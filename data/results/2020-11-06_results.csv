"title","abstract","authors","link","date","subject","source","initial_decision","q0","q1","q2","q3","q4","q5","q6","q7","q8","q9","q10","q11","q12","q13","q14","q15","q16","q17","q18","q19","q20","q21","q22","q23","q24","q25","q26","q27","q28","q29","q30","q31","q32","q33","q34","q35","q36","q37","q38","q39","q40","q41","q42","q43","q44","q45","q46","q47","q48","q49","q50","q51","q52","q53","q54","q55","q56","q57","q58","q59","q60","exclusion_reason","extraction_date","expert_decision","ID","o1"
"Use of Wikipedia categories on information retrieval research: a brief   review","Wikipedia categories, a classification scheme built for organizing and describing Wikpedia articles, are being applied in computer science research. This paper adopts a systematic literature review approach, in order to identify different approaches and uses of Wikipedia categories in information retrieval research. Several types of work are identified, depending on the intrinsic study of the categories structure, or its use as a tool for the processing and analysis of other documentary corpus different to Wikipedia. Information retrieval is identified as one of the major areas of use, in particular its application in the refinement and improvement of search expressions, and the construction of textual corpus. However, the set of available works shows that in many cases research approaches applied and results obtained can be integrated into a comprehensive and inclusive concept of information retrieval.","['JesÃºs Tramullas', 'Piedad Garrido-Picazo', 'Ana I. SÃ¡nchez-CasabÃ³n']","https://export.arxiv.org/abs/2004.09958","2020-04-21","cs.DL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17117,""
"Random Features for Kernel Approximation: A Survey on Algorithms,   Theory, and Beyond","Random features is one of the most popular techniques to speed up kernel methods in large-scale problems. Related works have been recognized by the NeurIPS Test-of-Time award in 2017 and the ICML Best Paper Finalist in 2019. The body of work on random features has grown rapidly, and hence it is desirable to have a comprehensive overview on this topic explaining the connections among various algorithms and theoretical results. In this survey, we systematically review the work on random features from the past ten years. First, the motivations, characteristics and contributions of representative random features based algorithms are summarized according to their sampling schemes, learning procedures, variance reduction properties and how they exploit training data. Second, we review theoretical results that center around the following key question: how many random features are needed to ensure a high approximation quality or no loss in the empirical/expected risks of the learned estimator. Third, we provide a comprehensive evaluation of popular random features based algorithms on several large-scale benchmark datasets and discuss their approximation quality and prediction performance for classification. Last, we discuss the relationship between random features and modern over-parameterized deep neural networks (DNNs), including the use of random features in the analysis DNNs as well as the gaps between current theoretical and empirical results. This survey may serve as a gentle introduction to this topic, and as a users' guide for practitioners interested in applying the representative algorithms and understanding theoretical results under various technical assumptions. We hope that this survey will facilitate discussion on the open problems in this topic, and more importantly, shed light on future research directions.","['Fanghui Liu', 'Xiaolin Huang', 'Yudong Chen', 'Johan A. K. Suykens']","https://export.arxiv.org/abs/2004.11154","2020-04-23","stat.ML cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17118,""
"Urban Anomaly Analytics: Description, Detection, and Prediction","Urban anomalies may result in loss of life or property if not handled properly. Automatically alerting anomalies in their early stage or even predicting anomalies before happening are of great value for populations. Recently, data-driven urban anomaly analysis frameworks have been forming, which utilize urban big data and machine learning algorithms to detect and predict urban anomalies automatically. In this survey, we make a comprehensive review of the state-of-the-art research on urban anomaly analytics. We first give an overview of four main types of urban anomalies, traffic anomaly, unexpected crowds, environment anomaly, and individual anomaly. Next, we summarize various types of urban datasets obtained from diverse devices, i.e., trajectory, trip records, CDRs, urban sensors, event records, environment data, social media and surveillance cameras. Subsequently, a comprehensive survey of issues on detecting and predicting techniques for urban anomalies is presented. Finally, research challenges and open problems as discussed.","['Mingyang Zhang', 'Tong Li', 'Yue Yu', 'Yong Li', 'Pan Hui', 'Yu Zheng']","https://export.arxiv.org/abs/2004.12094","2020-04-25","cs.SI cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17119,""
"ColBERT: Efficient and Effective Passage Search via Contextualized Late   Interaction over BERT","Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.","['Omar Khattab', 'Matei Zaharia']","https://export.arxiv.org/abs/2004.12832","2020-04-27","cs.IR cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17120,""
"Cross-lingual Information Retrieval with BERT","Multiple neural language models have been developed recently, e.g., BERT and XLNet, and achieved impressive results in various NLP tasks including sentence classification, question answering and document ranking. In this paper, we explore the use of the popular bidirectional language model, BERT, to model and learn the relevance between English queries and foreign-language documents in the task of cross-lingual information retrieval. A deep relevance matching model based on BERT is introduced and trained by finetuning a pretrained multilingual BERT model with weak supervision, using home-made CLIR training data derived from parallel corpora. Experimental results of the retrieval of Lithuanian documents against short English queries show that our model is effective and outperforms the competitive baseline approaches.","['Zhuolin Jiang', 'Amro El-Jaroudi', 'William Hartmann', 'Damianos Karakos', 'Lingjun Zhao']","https://export.arxiv.org/abs/2004.13005","2020-04-24","cs.IR cs.CL cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17121,""
"A scoping review of transfer learning research on medical image analysis   using ImageNet","Objective: Employing transfer learning (TL) with convolutional neural networks (CNNs), well-trained on non-medical ImageNet dataset, has shown promising results for medical image analysis in recent years. We aimed to conduct a scoping review to identify these studies and summarize their characteristics in terms of the problem description, input, methodology, and outcome. Materials and Methods: To identify relevant studies, MEDLINE, IEEE, and ACM digital library were searched. Two investigators independently reviewed articles to determine eligibility and to extract data according to a study protocol defined a priori. Results: After screening of 8,421 articles, 102 met the inclusion criteria. Of 22 anatomical areas, eye (18%), breast (14%), and brain (12%) were the most commonly studied. Data augmentation was performed in 72% of fine-tuning TL studies versus 15% of the feature-extracting TL studies. Inception models were the most commonly used in breast related studies (50%), while VGGNet was the common in eye (44%), skin (50%) and tooth (57%) studies. AlexNet for brain (42%) and DenseNet for lung studies (38%) were the most frequently used models. Inception models were the most frequently used for studies that analyzed ultrasound (55%), endoscopy (57%), and skeletal system X-rays (57%). VGGNet was the most common for fundus (42%) and optical coherence tomography images (50%). AlexNet was the most frequent model for brain MRIs (36%) and breast X-Rays (50%). 35% of the studies compared their model with other well-trained CNN models and 33% of them provided visualization for interpretation. Discussion: Various methods have been used in TL approaches from non-medical to medical image analysis. The findings of the scoping review can be used in future TL studies to guide the selection of appropriate research approaches, as well as identify research gaps and opportunities for innovation.","['Mohammad Amin Morid', 'Alireza Borjali', 'Guilherme Del Fiol']","https://export.arxiv.org/abs/2004.13175","2020-04-27","eess.IV cs.CV cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17122,""
"Conversational Word Embedding for Retrieval-Based Dialog System","Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs $ \left\langle{post, reply} \right\rangle$ to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply. To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level. We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems. The experiment results show that PR-Embedding can improve the quality of the selected response. PR-Embedding source code is available at https://github.com/wtma/PR-Embedding","['Wentao Ma', 'Yiming Cui', 'Ting Liu', 'Dong Wang', 'Shijin Wang', 'Guoping Hu']","https://export.arxiv.org/abs/2004.13249","2020-04-27","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17123,""
"Benchmarking LiDAR Sensors for Development and Evaluation of Automotive   Perception","Environment perception and representation are some of the most critical tasks in automated driving. To meet the stringent needs of safety standards such as ISO 26262 there is a need for efficient quantitative evaluation of the perceived information. However, to use typical methods of evaluation, such as comparing using annotated data, is not scalable due to the manual effort involved.   There is thus a need to automate the process of data annotation. This paper focuses on the LiDAR sensor and aims to identify the limitations of the sensor and provides a methodology to generate annotated data of a measurable quality. The limitations with the sensor are analysed in a Systematic Literature Review on available academic texts and refined by unstructured interviews with experts.   The main contributions are 1) the SLR with related interviews to identify LiDAR sensor limitations and 2) the associated methodology which allows us to generate world representations.","['Fredrik Schalling', 'Sebastian Ljungberg', 'Naveen Mohan']","https://export.arxiv.org/abs/2004.13433","2020-04-28","eess.SP cs.SY eess.SY","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17124,""
"Exploring Fine-tuning Techniques for Pre-trained Cross-lingual Models   via Continual Learning","Recently, fine-tuning pre-trained language models (e.g., multilingual BERT) to downstream cross-lingual tasks has shown promising results. However, the fine-tuning process inevitably changes the parameters of the pre-trained model and weakens its cross-lingual ability, which leads to sub-optimal performance. To alleviate this problem, we leverage continual learning to preserve the original cross-lingual ability of the pre-trained model when we fine-tune it to downstream tasks. The experimental result shows that our fine-tuning methods can better preserve the cross-lingual ability of the pre-trained model in a sentence retrieval task. Our methods also achieve better performance than other fine-tuning baselines on the zero-shot cross-lingual part-of-speech tagging and named entity recognition tasks.","['Zihan Liu', 'Genta Indra Winata', 'Andrea Madotto', 'Pascale Fung']","https://export.arxiv.org/abs/2004.14218","2020-04-29","cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17125,""
"Learning to Ask Screening Questions for Job Postings","At LinkedIn, we want to create economic opportunity for everyone in the global workforce. A critical aspect of this goal is matching jobs with qualified applicants. To improve hiring efficiency and reduce the need to manually screening each applicant, we develop a new product where recruiters can ask screening questions online so that they can filter qualified candidates easily. To add screening questions to all $20$M active jobs at LinkedIn, we propose a new task that aims to automatically generate screening questions for a given job posting. To solve the task of generating screening questions, we develop a two-stage deep learning model called Job2Questions, where we apply a deep learning model to detect intent from the text description, and then rank the detected intents by their importance based on other contextual features. Since this is a new product with no historical data, we employ deep transfer learning to train complex models with limited training data. We launched the screening question product and our AI models to LinkedIn users and observed significant impact in the job marketplace. During our online A/B test, we observed $+53.10\%$ screening question suggestion acceptance rate, $+22.17\%$ job coverage, $+190\%$ recruiter-applicant interaction, and $+11$ Net Promoter Score. In sum, the deployed Job2Questions model helps recruiters to find qualified applicants and job seekers to find jobs they are qualified for.","['Baoxu Shi', 'Shan Li', 'Jaewon Yang', 'Mustafa Emre Kazdagli', 'Qi He']","https://export.arxiv.org/abs/2004.14969","2020-04-30","cs.IR cs.AI cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17126,""
"Sparse, Dense, and Attentional Representations for Text Retrieval","Dual encoders perform retrieval by encoding documents and queries into dense lowdimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.","['Yi Luan', 'Jacob Eisenstein', 'Kristina Toutanova', 'Michael Collins']","https://export.arxiv.org/abs/2005.00181","2020-04-30","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17127,""
"Remote Sensing Image Scene Classification Meets Deep Learning:   Challenges, Methods, Benchmarks, and Opportunities","Remote sensing image scene classification, which aims at labeling remote sensing images with a set of semantic categories based on their contents, has broad applications in a range of fields. Propelled by the powerful feature learning capabilities of deep neural networks, remote sensing image scene classification driven by deep learning has drawn remarkable attention and achieved significant breakthroughs. However, to the best of our knowledge, a comprehensive review of recent achievements regarding deep learning for scene classification of remote sensing images is still lacking. Considering the rapid evolution of this field, this paper provides a systematic survey of deep learning methods for remote sensing image scene classification by covering more than 160 papers. To be specific, we discuss the main challenges of remote sensing image scene classification and survey (1) Autoencoder-based remote sensing image scene classification methods, (2) Convolutional Neural Network-based remote sensing image scene classification methods, and (3) Generative Adversarial Network-based remote sensing image scene classification methods. In addition, we introduce the benchmarks used for remote sensing image scene classification and summarize the performance of more than two dozen of representative algorithms on three commonly-used benchmark data sets. Finally, we discuss the promising opportunities for further research.","['Gong Cheng', 'Xingxing Xie', 'Junwei Han', 'Lei Guo', 'Gui-Song Xia']","https://export.arxiv.org/abs/2005.01094","2020-05-03","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17128,""
"MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering   Models","Retrieval question answering (ReQA) is the task of retrieving a sentence-level answer to a question from an open corpus (Ahmad et al.,2019).This paper presents MultiReQA, anew multi-domain ReQA evaluation suite com-posed of eight retrieval QA tasks drawn from publicly available QA datasets. We provide the first systematic retrieval based evaluation over these datasets using two supervised neural models, based on fine-tuning BERT andUSE-QA models respectively, as well as a surprisingly strong information retrieval baseline,BM25. Five of these tasks contain both train-ing and test data, while three contain test data only. Performance on the five tasks with train-ing data shows that while a general model covering all domains is achievable, the best performance is often obtained by training exclusively on in-domain data.","['Mandy Guo', 'Yinfei Yang', 'Daniel Cer', 'Qinlan Shen', 'Noah Constant']","https://export.arxiv.org/abs/2005.02507","2020-05-05","cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17129,""
"AI in society and culture: decision making and values","With the increased expectation of artificial intelligence, academic research face complex questions of human-centred, responsible and trustworthy technology embedded into society and culture. Several academic debates, social consultations and impact studies are available to reveal the key aspects of the changing human-machine ecosystem. To contribute to these studies, hundreds of related academic sources are summarized below regarding AI-driven decisions and valuable AI. In details, sociocultural filters, taxonomy of human-machine decisions and perspectives of value-based AI are in the focus of this literature review. For better understanding, it is proposed to invite stakeholders in the prepared large-scale survey about the next generation AI that investigates issues that go beyond the technology.","['Katalin Feher', 'Asta Zelenkauskaite']","https://export.arxiv.org/abs/2005.02777","2020-04-29","cs.OH cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17130,""
"Text Recognition in the Wild: A Survey","The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research field in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising in terms of innovation, practicality, and efficiency. This paper aims to (1) summarize the fundamental problems and the state-of-the-art associated with scene text recognition; (2) introduce new insights and ideas; (3) provide a comprehensive review of publicly available resources; (4) point out directions for future work. In summary, this literature review attempts to present the entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field, and could be helpful to inspire future research. Related resources are available at our Github repository: https://github.com/HCIILAB/Scene-Text-Recognition.","['Xiaoxue Chen', 'Lianwen Jin', 'Yuanzhi Zhu', 'Canjie Luo', 'Tianwei Wang']","https://export.arxiv.org/abs/2005.03492","2020-05-07","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17131,""
"Effective Data Fusion with Generalized Vegetation Index: Evidence from   Land Cover Segmentation in Agriculture","How can we effectively leverage the domain knowledge from remote sensing to better segment agriculture land cover from satellite images? In this paper, we propose a novel, model-agnostic, data-fusion approach for vegetation-related computer vision tasks. Motivated by the various Vegetation Indices (VIs), which are introduced by domain experts, we systematically reviewed the VIs that are widely used in remote sensing and their feasibility to be incorporated in deep neural networks. To fully leverage the Near-Infrared channel, the traditional Red-Green-Blue channels, and Vegetation Index or its variants, we propose a Generalized Vegetation Index (GVI), a lightweight module that can be easily plugged into many neural network architectures to serve as an additional information input. To smoothly train models with our GVI, we developed an Additive Group Normalization (AGN) module that does not require extra parameters of the prescribed neural networks. Our approach has improved the IoUs of vegetation-related classes by 0.9-1.3 percent and consistently improves the overall mIoU by 2 percent on our baseline.","['Hao Sheng', 'Xiao Chen', 'Jingyi Su', 'Ram Rajagopal', 'Andrew Ng']","https://export.arxiv.org/abs/2005.03743","2020-05-07","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17132,""
"Literature Triage on Genomic Variation Publications by   Knowledge-enhanced Multi-channel CNN","Background: To investigate the correlation between genomic variation and certain diseases or phenotypes, the fundamental task is to screen out the concerning publications from massive literature, which is called literature triage. Some knowledge bases, including UniProtKB/Swiss-Prot and NHGRI-EBI GWAS Catalog are created for collecting concerning publications. These publications are manually curated by experts, which is time-consuming. Moreover, the manual curation of information from literature is not scalable due to the rapidly increasing amount of publications. In order to cut down the cost of literature triage, machine-learning models were adopted to automatically identify biomedical publications. Methods: Comparing to previous studies utilizing machine-learning models for literature triage, we adopt a multi-channel convolutional network to utilize rich textual information and meanwhile bridge the semantic gaps from different corpora. In addition, knowledge embeddings learned from UMLS is also used to provide extra medical knowledge beyond textual features in the process of triage. Results: We demonstrate that our model outperforms the state-of-the-art models over 5 datasets with the help of knowledge embedding and multiple channels. Our model improves the accuracy of biomedical literature triage results. Conclusions: Multiple channels and knowledge embeddings enhance the performance of the CNN model in the task of biomedical literature triage. Keywords: Literature Triage; Knowledge Embedding; Multi-channel Convolutional Network","['Chenhui Lv', 'Qian Lu', 'Xiang Zhang']","https://export.arxiv.org/abs/2005.04044","2020-05-08","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17133,""
"Evidence Inference 20: More Data, Better Models","How do we most effectively treat a disease or condition? Ideally, we could consult a database of evidence gleaned from clinical trials to answer such questions. Unfortunately, no such database exists; clinical trial results are instead disseminated primarily via lengthy natural language articles. Perusing all such articles would be prohibitively time-consuming for healthcare practitioners; they instead tend to depend on manually compiled systematic reviews of medical literature to inform care.   NLP may speed this process up, and eventually facilitate immediate consult of published evidence. The Evidence Inference dataset was recently released to facilitate research toward this end. This task entails inferring the comparative performance of two treatments, with respect to a given outcome, from a particular article (describing a clinical trial) and identifying supporting evidence. For instance: Does this article report that chemotherapy performed better than surgery for five-year survival rates of operable cancers? In this paper, we collect additional annotations to expand the Evidence Inference dataset by 25\%, provide stronger baseline models, systematically inspect the errors that these make, and probe dataset quality. We also release an abstract only (as opposed to full-texts) version of the task for rapid model prototyping. The updated corpus, documentation, and code for new baselines and evaluations are available at http://evidence-inference.ebm-nlp.com/.","['Jay DeYoung', 'Eric Lehman', 'Ben Nye', 'Iain J. Marshall', 'Byron C. Wallace']","https://export.arxiv.org/abs/2005.04177","2020-05-08","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17134,""
"Local Self-Attention over Long Text for Efficient Document Retrieval","Neural networks, particularly Transformer-based architectures, have achieved significant performance improvements on several retrieval benchmarks. When the items being retrieved are documents, the time and memory cost of employing Transformers over a full sequence of document terms can be prohibitive. A popular strategy involves considering only the first n terms of the document. This can, however, result in a biased system that under retrieves longer documents. In this work, we propose a local self-attention which considers a moving window over the document terms and for each term attends only to other terms in the same window. This local attention incurs a fraction of the compute and memory cost of attention over the whole document. The windowed approach also leads to more compact packing of padded documents in minibatches resulting in additional savings. We also employ a learned saturation function and a two-staged pooling strategy to identify relevant regions of the document. The Transformer-Kernel pooling model with these changes can efficiently elicit relevance information from documents with thousands of tokens. We benchmark our proposed modifications on the document ranking task from the TREC 2019 Deep Learning track and observe significant improvements in retrieval quality as well as increased retrieval of longer documents at moderate increase in compute and memory costs.","['Sebastian HofstÃ¤tter', 'Hamed Zamani', 'Bhaskar Mitra', 'Nick Craswell', 'Allan Hanbury']","https://export.arxiv.org/abs/2005.04908","2020-05-11","cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17135,""
"ReadNet:Towards Accurate ReID with Limited and Noisy Samples","Person re-identification (ReID) is an essential cross-camera retrieval task to identify pedestrians. However, the photo number of each pedestrian usually differs drastically, and thus the data limitation and imbalance problem hinders the prediction accuracy greatly. Additionally, in real-world applications, pedestrian images are captured by different surveillance cameras, so the noisy camera related information, such as the lights, perspectives and resolutions, result in inevitable domain gaps for ReID algorithms. These challenges bring difficulties to current deep learning methods with triplet loss for coping with such problems. To address these challenges, this paper proposes ReadNet, an adversarial camera network (ACN) with an angular triplet loss (ATL). In detail, ATL focuses on learning the angular distance among different identities to mitigate the effect of data imbalance, and guarantees a linear decision boundary as well, while ACN takes the camera discriminator as a game opponent of feature extractor to filter camera related information to bridge the multi-camera gaps. ReadNet is designed to be flexible so that either ATL or ACN can be deployed independently or simultaneously. The experiment results on various benchmark datasets have shown that ReadNet can deliver better prediction performance than current state-of-the-art methods.","['Yitian Li', 'Ruini Xue', 'Mengmeng Zhu', 'Qing Xu', 'Zenglin Xu']","https://export.arxiv.org/abs/2005.05740","2020-05-12","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17136,""
"Why Fairness Cannot Be Automated: Bridging the Gap Between EU   Non-Discrimination Law and AI","This article identifies a critical incompatibility between European notions of discrimination and existing statistical measures of fairness. First, we review the evidential requirements to bring a claim under EU non-discrimination law. Due to the disparate nature of algorithmic and human discrimination, the EU's current requirements are too contextual, reliant on intuition, and open to judicial interpretation to be automated. Second, we show how the legal protection offered by non-discrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised stereotypes) which can act as a signal to victims that discrimination has occurred. Finally, we examine how existing work on fairness in machine learning lines up with procedures for assessing cases under EU non-discrimination law. We propose ""conditional demographic disparity"" (CDD) as a standard baseline statistical measurement that aligns with the European Court of Justice's ""gold standard."" Establishing a standard set of statistical evidence for automated discrimination cases can help ensure consistent procedures for assessment, but not judicial interpretation, of cases involving AI and automated systems. Through this proposal for procedural regularity in the identification and assessment of automated discrimination, we clarify how to build considerations of fairness into automated systems as far as possible while still respecting and enabling the contextual approach to judicial interpretation practiced under EU non-discrimination law.   N.B. Abridged abstract","['Sandra Wachter', 'Brent Mittelstadt', 'Chris Russell']","https://export.arxiv.org/abs/2005.05906","2020-05-12","cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17137,""
"Unlocking the Power of Deep PICO Extraction: Step-wise Medical NER   Identification","The PICO framework (Population, Intervention, Comparison, and Outcome) is usually used to formulate evidence in the medical domain. The major task of PICO extraction is to extract sentences from medical literature and classify them into each class. However, in most circumstances, there will be more than one evidences in an extracted sentence even it has been categorized to a certain class. In order to address this problem, we propose a step-wise disease Named Entity Recognition (DNER) extraction and PICO identification method. With our method, sentences in paper title and abstract are first classified into different classes of PICO, and medical entities are then identified and classified into P and O. Different kinds of deep learning frameworks are used and experimental results show that our method will achieve high performance and fine-grained extraction results comparing with conventional PICO extraction works.","['Tengteng Zhang', 'Yiqin Yu', 'Jing Mei', 'Zefang Tang', 'Xiang Zhang', 'Shaochun Li']","https://export.arxiv.org/abs/2005.06601","2020-04-29","cs.CL cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17138,""
"Literature Review and Implementation Overview: High Performance   Computing with Graphics Processing Units for Classroom and Research Use","In this report, I discuss the history and current state of GPU HPC systems. Although high-power GPUs have only existed a short time, they have found rapid adoption in deep learning applications. I also discuss an implementation of a commodity-hardware NVIDIA GPU HPC cluster for deep learning research and academic teaching use.","['Nathan George']","https://export.arxiv.org/abs/2005.07598","2020-05-13","cs.DC","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17139,""
"Cross-Lingual Low-Resource Set-to-Description Retrieval for Global   E-Commerce","With the prosperous of cross-border e-commerce, there is an urgent demand for designing intelligent approaches for assisting e-commerce sellers to offer local products for consumers from all over the world. In this paper, we explore a new task of cross-lingual information retrieval, i.e., cross-lingual set-to-description retrieval in cross-border e-commerce, which involves matching product attribute sets in the source language with persuasive product descriptions in the target language. We manually collect a new and high-quality paired dataset, where each pair contains an unordered product attribute set in the source language and an informative product description in the target language. As the dataset construction process is both time-consuming and costly, the new dataset only comprises of 13.5k pairs, which is a low-resource setting and can be viewed as a challenging testbed for model development and evaluation in cross-border e-commerce. To tackle this cross-lingual set-to-description retrieval task, we propose a novel cross-lingual matching network (CLMN) with the enhancement of context-dependent cross-lingual mapping upon the pre-trained monolingual BERT representations. Experimental results indicate that our proposed CLMN yields impressive results on the challenging task and the context-dependent cross-lingual mapping on BERT yields noticeable improvement over the pre-trained multi-lingual BERT model.","['Juntao Li', 'Chang Liu', 'Jian Wang', 'Lidong Bing', 'Hongsong Li', 'Xiaozhong Liu', 'Dongyan Zhao', 'Rui Yan']","https://export.arxiv.org/abs/2005.08188","2020-05-17","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17140,""
"Hybrid Sequential Recommender via Time-aware Attentive Memory Network","Recommendation systems aim to assist users to discover most preferred contents from an ever-growing corpus of items. Although recommenders have been greatly improved by deep learning, they still faces several challenges: (1) Behaviors are much more complex than words in sentences, so traditional attentive and recurrent models may fail in capturing the temporal dynamics of user preferences. (2) The preferences of users are multiple and evolving, so it is difficult to integrate long-term memory and short-term intent.   In this paper, we propose a temporal gating methodology to improve attention mechanism and recurrent units, so that temporal information can be considered in both information filtering and state transition. Additionally, we propose a Multi-hop Time-aware Attentive Memory network (MTAM) to integrate long-term and short-term preferences. We use the proposed time-aware GRU network to learn the short-term intent and maintain prior records in user memory. We treat the short-term intent as a query and design a multi-hop memory reading operation via the proposed time-aware attention to generate user representation based on the current intent and long-term memory. Our approach is scalable for candidate retrieval tasks and can be viewed as a non-linear generalization of latent factorization for dot-product based Top-K recommendation. Finally, we conduct extensive experiments on six benchmark datasets and the experimental results demonstrate the effectiveness of our MTAM and temporal gating methodology.","['Wendi Ji', 'Keqiang Wang', 'Xiaoling Wang', 'TingWei Chen', 'Alexandra Cristea']","https://export.arxiv.org/abs/2005.08598","2020-05-18","cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17141,""
"An Overview of Privacy in Machine Learning","Over the past few years, providers such as Google, Microsoft, and Amazon have started to provide customers with access to software interfaces allowing them to easily embed machine learning tasks into their applications. Overall, organizations can now use Machine Learning as a Service (MLaaS) engines to outsource complex tasks, e.g., training classifiers, performing predictions, clustering, etc. They can also let others query models trained on their data. Naturally, this approach can also be used (and is often advocated) in other contexts, including government collaborations, citizen science projects, and business-to-business partnerships. However, if malicious users were able to recover data used to train these models, the resulting information leakage would create serious issues. Likewise, if the inner parameters of the model are considered proprietary information, then access to the model should not allow an adversary to learn such parameters. In this document, we set to review privacy challenges in this space, providing a systematic review of the relevant research literature, also exploring possible countermeasures. More specifically, we provide ample background information on relevant concepts around machine learning and privacy. Then, we discuss possible adversarial models and settings, cover a wide range of attacks that relate to private and/or sensitive information leakage, and review recent results attempting to defend against such attacks. Finally, we conclude with a list of open problems that require more work, including the need for better evaluations, more targeted defenses, and the study of the relation to policy and data protection efforts.","['Emiliano De Cristofaro']","https://export.arxiv.org/abs/2005.08679","2020-05-18","cs.LG cs.AI cs.CR cs.CY stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17142,""
"On Restricting Real-Valued Genotypes in Evolutionary Algorithms","Real-valued genotypes together with the variation operators, mutation and crossover, constitute some of the fundamental building blocks of Evolutionary Algorithms. Real-valued genotypes are utilized in a broad range of contexts, from weights in Artificial Neural Networks to parameters in robot control systems. Shared between most uses of real-valued genomes is the need for limiting the range of individual parameters to allowable bounds. In this paper we will illustrate the challenge of limiting the parameters of real-valued genomes and analyse the most promising method to properly limit these values. We utilize both empirical as well as benchmark examples to demonstrate the utility of the proposed method and through a literature review show how the insight of this paper could impact other research within the field. The proposed method requires minimal intervention from Evolutionary Algorithm practitioners and behaves well under repeated application of variation operators, leading to better theoretical properties as well as significant differences in well-known benchmarks.","['JÃ¸rgen Nordmoen', 'TÃ¸nnes Frostad Nygaard', 'Eivind Samuelsen', 'Kyrre Glette']","https://export.arxiv.org/abs/2005.09380","2020-05-19","cs.NE","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17143,""
"GLEAKE: Global and Local Embedding Automatic Keyphrase Extraction","Automated methods for granular categorization of large corpora of text documents have become increasingly more important with the rate scientific, news, medical, and web documents are growing in the last few years. Automatic keyphrase extraction (AKE) aims to automatically detect a small set of single or multi-words from within a single textual document that captures the main topics of the document. AKE plays an important role in various NLP and information retrieval tasks such as document summarization and categorization, full-text indexing, and article recommendation. Due to the lack of sufficient human-labeled data in different textual contents, supervised learning approaches are not ideal for automatic detection of keyphrases from the content of textual bodies. With the state-of-the-art advances in text embedding techniques, NLP researchers have focused on developing unsupervised methods to obtain meaningful insights from raw datasets. In this work, we introduce Global and Local Embedding Automatic Keyphrase Extractor (GLEAKE) for the task of AKE. GLEAKE utilizes single and multi-word embedding techniques to explore the syntactic and semantic aspects of the candidate phrases and then combines them into a series of embedding-based graphs. Moreover, GLEAKE applies network analysis techniques on each embedding-based graph to refine the most significant phrases as a final set of keyphrases. We demonstrate the high performance of GLEAKE by evaluating its results on five standard AKE datasets from different domains and writing styles and by showing its superiority with regards to other state-of-the-art methods.","['Javad Rafiei Asl', 'Juan M. Banda']","https://export.arxiv.org/abs/2005.09740","2020-05-19","cs.IR cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17144,""
"Adversarial Machine Learning in Recommender Systems: State of the art   and Challenges","Latent-factor models (LFM) based on collaborative filtering (CF), such as matrix factorization (MF) and deep CF methods, are widely used in modern recommender systems (RS) due to their excellent performance and recommendation accuracy. Notwithstanding their great success, in recent years, it has been shown that these methods are vulnerable to adversarial examples, i.e., subtle but non-random perturbations designed to force recommendation models to produce erroneous outputs. The main reason for this behavior is that user interaction data used for training of LFM can be contaminated by malicious activities or users' misoperation that can induce an unpredictable amount of natural noise and harm recommendation outcomes. On the other side, it has been shown that these systems, conceived originally to attack machine learning applications, can be successfully adopted to strengthen their robustness against attacks as well as to train more precise recommendation engines. In this respect, the goal of this survey is two-fold: (i) to present recent advances on AML-RS for the security of RS (i.e., attacking and defense recommendation models), (ii) to show another successful application of AML in generative adversarial networks (GANs), which use the core concept of learning in AML (i.e., the min-max game) for generative applications. In this survey, we provide an exhaustive literature review of 60 articles published in major RS and ML journals and conferences. This review serves as a reference for the RS community, working on the security of RS and recommendation models leveraging generative models to improve their quality.","['Yashar Deldjoo', 'Tommaso Di Noia', 'Felice Antonio Merra']","https://export.arxiv.org/abs/2005.10322","2020-05-20","cs.IR cs.CR cs.LG cs.MM","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17145,""
"Pairwise Supervised Hashing with Bernoulli Variational Auto-Encoder and   Self-Control Gradient Estimator","Semantic hashing has become a crucial component of fast similarity search in many large-scale information retrieval systems, in particular, for text data. Variational auto-encoders (VAEs) with binary latent variables as hashing codes provide state-of-the-art performance in terms of precision for document retrieval. We propose a pairwise loss function with discrete latent VAE to reward within-class similarity and between-class dissimilarity for supervised hashing. Instead of solving the optimization relying on existing biased gradient estimators, an unbiased low-variance gradient estimator is adopted to optimize the hashing function by evaluating the non-differentiable loss function over two correlated sets of binary hashing codes to control the variance of gradient estimates. This new semantic hashing framework achieves superior performance compared to the state-of-the-arts, as demonstrated by our comprehensive experiments.","['Siamak Zamani Dadaneh', 'Shahin Boluki', 'Mingzhang Yin', 'Mingyuan Zhou', 'Xiaoning Qian']","https://export.arxiv.org/abs/2005.10477","2020-05-21","cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17146,""
"COVID-19 Public Sentiment Insights and Machine Learning for Tweets   Classification","Along with the Coronavirus pandemic, another crisis has manifested itself in the form of mass fear and panic phenomena, fueled by incomplete and often inaccurate information. There is therefore a tremendous need to address and better understand COVID-19's informational crisis and gauge public sentiment, so that appropriate messaging and policy decisions can be implemented. In this research article, we identify public sentiment associated with the pandemic using Coronavirus specific Tweets and R statistical software, along with its sentiment analysis packages. We demonstrate insights into the progress of fear-sentiment over time as COVID-19 approached peak levels in the United States, using descriptive textual analytics supported by necessary textual data visualizations. Furthermore, we provide a methodological overview of two essential machine learning (ML) classification methods, in the context of textual analytics, and compare their effectiveness in classifying Coronavirus Tweets of varying lengths. We observe a strong classification accuracy of 91% for short Tweets, with the Naive Bayes method. We also observe that the logistic regression classification method provides a reasonable accuracy of 74% with shorter Tweets, and both methods showed relatively weaker performance for longer Tweets. This research provides insights into Coronavirus fear sentiment progression, and outlines associated methods, implications, limitations and opportunities.","['Jim Samuel', 'G. G. Md. Nawaz Ali', 'Md. Mokhlesur Rahman', 'Ek Esawi', 'Yana Samuel']","https://export.arxiv.org/abs/2005.10898","2020-05-21","cs.IR cs.SI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17147,""
"A Survey of Information Cascade Analysis: Models, Predictions and Recent   Advances","The deluge of digital information in our daily life -- from user-generated content such as microblogs and scientific papers, to online business such as viral marketing and advertising -- offers unprecedented opportunities to explore and exploit the trajectories and structures of the evolution of information cascades. Abundant research efforts, both academic and industrial, have aimed to reach a better understanding of the mechanisms driving the spread of information and quantifying the outcome of information diffusion. This article presents a comprehensive review and categorization of information popularity prediction methods, from feature engineering and stochastic processes, through graph representation, to deep learning-based approaches. Specifically, we first formally define different types of information cascades and summarize the perspectives of existing studies. We then present a taxonomy that categorizes existing works into the aforementioned three main groups as well as the main subclasses in each group, and we systematically review cutting-edge research work. Finally, we summarize the pros and cons of existing research efforts and outline the open challenges and opportunities in this field.","['Fan Zhou', 'Xovee Xu', 'Goce Trajcevski', 'Kunpeng Zhang']","https://export.arxiv.org/abs/2005.11041","2020-05-22","cs.SI cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17148,""
"Data Mining with Big Data in Intrusion Detection Systems: A Systematic   Literature Review","Cloud computing has become a powerful and indispensable technology for complex, high performance and scalable computation. The exponential expansion in the deployment of cloud technology has produced a massive amount of data from a variety of applications, resources and platforms. In turn, the rapid rate and volume of data creation has begun to pose significant challenges for data management and security. The design and deployment of intrusion detection systems (IDS) in the big data setting has, therefore, become a topic of importance. In this paper, we conduct a systematic literature review (SLR) of data mining techniques (DMT) used in IDS-based solutions through the period 2013-2018. We employed criterion-based, purposive sampling identifying 32 articles, which constitute the primary source of the present survey. After a careful investigation of these articles, we identified 17 separate DMTs deployed in an IDS context. This paper also presents the merits and disadvantages of the various works of current research that implemented DMTs and distributed streaming frameworks (DSF) to detect and/or prevent malicious attacks in a big data environment.","['Fadi Salo', 'MohammadNoor Injadat', 'Ali Bou Nassif', 'Aleksander Essex']","https://export.arxiv.org/abs/2005.12267","2020-05-23","cs.CR cs.AI cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17149,""
"Explainable deep learning models in medical image analysis","Deep learning methods have been very effective for a variety of medical diagnostic tasks and has even beaten human experts on some of those. However, the black-box nature of the algorithms has restricted clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.","['Amitojdeep Singh', 'Sourya Sengupta', 'Vasudevan Lakshminarayanan']","https://export.arxiv.org/abs/2005.13799","2020-05-28","cs.CV cs.LG eess.IV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17150,""
"Explainable Artificial Intelligence: a Systematic Review","Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.","['Giulia Vilone', 'Luca Longo']","https://export.arxiv.org/abs/2006.00093","2020-05-29","cs.AI cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17151,""
"Conversational Machine Comprehension: a Literature Review","Conversational Machine Comprehension (CMC) is a research track in conversational AI which expects the machine to understand an open-domain text and thereafter engage in a multi-turn conversation to answer questions related to the text. While most of the research in Machine Reading Comprehension (MRC) revolves around single-turn question answering, multi-turn CMC has recently gained prominence, thanks to the advancement in natural language understanding via neural language models like BERT and the introduction of large-scale conversational datasets like CoQA and QuAC. The rise in interest has, however, led to a flurry of concurrent publications, each with a different yet structurally similar modeling approach and an inconsistent view of the surrounding literature. With the volume of model submissions to conversational datasets increasing every year, there exists a need to consolidate the scattered knowledge in this domain to streamline future research. This literature review, therefore, is a first-of-its-kind attempt at providing a holistic overview of CMC, with an emphasis on the common trends across recently published models, specifically in their approach to tackling conversational history. It focuses on synthesizing a generic framework for CMC models, rather than describing the models individually. The review is intended to serve as a compendium for future researchers in this domain.","['Somil Gupta', 'Bhanu Pratap Singh Rawat']","https://export.arxiv.org/abs/2006.00671","2020-05-31","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17152,""
"Eye Movements Biometrics: A Bibliometric Analysis from 2004 to 2019","Person identification based on eye movements is getting more and more attention, as it is anti-spoofing resistant and can be useful for continuous authentication. Therefore, it is noteworthy for researchers to know who and what is relevant in the field, including authors, journals, conferences, and institutions. This paper presents a comprehensive quantitative overview of the field of eye movement biometrics using a bibliometric approach. All data and analyses are based on documents written in English published between 2004 and 2019. Scopus was used to perform information retrieval. This research focused on temporal evolution, leading authors, most cited papers, leading journals, competitions and collaboration networks.","['Antonio Ricardo Alexandre Brasil', 'Jefferson Oliveira Andrade', 'Karin Satie Komati']","https://export.arxiv.org/abs/2006.01310","2020-06-01","cs.HC cs.CV cs.DL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17153,""
"Artificial Neural Network Based Breast Cancer Screening: A Comprehensive   Review","Breast cancer is a common fatal disease for women. Early diagnosis and detection is necessary in order to improve the prognosis of breast cancer affected people. For predicting breast cancer, several automated systems are already developed using different medical imaging modalities. This paper provides a systematic review of the literature on artificial neural network (ANN) based models for the diagnosis of breast cancer via mammography. The advantages and limitations of different ANN models including spiking neural network (SNN), deep belief network (DBN), convolutional neural network (CNN), multilayer neural network (MLNN), stacked autoencoders (SAE), and stacked de-noising autoencoders (SDAE) are described in this review. The review also shows that the studies related to breast cancer detection applied different deep learning models to a number of publicly available datasets. For comparing the performance of the models, different metrics such as accuracy, precision, recall, etc. were used in the existing studies. It is found that the best performance was achieved by residual neural network (ResNet)-50 and ResNet-101 models of CNN algorithm.","['Subrato Bharati', 'Prajoy Podder', 'M. Rubaiyat Hossain Mondal']","https://export.arxiv.org/abs/2006.01767","2020-05-29","eess.IV cs.IT cs.LG math.IT","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17154,""
"CSTNet: Contrastive Speech Translation Network for Self-Supervised   Speech Representation Learning","More than half of the 7,000 languages in the world are in imminent danger of going extinct. Traditional methods of documenting language proceed by collecting audio data followed by manual annotation by trained linguists at different levels of granularity. This time consuming and painstaking process could benefit from machine learning. Many endangered languages do not have any orthographic form but usually have speakers that are bi-lingual and trained in a high resource language. It is relatively easy to obtain textual translations corresponding to speech. In this work, we provide a multimodal machine learning framework for speech representation learning by exploiting the correlations between the two modalities namely speech and its corresponding text translation. Here, we construct a convolutional neural network audio encoder capable of extracting linguistic representations from speech. The audio encoder is trained to perform a speech-translation retrieval task in a contrastive learning framework. By evaluating the learned representations on a phone recognition task, we demonstrate that linguistic representations emerge in the audio encoder's internal representations as a by-product of learning to perform the retrieval task.","['Sameer Khurana', 'Antoine Laurent', 'James Glass']","https://export.arxiv.org/abs/2006.02814","2020-06-04","eess.AS cs.CL cs.LG cs.SD","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17155,""
"A Modified AUC for Training Convolutional Neural Networks: Taking   Confidence into Account","Receiver operating characteristic (ROC) curve is an informative tool in binary classification and Area Under ROC Curve (AUC) is a popular metric for reporting performance of binary classifiers. In this paper, first we present a comprehensive review of ROC curve and AUC metric. Next, we propose a modified version of AUC that takes confidence of the model into account and at the same time, incorporates AUC into Binary Cross Entropy (BCE) loss used for training a Convolutional neural Network for classification tasks. We demonstrate this on two datasets: MNIST and prostate MRI. Furthermore, we have published GenuineAI, a new python library, which provides the functions for conventional AUC and the proposed modified AUC along with metrics including sensitivity, specificity, recall, precision, and F1 for each point of the ROC curve.","['Khashayar Namdar', 'Masoom A. Haider', 'Farzad Khalvati']","https://export.arxiv.org/abs/2006.04836","2020-06-08","cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17156,""
"How Smart is the Grid?","Ancient Romans called 'urbs' the set of buildings and infrastructures, and 'civitas' the Roman citizens. Today instead, while the society is surfing the digital tsunami, 'urbs' and 'civitas' tend to become much closer, almost merging, that we might attempt to condensate these into a single concept: 'smart grid'. Internet of things, artificial intelligence, blockchain, quantum cryptography is only a few of the paradigms that are likely to contribute to determining the final portrait of the future smart grid. However, to understand the effective sustainability of complex grids, specific tools are required. To this end, in this article, a systematic review of the emerging paradigms is presented, identifying intersectoral synergies and limitations with respect to the `smart grid' concept. Further, a taxonomic framework for assessing the level of sustainability of the grid is proposed. Finally, from the scenario portrayed, a set of issues involving engineering, regulation, security, and social frameworks have been derived in a theoretical fashion. The findings are likely to suggest the urgent need for multidisciplinary cooperation to wisely address engineering and ontological challenges gravitating around the smart grid concept.","['Ermanno Lo Cascio', 'Zhenjun Ma', 'FranÃ§ois MarÃ©chal']","https://export.arxiv.org/abs/2006.04943","2020-06-04","cs.CY","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17157,""
"Human brain activity for machine attention","Cognitively inspired NLP leverages human-derived data to teach machines about language processing mechanisms. Recently, neural networks have been augmented with behavioral data to solve a range of NLP tasks spanning syntax and semantics. We are the first to exploit neuroscientific data, namely electroencephalography (EEG), to inform a neural attention model about language processing of the human brain. The challenge in working with EEG data is that features are exceptionally rich and need extensive pre-processing to isolate signals specific to text processing. We devise a method for finding such EEG features to supervise machine attention through combining theoretically motivated cropping with random forest tree splits. After this dimensionality reduction, the pre-processed EEG features are capable of distinguishing two reading tasks retrieved from a publicly available EEG corpus. We apply these features to regularise attention on relation classification and show that EEG is more informative than strong baselines. This improvement depends on both the cognitive load of the task and the EEG frequency domain. Hence, informing neural attention models with EEG signals is beneficial but requires further investigation to understand which dimensions are the most useful across NLP tasks.","['Lukas Muttenthaler', 'Nora Hollenstein', 'Maria Barrett']","https://export.arxiv.org/abs/2006.05113","2020-06-09","cs.CL cs.LG q-bio.NC","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17158,""
"Knowledge-Aided Open-Domain Question Answering","Open-domain question answering (QA) aims to find the answer to a question from a large collection of documents.Though many models for single-document machine comprehension have achieved strong performance, there is still much room for improving open-domain QA systems since document retrieval and answer reranking are still unsatisfactory. Golden documents that contain the correct answers may not be correctly scored by the retrieval component, and the correct answers that have been extracted may be wrongly ranked after other candidate answers by the reranking component. One of the reasons is derived from the independent principle in which each candidate document (or answer) is scored independently without considering its relationship to other documents (or answers). In this work, we propose a knowledge-aided open-domain QA (KAQA) method which targets at improving relevant document retrieval and candidate answer reranking by considering the relationship between a question and the documents (termed as question-document graph), and the relationship between candidate documents (termed as document-document graph). The graphs are built using knowledge triples from external knowledge resources. During document retrieval, a candidate document is scored by considering its relationship to the question and other documents. During answer reranking, a candidate answer is reranked using not only its own context but also the clues from other documents. The experimental results show that our proposed method improves document retrieval and answer reranking, and thereby enhances the overall performance of open-domain question answering.","['Mantong Zhou', 'Zhouxing Shi', 'Minlie Huang', 'Xiaoyan Zhu']","https://export.arxiv.org/abs/2006.05244","2020-06-09","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17159,""
"Dialog Policy Learning for Joint Clarification and Active Learning   Queries","Intelligent systems need to be able to recover from mistakes, resolve uncertainty, and adapt to novel concepts not seen during training. Dialog interaction can enable this by the use of clarifications for correction and resolving uncertainty, and active learning queries to learn new concepts encountered during operation. Prior work on dialog systems has either focused on exclusively learning how to perform clarification/ information seeking, or to perform active learning. In this work, we train a hierarchical dialog policy to jointly perform {\it both} clarification and active learning in the context of an interactive language-based image retrieval task motivated by an on-line shopping application, and demonstrate that jointly learning dialog policies for clarification and active learning is more effective than the use of static dialog policies for one or both of these functions.","['Aishwarya Padmakumar', 'Raymond J. Mooney']","https://export.arxiv.org/abs/2006.05456","2020-06-09","cs.CV cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17160,""
"Deep Learning for Change Detection in Remote Sensing Images:   Comprehensive Review and Meta-Analysis","Deep learning (DL) algorithms are considered as a methodology of choice for remote-sensing image analysis over the past few years. Due to its effective applications, deep learning has also been introduced for automatic change detection and achieved great success. The present study attempts to provide a comprehensive review and a meta-analysis of the recent progress in this subfield. Specifically, we first introduce the fundamentals of deep learning methods which arefrequently adopted for change detection. Secondly, we present the details of the meta-analysis conducted to examine the status of change detection DL studies. Then, we focus on deep learning-based change detection methodologies for remote sensing images by giving a general overview of the existing methods. Specifically, these deep learning-based methods were classified into three groups; fully supervised learning-based methods, fully unsupervised learning-based methods and transfer learning-based techniques. As a result of these investigations, promising new directions were identified for future research. This study will contribute in several ways to our understanding of deep learning for change detection and will provide a basis for further research.","['Lazhar Khelifi', 'Max Mignotte']","https://export.arxiv.org/abs/2006.05612","2020-06-09","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17161,""
"A systematic review on the role of artificial intelligence in   sonographic diagnosis of thyroid cancer: Past, present and future","Thyroid cancer is common worldwide, with a rapid increase in prevalence across North America in recent years. While most patients present with palpable nodules through physical examination, a large number of small and medium-sized nodules are detected by ultrasound examination. Suspicious nodules are then sent for biopsy through fine needle aspiration. Since biopsies are invasive and sometimes inconclusive, various research groups have tried to develop computer-aided diagnosis systems. Earlier approaches along these lines relied on clinically relevant features that were manually identified by radiologists. With the recent success of artificial intelligence (AI), various new methods are being developed to identify these features in thyroid ultrasound automatically. In this paper, we present a systematic review of state-of-the-art on AI application in sonographic diagnosis of thyroid cancer. This review follows a methodology-based classification of the different techniques available for thyroid cancer diagnosis. With more than 50 papers included in this review, we reflect on the trends and challenges of the field of sonographic diagnosis of thyroid malignancies and potential of computer-aided diagnosis to increase the impact of ultrasound applications on the future of thyroid cancer diagnosis. Machine learning will continue to play a fundamental role in the development of future thyroid cancer diagnosis frameworks.","['Fatemeh Abdolali', 'Atefeh Shahroudnejad', 'Abhilash Rakkunedeth Hareendranathan', 'Jacob L Jaremko', 'Michelle Noga', 'Kumaradevan Punithakumar']","https://export.arxiv.org/abs/2006.05861","2020-06-10","eess.IV cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17162,""
"Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of   Current Evaluation Protocols","As conversational AI-based dialogue management has increasingly become a trending topic, the need for a standardized and reliable evaluation procedure grows even more pressing. The current state of affairs suggests various evaluation protocols to assess chat-oriented dialogue management systems, rendering it difficult to conduct fair comparative studies across different approaches and gain an insightful understanding of their values. To foster this research, a more robust evaluation protocol must be set in place. This paper presents a comprehensive synthesis of both automated and human evaluation methods on dialogue systems, identifying their shortcomings while accumulating evidence towards the most effective evaluation dimensions. A total of 20 papers from the last two years are surveyed to analyze three types of evaluation protocols: automated, static, and interactive. Finally, the evaluation dimensions used in these papers are compared against our expert evaluation on the system-user dialogue data collected from the Alexa Prize 2020.","['Sarah E. Finch', 'Jinho D. Choi']","https://export.arxiv.org/abs/2006.06110","2020-06-10","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17163,""
"A Primer on Zeroth-Order Optimization in Signal Processing and Machine   Learning","Zeroth-order (ZO) optimization is a subset of gradient-free optimization that emerges in many signal processing and machine learning applications. It is used for solving optimization problems similarly to gradient-based methods. However, it does not require the gradient, using only function evaluations. Specifically, ZO optimization iteratively performs three major steps: gradient estimation, descent direction computation, and solution update. In this paper, we provide a comprehensive review of ZO optimization, with an emphasis on showing the underlying intuition, optimization principles and recent advances in convergence analysis. Moreover, we demonstrate promising applications of ZO optimization, such as evaluating robustness and generating explanations from black-box deep learning models, and efficient online sensor management.","['Sijia Liu', 'Pin-Yu Chen', 'Bhavya Kailkhura', 'Gaoyuan Zhang', 'Alfred Hero', 'Pramod K. Varshney']","https://export.arxiv.org/abs/2006.06224","2020-06-11","cs.LG eess.SP stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17164,""
"Guided Transformer: Leveraging Multiple External Sources for   Representation Learning in Conversational Search","Asking clarifying questions in response to ambiguous or faceted queries has been recognized as a useful technique for various information retrieval systems, especially conversational search systems with limited bandwidth interfaces. Analyzing and generating clarifying questions have been studied recently but the accurate utilization of user responses to clarifying questions has been relatively less explored. In this paper, we enrich the representations learned by Transformer networks using a novel attention mechanism from external information sources that weights each term in the conversation. We evaluate this Guided Transformer model in a conversational search scenario that includes clarifying questions. In our experiments, we use two separate external sources, including the top retrieved documents and a set of different possible clarifying questions for the query. We implement the proposed representation learning model for two downstream tasks in conversational search; document retrieval and next clarifying question selection. Our experiments use a public dataset for search clarification and demonstrate significant improvements compared to competitive baselines.","['Helia Hashemi', 'Hamed Zamani', 'W. Bruce Croft']","https://export.arxiv.org/abs/2006.07548","2020-06-12","cs.IR cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17165,""
"Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually   Grounded Speech","The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network's architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.","['William N. Havard', 'Jean-Pierre Chevrot', 'Laurent Besacier']","https://export.arxiv.org/abs/2006.08387","2020-06-15","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17166,""
"A systematic review and taxonomy of explanations in decision support and   recommender systems","With the recent advances in the field of artificial intelligence, an increasing number of decision-making tasks are delegated to software systems. A key requirement for the success and adoption of such systems is that users must trust system choices or even fully automated decisions. To achieve this, explanation facilities have been widely investigated as a means of establishing trust in these systems since the early years of expert systems. With today's increasingly sophisticated machine learning algorithms, new challenges in the context of explanations, accountability, and trust towards such systems constantly arise. In this work, we systematically review the literature on explanations in advice-giving systems. This is a family of systems that includes recommender systems, which is one of the most successful classes of advice-giving software in practice. We investigate the purposes of explanations as well as how they are generated, presented to users, and evaluated. As a result, we derive a novel comprehensive taxonomy of aspects to be considered when designing explanation facilities for current and future decision support systems. The taxonomy includes a variety of different facets, such as explanation objective, responsiveness, content and presentation. Moreover, we identified several challenges that remain unaddressed so far, for example related to fine-grained issues associated with the presentation of explanations and how explanation facilities are evaluated.","['Ingrid Nunes', 'Dietmar Jannach']","https://export.arxiv.org/abs/2006.08672","2020-06-15","cs.AI cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17167,""
"Cross-lingual Retrieval for Iterative Self-Supervised Training","Recent studies have demonstrated the cross-lingual alignment ability of multilingual pretrained language models. In this work, we found that the cross-lingual alignment can be further improved by training seq2seq models on sentence pairs mined using their own encoder outputs. We utilized these findings to develop a new approach -- cross-lingual retrieval for iterative self-supervised training (CRISS), where mining and training processes are applied iteratively, improving cross-lingual alignment and translation ability at the same time. Using this method, we achieved state-of-the-art unsupervised machine translation results on 9 language directions with an average improvement of 2.4 BLEU, and on the Tatoeba sentence retrieval task in the XTREME benchmark on 16 languages with an average improvement of 21.5% in absolute accuracy. Furthermore, CRISS also brings an additional 1.8 BLEU improvement on average compared to mBART, when finetuned on supervised machine translation downstream tasks.","['Chau Tran', 'Yuqing Tang', 'Xian Li', 'Jiatao Gu']","https://export.arxiv.org/abs/2006.09526","2020-06-16","cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17168,""
"Neural Topic Modeling with Continual Lifelong Learning","Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task.","['Pankaj Gupta', 'Yatin Chaudhary', 'Thomas Runkler', 'Hinrich SchÃ¼tze']","https://export.arxiv.org/abs/2006.10909","2020-06-18","cs.CL cs.IR cs.LG cs.NE","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17169,""
"Digital personal health libraries: a systematic literature review","Objective: This paper gives context on recent literature regarding the development of digital personal health libraries (PHL) and provides insights into the potential application of consumer health informatics in diverse clinical specialties. Materials and Methods: A systematic literature review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. Here, 2,850 records were retrieved from PubMed and EMBASE in March 2020 using search terms: personal, health, and library. Information related to the health topic, target population, study purpose, library function, data source, data science method, evaluation measure, and status were extracted from each eligible study. In addition, knowledge discovery methods, including co-occurrence analysis and multiple correspondence analysis, were used to explore research trends of PHL. Results: After screening, this systematic review focused on a dozen articles related to PHL. These encompassed health topics such as infectious diseases, congestive heart failure, electronic prescribing. Data science methods included relational database, information retrieval technology, ontology construction technology. Evaluation measures were heterogeneous regarding PHL functions and settings. At the time of writing, only one of the PHLs described in these articles is available for the public while the others are either prototypes or in the pilot stage. Discussion: Although PHL researches have used different methods to address problems in diverse health domains, there is a lack of an effective PHL to meet the needs of older adults. Conclusion: The development of PHLs may create an unprecedented opportunity for promoting the health of older consumers by providing diverse health information.","['Huitong Ding', 'Chi Zhang', 'Ning An', 'Lingling Zhang', 'Ning Xie', 'Gil Alterovitz']","https://export.arxiv.org/abs/2006.11686","2020-06-20","cs.DL cs.CY","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17170,""
"Success and Failure in Software Engineering: a Followup Systematic   Literature Review","Success and failure in software engineering are still among the least understood phenomena in the discipline. In a recent special journal issue on the topic, Mantyla et al. started discussing these topics from different angles; the authors focused their contributions on offering a general overview of both topics without deeper detail. Recognising the importance and impact of the topic, we have executed a followup, more in-depth systematic literature review with additional analyses beyond what was previously provided. These new analyses offer: (a) a grounded-theory of success and failure factors, harvesting over 500+ factors from the literature; (b) 14 manually-validated clusters of factors that provide relevant areas for success- and failure-specific measurement and risk-analysis; (c) a quality model composed of previously unmeasured organizational structure quantities which are germane to software product, process, and community quality. We show that the topics of success and failure deserve further study as well as further automated tool support, e.g., monitoring tools and metrics able to track the factors and patterns emerging from our study. This paper provides managers with risks as well as a more fine-grained analysis of the parameters that can be appraised to anticipate the risks.","['Damian A. Tamburri', '$UNK Member-at-Large', '$UNK IEEE', 'Fabio Palomba', 'Member IEEE', 'Rick Kazman', 'Senior Member', '$UNK IEEE']","https://export.arxiv.org/abs/2006.12086","2020-06-22","cs.SE","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17171,""
"ReCO: A Large Scale Chinese Reading Comprehension Dataset on Opinion","This paper presents the ReCO, a human-curated ChineseReading Comprehension dataset on Opinion. The questions in ReCO are opinion based queries issued to the commercial search engine. The passages are provided by the crowdworkers who extract the support snippet from the retrieved documents. Finally, an abstractive yes/no/uncertain answer was given by the crowdworkers. The release of ReCO consists of 300k questions that to our knowledge is the largest in Chinese reading comprehension. A prominent characteristic of ReCO is that in addition to the original context paragraph, we also provided the support evidence that could be directly used to answer the question. Quality analysis demonstrates the challenge of ReCO that requires various types of reasoning skills, such as causal inference, logical reasoning, etc. Current QA models that perform very well on many question answering problems, such as BERT, only achieve 77% accuracy on this dataset, a large margin behind humans nearly 92% performance, indicating ReCO presents a good challenge for machine reading comprehension. The codes, datasets are freely available at https://github.com/benywon/ReCO.","['$UNK BingningWang', 'Ting Yao', 'Qi Zhang', 'Jingfang Xu', 'Xiaochuan Wang']","https://export.arxiv.org/abs/2006.12146","2020-06-22","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17172,""
"ASReview: Open Source Software for Efficient and Transparent Active   Learning for Systematic Reviews","For many tasks - including but not limited to systematic reviews for research fields - the scientific literature needs to be checked systematically. Currently, scholars and practitioners might screen thousands of studies by hand to determine which studies to include in their review. This process is error prone and inefficient, because of the extremely imbalanced data: only a very small fraction of the studies screened will be relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that ASReview can yield far more efficient reviewing than manual reviewing, while exhibiting adequate quality. Furthermore, we describe the different options of the free and open source research software, we show how it can be used for screening the COVID19 literature, and we present the results from a series of user experience tests. We invite the community to contribute to open source projects such as our own, that provide measurable and reproducible improvement over current practice.","['Rens van de Schoot', 'Jonathan de Bruin', 'Raoul Schram', 'Parisa Zahedi', 'Jan de Boer', 'Felix Weijdema', 'Bianca Kramer', 'Martijn Huijts', 'Maarten Hoogerwerf', 'Gerbrich Ferdinands', 'Albert Harkema', 'Joukje Willemsen', 'Yongchao Ma', 'Qixiang Fang', 'Sybren Hindriks', 'Lars Tummers', 'Daniel Oberski']","https://export.arxiv.org/abs/2006.12166","2020-06-22","cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17173,""
"Artificial Intelligence-Assisted Energy and Thermal Comfort Control for   Sustainable Buildings: An Extended Representation of the Systematic Review","Different factors such as thermal comfort, humidity, air quality, and noise have significant combined effects on the acceptability and quality of the activities performed by the building occupants who spend most of their times indoors. Among the factors cited, thermal comfort, which contributes to the human well-being because of its connection with the thermoregulation of the human body. Therefore, the creation of thermally comfortable and energy efficient environments is of great importance in the design of the buildings and hence the heating, ventilation and air-conditioning systems. Recent works have been directed towards more advanced control strategies, based mainly on artificial intelligence which has the ability to imitate human behavior. This systematic literature review aims to provide an overview of the intelligent control strategies inside building and to investigate their ability to balance thermal comfort and energy efficiency optimization in indoor environments. Methods. A systematic literature review examined the peer-reviewed research works using ACM Digital Library, Scopus, Google Scholar, IEEE Xplore (IEOL), Web of Science, and Science Direct (SDOL), besides other sources from manual search. With the following string terms: thermal comfort, comfort temperature, preferred temperature, intelligent control, advanced control, artificial intelligence, computational intelligence, building, indoors, and built environment. Inclusion criteria were: English, studies monitoring, mainly, human thermal comfort in buildings and energy efficiency simultaneously based on control strategies using the intelligent approaches. Preferred Reporting Items for Systematic Reviews and Meta-Analysis guidelines were used. Initially, 1,077 articles were yielded, and 120 ultimately met inclusion criteria and were reviewed.","['Ghezlane Halhoul Merabet', 'Mohamed Essaaidi', 'Mohamed Ben-Haddou', 'Basheer Qolomany', 'Junaid Qadir', 'Muhammad Anan', 'Ala Al-Fuqaha', 'Riduan Mohamed Abid', 'Driss Benhaddou']","https://export.arxiv.org/abs/2006.12559","2020-06-22","eess.SP cs.CY cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17174,""
"A Mean-Field Theory for Learning the Sch\""{o}nberg Measure of Radial   Basis Functions","We develop and analyze a projected particle Langevin optimization method to learn the distribution in the Sch\""{o}nberg integral representation of the radial basis functions from training samples. More specifically, we characterize a distributionally robust optimization method with respect to the Wasserstein distance to optimize the distribution in the Sch\""{o}nberg integral representation. To provide theoretical performance guarantees, we analyze the scaling limits of a projected particle online (stochastic) optimization method in the mean-field regime. In particular, we prove that in the scaling limits, the empirical measure of the Langevin particles converges to the law of a reflected It\^{o} diffusion-drift process. Moreover, the drift is also a function of the law of the underlying process. Using It\^{o} lemma for semi-martingales and Grisanov's change of measure for the Wiener processes, we then derive a Mckean-Vlasov type partial differential equation (PDE) with Robin boundary conditions that describes the evolution of the empirical measure of the projected Langevin particles in the mean-field regime. In addition, we establish the existence and uniqueness of the steady-state solutions of the derived PDE in the weak sense. We apply our learning approach to train radial kernels in the kernel locally sensitive hash (LSH) functions, where the training data-set is generated via a $k$-mean clustering method on a small subset of data-base. We subsequently apply our kernel LSH with a trained kernel for image retrieval task on MNIST data-set, and demonstrate the efficacy of our kernel learning approach. We also apply our kernel learning approach in conjunction with the kernel support vector machines (SVMs) for classification of benchmark data-sets.","['Masoud Badiei Khuzani', 'Yinyu Ye', 'Sandy Napel', 'Lei Xing']","https://export.arxiv.org/abs/2006.13330","2020-06-23","math.ST cs.LG math.OC stat.ML stat.TH","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17175,""
"Mining Misdiagnosis Patterns from Biomedical Literature","Diagnostic errors can pose a serious threat to patient safety, leading to serious harm and even death. Efforts are being made to develop interventions that allow physicians to reassess for errors and improve diagnostic accuracy. Our study presents an exploration of misdiagnosis patterns mined from PubMed abstracts. Article titles containing certain phrases indicating misdiagnosis were selected and frequencies of these misdiagnoses calculated. We present the resulting patterns in the form of a directed graph with frequency-weighted misdiagnosis edges connecting diagnosis vertices. We find that the most commonly misdiagnosed diseases were often misdiagnosed as many different diseases, with each misdiagnosis having a relatively low frequency, rather than as a single disease with greater probability. Additionally, while a misdiagnosis relationship may generally exist, the relationship was often found to be one-sided.","['Cindy Li', 'Elizabeth Chen', 'Guergana Savova', 'Hamish Fraser', 'Carsten Eickhoff']","https://export.arxiv.org/abs/2006.13721","2020-06-24","cs.IR cs.CY cs.DL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17176,""
"On the Replicability and Reproducibility of Deep Learning in Software   Engineering","Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain knowledge. Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) replicability - whether the reported experimental result can be approximately reproduced in high probability with the same DL model and the same data; and (2) reproducibility - whether one reported experimental findings can be reproduced by new experiments with the same experimental protocol and DL model, but different sampled real-world data. Unlike traditional machine learning (ML) models, DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming optimization process. In this study, we conducted a literature review on 93 DL studies recently published in twenty SE journals or conferences. Our statistics show the urgency of investigating these two factors in SE. Moreover, we re-ran four representative DL models in SE. Experimental results show the importance of replicability and reproducibility, where the reported performance of a DL model could not be replicated for an unstable optimization process. Reproducibility could be substantially compromised if the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data. It is therefore urgent for the SE community to provide a long-lasting link to a replication package, enhance DL-based solution stability and convergence, and avoid performance sensitivity on different sampled data.","['Chao Liu', 'Cuiyun Gao', 'Xin Xia', 'David Lo', 'John Grundy', 'Xiaohu Yang']","https://export.arxiv.org/abs/2006.14244","2020-06-25","cs.SE cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17177,""
"Towards an automated repository for indexing, analysis and   characterization of municipal e-government websites in Mexico","This article addresses a problem in the electronic government discipline with special interest in Mexico: the need for a concentrated and updated information source about municipal e-government websites. One reason for this is the lack of a complete and updated database containing the electronic addresses (web domain names) of the municipal governments having a website. Due to diverse causes, not all the Mexican municipalities have one, and a number of those having it do not present information corresponding to the current governments but, instead, to other previous ones. The scarce official lists of municipal websites are not updated with the sufficient frequency, and manually determining which municipalities have an operating and valid website in a given moment is a time-consuming process. Besides, website contents do not always comply with legal requirements and are considerably heterogeneous. In turn, the evolution development level of municipal websites is valuable information that can be harnessed for diverse theoretical and practical purposes in the public administration field. Obtaining all these pieces of information requires website content analysis. Therefore, this article investigates the need for and the feasibility to automate implementation and updating of a digital repository to perform diverse analyses of these websites. Its technological feasibility is addressed by means of a literature review about web scraping and by proposing a preliminary manual methodology. This takes into account known, proven, techniques and software tools for web crawling and scraping. No new techniques for crawling or scraping are proposed because the existing ones satisfy the current needs. Finally, software requirements are specified in order to automate the creation, updating, indexing, and analyses of the repository.","['Sergio R. Coria', 'Leonardo Marcos-Santiago', 'Christian A. Cruz-Melendez', 'Juan M. Jimenez-Canseco']","https://export.arxiv.org/abs/2006.14746","2020-06-25","cs.CY cs.DL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17178,""
"MIRA: Leveraging Multi-Intention Co-click Information in Web-scale   Document Retrieval using Deep Neural Networks","We study the problem of deep recall model in industrial web search, which is, given a user query, retrieve hundreds of most relevance documents from billions of candidates. The common framework is to train two encoding models based on neural embedding which learn the distributed representations of queries and documents separately and match them in the latent semantic space. However, all the exiting encoding models only leverage the information of the document itself, which is often not sufficient in practice when matching with query terms, especially for the hard tail queries. In this work we aim to leverage the additional information for each document from its co-click neighbour to help document retrieval. The challenges include how to effectively extract information and eliminate noise when involving co-click information in deep model while meet the demands of billion-scale data size for real time online inference.   To handle the noise in co-click relations, we firstly propose a web-scale Multi-Intention Co-click document Graph(MICG) which builds the co-click connections between documents on click intention level but not on document level. Then we present an encoding framework MIRA based on Bert and graph attention networks which leverages a two-factor attention mechanism to aggregate neighbours. To meet the online latency requirements, we only involve neighbour information in document side, which can save the time-consuming query neighbor search in real time serving. We conduct extensive offline experiments on both public dataset and private web-scale dataset from two major commercial search engines demonstrating the effectiveness and scalability of the proposed method compared with several baselines. And a further case study reveals that co-click relations mainly help improve web search quality from two aspects: key concept enhancing and query term complementary.","['Yusi Zhang', 'Chuanjie Liu', 'Angen Luo', 'Hui Xue', 'Xuan Shan', 'Yuxiang Luo', 'Yiqian Xia', 'Yuanchi Yan', 'Haidong Wang']","https://export.arxiv.org/abs/2007.01510","2020-07-03","cs.IR cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17179,""
"Structured (De)composable Representations Trained with Neural Networks","The paper proposes a novel technique for representing templates and instances of concept classes. A template representation refers to the generic representation that captures the characteristics of an entire class. The proposed technique uses end-to-end deep learning to learn structured and composable representations from input images and discrete labels. The obtained representations are based on distance estimates between the distributions given by the class label and those given by contextual information, which are modeled as environments. We prove that the representations have a clear structure allowing to decompose the representation into factors that represent classes and environments. We evaluate our novel technique on classification and retrieval tasks involving different modalities (visual and language data).","['Graham Spinks', 'Marie-Francine Moens']","https://export.arxiv.org/abs/2007.03325","2020-07-07","cs.LG cs.CL cs.CV stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17180,""
"Neural relation extraction: a survey","Neural relation extraction discovers semantic relations between entities from unstructured text using deep learning methods. In this study, we present a comprehensive review of methods on neural network based relation extraction. We discuss advantageous and incompetent sides of existing studies and investigate additional research directions and improvement ideas in this field.","['Mehmet Aydar', 'Ozge Bozal', 'Furkan Ozbay']","https://export.arxiv.org/abs/2007.04247","2020-06-23","cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17181,""
"Less is More: Rejecting Unreliable Reviews for Product Question   Answering","Promptly and accurately answering questions on products is important for e-commerce applications. Manually answering product questions (e.g. on community question answering platforms) results in slow response and does not scale. Recent studies show that product reviews are a good source for real-time, automatic product question answering (PQA). In the literature, PQA is formulated as a retrieval problem with the goal to search for the most relevant reviews to answer a given product question. In this paper, we focus on the issue of answerability and answer reliability for PQA using reviews. Our investigation is based on the intuition that many questions may not be answerable with a finite set of reviews. When a question is not answerable, a system should return nil answers rather than providing a list of irrelevant reviews, which can have significant negative impact on user experience. Moreover, for answerable questions, only the most relevant reviews that answer the question should be included in the result. We propose a conformal prediction based framework to improve the reliability of PQA systems, where we reject unreliable answers so that the returned results are more concise and accurate at answering the product question, including returning nil answers for unanswerable questions. Experiments on a widely used Amazon dataset show encouraging results of our proposed framework. More broadly, our results demonstrate a novel and effective application of conformal methods to a retrieval task.","['Shiwei Zhang', 'Xiuzhen Zhang', 'Jey Han Lau', 'Jeffrey Chan', 'Cecile Paris']","https://export.arxiv.org/abs/2007.04526","2020-07-08","cs.CL cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17182,""
"A Systematic Review on Context-Aware Recommender Systems using Deep   Learning and Embeddings","Recommender Systems are tools that improve how users find relevant information in web systems, so they do not face too much information. In order to generate better recommendations, the context of information should be used in the recommendation process. Context-Aware Recommender Systems were created, accomplishing state-of-the-art results and improving traditional recommender systems. There are many approaches to build recommender systems, and two of the most prominent advances in area have been the use of Embeddings to represent the data in the recommender system, and the use of Deep Learning architectures to generate the recommendations to the user. A systematic review adopts a formal and systematic method to perform a bibliographic review, and it is used to identify and evaluate all the research in certain area of study, by analyzing the relevant research published. A systematic review was conducted to understand how the Deep Learning and Embeddings techniques are being applied to improve Context-Aware Recommender Systems. We summarized the architectures that are used to create those and the domains that they are used.","['Igor AndrÃ© Pegoraro Santana', 'Marcos Aurelio Domingues']","https://export.arxiv.org/abs/2007.04782","2020-07-09","cs.IR cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17183,""
"Medical Instrument Detection in Ultrasound-Guided Interventions: A   Review","Medical instrument detection is essential for computer-assisted interventions since it would facilitate the surgeons to find the instrument efficiently with a better interpretation, which leads to a better outcome. This article reviews medical instrument detection methods in the ultrasound-guided intervention. First, we present a comprehensive review of instrument detection methodologies, which include traditional non-data-driven methods and data-driven methods. The non-data-driven methods were extensively studied prior to the era of machine learning, i.e. data-driven approaches. We discuss the main clinical applications of medical instrument detection in ultrasound, including anesthesia, biopsy, prostate brachytherapy, and cardiac catheterization, which were validated on clinical datasets. Finally, we selected several principal publications to summarize the key issues and potential research directions for the computer-assisted intervention community.","['Hongxu Yang', 'Caifeng Shan', 'Alexander F. Kolen', 'Peter H. N. de With']","https://export.arxiv.org/abs/2007.04807","2020-07-09","eess.IV cs.CV physics.med-ph","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17184,""
"A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy:   From Physics-Based to AI-Guided Driving Policy Learning","This paper serves as an introduction and overview of the potentially useful models and methodologies from artificial intelligence (AI) into the field of transportation engineering for autonomous vehicle (AV) control in the era of mixed autonomy. We will discuss state-of-the-art applications of AI-guided methods, identify opportunities and obstacles, raise open questions, and help suggest the building blocks and areas where AI could play a role in mixed autonomy. We divide the stage of autonomous vehicle (AV) deployment into four phases: the pure HVs, the HV-dominated, the AVdominated, and the pure AVs. This paper is primarily focused on the latter three phases. It is the first-of-its-kind survey paper to comprehensively review literature in both transportation engineering and AI for mixed traffic modeling. Models used for each phase are summarized, encompassing game theory, deep (reinforcement) learning, and imitation learning. While reviewing the methodologies, we primarily focus on the following research questions: (1) What scalable driving policies are to control a large number of AVs in mixed traffic comprised of human drivers and uncontrollable AVs? (2) How do we estimate human driver behaviors? (3) How should the driving behavior of uncontrollable AVs be modeled in the environment? (4) How are the interactions between human drivers and autonomous vehicles characterized? Hopefully this paper will not only inspire our transportation community to rethink the conventional models that are developed in the data-shortage era, but also reach out to other disciplines, in particular robotics and machine learning, to join forces towards creating a safe and efficient mixed traffic ecosystem.","['Xuan Di', 'Rongye Shi']","https://export.arxiv.org/abs/2007.05156","2020-07-10","cs.AI cs.RO","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17185,""
"BISON:BM25-weighted Self-Attention Framework for Multi-Fields Document   Search","Recent breakthrough in natural language processing has advanced the information retrieval from keyword match to semantic vector search. To map query and documents into semantic vectors, self-attention models are being widely used. However, typical self-attention models, like Transformer, lack prior knowledge to distinguish the importance of different tokens, which has been proved to play a critical role in information retrieval tasks. In addition to this, when applying WordPiece tokenization, a rare word may be split into several different tokens. How to translate word-level prior knowledge into WordPiece tokens becomes a new challenge for the semantic representation generation. Moreover, web documents usually have multiple fields. Due to the heterogeneity of different fields, simple combination is not a good choice. In this paper, We propose a novel BM25-weighted Self-Attention framework (BISON) for web document search. By leveraging BM25 as prior weights, BISON learns weighted attention scores jointly with query matrix Q and key matrix K. We also present an efficient whole word weight sharing solution to mitigate prior knowledge discrepancy between words and WordPiece tokens. Furthermore, BISON effectively combines multiple fields by placing different fields into different segments. We demonstrate BISON is more efficient to capture the topical and semantic representation both in query and document. Intrinsic evaluation and experiments conducted on public data sets reveal BISON to be a general framework for document ranking task. It outperforms BERT and other modern models while retaining the same model complexity with BERT.","['Xuan Shan', 'Chuanjie Liu', 'Yiqian Xia', 'Qi Chen', 'Yusi Zhang', 'Angen Luo', 'Yuxiang Luo']","https://export.arxiv.org/abs/2007.05186","2020-07-10","cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17186,""
"Smart technology in the classroom: a systematic reviewProspects for   algorithmic accountability","Artificial intelligence (AI) algorithms have emerged in the educational domain as a tool to make learning more efficient. Different applications for mastering particular skills, learning new languages, and tracking their progress are used by children. What is the impact on children from using this smart technology? We conducted a systematic review to understand the state of the art. We explored the literature in several sub-disciplines: wearables, child psychology, AI and education, school surveillance, and accountability. Our review identified the need for more research for each established topic. We managed to find both positive and negative effects of using wearables, but cannot conclude if smart technology use leads to lowering the young children's performance. Based on our insights we propose a framework to effectively identify accountability for smart technology in education.","['Arian Garshi', 'Malin Wist Jakobsen', 'JÃ¸rgen Nyborg-Christensen', 'Daniel Ostnes', 'Maria Ovchinnikova']","https://export.arxiv.org/abs/2007.06374","2020-07-13","cs.CY cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17187,""
"A Feature Analysis for Multimodal News Retrieval","Content-based information retrieval is based on the information contained in documents rather than using metadata such as keywords. Most information retrieval methods are either based on text or image. In this paper, we investigate the usefulness of multimodal features for cross-lingual news search in various domains: politics, health, environment, sport, and finance. To this end, we consider five feature types for image and text and compare the performance of the retrieval system using different combinations. Experimental results show that retrieval results can be improved when considering both visual and textual information. In addition, it is observed that among textual features entity overlap outperforms word embeddings, while geolocation embeddings achieve better performance among visual features in the retrieval task.","['Golsa Tahmasebzadeh', 'Sherzod Hakimov', 'Eric MÃ¼ller-Budack', 'Ralph Ewerth']","https://export.arxiv.org/abs/2007.06390","2020-07-13","cs.CL cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17188,""
"The Notary in the Haystack -- Countering Class Imbalance in Document   Processing with CNNs","Notarial instruments are a category of documents. A notarial instrument can be distinguished from other documents by its notary sign, a prominent symbol in the certificate, which also allows to identify the document's issuer. Naturally, notarial instruments are underrepresented in regard to other documents. This makes a classification difficult because class imbalance in training data worsens the performance of Convolutional Neural Networks. In this work, we evaluate different countermeasures for this problem. They are applied to a binary classification and a segmentation task on a collection of medieval documents. In classification, notarial instruments are distinguished from other documents, while the notary sign is separated from the certificate in the segmentation task. We evaluate different techniques, such as data augmentation, under- and oversampling, as well as regularizing with focal loss. The combination of random minority oversampling and data augmentation leads to the best performance. In segmentation, we evaluate three loss-functions and their combinations, where only class-weighted dice loss was able to segment the notary sign sufficiently.","['Martin Leipert', 'Georg Vogeler', 'Mathias Seuret', 'Andreas Maier', 'Vincent Christlein']","https://export.arxiv.org/abs/2007.07943","2020-07-15","cs.CV eess.IV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17189,""
"Deep Reinforced Query Reformulation for Information Retrieval","Query reformulations have long been a key mechanism to alleviate the vocabulary-mismatch problem in information retrieval, for example by expanding the queries with related query terms or by generating paraphrases of the queries. In this work, we propose a deep reinforced query reformulation (DRQR) model to automatically generate new reformulations of the query. To encourage the model to generate queries which can achieve high performance when performing the retrieval task, we incorporate query performance prediction into our reward function. In addition, to evaluate the quality of the reformulated query in the context of information retrieval, we first train our DRQR model, then apply the retrieval ranking model on the obtained reformulated query. Experiments are conducted on the TREC 2020 Deep Learning track MSMARCO document ranking dataset. Our results show that our proposed model outperforms several query reformulation model baselines when performing retrieval task. In addition, improvements are also observed when combining with various retrieval models, such as query expansion and BERT.","['Xiao Wang', 'Craig Macdonald', 'Iadh Ounis']","https://export.arxiv.org/abs/2007.07987","2020-07-15","cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17190,""
"Overview of CheckThat! 2020: Automatic Identification and Verification   of Claims in Social Media","We present an overview of the third edition of the CheckThat! Lab at CLEF 2020. The lab featured five tasks in two different languages: English and Arabic. The first four tasks compose the full pipeline of claim verification in social media: Task 1 on check-worthiness estimation, Task 2 on retrieving previously fact-checked claims, Task 3 on evidence retrieval, and Task 4 on claim verification. The lab is completed with Task 5 on check-worthiness estimation in political debates and speeches. A total of 67 teams registered to participate in the lab (up from 47 at CLEF 2019), and 23 of them actually submitted runs (compared to 14 at CLEF 2019). Most teams used deep neural networks based on BERT, LSTMs, or CNNs, and achieved sizable improvements over the baselines on all tasks. Here we describe the tasks setup, the evaluation results, and a summary of the approaches used by the participants, and we discuss some lessons learned. Last but not least, we release to the research community all datasets from the lab as well as the evaluation scripts, which should enable further research in the important tasks of check-worthiness estimation and automatic claim verification.","['Alberto Barron-Cedeno', 'Tamer Elsayed', 'Preslav Nakov', 'Giovanni Da San Martino', 'Maram Hasanain', 'Reem Suwaileh', 'Fatima Haouari', 'Nikolay Babulkov', 'Bayan Hamdan', 'Alex Nikolov', 'Shaden Shaar', 'Zien Sheikh Ali']","https://export.arxiv.org/abs/2007.07997","2020-07-15","cs.CL cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17191,""
"Learning from Noisy Labels with Deep Neural Networks: A Survey","Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 46 state-of-the-art robust training methods, all of which are categorized into seven groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.","['Hwanjun Song', 'Minseok Kim', 'Dongmin Park', 'Jae-Gil Lee']","https://export.arxiv.org/abs/2007.08199","2020-07-16","cs.LG cs.CV stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17192,""
"Backdoor Learning: A Survey","Backdoor attack intends to embed hidden backdoor into deep neural networks (DNNs), such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger. Backdoor attack could happen when the training process is not fully controlled by the user, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, its systematic review, however, remains blank. In this paper, we present the first comprehensive survey of this realm. We summarize and categorize existing backdoor attacks and defenses based on their characteristics, and provide a unified framework for analyzing poisoning-based backdoor attacks. Besides, we also analyze the relation between backdoor attacks and the relevant fields ($i.e.,$ adversarial attack and data poisoning), and summarize the benchmark datasets. Finally, we briefly outline certain future research directions relying upon reviewed works.","['Yiming Li', 'Baoyuan Wu', 'Yong Jiang', 'Zhifeng Li', 'Shu-Tao Xia']","https://export.arxiv.org/abs/2007.08745","2020-07-17","cs.CR cs.CV cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17193,""
"Always-On 674uW @ 4GOP/s Error Resilient Binary Neural Networks with   Aggressive SRAM Voltage Scaling on a 22nm IoT End-Node","Binary Neural Networks (BNNs) have been shown to be robust to random bit-level noise, making aggressive voltage scaling attractive as a power-saving technique for both logic and SRAMs. In this work, we introduce the first fully programmable IoT end-node system-on-chip (SoC) capable of executing software-defined, hardware-accelerated BNNs at ultra-low voltage. Our SoC exploits a hybrid memory scheme where error-vulnerable SRAMs are complemented by reliable standard-cell memories to safely store critical data under aggressive voltage scaling. On a prototype in 22nm FDX technology, we demonstrate that both the logic and SRAM voltage can be dropped to 0.5Vwithout any accuracy penalty on a BNN trained for the CIFAR-10 dataset, improving energy efficiency by 2.2X w.r.t. nominal conditions. Furthermore, we show that the supply voltage can be dropped to 0.42V (50% of nominal) while keeping more than99% of the nominal accuracy (with a bit error rate ~1/1000). In this operating point, our prototype performs 4Gop/s (15.4Inference/s on the CIFAR-10 dataset) by computing up to 13binary ops per pJ, achieving 22.8 Inference/s/mW while keeping within a peak power envelope of 674uW - low enough to enable always-on operation in ultra-low power smart cameras, long-lifetime environmental sensors, and insect-sized pico-drones.","['Alfio Di Mauro', 'Francesco Conti', 'Pasquale Davide Schiavone', 'Davide Rossi', 'Luca Benini']","https://export.arxiv.org/abs/2007.08952","2020-07-17","cs.AR cs.LG eess.SP","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17194,""
"A Systematic Review of Natural Language Processing for Knowledge   Management in Healthcare","Driven by the visions of Data Science, recent years have seen a paradigm shift in Natural Language Processing (NLP). NLP has set the milestone in text processing and proved to be the preferred choice for researchers in the healthcare domain. The objective of this paper is to identify the potential of NLP, especially, how NLP is used to support the knowledge management process in the healthcare domain, making data a critical and trusted component in improving the health outcomes. This paper provides a comprehensive survey of the state-of-the-art NLP research with a particular focus on how knowledge is created, captured, shared, and applied in the healthcare domain. Our findings suggest, first, the techniques of NLP those supporting knowledge management extraction and knowledge capture processes in healthcare. Second, we propose a conceptual model for the knowledge extraction process through NLP. Finally, we discuss a set of issues, challenges, and proposed future research areas.","['Ganga Prasad Basyal', 'Bhaskar P. Rimal', 'David Zeng']","https://export.arxiv.org/abs/2007.09134","2020-07-17","cs.CY","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17195,""
"Software Development Analytics in Practice: A Systematic Literature   Review","Context:Software Development Analytics is a research area concerned with providing insights to improve product deliveries and processes. Many types of studies, data sources and mining methods have been used for that purpose. Objective:This systematic literature review aims at providing an aggregate view of the relevant studies on Software Development Analytics in the past decade (2010-2019), with an emphasis on its application in practical settings. Method:Definition and execution of a search string upon several digital libraries, followed by a quality assessment criteria to identify the most relevant papers. On those, we extracted a set of characteristics (study type, data source, study perspective, development life-cycle activities covered, stakeholders, mining methods, and analytics scope) and classified their impact against a taxonomy. Results:Source code repositories, experimental case studies, and developers are the most common data sources, study types, and stakeholders, respectively. Product and project managers are also often present, but less than expected. Mining methods are evolving rapidly and that is reflected in the long list identified. Descriptive statistics are the most usual method followed by correlation analysis. Being software development an important process in every organization, it was unexpected to find that process mining was present in only one study. Most contributions to the software development life cycle were given in the quality dimension. Time management and costs control were lightly debated. The analysis of security aspects suggests it is an increasing topic of concern for practitioners. Risk management contributions are scarce. Conclusions:There is a wide improvement margin for software development analytics in practice. For instance, mining and analyzing the activities performed by software developers in their actual workbench, the IDE.","['Joao Caldeira', 'Fernando Brito e Abreu', 'Jorge Cardoso', 'Toacy Oliveira']","https://export.arxiv.org/abs/2007.10213","2020-07-20","cs.SE","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17196,""
"Conformer-Kernel with Query Term Independence for Document Retrieval","The Transformer-Kernel (TK) model has demonstrated strong reranking performance on the TREC Deep Learning benchmark---and can be considered to be an efficient (but slightly less effective) alternative to BERT-based ranking models. In this work, we extend the TK architecture to the full retrieval setting by incorporating the query term independence assumption. Furthermore, to reduce the memory complexity of the Transformer layers with respect to the input sequence length, we propose a new Conformer layer. We show that the Conformer's GPU memory requirement scales linearly with input sequence length, making it a more viable option when ranking long documents. Finally, we demonstrate that incorporating explicit term matching signal into the model can be particularly useful in the full retrieval setting. We present preliminary results from our work in this paper.","['Bhaskar Mitra', 'Sebastian Hofstatter', 'Hamed Zamani', 'Nick Craswell']","https://export.arxiv.org/abs/2007.10434","2020-07-20","cs.IR cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17197,""
"Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive   Review","This work provides the community with a timely comprehensive review of backdoor attacks and countermeasures on deep learning. According to the attacker's capability and affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into six categorizations: code poisoning, outsourcing, pretrained, data collection, collaborative learning and post-deployment. Accordingly, attacks under each categorization are combed. The countermeasures are categorized into four general classes: blind backdoor removal, offline backdoor inspection, online backdoor inspection, and post backdoor removal. Accordingly, we review countermeasures, and compare and analyze their advantages and disadvantages. We have also reviewed the flip side of backdoor attacks, which are explored for i) protecting intellectual property of deep learning models, ii) acting as a honeypot to catch adversarial example attacks, and iii) verifying data deletion requested by the data contributor.Overall, the research on defense is far behind the attack, and there is no single defense that can prevent all types of backdoor attacks. In some cases, an attacker can intelligently bypass existing defenses with an adaptive attack. Drawing the insights from the systematic review, we also present key areas for future research on the backdoor, such as empirical security evaluations from physical trigger attacks, and in particular, more efficient and practical countermeasures are solicited.","['Yansong Gao', 'Bao Gia Doan', 'Zhi Zhang', 'Siqi Ma', 'Jiliang Zhang', 'Anmin Fu', 'Surya Nepal', 'Hyoungshick Kim']","https://export.arxiv.org/abs/2007.10760","2020-07-21","cs.CR cs.CV cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17198,""
"Conformance checking: A state-of-the-art literature review","Conformance checking is a set of process mining functions that compare process instances with a given process model. It identifies deviations between the process instances' actual behaviour (""as-is"") and its modelled behaviour (""to-be""). Especially in the context of analyzing compliance in organizations, it is currently gaining momentum -- e.g. for auditors. Researchers have proposed a variety of conformance checking techniques that are geared towards certain process model notations or specific applications such as process model evaluation. This article reviews a set of conformance checking techniques described in 37 scholarly publications. It classifies the techniques along the dimensions ""modelling language"", ""algorithm type"", ""quality metric"", and ""perspective"" using a concept matrix so that the techniques can be better accessed by practitioners and researchers. The matrix highlights the dimensions where extant research concentrates and where blind spots exist. For instance, process miners use declarative process modelling languages often, but applications in conformance checking are rare. Likewise, process mining can investigate process roles or process metrics such as duration, but conformance checking techniques narrow on analyzing control-flow. Future research may construct techniques that support these neglected approaches to conformance checking.","['Sebastian Dunzer', 'Matthias Stierle', 'Martin Matzner', 'Stephan Baier']","https://export.arxiv.org/abs/2007.10903","2020-07-21","cs.SE cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17199,""
"Wireless Image Retrieval at the Edge","We study the image retrieval problem at the wireless edge, where an edge device captures an image, which is then used to retrieve similar images from an edge server. These can be images of the same person or a vehicle taken from other cameras at different times and locations. Our goal is to maximize the accuracy of the retrieval task under power and bandwidth constraints over the wireless link. Due to the stringent delay constraint of the underlying application, sending the whole image at a sufficient quality is not possible. To address this problem we propose two communication schemes, based on digital and analog communications, respectively. In the digital approach, we first propose a deep neural network (DNN) for retrieval-oriented image compression, whose output bit sequence is transmitted over the channel as reliably as possible using conventional channel codes. In the analog joint source and channel coding (JSCC) approach, the feature vectors are directly mapped into channel symbols. We evaluate both schemes on image based re-identification (re-ID) tasks under different channel conditions, including both static and fading channels. We show that the JSCC scheme significantly increases the end-to-end accuracy, speeds up the encoding process, and provides graceful degradation with channel conditions. The proposed architecture is evaluated through extensive simulations on different datasets and channel conditions, as well as through ablation studies.","['Mikolaj Jankowski', 'Deniz Gunduz', 'Krystian Mikolajczyk']","https://export.arxiv.org/abs/2007.10915","2020-07-21","cs.IT cs.LG math.IT","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17200,""
"A Systematic Literature Review on Federated Machine Learning: From A   Software Engineering Perspective","Federated learning is an emerging machine learning paradigm where multiple clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning from a software engineering perspective, we performed a systematic literature review with the extracted 231 primary studies. The results show that most of the known motivations of federated learning appear to be the most studied federated learning challenges, such as communication efficiency and statistical heterogeneity. Also, there are only a few real-world applications of federated learning. Hence, more studies in this area are needed before the actual industrial-level adoption of federated learning.","['Sin Kit Lo', 'Qinghua Lu', 'Chen Wang', 'Hye-Young Paik', 'Liming Zhu']","https://export.arxiv.org/abs/2007.11354","2020-07-22","cs.SE cs.DC cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17201,""
"Complex Sequential Data Analysis: A Systematic Literature Review of   Existing Algorithms","This paper provides a review of past approaches to the use of deep-learning frameworks for the analysis of discrete irregular-patterned complex sequential datasets. A typical example of such a dataset is financial data where specific events trigger sudden irregular changes in the sequence of the data. Traditional deep-learning methods perform poorly or even fail when trying to analyse these datasets. The results of a systematic literature review reveal the dominance of frameworks based on recurrent neural networks. The performance of deep-learning frameworks was found to be evaluated mainly using mean absolute error and root mean square error accuracy metrics. Underlying challenges that were identified are: lack of performance robustness, non-transparency of the methodology, internal and external architectural design and configuration issues. These challenges provide an opportunity to improve the framework for complex irregular-patterned sequential datasets.","['Kudakwashe Dandajena', 'Isabella M. Venter', 'Mehrdad Ghaziasgar', 'Reg Dodds']","https://export.arxiv.org/abs/2007.11572","2020-07-22","cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17202,""
"A Comprehensive Review of Deep Learning Applications in Hydrology and   Water Resources","The global volume of digital data is expected to reach 175 zettabytes by 2025. The volume, variety, and velocity of water-related data are increasing due to large-scale sensor networks and increased attention to topics such as disaster response, water resources management, and climate change. Combined with the growing availability of computational resources and popularity of deep learning, these data are transformed into actionable and practical knowledge, revolutionizing the water industry. In this article, a systematic review of literature is conducted to identify existing research which incorporates deep learning methods in the water sector, with regard to monitoring, management, governance and communication of water resources. The study provides a comprehensive review of state-of-the-art deep learning approaches used in the water industry for generation, prediction, enhancement, and classification tasks, and serves as a guide for how to utilize available deep learning methods for future water resources challenges. Key issues and challenges in the application of these techniques in the water domain are discussed, including the ethics of these technologies for decision-making in water resources management and governance. Finally, we provide recommendations and future directions for the application of deep learning models in hydrology and water resources.","['Muhammed Sit', 'Bekir Z. Demiray', 'Zhongrun Xiang', 'Gregory J. Ewing', 'Yusuf Sermet', 'Ibrahim Demir']","https://export.arxiv.org/abs/2007.12269","2020-06-17","physics.geo-ph cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17203,""
"COVID-19 Knowledge Graph: Accelerating Information Retrieval and   Discovery for Scientific Literature","The coronavirus disease (COVID-19) has claimed the lives of over 350,000 people and infected more than 6 million people worldwide. Several search engines have surfaced to provide researchers with additional tools to find and retrieve information from the rapidly growing corpora on COVID-19. These engines lack extraction and visualization tools necessary to retrieve and interpret complex relations inherent to scientific literature. Moreover, because these engines mainly rely upon semantic information, their ability to capture complex global relationships across documents is limited, which reduces the quality of similarity-based article recommendations for users. In this work, we present the COVID-19 Knowledge Graph (CKG), a heterogeneous graph for extracting and visualizing complex relationships between COVID-19 scientific articles. The CKG combines semantic information with document topological information for the application of similar document retrieval. The CKG is constructed using the latent schema of the data, and then enriched with biomedical entity information extracted from the unstructured text of articles using scalable AWS technologies to form relations in the graph. Finally, we propose a document similarity engine that leverages low-dimensional graph embeddings from the CKG with semantic embeddings for similar article retrieval. Analysis demonstrates the quality of relationships in the CKG and shows that it can be used to uncover meaningful information in COVID-19 scientific articles. The CKG helps power www.cord19.aws and is publicly available.","['Colby Wise', 'Vassilis N. Ioannidis', 'Miguel Romero Calvo', 'Xiang Song', 'George Price', 'Ninad Kulkarni', 'Ryan Brand', 'Parminder Bhatia', 'George Karypis']","https://export.arxiv.org/abs/2007.12731","2020-07-24","cs.IR cs.AI cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17204,""
"Leveraging the Power of Place: A Data-Driven Decision Helper to Improve   the Location Decisions of Economic Immigrants","A growing number of countries have established programs to attract immigrants who can contribute to their economy. Research suggests that an immigrant's initial arrival location plays a key role in shaping their economic success. Yet immigrants currently lack access to personalized information that would help them identify optimal destinations. Instead, they often rely on availability heuristics, which can lead to the selection of sub-optimal landing locations, lower earnings, elevated outmigration rates, and concentration in the most well-known locations. To address this issue and counteract the effects of cognitive biases and limited information, we propose a data-driven decision helper that draws on behavioral insights, administrative data, and machine learning methods to inform immigrants' location decisions. The decision helper provides personalized location recommendations that reflect immigrants' preferences as well as data-driven predictions of the locations where they maximize their expected earnings given their profile. We illustrate the potential impact of our approach using backtests conducted with administrative data that links landing data of recent economic immigrants from Canada's Express Entry system with their earnings retrieved from tax records. Simulations across various scenarios suggest that providing location recommendations to incoming economic immigrants can increase their initial earnings and lead to a mild shift away from the most populous landing destinations. Our approach can be implemented within existing institutional structures at minimal cost, and offers governments an opportunity to harness their administrative data to improve outcomes for economic immigrants.","['Jeremy Ferwerda', 'Nicholas Adams-Cohen', 'Kirk Bansak', 'Jennifer Fei', 'Duncan Lawrence', 'Jeremy M. Weinstein', 'Jens Hainmueller']","https://export.arxiv.org/abs/2007.13902","2020-07-27","cs.CY cs.LG econ.GN q-fin.EC stat.AP","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17205,""
"A Process Mining Software Comparison","www.processmining-software.com is a dedicated website for process mining software comparison and was developed to give practitioners and researchers an overview of commercial tools available on the market. Based on literature review and experimental tool testing, a set of criteria was developed in order to assess the tools' functional capabilities in an objective manner. With our publicly accessible website, we intend to increase the transparency of tool functionality. Being an academic endeavour, the non-commercial nature of the study ensures a less biased assessment as compared with reports from analyst firms.","['Daniel Viner', 'Matthias Stierle', 'Martin Matzner']","https://export.arxiv.org/abs/2007.14038","2020-07-28","cs.SE cs.HC","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17206,""
"Is there something I'm missing? Topic Modeling in eDiscovery","In legal eDiscovery, the parties are required to search through their electronically stored information to find documents that are relevant to a specific case. Negotiations over the scope of these searches are often based on a fear that something will be missed. This paper continues an argument that discovery should be based on identifying the facts of a case. If a search process is less than complete (if it has Recall less than 100%), it may still be complete in presenting all of the relevant available topics. In this study, Latent Dirichlet Allocation was used to identify 100 topics from all of the known relevant documents. The documents were then categorized to about 80% Recall (i.e., 80% of the relevant documents were found by the categorizer, designated the hit set and 20% were missed, designated the missed set). Despite the fact that less than all of the relevant documents were identified by the categorizer, the documents that were identified contained all of the topics derived from the full set of documents. This same pattern held whether the categorizer was a na\""ive Bayes categorizer trained on a random selection of documents or a Support Vector Machine trained with Continuous Active Learning (which focuses evaluation on the most-likely-to-be-relevant documents). No topics were identified in either categorizer's missed set that were not already seen in the hit set. Not only is a computer-assisted search process reasonable (as required by the Federal Rules of Civil Procedure), it is also complete when measured by topics.","['Herbert L. Roitblat']","https://export.arxiv.org/abs/2007.15731","2020-07-30","cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17207,""
"Cluster-Based Information Retrieval by using (K-means)- Hierarchical   Parallel Genetic Algorithms Approach","Cluster-based information retrieval is one of the Information retrieval(IR) tools that organize, extract features and categorize the web documents according to their similarity. Unlike traditional approaches, cluster-based IR is fast in processing large datasets of document. To improve the quality of retrieved documents, increase the efficiency of IR and reduce irrelevant documents from user search. in this paper, we proposed a (K-means) - Hierarchical Parallel Genetic Algorithms Approach (HPGA) that combines the K-means clustering algorithm with hybrid PG of multi-deme and master/slave PG algorithms. K-means uses to cluster the population to k subpopulations then take most clusters relevant to the query to manipulate in a parallel way by the two levels of genetic parallelism, thus, irrelevant documents will not be included in subpopulations, as a way to improve the quality of results. Three common datasets (NLP, CISI, and CACM) are used to compute the recall, precision, and F-measure averages. Finally, we compared the precision values of three datasets with Genetic-IR and classic-IR. The proposed approach precision improvements with IR-GA were 45% in the CACM, 27% in the CISI, and 25% in the NLP. While, by comparing with Classic-IR, (k-means)-HPGA got 47% in CACM, 28% in CISI, and 34% in NLP.","['Sarah Hussein Toman', 'Mohammed Hamzah Abed', 'Zinah Hussein Toman']","https://export.arxiv.org/abs/2008.00150","2020-07-31","cs.AI cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17208,""
"Video Question Answering on Screencast Tutorials","This paper presents a new video question answering task on screencast tutorials. We introduce a dataset including question, answer and context triples from the tutorial videos for a software. Unlike other video question answering works, all the answers in our dataset are grounded to the domain knowledge base. An one-shot recognition algorithm is designed to extract the visual cues, which helps enhance the performance of video question answering. We also propose several baseline neural network architectures based on various aspects of video contexts from the dataset. The experimental results demonstrate that our proposed models significantly improve the question answering performances by incorporating multi-modal contexts and domain knowledge.","['Wentian Zhao', 'Seokhwan Kim', 'Ning Xu', 'Hailin Jin']","https://export.arxiv.org/abs/2008.00544","2020-08-02","cs.CL cs.AI cs.CV cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17209,""
"Musical Word Embedding: Bridging the Gap between Listening Contexts and   Music","Word embedding pioneered by Mikolov et al. is a staple technique for word representations in natural language processing (NLP) research which has also found popularity in music information retrieval tasks. Depending on the type of text data for word embedding, however, vocabulary size and the degree of musical pertinence can significantly vary. In this work, we (1) train the distributed representation of words using combinations of both general text data and music-specific data and (2) evaluate the system in terms of how they associate listening contexts with musical compositions.","['Seungheon Doh', 'Jongpil Lee', 'Tae Hong Park', 'Juhan Nam']","https://export.arxiv.org/abs/2008.01190","2020-07-23","cs.IR cs.LG cs.MM stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17210,""
"Temporal Context Aggregation for Video Retrieval with Contrastive   Learning","The current research focus on Content-Based Video Retrieval requires higher-level video representation describing the long-range semantic dependencies of relevant incidents, events, etc. However, existing methods commonly process the frames of a video as individual images or short clips, making the modeling of long-range semantic dependencies difficult. In this paper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a video representation learning framework that incorporates long-range temporal information between frame-level features using the self-attention mechanism. To train it on video retrieval datasets, we propose a supervised contrastive learning method that performs automatic hard negative mining and utilizes the memory bank mechanism to increase the capacity of negative samples. Extensive experiments are conducted on multiple video retrieval tasks, such as CC_WEB_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant performance advantage (~17% mAP on FIVR-200K) over state-of-the-art methods with video-level features, and deliver competitive results with 22x faster inference time comparing with frame-level features.","['Jie Shao', 'Xin Wen', 'Bingchen Zhao', 'Xiangyang Xue']","https://export.arxiv.org/abs/2008.01334","2020-08-04","cs.CV cs.LG cs.MM","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17211,""
"Neuromorphic Computing for Content-based Image Retrieval","Neuromorphic computing mimics the neural activity of the brain through emulating spiking neural networks. In numerous machine learning tasks, neuromorphic chips are expected to provide superior solutions in terms of cost and power efficiency. Here, we explore the application of Loihi, a neuromorphic computing chip developed by Intel, for the computer vision task of image retrieval. We evaluated the functionalities and the performance metrics that are critical in context-based visual search and recommender systems using deep-learning embeddings. Our results show that the neuromorphic solution is about 3.2 times more energy-efficient compared with an Intel Core i7 CPU and 12.5 times more energy-efficient compared with Nvidia T4 GPU for inference by a lightweight convolutional neural network without batching, while maintaining the same level of matching accuracy. The study validates the longterm potential of neuromorphic computing in machine learning, as a complementary paradigm to the existing Von Neumann architectures.","['Te-Yuan Liu', 'Ata Mahjoubfar', 'Daniel Prusinski', 'Luis Stevens']","https://export.arxiv.org/abs/2008.01380","2020-08-04","cs.NE cs.CV cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17212,""
"Enforcing exact boundary and initial conditions in the deep mixed   residual method","In theory, boundary and initial conditions are important for the wellposedness of partial differential equations (PDEs). Numerically, these conditions can be enforced exactly in classical numerical methods, such as finite difference method and finite element method. Recent years have witnessed growing interests in solving PDEs by deep neural networks (DNNs), especially in the high-dimensional case. However, in the generic situation, a careful literature review shows that boundary conditions cannot be enforced exactly for DNNs, which inevitably leads to a modeling error. In this work, based on the recently developed deep mixed residual method (MIM), we demonstrate how to make DNNs satisfy boundary and initial conditions automatically in a systematic manner. As a consequence, the loss function in MIM is free of the penalty term and does not have any modeling error. Using numerous examples, including Dirichlet, Neumann, mixed, Robin, and periodic boundary conditions for elliptic equations, and initial conditions for parabolic and hyperbolic equations, we show that enforcing exact boundary and initial conditions not only provides a better approximate solution but also facilitates the training process.","['Liyao Lyu', 'Keke Wu', 'Rui Du', 'Jingrun Chen']","https://export.arxiv.org/abs/2008.01491","2020-08-04","math.NA cs.NA","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17213,""
"Multiple Code Hashing for Efficient Image Retrieval","Due to its low storage cost and fast query speed, hashing has been widely used in large-scale image retrieval tasks. Hash bucket search returns data points within a given Hamming radius to each query, which can enable search at a constant or sub-linear time cost. However, existing hashing methods cannot achieve satisfactory retrieval performance for hash bucket search in complex scenarios, since they learn only one hash code for each image. More specifically, by using one hash code to represent one image, existing methods might fail to put similar image pairs to the buckets with a small Hamming distance to the query when the semantic information of images is complex. As a result, a large number of hash buckets need to be visited for retrieving similar images, based on the learned codes. This will deteriorate the efficiency of hash bucket search. In this paper, we propose a novel hashing framework, called multiple code hashing (MCH), to improve the performance of hash bucket search. The main idea of MCH is to learn multiple hash codes for each image, with each code representing a different region of the image. Furthermore, we propose a deep reinforcement learning algorithm to learn the parameters in MCH. To the best of our knowledge, this is the first work that proposes to learn multiple hash codes for each image in image retrieval. Experiments demonstrate that MCH can achieve a significant improvement in hash bucket search, compared with existing methods that learn only one hash code for each image.","['Ming-Wei Li', 'Qing-Yuan Jiang', 'Wu-Jun Li']","https://export.arxiv.org/abs/2008.01503","2020-08-04","cs.LG cs.CV stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17214,""
"Multi-Perspective Semantic Information Retrieval in the Biomedical   Domain","Information Retrieval (IR) is the task of obtaining pieces of data (such as documents) that are relevant to a particular query or need from a large repository of information. IR is a valuable component of several downstream Natural Language Processing (NLP) tasks. Practically, IR is at the heart of many widely-used technologies like search engines. While probabilistic ranking functions like the Okapi BM25 function have been utilized in IR systems since the 1970's, modern neural approaches pose certain advantages compared to their classical counterparts. In particular, the release of BERT (Bidirectional Encoder Representations from Transformers) has had a significant impact in the NLP community by demonstrating how the use of a Masked Language Model trained on a large corpus of data can improve a variety of downstream NLP tasks, including sentence classification and passage re-ranking. IR Systems are also important in the biomedical and clinical domains. Given the increasing amount of scientific literature across biomedical domain, the ability find answers to specific clinical queries from a repository of millions of articles is a matter of practical value to medical professionals. Moreover, there are domain-specific challenges present, including handling clinical jargon and evaluating the similarity or relatedness of various medical symptoms when determining the relevance between a query and a sentence. This work presents contributions to several aspects of the Biomedical Semantic Information Retrieval domain. First, it introduces Multi-Perspective Sentence Relevance, a novel methodology of utilizing BERT-based models for contextual IR. The system is evaluated using the BioASQ Biomedical IR Challenge. Finally, practical contributions in the form of a live IR system for medics and a proposed challenge on the Living Systematic Review clinical task are provided.","['Samarth Rawal']","https://export.arxiv.org/abs/2008.01526","2020-07-17","cs.IR cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17215,""
"Exploring Relations in Untrimmed Videos for Self-Supervised Learning","Existing video self-supervised learning methods mainly rely on trimmed videos for model training. However, trimmed datasets are manually annotated from untrimmed videos. In this sense, these methods are not really self-supervised. In this paper, we propose a novel self-supervised method, referred to as Exploring Relations in Untrimmed Videos (ERUV), which can be straightforwardly applied to untrimmed videos (real unlabeled) to learn spatio-temporal features. ERUV first generates single-shot videos by shot change detection. Then a designed sampling strategy is used to model relations for video clips. The strategy is saved as our self-supervision signals. Finally, the network learns representations by predicting the category of relations between the video clips. ERUV is able to compare the differences and similarities of videos, which is also an essential procedure for action and video related tasks. We validate our learned models with action recognition and video retrieval tasks with three kinds of 3D CNNs. Experimental results show that ERUV is able to learn richer representations and it outperforms state-of-the-art self-supervised methods with significant margins.","['Dezhao Luo', 'Bo Fang', 'Yu Zhou', 'Yucan Zhou', 'Dayan Wu', 'Weiping Wang']","https://export.arxiv.org/abs/2008.02711","2020-08-06","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17216,""
"Efficient Neural Query Auto Completion","Query Auto Completion (QAC), as the starting point of information retrieval tasks, is critical to user experience. Generally it has two steps: generating completed query candidates according to query prefixes, and ranking them based on extracted features. Three major challenges are observed for a query auto completion system: (1) QAC has a strict online latency requirement. For each keystroke, results must be returned within tens of milliseconds, which poses a significant challenge in designing sophisticated language models for it. (2) For unseen queries, generated candidates are of poor quality as contextual information is not fully utilized. (3) Traditional QAC systems heavily rely on handcrafted features such as the query candidate frequency in search logs, lacking sufficient semantic understanding of the candidate.   In this paper, we propose an efficient neural QAC system with effective context modeling to overcome these challenges. On the candidate generation side, this system uses as much information as possible in unseen prefixes to generate relevant candidates, increasing the recall by a large margin. On the candidate ranking side, an unnormalized language model is proposed, which effectively captures deep semantics of queries. This approach presents better ranking performance over state-of-the-art neural ranking methods and reduces $\sim$95\% latency compared to neural language modeling methods. The empirical results on public datasets show that our model achieves a good balance between accuracy and efficiency. This system is served in LinkedIn job search with significant product impact observed.","['Sida Wang', 'Weiwei Guo', 'Huiji Gao', 'Bo Long']","https://export.arxiv.org/abs/2008.02879","2020-08-06","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17217,""
"Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to   Cold-Start Search Retrieval","Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.","['Tao Wu', 'Ellie Ka-In Chio', 'Heng-Tze Cheng', 'Yu Du', 'Steffen Rendle', 'Dima Kuzmin', 'Ritesh Agarwal', 'Li Zhang', 'John Anderson', 'Sarvjeet Singh', 'Tushar Chandra', 'Ed H. Chi', 'Wen Li', 'Ankit Kumar', 'Xiang Ma', 'Alex Soares', 'Nitin Jindal', 'Pei Cao']","https://export.arxiv.org/abs/2008.02930","2020-08-06","cs.LG cs.IR stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17218,""
"A Survey on Device Behavior Fingerprinting: Data Sources, Techniques,   Application Scenarios, and Datasets","In the current network-based computing world, where the number of interconnected devices grows exponentially, their diversity, malfunctions, and cybersecurity threats are increasing at the same rate. To guarantee the correct functioning and performance of novel environments such as Smart Cities, Industry 4.0, or crowdsensing, it is crucial to identify the capabilities of their devices (e.g., sensors, actuators) and detect potential misbehavior that may arise due to cyberattacks, system faults, or misconfigurations. With this goal in mind, a promising research field emerged focusing on creating and managing fingerprints that model the behavior of both the device actions and its components. The article at hand studies the recent growth of the device behavior fingerprinting field in terms of application scenarios, behavioral sources, and processing and evaluation techniques. First, it performs a comprehensive review of the device types, behavioral data, and processing and evaluation techniques used by the most recent and representative research works dealing with two major scenarios: device identification and device misbehavior detection. After that, each work is deeply analyzed and compared, emphasizing its characteristics, advantages, and limitations. This article also provides researchers with a review of the most relevant characteristics of existing datasets as most of the novel processing techniques are based on machine learning and deep learning. Finally, it studies the evolution of these two scenarios in recent years, providing lessons learned, current trends, and future research challenges to guide new solutions in the area.","['Pedro Miguel SÃ¡nchez SÃ¡nchez', 'Jose MarÃ­a Jorquera Valero', 'Alberto Huertas CeldrÃ¡n', 'GÃ©rÃ´me Bovet', 'Manuel Gil PÃ©rez', 'Gregorio MartÃ­nez PÃ©rez']","https://export.arxiv.org/abs/2008.03343","2020-08-07","cs.CR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17219,""
"Recent Advances and New Guidelines on Hyperspectral and Multispectral   Image Fusion","Hyperspectral image (HSI) with high spectral resolution often suffers from low spatial resolution owing to the limitations of imaging sensors. Image fusion is an effective and economical way to enhance the spatial resolution of HSI, which combines HSI with higher spatial resolution multispectral image (MSI) of the same scenario. In the past years, many HSI and MSI fusion algorithms are introduced to obtain high-resolution HSI. However, it lacks a full-scale review for the newly proposed HSI and MSI fusion approaches. To tackle this problem,this work gives a comprehensive review and new guidelines for HSI-MSI fusion. According to the characteristics of HSI-MSI fusion methods, they are categorized as four categories, including pan-sharpening based approaches, matrix factorization based approaches, tensor representation based approaches, and deep convolution neural network based approaches. We make a detailed introduction, discussions, and comparison for the fusion methods in each category. Additionally, the existing challenges and possible future directions for the HSI-MSI fusion are presented.","['Renwei Dian', 'Shutao Li', 'Bin Sun', 'Anjing Guo']","https://export.arxiv.org/abs/2008.03426","2020-08-07","eess.IV cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17220,""
"Metric Learning vs Classification for Disentangled Music Representation   Learning","Deep representation learning offers a powerful paradigm for mapping input data onto an organized embedding space and is useful for many music information retrieval tasks. Two central methods for representation learning include deep metric learning and classification, both having the same goal of learning a representation that can generalize well across tasks. Along with generalization, the emerging concept of disentangled representations is also of great interest, where multiple semantic concepts (e.g., genre, mood, instrumentation) are learned jointly but remain separable in the learned representation space. In this paper we present a single representation learning framework that elucidates the relationship between metric learning, classification, and disentanglement in a holistic manner. For this, we (1) outline past work on the relationship between metric learning and classification, (2) extend this relationship to multi-label data by exploring three different learning approaches and their disentangled versions, and (3) evaluate all models on four tasks (training time, similarity retrieval, auto-tagging, and triplet prediction). We find that classification-based models are generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Finally, we show that our proposed approach yields state-of-the-art results for music auto-tagging.","['Jongpil Lee', 'Nicholas J. Bryan', 'Justin Salamon', 'Zeyu Jin', 'Juhan Nam']","https://export.arxiv.org/abs/2008.03729","2020-08-09","cs.SD cs.IR cs.LG eess.AS","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17221,""
"An Overview on Evaluating and Predicting Scholarly Article Impact","Scholarly article impact reflects the significance of academic output recognised by academic peers, and it often plays a crucial role in assessing the scientific achievements of researchers, teams, institutions and countries. It is also used for addressing various needs in the academic and scientific arena, such as recruitment decisions, promotions, and funding allocations. This article provides a comprehensive review of recent progresses related to article impact assessment and prediction. The~review starts by sharing some insight into the article impact research and outlines current research status. Some core methods and recent progress are presented to outline how article impact metrics and prediction have evolved to consider integrating multiple networks. Key techniques, including statistical analysis, machine learning, data mining and network science, are discussed. In particular, we highlight important applications of each technique in article impact research. Subsequently, we discuss the open issues and challenges of article impact research. At the same time, this review points out some important research directions, including article impact evaluation by considering Conflict of Interest, time and location information, various distributions of scholarly entities, and rising stars.","['Xiaomei Bai', 'Hui Liu', 'Fuli Zhang', 'Zhaolong Ning', 'Xiangjie Kong', 'Ivan Lee', 'Feng Xia']","https://export.arxiv.org/abs/2008.03867","2020-08-09","cs.DL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17222,""
"Beyond Lexical: A Semantic Retrieval Framework for Textual SearchEngine","Search engine has become a fundamental component in various web and mobile applications. Retrieving relevant documents from the massive datasets is challenging for a search engine system, especially when faced with verbose or tail queries. In this paper, we explore a vector space search framework for document retrieval. Specifically, we trained a deep semantic matching model so that each query and document can be encoded as a low dimensional embedding. Our model was trained based on BERT architecture. We deployed a fast k-nearest-neighbor index service for online serving. Both offline and online metrics demonstrate that our method improved retrieval performance and search quality considerably, particularly for tail","['Kuan Fang', 'Long Zhao', 'Zhan Shen', 'RuiXing Wang', 'RiKang Zhour', 'LiWen Fan']","https://export.arxiv.org/abs/2008.03917","2020-08-10","cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17223,""
"Learning to Learn in Collective Adaptive Systems: Mining Design Patterns   for Data-driven Reasoning","Engineering collective adaptive systems (CAS) with learning capabilities is a challenging task due to their multi-dimensional and complex design space. Data-driven approaches for CAS design could introduce new insights enabling system engineers to manage the CAS complexity more cost-effectively at the design-phase. This paper introduces a systematic approach to reason about design choices and patterns of learning-based CAS. Using data from a systematic literature review, reasoning is performed with a novel application of data-driven methodologies such as clustering, multiple correspondence analysis and decision trees. The reasoning based on past experience as well as supporting novel and innovative design choices are demonstrated.","[""Mirko D'Angelo"", 'Sona Ghahremani', 'Simos Gerasimou', 'Johannes Grohmann', 'Ingrid Nunes', 'Sven Tomforde', 'Evangelos Pournaras']","https://export.arxiv.org/abs/2008.03995","2020-08-10","cs.SE","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17224,""
"LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for   Multi-Granular Propaganda Span Identification","In this paper we describe our submission for the task of Propaganda Span Identification in news articles. We introduce a BERT-BiLSTM based span-level propaganda classification model that identifies which token spans within the sentence are indicative of propaganda. The ""multi-granular"" model incorporates linguistic knowledge at various levels of text granularity, including word, sentence and document level syntactic, semantic and pragmatic affect features, which significantly improve model performance, compared to its language-agnostic variant. To facilitate better representation learning, we also collect a corpus of 10k news articles, and use it for fine-tuning the model. The final model is a majority-voting ensemble which learns different propaganda class boundaries by leveraging different subsets of incorporated knowledge and attains $4^{th}$ position on the test leaderboard. Our final model and code is released at https://github.com/sopu/PropagandaSemEval2020.","['Sopan Khosla', 'Rishabh Joshi', 'Ritam Dutt', 'Alan W Black', 'Yulia Tsvetkov']","https://export.arxiv.org/abs/2008.04820","2020-08-11","cs.CL cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17225,""
"Content-based Music Similarity with Triplet Networks","We explore the feasibility of using triplet neural networks to embed songs based on content-based music similarity. Our network is trained using triplets of songs such that two songs by the same artist are embedded closer to one another than to a third song by a different artist. We compare two models that are trained using different ways of picking this third song: at random vs. based on shared genre labels. Our experiments are conducted using songs from the Free Music Archive and use standard audio features. The initial results show that shallow Siamese networks can be used to embed music for a simple artist retrieval task.","['Joseph Cleveland', 'Derek Cheng', 'Michael Zhou', 'Thorsten Joachims', 'Douglass Turnbull']","https://export.arxiv.org/abs/2008.04938","2020-08-11","cs.LG cs.SD eess.AS","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17226,""
"Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using   Transformer Encoders","Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the problem of accurate cross-media retrieval through image-sentence matching based on word-region alignments using supervision only at the global image-sentence level. In particular, we present an approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences, i.e., image regions and words, respectively, in order to preserve the informative richness of both modalities. The proposed approach obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k. Moreover, on MS-COCO, it defeats current approaches also on the sentence retrieval task. Given our long-term interest in scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. In fact, cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way towards the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against the best eight methods in this research area. On the MS-COCO 1K test set, we obtain an improvement of 3.5% and 1.2% respectively on the image and the sentence retrieval tasks on the Recall@1 metric. The code used for the experiments is publicly available on GitHub at https://github.com/mesnico/TERAN.","['Nicola Messina', 'Giuseppe Amato', 'Andrea Esuli', 'Fabrizio Falchi', 'Claudio Gennaro', 'StÃ©phane Marchand-Maillet']","https://export.arxiv.org/abs/2008.05231","2020-08-12","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17227,""
"Synergy between Machine/Deep Learning and Software Engineering: How Far   Are We?","Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Machine Learning (ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the quality (especially the applicability and generalizability) of ML/DL-related SE studies, and to stimulate and enhance future collaborations between SE/AI researchers and industry practitioners, we conducted a 10-year Systematic Literature Review (SLR) on 906 ML/DL-related SE papers published between 2009 and 2018. Our trend analysis demonstrated the mutual impacts that ML/DL and SE have had on each other. At the same time, however, we also observed a paucity of replicable and reproducible ML/DL-related SE studies and identified five factors that influence their replicability and reproducibility. To improve the applicability and generalizability of research results, we analyzed what ingredients in a study would facilitate an understanding of why a ML/DL technique was selected for a specific SE problem. In addition, we identified the unique trends of impacts of DL models on SE tasks, as well as five unique challenges that needed to be met in order to better leverage DL to improve the productivity of SE tasks. Finally, we outlined a road-map that we believe can facilitate the transfer of ML/DL-based SE research results into real-world industry practices.","['Simin Wang', 'Liguo Huang', 'Jidong Ge', 'Tengfei Zhang', 'Haitao Feng', 'Ming Li', 'He Zhang', 'Vincent Ng']","https://export.arxiv.org/abs/2008.05515","2020-08-12","cs.SE cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17228,""
"Self-supervised Video Representation Learning by Pace Prediction","This paper addresses the problem of self-supervised video representation learning from a new perspective -- by video pace prediction. It stems from the observation that human visual system is sensitive to video pace, e.g., slow motion, a widely used technique in film making. Specifically, given a video played in natural pace, we randomly sample training clips in different paces and ask a neural network to identify the pace for each video clip. The assumption here is that the network can only succeed in such a pace reasoning task when it understands the underlying video content and learns representative spatio-temporal features. In addition, we further introduce contrastive learning to push the model towards discriminating different paces by maximizing the agreement on similar video content. To validate the effectiveness of the proposed method, we conduct extensive experiments on action recognition and video retrieval tasks with several alternative network architectures. Experimental evaluations show that our approach achieves state-of-the-art performance for self-supervised video representation learning across different network architectures and different benchmarks. The code and pre-trained models are available at https://github.com/laura-wang/video-pace.","['Jiangliu Wang', 'Jianbo Jiao', 'Yun-Hui Liu']","https://export.arxiv.org/abs/2008.05861","2020-08-13","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17229,""
"Semi-supervised learning using teacher-student models for vocal melody   extraction","The lack of labeled data is a major obstacle in many music information retrieval tasks such as melody extraction, where labeling is extremely laborious or costly. Semi-supervised learning (SSL) provides a solution to alleviate the issue by leveraging a large amount of unlabeled data. In this paper, we propose an SSL method using teacher-student models for vocal melody extraction. The teacher model is pre-trained with labeled data and guides the student model to make identical predictions given unlabeled input in a self-training setting. We examine three setups of teacher-student models with different data augmentation schemes and loss functions. Also, considering the scarcity of labeled data in the test phase, we artificially generate large-scale testing data with pitch labels from unlabeled data using an analysis-synthesis method. The results show that the SSL method significantly increases the performance against supervised learning only and the improvement depends on the teacher-student models, the size of unlabeled data, the number of self-training iterations, and other training details. We also find that it is essential to ensure that the unlabeled audio has vocal parts. Finally, we show that the proposed SSL method enables a baseline convolutional recurrent neural network model to achieve performance comparable to state-of-the-arts.","['Sangeun Kum', 'Jing-Hua Lin', 'Li Su', 'Juhan Nam']","https://export.arxiv.org/abs/2008.06358","2020-08-14","eess.AS cs.SD","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17230,""
"Machine learning for COVID-19 detection and prognostication using chest   radiographs and CT scans: a systematic methodological review","Background: Machine learning methods offer great potential for fast and accurate detection and prognostication of COVID-19 from standard-of-care chest radiographs (CXR) and computed tomography (CT) images. In this systematic review we critically evaluate the machine learning methodologies employed in the rapidly growing literature.   Methods: In this systematic review we reviewed EMBASE via OVID, MEDLINE via PubMed, bioRxiv, medRxiv and arXiv for published papers and preprints uploaded from Jan 1, 2020 to June 24, 2020. Studies which consider machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images were included. A methodology quality review of each paper was performed against established benchmarks to ensure the review focusses only on high-quality reproducible papers. This study is registered with PROSPERO [CRD42020188887].   Interpretation: Our review finds that none of the developed models discussed are of potential clinical use due to methodological flaws and underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. Typically, we find that the documentation of a model's development is not sufficient to make the results reproducible and therefore of 168 candidate papers only 29 are deemed to be reproducible and subsequently considered in this review. We therefore encourage authors to use established machine learning checklists to ensure sufficient documentation is made available, and to follow the PROBAST (prediction model risk of bias assessment tool) framework to determine the underlying biases in their model development process and to mitigate these where possible. This is key to safe clinical implementation which is urgently needed.","['Michael Roberts', 'Derek Driggs', 'Matthew Thorpe', 'Julian Gilbey', 'Michael Yeung', 'Stephan Ursprung', 'Angelica I. Aviles-Rivero', 'Christian Etmann', 'Cathal McCague', 'Lucian Beer', 'Jonathan R. Weir-McCall', 'Zhongzhao Teng', 'James H. F. Rudd', 'Evis Sala', 'Carola-Bibiane SchÃ¶nlieb']","https://export.arxiv.org/abs/2008.06388","2020-08-14","cs.LG cs.CV eess.IV stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17231,""
"A Survey of Machine Learning Methods for Detecting False Data Injection   Attacks in Power Systems","Over the last decade, the number of cyberattacks targeting power systems and causing physical and economic damages has increased rapidly. Among them, False Data Injection Attacks (FDIAs) is a class of cyberattacks against power grid monitoring systems. Adversaries can successfully perform FDIAs in order to manipulate the power system State Estimation (SE) by compromising sensors or modifying system data. SE is an essential process performed by the Energy Management System (EMS) towards estimating unknown state variables based on system redundant measurements and network topology. SE routines include Bad Data Detection (BDD) algorithms to eliminate errors from the acquired measurements, e.g., in case of sensor failures. FDIAs can bypass BDD modules to inject malicious data vectors into a subset of measurements without being detected, and thus manipulate the results of the SE process. In order to overcome the limitations of traditional residual-based BDD approaches, data-driven solutions based on machine learning algorithms have been widely adopted for detecting malicious manipulation of sensor data due to their fast execution times and accurate results. This paper provides a comprehensive review of the most up-to-date machine learning methods for detecting FDIAs against power system SE algorithms.","['Ali Sayghe', 'Yaodan Hu', 'Ioannis Zografopoulos', 'XiaoRui Liu', 'Raj Gautam Dutta', 'Yier Jin', 'Charalambos Konstantinou']","https://export.arxiv.org/abs/2008.06926","2020-08-16","eess.SY cs.CR cs.SY","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17232,""
"COVID-SEE: Scientific Evidence Explorer for COVID-19 Related Research","We present COVID-SEE, a system for medical literature discovery based on the concept of information exploration, which builds on several distinct text analysis and natural language processing methods to structure and organise information in publications, and augments search by providing a visual overview supporting exploration of a collection to identify key articles of interest. We developed this system over COVID-19 literature to help medical professionals and researchers explore the literature evidence, and improve findability of relevant information. COVID-SEE is available at http://covid-see.com.","['Karin Verspoor', 'Simon Å uster', 'Yulia Otmakhova', 'Shevon Mendis', 'Zenan Zhai', 'Biaoyan Fang', 'Jey Han Lau', 'Timothy Baldwin', 'Antonio Jimeno Yepes', 'David Martinez']","https://export.arxiv.org/abs/2008.07880","2020-08-18","cs.CL cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17233,""
"Assigning function to protein-protein interactions: a weakly supervised   BioBERT based approach using PubMed abstracts","Motivation: Protein-protein interactions (PPI) are critical to the function of proteins in both normal and diseased cells, and many critical protein functions are mediated by interactions.Knowledge of the nature of these interactions is important for the construction of networks to analyse biological data. However, only a small percentage of PPIs captured in protein interaction databases have annotations of function available, e.g. only 4% of PPI are functionally annotated in the IntAct database. Here, we aim to label the function type of PPIs by extracting relationships described in PubMed abstracts.   Method: We create a weakly supervised dataset from the IntAct PPI database containing interacting protein pairs with annotated function and associated abstracts from the PubMed database. We apply a state-of-the-art deep learning technique for biomedical natural language processing tasks, BioBERT, to build a model - dubbed PPI-BioBERT - for identifying the function of PPIs. In order to extract high quality PPI functions at large scale, we use an ensemble of PPI-BioBERT models to improve uncertainty estimation and apply an interaction type-specific threshold to counteract the effects of variations in the number of training samples per interaction type.   Results: We scan 18 million PubMed abstracts to automatically identify 3253 new typed PPIs, including phosphorylation and acetylation interactions, with an overall precision of 46% (87% for acetylation) based on a human-reviewed sample. This work demonstrates that analysis of biomedical abstracts for PPI function extraction is a feasible approach to substantially increasing the number of interactions annotated with function captured in online databases.","['Aparna Elangovan', 'Melissa Davis', 'Karin Verspoor']","https://export.arxiv.org/abs/2008.08727","2020-08-19","cs.CL cs.LG q-bio.GN","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17234,""
"Document Visual Question Answering Challenge 2020","This paper presents results of Document Visual Question Answering Challenge organized as part of ""Text and Documents in the Deep Learning Era"" workshop, in CVPR 2020. The challenge introduces a new problem - Visual Question Answering on document images. The challenge comprised two tasks. The first task concerns with asking questions on a single document image. On the other hand, the second task is set as a retrieval task where the question is posed over a collection of images. For the task 1 a new dataset is introduced comprising 50,000 questions-answer(s) pairs defined over 12,767 document images. For task 2 another dataset has been created comprising 20 questions over 14,362 document images which share the same document template.","['Minesh Mathew', 'Ruben Tito', 'Dimosthenis Karatzas', 'R. Manmatha', 'C. V. Jawahar']","https://export.arxiv.org/abs/2008.08899","2020-08-20","cs.CV cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17235,""
"Constructing a Knowledge Graph from Unstructured Documents without   External Alignment","Knowledge graphs (KGs) are relevant to many NLP tasks, but building a reliable domain-specific KG is time-consuming and expensive. A number of methods for constructing KGs with minimized human intervention have been proposed, but still require a process to align into the human-annotated knowledge base. To overcome this issue, we propose a novel method to automatically construct a KG from unstructured documents that does not require external alignment and explore its use to extract desired information. To summarize our approach, we first extract knowledge tuples in their surface form from unstructured documents, encode them using a pre-trained language model, and link the surface-entities via the encoding to form the graph structure. We perform experiments with benchmark datasets such as WikiMovies and MetaQA. The experimental results show that our method can successfully create and search a KG with 18K documents and achieve 69.7% hits@10 (close to an oracle model) on a query retrieval task.","['Seunghak Yu', 'Tianxing He', 'James Glass']","https://export.arxiv.org/abs/2008.08995","2020-08-20","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17236,""
"A summary of the prevalence of Genetic Algorithms in Bioinformatics from   2015 onwards","In recent years, machine learning has seen an increasing presencein a large variety of fields, especially in health care and bioinformatics.More specifically, the field where machine learning algorithms have found most applications is Genetic Algorithms.The objective of this paper is to conduct a survey of articles published from 2015 onwards that deal with Genetic Algorithms(GA) and how they are used in bioinformatics.To achieve the objective, a scoping review was conducted that utilized Google Scholar alongside Publish or Perish and the Scimago Journal & CountryRank to search for respectable sources. Upon analyzing 31 articles from the field of bioinformatics, it became apparent that genetic algorithms rarely form a full application, instead they rely on other vital algorithms such as support vector machines.Indeed, support vector machines were the most prevalent algorithms used alongside genetic algorithms; however, while the usage of such algorithms contributes to the heavy focus on accuracy by GA programs, it often sidelines computation times in the process. In fact, most applications employing GAs for classification and feature selectionare nearing or at 100% success rate, and the focus of future GA development should be directed elsewhere. Population-based searches, like GA, are often combined with other machine learning algorithms. In this scoping review, genetic algorithms combined with Support Vector Machines were found to perform best. The performance metric that was evaluated most often was accuracy. Measuring the accuracy avoids measuring the main weakness of GAs, which is computational time. The future of genetic algorithms could be open-ended evolutionary algorithms, which attempt to increase complexity and find diverse solutions, rather than optimize a fitness function and converge to a single best solution from the initial population of solutions.","['Mekaal Swerhun', 'Jasmine Foley', 'Brandon Massop', 'Vijay Mago']","https://export.arxiv.org/abs/2008.09017","2020-08-20","cs.NE cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17237,""
"A Survey of Visual Analytics Techniques for Machine Learning","Visual analytics for machine learning has recently evolved as one of the most exciting areas in the field of visualization. To better identify which research topics are promising and to learn how to apply relevant techniques in visual analytics, we systematically review 259 papers published in the last ten years together with representative works before 2010. We build a taxonomy, which includes three first-level categories: techniques before model building, techniques during model building, and techniques after model building. Each category is further characterized by representative analysis tasks, and each task is exemplified by a set of recent influential works. We also discuss and highlight research challenges and promising potential future research opportunities useful for visual analytics researchers.","['Jun Yuan', 'Changjian Chen', 'Weikai Yang', 'Mengchen Liu', 'Jiazhi Xia', 'Shixia Liu']","https://export.arxiv.org/abs/2008.09632","2020-08-21","cs.HC","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17238,""
"A systematic literature review on machine learning applications for   consumer sentiment analysis using online reviews","Consumer sentiment analysis is a recent fad for social media related applications such as healthcare, crime, finance, travel, and academics. Disentangling consumer perception to gain insight into the desired objective and reviews is significant. With the advancement of technology, a massive amount of social web-data increasing in terms of volume, subjectivity, and heterogeneity, becomes challenging to process it manually. Machine learning techniques have been utilized to handle this difficulty in real-life applications. This paper presents the study to find out the usefulness, scope, and applicability of this alliance of Machine Learning techniques for consumer sentiment analysis on online reviews in the domain of hospitality and tourism. We have shown a systematic literature review to compare, analyze, explore, and understand the attempts and direction in a proper way to find research gaps to illustrating the future scope of this pairing. This work is contributing to the extant literature in two ways; firstly, the primary objective is to read and analyze the use of machine learning techniques for consumer sentiment analysis on online reviews in the domain of hospitality and tourism. Secondly, in this work, we presented a systematic approach to identify, collect observational evidence, results from the analysis, and assimilate observations of all related high-quality research to address particular research queries referring to the described research area.","['Praphula Kumar Jain', 'Rajendra Pamula']","https://export.arxiv.org/abs/2008.10282","2020-08-24","cs.HC","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17239,""
"Buy Me That Look: An Approach for Recommending Similar Fashion Products","The recent proliferation of numerous fashion e-commerce platforms has led to a surge in online shopping of fashion products. Fashion being the dominant aspect in online retail sales, demands for efficient and effective fashion products recommendation systems that could boost revenue, improve customer experience and engagement. In this paper, we focus on the problem of similar fashion item recommendation for multiple fashion items. Given a Product Display Page for a fashion item in an online e-commerce platform, we identify the images with a full-shot look, i.e., the one with a full human model wearing the fashion item. While the majority of existing works in this domain focus on retrieving similar products corresponding to a single item present in a query, we focus on the retrieval of multiple fashion items at once. This is an important problem because while a user might have searched for a particular primary article type (e.g., men's shorts), the human model in the full-shot look image would usually be wearing secondary fashion items as well (e.g., t-shirts, shoes etc). Upon looking at the full-shot look image in the PDP, the user might also be interested in viewing similar items for the secondary article types. To address this need, we use human keypoint detection to first identify the fullshot images, from which we subsequently select the front facing ones. An article detection and localisation module pretrained on a large-dataset is then used to identify different articles in the image. The detected articles and the catalog database images are then represented in a common embedding space, for the purpose of similarity based retrieval. We make use of a triplet-based neural network to obtain the embeddings. Our embedding network by virtue of an active-learning component achieves further improvements in the retrieval performance.","['Abhinav Ravi', 'Sandeep Repakula', 'Ujjal Kr Dutta', 'Maulik Parmar']","https://export.arxiv.org/abs/2008.11638","2020-08-26","cs.CV cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17240,""
"Automatic Speech Summarisation: A Scoping Review","Speech summarisation techniques take human speech as input and then output an abridged version as text or speech. Speech summarisation has applications in many domains from information technology to health care, for example improving speech archives or reducing clinical documentation burden. This scoping review maps the speech summarisation literature, with no restrictions on time frame, language summarised, research method, or paper type. We reviewed a total of 110 papers out of a set of 153 found through a literature search and extracted speech features used, methods, scope, and training corpora. Most studies employ one of four speech summarisation architectures: (1) Sentence extraction and compaction; (2) Feature extraction and classification or rank-based sentence selection; (3) Sentence compression and compression summarisation; and (4) Language modelling. We also discuss the strengths and weaknesses of these different methods and speech features. Overall, supervised methods (e.g. Hidden Markov support vector machines, Ranking support vector machines, Conditional random fields) performed better than unsupervised methods. As supervised methods require manually annotated training data which can be costly, there was more interest in unsupervised methods. Recent research into unsupervised methods focusses on extending language modelling, for example by combining Uni-gram modelling with deep neural networks. Protocol registration: The protocol for this scoping review is registered at https://osf.io.","['Dana Rezazadegan', 'Shlomo Berkovsky', 'Juan C. Quiroz', 'A. Baki Kocaballi', 'Ying Wang', 'Liliana Laranjo', 'Enrico Coiera']","https://export.arxiv.org/abs/2008.11897","2020-08-26","cs.CL cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17241,""
"Repurposing TREC-COVID Annotations to Answer the Key Questions of   CORD-19","The novel coronavirus disease 2019 (COVID-19) began in Wuhan, China in late 2019 and to date has infected over 14M people worldwide, resulting in over 750,000 deaths. On March 10, 2020 the World Health Organization (WHO) declared the outbreak a global pandemic. Many academics and researchers, not restricted to the medical domain, began publishing papers describing new discoveries. However, with the large influx of publications, it was hard for these individuals to sift through the large amount of data and make sense of the findings. The White House and a group of industry research labs, lead by the Allen Institute for AI, aggregated over 200,000 journal articles related to a variety of coronaviruses and tasked the community with answering key questions related to the corpus, releasing the dataset as CORD-19. The information retrieval (IR) community repurposed the journal articles within CORD-19 to more closely resemble a classic TREC-style competition, dubbed TREC-COVID, with human annotators providing relevancy judgements at the end of each round of competition. Seeing the related endeavors, we set out to repurpose the relevancy annotations for TREC-COVID tasks to identify journal articles in CORD-19 which are relevant to the key questions posed by CORD-19. A BioBERT model trained on this repurposed dataset prescribes relevancy annotations for CORD-19 tasks that have an overall agreement of 0.4430 with majority human annotations in terms of Cohen's kappa. We present the methodology used to construct the new dataset and describe the decision process used throughout.","['Connor T. Heaton', 'Prasenjit Mitra']","https://export.arxiv.org/abs/2008.12353","2020-08-27","cs.CL cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17242,""
"Dynamical Variational Autoencoders: A Comprehensive Review","The Variational Autoencoder (VAE) is a powerful deep generative model that is now extensively used to represent high-dimensional complex data via a low-dimensional latent space that is learned in an unsupervised manner. In the original VAE model, input data vectors are processed independently. In the recent years, a series of papers have presented different extensions of the VAE to sequential data, that not only model the latent space, but also model the temporal dependencies within a sequence of data vectors and/or corresponding latent vectors, relying on recurrent neural networks or state space models. In this paper we perform an extensive literature review of these models. Importantly, we introduce and discuss a general class of models called Dynamical Variational Autoencoders (DVAEs) that encompass a large subset of these temporal VAE extensions. Then we present in details seven different instances of DVAE that were recently proposed in the literature, with an effort to homogenize the notations and presentation lines, as well as to relate those models with existing classical temporal models (that are also presented for the sake of completeness). We reimplemented those seven DVAE models and we present the results of an experimental benchmark that we conducted on the speech analysis-resynthesis task (the PyTorch code will be made publicly available). An extensive discussion is presented at the end of the paper, aiming to comment on important issues concerning the DVAE class of models and to describe future research guidelines.","['Laurent Girin', 'Simon Leglaive', 'Xiaoyu Bie', 'Julien Diard', 'Thomas Hueber', 'Xavier Alameda-Pineda']","https://export.arxiv.org/abs/2008.12595","2020-08-28","cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17243,""
"Characterizing Automated Data Insights","Many researchers have explored tools that aim to recommend data insights to users. These tools automatically communicate a rich diversity of data insights and offer such insights for many different purposes. However, there is a lack of structured understanding concerning what researchers of these tools mean by ""insight"" and what tasks in the analysis workflow these tools aim to support. We conducted a systematic review of existing systems that seek to recommend data insights. Grounded in the review, we propose 12 types of automated insights and four purposes of automating insights. We further discuss the design opportunities emerged from our analysis.","['Po-Ming Law', 'Alex Endert', 'John Stasko']","https://export.arxiv.org/abs/2008.13060","2020-08-29","cs.HC","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17244,""
"A Survey of Deep Active Learning","Active learning (AL) attempts to maximize the performance gain of the model by marking the fewest samples. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize massive parameters, so that the model learns how to extract high-quality features. In recent years, due to the rapid development of internet technology, we are in an era of information torrents and we have massive amounts of data. In this way, DL has aroused strong interest of researchers and has been rapidly developed. Compared with DL, researchers have relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples. Therefore, early AL is difficult to reflect the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to the publicity of the large number of existing annotation datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, which is not allowed in some fields that require high expertise, especially in the fields of speech recognition, information extraction, medical images, etc. Therefore, AL has gradually received due attention. A natural idea is whether AL can be used to reduce the cost of sample annotations, while retaining the powerful learning capabilities of DL. Therefore, deep active learning (DAL) has emerged. Although the related research has been quite abundant, it lacks a comprehensive survey of DAL. This article is to fill this gap, we provide a formal classification method for the existing work, and a comprehensive and systematic overview. In addition, we also analyzed and summarized the development of DAL from the perspective of application. Finally, we discussed the confusion and problems in DAL, and gave some possible development directions for DAL.","['Pengzhen Ren', 'Yun Xiao', 'Xiaojun Chang', 'Po-Yao Huang', 'Zhihui Li', 'Xiaojiang Chen', 'Xin Wang']","https://export.arxiv.org/abs/2009.00236","2020-08-30","cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17245,""
"Uncovering Hidden Challenges in Query-Based Video Moment Retrieval","The query-based moment retrieval is a problem of localising a specific clip from an untrimmed video according a query sentence. This is a challenging task that requires interpretation of both the natural language query and the video content. Like in many other areas in computer vision and machine learning, the progress in query-based moment retrieval is heavily driven by the benchmark datasets and, therefore, their quality has significant impact on the field. In this paper, we present a series of experiments assessing how well the benchmark results reflect the true progress in solving the moment retrieval task. Our results indicate substantial biases in the popular datasets and unexpected behaviour of the state-of-the-art models. Moreover, we present new sanity check experiments and approaches for visualising the results. Finally, we suggest possible directions to improve the temporal sentence grounding in the future. Our code for this paper is available at https://mayu-ot.github.io/hidden-challenges-MR .","['Mayu Otani', 'Yuta Nakashima', 'Esa Rahtu', 'Janne HeikkilÃ¤']","https://export.arxiv.org/abs/2009.00325","2020-09-01","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17246,""
"Identifying Documents In-Scope of a Collection from Web Archives","Web archive data usually contains high-quality documents that are very useful for creating specialized collections of documents, e.g., scientific digital libraries and repositories of technical reports. In doing so, there is a substantial need for automatic approaches that can distinguish the documents of interest for a collection out of the huge number of documents collected by web archiving institutions. In this paper, we explore different learning models and feature representations to determine the best performing ones for identifying the documents of interest from the web archived data. Specifically, we study both machine learning and deep learning models and ""bag of words"" (BoW) features extracted from the entire document or from specific portions of the document, as well as structural features that capture the structure of documents. We focus our evaluation on three datasets that we created from three different Web archives. Our experimental results show that the BoW classifiers that focus only on specific portions of the documents (rather than the full text) outperform all compared methods on all three datasets.","['Krutarth Patel', 'Cornelia Caragea', 'Mark Phillips', 'Nathaniel Fox']","https://export.arxiv.org/abs/2009.00611","2020-09-02","cs.IR cs.CL cs.DL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17247,""
"Clustering in VANET: Algorithms and Challenges","Clustering is an important concept in vehicular ad hoc network (VANET) where several vehicles join to form a group based on common features. Mobility-based clustering strategies are the most common in VANET clustering; however, machine learning and fuzzy logic algorithms are also the basis of many VANET clustering algorithms. Some VANET clustering algorithms integrate machine learning and fuzzy logic algorithms to make the cluster more stable and efficient. Network mobility (NEMO) and multi-hop-based strategies are also used for VANET clustering. Mobility and some other clustering strategies are presented in the existing literature reviews; however, extensive study of intelligence-based, mobility-based, and multi-hop-based strategies still missing in the VANET clustering reviews. In this paper, we presented a classification of intelligence-based clustering algorithms, mobility-based algorithms, and multi-hop-based algorithms with an analysis on the mobility metrics, evaluation criteria, challenges, and future directions of machine learning, fuzzy logic, mobility, NEMO, and multi-hop clustering algorithms.","['Mohammad Mukhtaruzzaman', 'Mohammed Atiquzzaman']","https://export.arxiv.org/abs/2009.01964","2020-09-03","cs.NI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17248,""
"Brown University at TREC Deep Learning 2019","This paper describes Brown University's submission to the TREC 2019 Deep Learning track. We followed a 2-phase method for producing a ranking of passages for a given input query: In the the first phase, the user's query is expanded by appending 3 queries generated by a transformer model which was trained to rephrase an input query into semantically similar queries. The expanded query can exhibit greater similarity in surface form and vocabulary overlap with the passages of interest and can therefore serve as enriched input to any downstream information retrieval method. In the second phase, we use a BERT-based model pre-trained for language modeling but fine-tuned for query - document relevance prediction to compute relevance scores for a set of 1000 candidate passages per query and subsequently obtain a ranking of passages by sorting them based on the predicted relevance scores. According to the results published in the official Overview of the TREC Deep Learning Track 2019, our team ranked 3rd in the passage retrieval task (including full ranking and re-ranking), and 2nd when considering only re-ranking submissions.","['George Zerveas', 'Ruochen Zhang', 'Leila Kim', 'Carsten Eickhoff']","https://export.arxiv.org/abs/2009.04016","2020-09-08","cs.IR cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17249,""
"Plant Diseases recognition on images using Convolutional Neural   Networks: A Systematic Review","Plant diseases are considered one of the main factors influencing food production and minimize losses in production, and it is essential that crop diseases have fast detection and recognition. The recent expansion of deep learning methods has found its application in plant disease detection, offering a robust tool with highly accurate results. In this context, this work presents a systematic review of the literature that aims to identify the state of the art of the use of convolutional neural networks(CNN) in the process of identification and classification of plant diseases, delimiting trends, and indicating gaps. In this sense, we present 121 papers selected in the last ten years with different approaches to treat aspects related to disease detection, characteristics of the data set, the crops and pathogens investigated. From the results of the systematic review, it is possible to understand the innovative trends regarding the use of CNNs in the identification of plant diseases and to identify the gaps that need the attention of the research community.","['Andre S. Abade', 'Paulo Afonso Ferreira', 'Flavio de Barros Vidal']","https://export.arxiv.org/abs/2009.04365","2020-09-09","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17250,""
"Patient Cohort Retrieval using Transformer Language Models","We apply deep learning-based language models to the task of patient cohort retrieval (CR) with the aim to assess their efficacy. The task of CR requires the extraction of relevant documents from the electronic health records (EHRs) on the basis of a given query. Given the recent advancements in the field of document retrieval, we map the task of CR to a document retrieval task and apply various deep neural models implemented for the general domain tasks. In this paper, we propose a framework for retrieving patient cohorts using neural language models without the need of explicit feature engineering and domain expertise. We find that a majority of our models outperform the BM25 baseline method on various evaluation metrics.","['Sarvesh Soni', 'Kirk Roberts']","https://export.arxiv.org/abs/2009.05121","2020-09-10","cs.IR cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17251,""
"CONDA-PM -- A Systematic Review and Framework for Concept Drift Analysis   in Process Mining","Business processes evolve over time to adapt to changing business environments. This requires continuous monitoring of business processes to gain insights into whether they conform to the intended design or deviate from it. The situation when a business process changes while being analysed is denoted as Concept Drift. Its analysis is concerned with studying how a business process changes, in terms of detecting and localising changes and studying the effects of the latter. Concept drift analysis is crucial to enable early detection and management of changes, that is, whether to promote a change to become part of an improved process, or to reject the change and make decisions to mitigate its effects. Despite its importance, there exists no comprehensive framework for analysing concept drift types, affected process perspectives, and granularity levels of a business process. This article proposes the CONcept Drift Analysis in Process Mining (CONDA-PM) framework describing phases and requirements of a concept drift analysis approach. CONDA-PM was derived from a Systematic Literature Review (SLR) of current approaches analysing concept drift. We apply the CONDA-PM framework on current approaches to concept drift analysis and evaluate their maturity. Applying CONDA-PM framework highlights areas where research is needed to complement existing efforts.","['Ghada Elkhawaga', 'Mervat Abuelkheir', 'Sherif I. Barakat', 'Alaa M. Riad', 'Manfred Reichert']","https://export.arxiv.org/abs/2009.05438","2020-09-08","cs.LG cs.AI cs.SE stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17252,""
"Fine-tuning Pre-trained Contextual Embeddings for Citation Content   Analysis in Scholarly Publication","Citation function and citation sentiment are two essential aspects of citation content analysis (CCA), which are useful for influence analysis, the recommendation of scientific publications. However, existing studies are mostly traditional machine learning methods, although deep learning techniques have also been explored, the improvement of the performance seems not significant due to insufficient training data, which brings difficulties to applications. In this paper, we propose to fine-tune pre-trained contextual embeddings ULMFiT, BERT, and XLNet for the task. Experiments on three public datasets show that our strategy outperforms all the baselines in terms of the F1 score. For citation function identification, the XLNet model achieves 87.2%, 86.90%, and 81.6% on DFKI, UMICH, and TKDE2019 datasets respectively, while it achieves 91.72% and 91.56% on DFKI and UMICH in term of citation sentiment identification. Our method can be used to enhance the influence analysis of scholars and scholarly publications.","['Haihua Chen', 'Huyen Nguyen']","https://export.arxiv.org/abs/2009.05836","2020-09-12","cs.CL cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17253,""
"Should We Trust (X)AI? Design Dimensions for Structured Experimental   Evaluations","This paper systematically derives design dimensions for the structured evaluation of explainable artificial intelligence (XAI) approaches. These dimensions enable a descriptive characterization, facilitating comparisons between different study designs. They further structure the design space of XAI, converging towards a precise terminology required for a rigorous study of XAI. Our literature review differentiates between comparative studies and application papers, revealing methodological differences between the fields of machine learning, human-computer interaction, and visual analytics. Generally, each of these disciplines targets specific parts of the XAI process. Bridging the resulting gaps enables a holistic evaluation of XAI in real-world scenarios, as proposed by our conceptual model characterizing bias sources and trust-building. Furthermore, we identify and discuss the potential for future work based on observed research gaps that should lead to better coverage of the proposed model.","['Fabian Sperrle', 'Mennatallah El-Assady', 'Grace Guo', 'Duen Horng Chau', 'Alex Endert', 'Daniel Keim']","https://export.arxiv.org/abs/2009.06433","2020-09-14","cs.HC cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17254,""
"A Systematic Literature Review on the Use of Deep Learning in Software   Engineering Research","An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross-cutting area of work, from its modern inception to the present, this paper presents a systematic literature review of research at the intersection of SE & DL. The review canvases work appearing in the most prominent SE and DL conferences and journals and spans 84 papers across 22 unique SE tasks. We center our analysis around the components of learning, a set of principles that govern the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research, and likely areas of fertile exploration for the future.","['Cody Watson', 'Nathan Cooper', 'David Nader Palacio', 'Kevin Moran', 'Denys Poshyvanyk']","https://export.arxiv.org/abs/2009.06520","2020-09-14","cs.SE cs.AI cs.LG cs.NE","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17255,""
"Image Based Artificial Intelligence in Wound Assessment: A Systematic   Review","Efficient and effective assessment of acute and chronic wounds can help wound care teams in clinical practice to greatly improve wound diagnosis, optimize treatment plans, ease the workload and achieve health related quality of life to the patient population. While artificial intelligence (AI) has found wide applications in health-related sciences and technology, AI-based systems remain to be developed clinically and computationally for high-quality wound care. To this end, we have carried out a systematic review of intelligent image-based data analysis and system developments for wound assessment. Specifically, we provide an extensive review of research methods on wound measurement (segmentation) and wound diagnosis (classification). We also reviewed recent work on wound assessment systems (including hardware, software, and mobile apps). More than 250 articles were retrieved from various publication databases and online resources, and 115 of them were carefully selected to cover the breadth and depth of most recent and relevant work to convey the current review to its fulfillment.","['D. M. Anisuzzaman', 'Chuanbo Wang', 'Behrouz Rostami', 'Sandeep Gopalakrishnan', 'Jeffrey Niezgoda', 'Zeyun Yu']","https://export.arxiv.org/abs/2009.07141","2020-09-15","cs.CY cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17256,""
"BERT-QE: Contextualized Query Expansion for Document Re-ranking","Query expansion aims to mitigate the mismatch between the language used in a query and in a document. Query expansion methods can suffer from introducing non-relevant information when expanding the query, however. To bridge this gap, inspired by recent advances in applying contextualized models like BERT to the document retrieval task, this paper proposes a novel query expansion model that leverages the strength of the BERT model to better select relevant information for expansion. In evaluations on the standard TREC Robust04 and GOV2 test collections, the proposed BERT-QE model significantly outperforms BERT-Large models commonly used for document retrieval.","['Zhi Zheng', 'Kai Hui', 'Ben He', 'Xianpei Han', 'Le Sun', 'Andrew Yates']","https://export.arxiv.org/abs/2009.07258","2020-09-15","cs.IR cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17257,""
"DDRQA: Dynamic Document Reranking for Open-domain Multi-hop Question   Answering","Open-domain multi-hop question answering (QA) requires to retrieve multiple supporting documents, some of which have little lexical overlap with the question and can only be located by iterative document retrieval. However, multi-step document retrieval often incurs more relevant but non-supporting documents, which dampens the downstream noise-sensitive reader module for answer extraction. To address this challenge, we propose Dynamic Document Reranking (DDR) to iteratively retrieve, rerank and filter documents, and adaptively determine when to stop the retrieval process. DDR employs an entity-linked document graph for multi-document interaction, which boosts up the retrieval performance. Experiments on HotpotQA full wiki setting show that our method achieves more than 7 points higher reranking performance over the previous best retrieval model, and also achieves state-of-the-art question answering performance on the official leaderboard.","['Yuyu Zhang', 'Ping Nie', 'Arun Ramamurthy', 'Le Song']","https://export.arxiv.org/abs/2009.07465","2020-09-16","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17258,""
"Simplified TinyBERT: Knowledge Distillation for Document Retrieval","Despite the effectiveness of utilizing BERT for document ranking, the computational cost of such approaches is non-negligible when compared to other retrieval methods. To this end, this paper first empirically investigates the applications of knowledge distillation models on document ranking task. In addition, on top of the recent TinyBERT, two simplifications are proposed. Evaluation on MS MARCO document re-ranking task confirms the effectiveness of the proposed simplifications.","['Xuanang Chen', 'Ben He', 'Kai Hui', 'Le Sun', 'Yingfei Sun']","https://export.arxiv.org/abs/2009.07531","2020-09-16","cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17259,""
"Building power consumption datasets: Survey, taxonomy and future   directions","In the last decade, extended efforts have been poured into energy efficiency. Several energy consumption datasets were henceforth published, with each dataset varying in properties, uses and limitations. For instance, building energy consumption patterns are sourced from several sources, including ambient conditions, user occupancy, weather conditions and consumer preferences. Thus, a proper understanding of the available datasets will result in a strong basis for improving energy efficiency. Starting from the necessity of a comprehensive review of existing databases, this work is proposed to survey, study and visualize the numerical and methodological nature of building energy consumption datasets. A total of thirty-one databases are examined and compared in terms of several features, such as the geographical location, period of collection, number of monitored households, sampling rate of collected data, number of sub-metered appliances, extracted features and release date. Furthermore, data collection platforms and related modules for data transmission, data storage and privacy concerns used in different datasets are also analyzed and compared. Based on the analytical study, a novel dataset has been presented, namely Qatar university dataset, which is an annotated power consumption anomaly detection dataset. The latter will be very useful for testing and training anomaly detection algorithms, and hence reducing wasted energy. Moving forward, a set of recommendations is derived to improve datasets collection, such as the adoption of multi-modal data collection, smart Internet of things data collection, low-cost hardware platforms and privacy and security mechanisms. In addition, future directions to improve datasets exploitation and utilization are identified, including the use of novel machine learning solutions, innovative visualization tools and explainable recommender systems.","['Yassine Himeur', 'Abdullah Alsalemi', 'Faycal Bensaali', 'Abbes Amira']","https://export.arxiv.org/abs/2009.08192","2020-09-17","cs.CY","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17260,""
"Interpretable Machine Learning Approaches to Prediction of Chronic   Homelessness","We introduce a machine learning approach to predict chronic homelessness from de-identified client shelter records drawn from a commonly used Canadian homelessness management information system. Using a 30-day time step, a dataset for 6521 individuals was generated. Our model, HIFIS-RNN-MLP, incorporates both static and dynamic features of a client's history to forecast chronic homelessness 6 months into the client's future. The training method was fine-tuned to achieve a high F1-score, giving a desired balance between high recall and precision. Mean recall and precision across 10-fold cross validation were 0.921 and 0.651 respectively. An interpretability method was applied to explain individual predictions and gain insight into the overall factors contributing to chronic homelessness among the population studied. The model achieves state-of-the-art performance and improved stakeholder trust of what is usually a ""black box"" neural network model through interpretable AI.","['Blake VanBerlo', 'Matthew A. S. Ross', 'Jonathan Rivard', 'Ryan Booker']","https://export.arxiv.org/abs/2009.09072","2020-09-12","cs.CY cs.AI cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17261,""
"Tradeoffs in Sentence Selection Techniques for Open-Domain Question   Answering","Current methods in open-domain question answering (QA) usually employ a pipeline of first retrieving relevant documents, then applying strong reading comprehension (RC) models to that retrieved text. However, modern RC models are complex and expensive to run, so techniques to prune the space of retrieved text are critical to allow this approach to scale. In this paper, we focus on approaches which apply an intermediate sentence selection step to address this issue, and investigate the best practices for this approach. We describe two groups of models for sentence selection: QA-based approaches, which run a full-fledged QA system to identify answer candidates, and retrieval-based models, which find parts of each passage specifically related to each question. We examine trade-offs between processing speed and task performance in these two approaches, and demonstrate an ensemble module that represents a hybrid of the two. From experiments on Open-SQuAD and TriviaQA, we show that very lightweight QA models can do well at this task, but retrieval-based models are faster still. An ensemble module we describe balances between the two and generalizes well cross-domain.","['Shih-Ting Lin', 'Greg Durrett']","https://export.arxiv.org/abs/2009.09120","2020-09-18","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17262,""
"City-Scale Visual Place Recognition with Deep Local Features Based on   Multi-Scale Ordered VLAD Pooling","Visual place recognition is the task of recognizing a place depicted in an image based on its pure visual appearance without metadata. In visual place recognition, the challenges lie upon not only the changes in lighting conditions, camera viewpoint, and scale, but also the characteristic of scene level images and the distinct features of the area. To resolve these challenges, one must consider both the local discriminativeness and the global semantic context of images. On the other hand, the diversity of the datasets is also particularly important to develop more general models and advance the progress of the field. In this paper, we present a fully-automated system for place recognition at a city-scale based on content-based image retrieval. Our main contributions to the community lie in three aspects. Firstly, we take a comprehensive analysis of visual place recognition and sketch out the unique challenges of the task compared to general image retrieval tasks. Next, we propose yet a simple pooling approach on top of convolutional neural network activations to embed the spatial information into the image representation vector. Finally, we introduce new datasets for place recognition, which are particularly essential for application-based research. Furthermore, throughout extensive experiments, various issues in both image retrieval and place recognition are analyzed and discussed to give some insights for improving the performance of retrieval models in reality.","['Duc Canh Le', 'Chan Hyun Youn']","https://export.arxiv.org/abs/2009.09255","2020-09-19","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17263,""
"A Technical Review of Wireless security for the Internet of things:   Software Defined Radio perspective","The increase of cyberattacks using IoT devices has exposed the vulnerabilities in the infrastructures that make up the IoT and have shown how small devices can affect networks and services functioning. This paper presents a review of the vulnerabilities of the wireless technologies that bear the IoT and assessing the experiences in implementing wireless attacks targeting the Internet of Things using Software-Defined Radio (SDR) technologies. A systematic literature review was conducted. The types of vulnerabilities and attacks that can affect the wireless technologies that stand the IoT ecosystem and SDR radio platforms were compared. On the IoT system model layer, perception layer was identified as the most vulnerable. Most attacks at this level occur due to limitations in hardware, physical exposure of devices, and heterogeneity of technologies. Future cybersecurity systems based on SDR radios have notable advantages due to their flexibility to adapt to new communication technologies and their potential for the development of advanced tools. However, cybersecurity challenges for the Internet of Things are so complex that it is needed to merge SDR hardware with cognitive techniques and intelligent techniques such as deep learning to adapt to rapid technological changes.","['Jose de Jesus Rugeles', 'Edward Paul Guillen', 'Leonardo S Cardoso']","https://export.arxiv.org/abs/2009.10171","2020-09-21","cs.CR eess.SP","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17264,""
"A Survey and Taxonomy of Distributed Data Mining Research Studies: A   Systematic Literature Review","Context: Data Mining (DM) method has been evolving year by year and as of today there is also the enhancement of DM technique that can be run several times faster than the traditional one, called Distributed Data Mining (DDM). It is not a new field in data processing actually, but in the recent years many researchers have been paying more attention on this area. Problems: The number of publication regarding DDM in high reputation journals and conferences has increased significantly. It makes difficult for researchers to gain a comprehensive view of DDM that require further research. Solution: We conducted a systematic literature review to map the previous research in DDM field. Our objective is to provide the motivation for new research by identifying the gap in DDM field as well as the hot area itself. Result: Our analysis came up with some conclusions by answering 7 research questions proposed in this literature review. In addition, the taxonomy of DDM research area is presented in this paper. Finally, this systematic literature review provides the statistic of development of DDM since 2000 to 2015, in which this will help the future researchers to have a comprehensive overview of current situation of DDM.","['Fauzi Adi Rafrastara', 'Qi Deyu']","https://export.arxiv.org/abs/2009.10618","2020-09-14","cs.DC cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17265,""
"A Survey on Model Watermarking Neural Networks","Machine learning (ML) models are applied in an increasing variety of domains. The availability of large amounts of data and computational resources encourages the development of ever more complex and valuable models. These models are considered intellectual property of the legitimate parties who have trained them, which makes their protection against stealing, illegitimate redistribution, and unauthorized application an urgent need. Digital watermarking presents a strong mechanism for marking model ownership and, thereby, offers protection against those threats. The emergence of numerous watermarking schemes and attacks against them is pushed forward by both academia and industry, which motivates a comprehensive survey on this field. This document at hand provides the first extensive literature review on ML model watermarking schemes and attacks against them. It offers a taxonomy of existing approaches and systemizes general knowledge around them. Furthermore, it assembles the security requirements to watermarking approaches and evaluates schemes published by the scientific community according to them in order to present systematic shortcomings and vulnerabilities. Thus, it can not only serve as valuable guidance in choosing the appropriate scheme for specific scenarios, but also act as an entry point into developing new mechanisms that overcome presented shortcomings, and thereby contribute in advancing the field.","['Franziska Boenisch']","https://export.arxiv.org/abs/2009.12153","2020-09-25","cs.CR cs.LG cs.MM","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17266,""
"A Comprehensive Survey of the Tactile Internet: State of the art and   Research Directions","The Internet has made several giant leaps over the years, from a fixed to a mobile Internet, then to the Internet of Things, and now to a Tactile Internet. The Tactile Internet goes far beyond data, audio and video delivery over fixed and mobile networks, and even beyond allowing communication and collaboration among things. It is expected to enable haptic communication and allow skill set delivery over networks. Some examples of potential applications are tele-surgery, vehicle fleets, augmented reality and industrial process automation. Several papers already cover many of the Tactile Internet-related concepts and technologies, such as haptic codecs, applications, and supporting technologies. However, none of them offers a comprehensive survey of the Tactile Internet, including its architectures and algorithms. Furthermore, none of them provides a systematic and critical review of the existing solutions. To address these lacunae, we provide a comprehensive survey of the architectures and algorithms proposed to date for the Tactile Internet. In addition, we critically review them using a well-defined set of requirements and discuss some of the lessons learned as well as the most promising research directions.","['N. Promwongsa', 'A. Ebrahimzadeh', 'D. Naboulsi', 'S. Kianpisheh', 'F. Belqasmi', 'R. Glitho', 'N. Crespi', 'O. Alfandi']","https://export.arxiv.org/abs/2009.12164","2020-09-22","eess.SP cs.NI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17267,""
"Generating Realistic COVID19 X-rays with a Mean Teacher + Transfer   Learning GAN","COVID-19 is a novel infectious disease responsible for over 800K deaths worldwide as of August 2020. The need for rapid testing is a high priority and alternative testing strategies including X-ray image classification are a promising area of research. However, at present, public datasets for COVID19 x-ray images have low data volumes, making it challenging to develop accurate image classifiers. Several recent papers have made use of Generative Adversarial Networks (GANs) in order to increase the training data volumes. But realistic synthetic COVID19 X-rays remain challenging to generate. We present a novel Mean Teacher + Transfer GAN (MTT-GAN) that generates COVID19 chest X-ray images of high quality. In order to create a more accurate GAN, we employ transfer learning from the Kaggle Pneumonia X-Ray dataset, a highly relevant data source orders of magnitude larger than public COVID19 datasets. Furthermore, we employ the Mean Teacher algorithm as a constraint to improve stability of training. Our qualitative analysis shows that the MTT-GAN generates X-ray images that are greatly superior to a baseline GAN and visually comparable to real X-rays. Although board-certified radiologists can distinguish MTT-GAN fakes from real COVID19 X-rays. Quantitative analysis shows that MTT-GAN greatly improves the accuracy of both a binary COVID19 classifier as well as a multi-class Pneumonia classifier as compared to a baseline GAN. Our classification accuracy is favourable as compared to recently reported results in the literature for similar binary and multi-class COVID19 screening tasks.","['Sumeet Menon', 'Joshua Galita', 'David Chapman', 'Aryya Gangopadhyay', 'Jayalakshmi Mangalagiri', 'Phuong Nguyen', 'Yaacov Yesha', 'Yelena Yesha', 'Babak Saboury', 'Michael Morris']","https://export.arxiv.org/abs/2009.12478","2020-09-25","cs.LG cs.CV eess.IV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17268,""
"Normalization Techniques in Training DNNs: Methodology, Analysis and   Application","Normalization techniques are essential for accelerating the training and improving the generalization of deep neural networks (DNNs), and have successfully been used in various applications. This paper reviews and comments on the past, present and future of normalization methods in the context of DNN training. We provide a unified picture of the main motivation behind different approaches from the perspective of optimization, and present a taxonomy for understanding the similarities and differences between them. Specifically, we decompose the pipeline of the most representative normalizing activation methods into three components: the normalization area partitioning, normalization operation and normalization representation recovery. In doing so, we provide insight for designing new normalization technique. Finally, we discuss the current progress in understanding normalization methods, and provide a comprehensive review of the applications of normalization for particular tasks, in which it can effectively solve the key issues.","['Lei Huang', 'Jie Qin', 'Yi Zhou', 'Fan Zhu', 'Li Liu', 'Ling Shao']","https://export.arxiv.org/abs/2009.12836","2020-09-27","cs.LG cs.CV stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17269,""
"SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer   Matching Retrieval","We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, Natuarl Question, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.","['Tiancheng Zhao', 'Xiaopeng Lu', 'Kyusong Lee']","https://export.arxiv.org/abs/2009.13013","2020-09-27","cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17270,""
"What Disease does this Patient Have? A Large-scale Open Domain Question   Answering Dataset from Medical Exams","Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7\%, 42.0\%, and 70.1\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.","['Di Jin', 'Eileen Pan', 'Nassim Oufattole', 'Wei-Hung Weng', 'Hanyi Fang', 'Peter Szolovits']","https://export.arxiv.org/abs/2009.13081","2020-09-28","cs.CL cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17271,""
"Deep Learning for Predictive Business Process Monitoring: Review and   Benchmark","Predictive monitoring of business processes is concerned with the prediction of ongoing cases on a business process. Lately, the popularity of deep learning techniques has propitiated an ever-growing set of approaches focused on predictive monitoring based on these techniques. However, the high disparity of process logs and experimental setups used to evaluate these approaches makes it especially difficult to make a fair comparison. Furthermore, it also difficults the selection of the most suitable approach to solve a specific problem. In this paper, we provide both a systematic literature review of approaches that use deep learning to tackle the predictive monitoring tasks. In addition, we performed an exhaustive experimental evaluation of 10 different approaches over 12 publicly available process logs.","['EfrÃ©n Rama-Maneiro', 'Juan C. Vidal', 'Manuel Lama']","https://export.arxiv.org/abs/2009.13251","2020-09-24","cs.LG cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17272,""
"Neural Networks based approaches for Major Depressive Disorder and   Bipolar Disorder Diagnosis using EEG signals: A review","Mental disorders represent critical public health challenges as they are leading contributors to the global burden of disease and intensely influence social and financial welfare of individuals. The present comprehensive review concentrate on the two mental disorders: Major depressive Disorder (MDD) and Bipolar Disorder (BD) with noteworthy publications during the last ten years. There's a big need nowadays for phenotypic characterization of psychiatric disorders with biomarkers. Electroencephalography (EEG) signals could offer a rich signature for MDD and BD and then they could improve understanding of pathophysiological mechanisms underling these mental disorders. In this work, we focus on the literature works adopting neural networks fed by EEG signals. Among those studies using EEG and neural networks, we have discussed a variety of EEG based protocols, biomarkers and public datasets for depression and bipolar disorder detection. We conclude with a discussion and valuable recommendations that will help to improve the reliability of developed models and for more accurate and more deterministic computational intelligence based systems in psychiatry. This review will prove to be a structured and valuable initial point for the researchers working on depression and bipolar disorders recognition by using EEG signals.","['Sana Yasin', 'Syed Asad Hussain', 'Sinem Aslan', 'Imran Raza', 'Muhammad Muzammel', 'Alice Othmani']","https://export.arxiv.org/abs/2009.13402","2020-09-28","q-bio.NC cs.LG eess.SP","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17273,""
"A Comprehensive Review for MRF and CRF Approaches in Pathology Image   Analysis","Pathology image analysis is an essential procedure for clinical diagnosis of many diseases. To boost the accuracy and objectivity of detection, nowadays, an increasing number of computer-aided diagnosis (CAD) system is proposed. Among these methods, random field models play an indispensable role in improving the analysis performance. In this review, we present a comprehensive overview of pathology image analysis based on the markov random fields (MRFs) and conditional random fields (CRFs), which are two popular random field models. Firstly, we introduce the background of two random fields and pathology images. Secondly, we summarize the basic mathematical knowledge of MRFs and CRFs from modelling to optimization. Then, a thorough review of the recent research on the MRFs and CRFs of pathology images analysis is presented. Finally, we investigate the popular methodologies in the related works and discuss the method migration among CAD field.","['Chen Li', 'Yixin Li', 'Changhao Sun', 'Hao Chen', 'Hong Zhang']","https://export.arxiv.org/abs/2009.13721","2020-09-28","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17274,""
"Deep matrix factorizations","Constrained low-rank matrix approximations have been known for decades as powerful linear dimensionality reduction techniques to be able to extract the information contained in large data sets in a relevant way. However, such low-rank approaches are unable to mine complex, interleaved features that underlie hierarchical semantics. Recently, deep matrix factorization (deep MF) was introduced to deal with the extraction of several layers of features and has been shown to reach outstanding performances on unsupervised tasks. Deep MF was motivated by the success of deep learning, as it is conceptually close to some neural networks paradigms. In this paper, we present the main models, algorithms, and applications of deep MF through a comprehensive literature review. We also discuss theoretical questions and perspectives of research.","['Pierre De Handschutter', 'Nicolas Gillis', 'Xavier Siebert']","https://export.arxiv.org/abs/2010.00380","2020-10-01","cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17275,""
"A Survey on Explainability in Machine Reading Comprehension","This paper presents a systematic review of benchmarks and approaches for explainability in Machine Reading Comprehension (MRC). We present how the representation and inference challenges evolved and the steps which were taken to tackle these challenges. We also present the evaluation methodologies to assess the performance of explainable systems. In addition, we identify persisting open research questions and highlight critical directions for future work.","['Mokanarangan Thayaparan', 'Marco Valentino', 'AndrÃ© Freitas']","https://export.arxiv.org/abs/2010.00389","2020-10-01","cs.CL cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17276,""
"Autoregressive Entity Retrieval","Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity information such as descriptions. This approach leads to several shortcomings: i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; ii) a large memory footprint is needed to store dense representations when considering large entity sets; iii) an appropriately hard set of negative data has to be subsampled at training time. We propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion, and conditioned on the context. This enables to mitigate the aforementioned technical issues: i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new SOTA, or very competitive results while using a tiny fraction of the memory of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name.","['Nicola De Cao', 'Gautier Izacard', 'Sebastian Riedel', 'Fabio Petroni']","https://export.arxiv.org/abs/2010.00904","2020-10-02","cs.CL cs.IR cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17277,""
"Evaluating Progress on Machine Learning for Longitudinal Electronic   Healthcare Data","The Large Scale Visual Recognition Challenge based on the well-known Imagenet dataset catalyzed an intense flurry of progress in computer vision. Benchmark tasks have propelled other sub-fields of machine learning forward at an equally impressive pace, but in healthcare it has primarily been image processing tasks, such as in dermatology and radiology, that have experienced similar benchmark-driven progress. In the present study, we performed a comprehensive review of benchmarks in medical machine learning for structured data, identifying one based on the Medical Information Mart for Intensive Care (MIMIC-III) that allows the first direct comparison of predictive performance and thus the evaluation of progress on four clinical prediction tasks: mortality, length of stay, phenotyping, and patient decompensation. We find that little meaningful progress has been made over a 3 year period on these tasks, despite significant community engagement. Through our meta-analysis, we find that the performance of deep recurrent models is only superior to logistic regression on certain tasks. We conclude with a synthesis of these results, possible explanations, and a list of desirable qualities for future benchmarks in medical machine learning.","['David Bellamy', 'Leo Celi', 'Andrew L. Beam']","https://export.arxiv.org/abs/2010.01149","2020-10-02","cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17278,""
"Leveraging Semantic and Lexical Matching to Improve the Recall of   Document Retrieval Systems: A Hybrid Approach","Search engines often follow a two-phase paradigm where in the first stage (the retrieval stage) an initial set of documents is retrieved and in the second stage (the re-ranking stage) the documents are re-ranked to obtain the final result list. While deep neural networks were shown to improve the performance of the re-ranking stage in previous works, there is little literature about using deep neural networks to improve the retrieval stage. In this paper, we study the merits of combining deep neural network models and lexical models for the retrieval stage. A hybrid approach, which leverages both semantic (deep neural network-based) and lexical (keyword matching-based) retrieval models, is proposed. We perform an empirical study, using a publicly available TREC collection, which demonstrates the effectiveness of our approach and sheds light on the different characteristics of the semantic approach, the lexical approach, and their combination.","['Saar Kuzi', 'Mingyang Zhang', 'Cheng Li', 'Michael Bendersky', 'Marc Najork']","https://export.arxiv.org/abs/2010.01195","2020-10-02","cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17279,""
"Aspect-Based Sentiment Analysis in Education Domain","Analysis of a large amount of data has always brought value to institutions and organizations. Lately, people's opinions expressed through text have become a very important aspect of this analysis. In response to this challenge, a natural language processing technique known as Aspect-Based Sentiment Analysis (ABSA) has emerged. Having the ability to extract the polarity for each aspect of opinions separately, ABSA has found itself useful in a wide range of domains. Education is one of the domains in which ABSA can be successfully utilized. Being able to understand and find out what students like and don't like most about a course, professor, or teaching methodology can be of great importance for the respective institutions. While this task represents a unique NLP challenge, many studies have proposed different approaches to tackle the problem. In this work, we present a comprehensive review of the existing work in ABSA with a focus in the education domain. A wide range of methodologies are discussed and conclusions are drawn.","['Rinor Hajrizi', 'Krenare Pireva NuÃ§i']","https://export.arxiv.org/abs/2010.01429","2020-10-03","cs.CL cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17280,""
"Multi-Modal Retrieval using Graph Neural Networks","Most real world applications of image retrieval such as Adobe Stock, which is a marketplace for stock photography and illustrations, need a way for users to find images which are both visually (i.e. aesthetically) and conceptually (i.e. containing the same salient objects) as a query image. Learning visual-semantic representations from images is a well studied problem for image retrieval. Filtering based on image concepts or attributes is traditionally achieved with index-based filtering (e.g. on textual tags) or by re-ranking after an initial visual embedding based retrieval. In this paper, we learn a joint vision and concept embedding in the same high-dimensional space. This joint model gives the user fine-grained control over the semantics of the result set, allowing them to explore the catalog of images more rapidly. We model the visual and concept relationships as a graph structure, which captures the rich information through node neighborhood. This graph structure helps us learn multi-modal node embeddings using Graph Neural Networks. We also introduce a novel inference time control, based on selective neighborhood connectivity allowing the user control over the retrieval algorithm. We evaluate these multi-modal embeddings quantitatively on the downstream relevance task of image retrieval on MS-COCO dataset and qualitatively on MS-COCO and an Adobe Stock dataset.","['Aashish Kumar Misraa', 'Ajinkya Kale', 'Pranav Aggarwal', 'Ali Aminian']","https://export.arxiv.org/abs/2010.01666","2020-10-04","cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17281,""
"PolicyQA: A Reading Comprehension Dataset for Privacy Policies","Privacy policy documents are long and verbose. A question answering (QA) system can assist users in finding the information that is relevant and important to them. Prior studies in this domain frame the QA task as retrieving the most relevant text segment or a list of sentences from the policy document given a question. On the contrary, we argue that providing users with a short text span from policy documents reduces the burden of searching the target information from a lengthy text segment. In this paper, we present PolicyQA, a dataset that contains 25,017 reading comprehension style examples curated from an existing corpus of 115 website privacy policies. PolicyQA provides 714 human-annotated questions written for a wide range of privacy practices. We evaluate two existing neural QA models and perform rigorous analysis to reveal the advantages and challenges offered by PolicyQA.","['Wasi Uddin Ahmad', 'Jianfeng Chi', 'Yuan Tian', 'Kai-Wei Chang']","https://export.arxiv.org/abs/2010.02557","2020-10-06","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17282,""
"Deep Representation Learning of Patient Data from Electronic Health   Records (EHR): A Systematic Review","Patient representation learning refers to learning a dense mathematical representation of a patient that encodes meaningful information from Electronic Health Records (EHRs). This is generally performed using advanced deep learning methods. This study presents a systematic review of this field and provides both qualitative and quantitative analyses from a methodological perspective. We identified studies developing patient representations from EHRs with deep learning methods from MEDLINE, EMBASE, Scopus, the Association for Computing Machinery (ACM) Digital Library, and Institute of Electrical and Electronics Engineers (IEEE) Xplore Digital Library. After screening 362 articles, 48 papers were included for a comprehensive data collection. We noticed a typical workflow starting with feeding raw data, applying deep learning models, and ending with clinical outcome predictions as evaluations of the learned representations. Specifically, learning representations from structured EHR data was dominant (36 out of 48 studies). Recurrent Neural Networks were widely applied as the deep learning architecture (LSTM: 13 studies, GRU: 11 studies). Disease prediction was the most common application and evaluation (30 studies). Benchmark datasets were mostly unavailable (28 studies) due to privacy concerns of EHR data, and code availability was assured in 20 studies. We show the importance and feasibility of learning comprehensive representations of patient EHR data through a systematic review. Advances in patient representation learning techniques will be essential for powering patient-level EHR analyses. Future work will still be devoted to leveraging the richness and potential of available EHR data. Knowledge distillation and advanced learning techniques will be exploited to assist the capability of learning patient representation further.","['Yuqi Si', 'Jingcheng Du', 'Zhao Li', 'Xiaoqian Jiang', 'Timothy Miller', 'Fei Wang', 'W. Jim Zheng', 'Kirk Roberts']","https://export.arxiv.org/abs/2010.02809","2020-10-06","cs.LG cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17283,""
"Inductive Entity Representations from Text via Link Prediction","We present a method for learning representations of entities, that uses a Transformer-based architecture as an entity encoder, and link prediction training on a knowledge graph with textual entity descriptions. We demonstrate that our approach can be applied effectively for link prediction in different inductive settings involving entities not seen during training, outperforming related state-of-the-art methods (22% MRR improvement on average). We provide evidence that the learned representations transfer to other tasks that do not require fine-tuning the entity encoder. In an entity classification task we obtain an average improvement of 16% accuracy compared with baselines that also employ pre-trained models. For an information retrieval task, significant improvements of up to 8.8% in NDCG@10 were obtained for natural language queries.","['Daniel Daza', 'Michael Cochez', 'Paul Groth']","https://export.arxiv.org/abs/2010.03496","2020-10-07","cs.CL cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17284,""
"A Survey on Deep Neural Network Compression: Challenges, Overview, and   Solutions","Deep Neural Network (DNN) has gained unprecedented performance due to its automated feature extraction capability. This high order performance leads to significant incorporation of DNN models in different Internet of Things (IoT) applications in the past decade. However, the colossal requirement of computation, energy, and storage of DNN models make their deployment prohibitive on resource constraint IoT devices. Therefore, several compression techniques were proposed in recent years for reducing the storage and computation requirements of the DNN model. These techniques on DNN compression have utilized a different perspective for compressing DNN with minimal accuracy compromise. It encourages us to make a comprehensive overview of the DNN compression techniques. In this paper, we present a comprehensive review of existing literature on compressing DNN model that reduces both storage and computation requirements. We divide the existing approaches into five broad categories, i.e., network pruning, sparse representation, bits precision, knowledge distillation, and miscellaneous, based upon the mechanism incorporated for compressing the DNN model. The paper also discussed the challenges associated with each category of DNN compression techniques. Finally, we provide a quick summary of existing work under each category with the future direction in DNN compression.","['Rahul Mishra', 'Hari Prabhat Gupta', 'Tanima Dutta']","https://export.arxiv.org/abs/2010.03954","2020-10-05","cs.LG cs.NI eess.SP","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17285,""
"A survey of algorithmic recourse: definitions, formulations, solutions,   and prospects","Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role for the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavourably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions towards which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.","['Amir-Hossein Karimi', 'Gilles Barthe', 'Bernhard SchÃ¶lkopf', 'Isabel Valera']","https://export.arxiv.org/abs/2010.04050","2020-10-08","cs.LG cs.AI stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17286,""
"A Survey of Knowledge-Enhanced Text Generation","The goal of text generation is to make machines express in human language. It is one of the most important yet challenging tasks in natural language processing (NLP). Since 2014, various neural encoder-decoder models pioneered by Seq2Seq have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating various forms of knowledge beyond the input text into the generation models. This research direction is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on knowledge enhanced text generation over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.","['Wenhao Yu', 'Chenguang Zhu', 'Zaitang Li', 'Zhiting Hu', 'Qingyun Wang', 'Heng Ji', 'Meng Jiang']","https://export.arxiv.org/abs/2010.04389","2020-10-09","cs.CL cs.AI cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17287,""
"Scaling Systematic Literature Reviews with Machine Learning Pipelines","Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a systematic review are easily done automatically: searching for documents can be done via APIs and scrapers, selection of relevant documents can be done via binary classification, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these tasks. We construct a pipeline that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of classifiers to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of data extraction with varying difficulty in annotation, and five different neural architectures to do the extraction. We find that we can get surprising accuracy and generalisability of the whole pipeline system with only 2 weeks of human-expert annotation, which is only 15% of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.","['Seraphina Goldfarb-Tarrant', 'Alexander Robertson', 'Jasmina Lazic', 'Theodora Tsouloufi', 'Louise Donnison', 'Karen Smyth']","https://export.arxiv.org/abs/2010.04665","2020-10-09","cs.CL cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17288,""
"Light Field Salient Object Detection: A Review and Benchmark","Salient object detection (SOD) is a long-standing research topic in computer vision and has drawn an increasing amount of research interest in the past decade. This paper provides the first comprehensive review and benchmark for SOD on light field, which has long been lacking in the saliency community. Firstly, we introduce preliminary knowledge on lights, including theory and data forms, and then review existing studies on light field SOD, covering ten traditional models, seven deep learning-based models, one comparative study, and one brief review. Existing datasets for light field SOD are also summarized with detailed information and statistical analyses. Secondly, we benchmark seven representative light field SOD models together with several cutting-edge RGB-D SOD models on four widely used light field datasets, from which insightful discussions and analyses, including a comparison between light field SOD and RGB-D SOD models, are achieved. Besides, due to the inconsistency of datasets in their current forms, we further generate complete data and supplement focal stacks, depth maps and multi-view images for the inconsistent datasets, making them consistent and unified. Our supplemental data makes a universal benchmark possible. Lastly, because light field SOD is quite a special problem attributed to its diverse data representations and high dependency on acquisition hardware, making it differ greatly from other saliency detection tasks, we provide nine hints into the challenges and future directions, and outline several open issues. We hope our review and benchmarking could serve as a catalyst to advance research in this field. All the materials including collected models, datasets, benchmarking results, and supplemented light field datasets will be publicly available on our project site https://github.com/kerenfu/LFSOD-Survey.","['Yao Jiang', 'Tao Zhou', 'Ge-Peng Ji', 'Keren Fu', 'Qijun Zhao', 'Deng-Ping Fan']","https://export.arxiv.org/abs/2010.04968","2020-10-10","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17289,""
"Contrastive Representation Learning: A Framework and Review","Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.","['Phuc H. Le-Khac', 'Graham Healy', 'Alan F. Smeaton']","https://export.arxiv.org/abs/2010.05113","2020-10-10","cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17290,""
"A Comprehensive Survey on Local Differential Privacy Toward Data   Statistics and Analysis in Crowdsensing","Collecting and analyzing massive data generated from smart devices have become increasingly pervasive in crowdsensing, which are the building blocks for data-driven decision-making. However, extensive statistics and analysis of such data will seriously threaten the privacy of participating users. Local differential privacy (LDP) has been proposed as an excellent and prevalent privacy model with distributed architecture, which can provide strong privacy guarantees for each user while collecting and analyzing data. LDP ensures that each user's data is locally perturbed first in the client-side and then sent to the server-side, thereby protecting data from privacy leaks on both the client-side and server-side. This survey presents a comprehensive and systematic overview of LDP with respect to privacy models, research tasks, enabling mechanisms, and various applications. Specifically, we first provide a theoretical summarization of LDP, including the LDP model, the variants of LDP, and the basic framework of LDP algorithms. Then, we investigate and compare the diverse LDP mechanisms for various data statistics and analysis tasks from the perspectives of frequency estimation, mean estimation, and machine learning. What's more, we also summarize practical LDP-based application scenarios. Finally, we outline several future research directions under LDP.","['Teng Wang', 'Xuefeng Zhang', 'Jingyu Feng', 'Xinyu Yang']","https://export.arxiv.org/abs/2010.05253","2020-10-11","cs.CR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17291,""
"PECOS: Prediction for Enormous and Correlated Output Spaces","Many challenging problems in modern applications amount to finding relevant results from an enormous output space of potential candidates. The size of the output space for these problems can range from millions to billions. Moreover, training data is often limited for many of the so-called ``long-tail'' of items in the output space. Given the inherent paucity of training data for most of the items in the output space, developing machine learned models that perform well for spaces of this size is challenging. Fortunately, items in the output space are often correlated thereby presenting an opportunity to alleviate the data sparsity issue. In this paper, we propose the Prediction for Enormous and Correlated Output Spaces (PECOS) framework, a versatile and modular machine learning framework for solving prediction problems for very large output spaces, and apply it to the eXtreme Multilabel Ranking (XMR) problem: given an input instance, find and rank the most relevant items from an enormous but fixed and finite output space. PECOS is a three-phase framework: (i) in the first phase, PECOS organizes the output space using a semantic indexing scheme, (ii) in the second phase, PECOS uses the indexing to narrow down the output space by orders of magnitude using a machine learned matching scheme, and (iii) in the third phase, PECOS ranks the matched items using a final ranking scheme. The versatility and modularity of PECOS allows for easy plug-and-play of various choices for the indexing, matching, and ranking phases. On a dataset where the output space is of size 2.8 million, PECOS with a neural matcher results in a 10% increase in precision@1 (from 46% to 51.2%) over PECOS with a recursive linear matcher but takes 265x more time to train. We also develop fast real time inference procedures; for example, inference takes less than 10 milliseconds on the data set with 2.8 million labels.","['Hsiang-Fu Yu', 'Kai Zhong', 'Inderjit S. Dhillon']","https://export.arxiv.org/abs/2010.05878","2020-10-12","cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17292,""
"Artificial Intelligence, speech and language processing approaches to   monitoring Alzheimer's Disease: a systematic review","Language is a valuable source of clinical information in Alzheimer's Disease, as it declines concurrently with neurodegeneration. Consequently, speech and language data have been extensively studied in connection with its diagnosis. This paper summarises current findings on the use of artificial intelligence, speech and language processing to predict cognitive decline in the context of Alzheimer's Disease, detailing current research procedures, highlighting their limitations and suggesting strategies to address them. We conducted a systematic review of original research between 2000 and 2019, registered in PROSPERO (reference CRD42018116606). An interdisciplinary search covered six databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine (PubMed and Embase) and Web of Science. Bibliographies of relevant papers were screened until December 2019. From 3,654 search results 51 articles were selected against the eligibility criteria. Four tables summarise their findings: study details (aim, population, interventions, comparisons, methods and outcomes), data details (size, type, modalities, annotation, balance, availability and language of study), methodology (pre-processing, feature generation, machine learning, evaluation and results) and clinical applicability (research implications, clinical potential, risk of bias and strengths/limitations). While promising results are reported across nearly all 51 studies, very few have been implemented in clinical research or practice. We concluded that the main limitations of the field are poor standardisation, limited comparability of results, and a degree of disconnect between study aims and clinical applications. Attempts to close these gaps should support translation of future research into clinical practice.","['Sofia de la Fuente Garcia', 'Craig Ritchie', 'Saturnino Luz']","https://export.arxiv.org/abs/2010.06047","2020-10-12","cs.AI cs.CL eess.AS","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17293,""
"Machine learning for the diagnosis of Parkinson's disease: A systematic   review","Diagnosis of Parkinson's disease (PD) is commonly based on medical observations and assessment of clinical signs, including the characterization of a variety of motor symptoms. However, traditional diagnostic approaches may suffer from subjectivity as they rely on the evaluation of movements that are sometimes subtle to human eyes and therefore difficult to classify, leading to possible misclassification. In the meantime, early non-motor symptoms of PD may be mild and can be caused by many other conditions. Therefore, these symptoms are often overlooked, making diagnosis of PD at an early stage challenging. To address these difficulties and to refine the diagnosis and assessment procedures of PD, machine learning methods have been implemented for the classification of PD and healthy controls or patients with similar clinical presentations (e.g., movement disorders or other Parkinsonian syndromes). To provide a comprehensive overview of data modalities and machine learning methods that have been used in the diagnosis and differential diagnosis of PD, in this study, we conducted a systematic literature review of studies published until February 14, 2020, using the PubMed and IEEE Xplore databases. A total of 209 studies were included, extracted for relevant information and presented in this systematic review, with an investigation of their aims, sources of data, types of data, machine learning methods and associated outcomes. These studies demonstrate a high potential for adaptation of machine learning methods and novel biomarkers in clinical decision making, leading to increasingly systematic, informed diagnosis of PD.","['Jie Mei', 'Christian Desrosiers', 'Johannes Frasnelli']","https://export.arxiv.org/abs/2010.06101","2020-10-12","cs.LG stat.ML","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17294,""
"Self-Supervised Ranking for Representation Learning","We present a new framework for self-supervised representation learning by positing it as a ranking problem in an image retrieval context on a large number of random views from random sets of images. Our work is based on two intuitive observations: first, a good representation of images must yield a high-quality image ranking in a retrieval task; second, we would expect random views of an image to be ranked closer to a reference view of that image than random views of other images. Hence, we model representation learning as a learning-to-rank problem in an image retrieval context, and train it by maximizing average precision (AP) for ranking. Specifically, given a mini-batch of images, we generate a large number of positive/negative samples and calculate a ranking loss term by separately treating each image view as a retrieval query. The new framework, dubbed S2R2, enables computing a global objective compared to the local objective in the popular contrastive learning framework calculated on pairs of views. A global objective leads S2R2 to faster convergence in terms of the number of epochs. In principle, by using a ranking criterion, we eliminate reliance on object-centered curated datasets (e.g., ImageNet). When trained on STL10 and MS-COCO, S2R2 outperforms SimCLR and performs on par with the state-of-the-art clustering-based contrastive learning model, SwAV, while being much simpler both conceptually and implementation-wise. Furthermore, when trained on a small subset of MS-COCO with fewer similar scenes, S2R2 significantly outperforms both SwAV and SimCLR. This indicates that S2R2 is potentially more effective on diverse scenes and decreases the need for a large training dataset for self-supervised learning.","['Ali Varamesh', 'Ali Diba', 'Tinne Tuytelaars', 'Luc Van Gool']","https://export.arxiv.org/abs/2010.07258","2020-10-14","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17295,""
"Impact of Action Unit Occurrence Patterns on Detection","Detecting action units is an important task in face analysis, especially in facial expression recognition. This is due, in part, to the idea that expressions can be decomposed into multiple action units. In this paper we investigate the impact of action unit occurrence patterns on detection of action units. To facilitate this investigation, we review state of the art literature, for AU detection, on 2 state-of-the-art face databases that are commonly used for this task, namely DISFA, and BP4D. Our findings, from this literature review, suggest that action unit occurrence patterns strongly impact evaluation metrics (e.g. F1-binary). Along with the literature review, we also conduct multi and single action unit detection, as well as propose a new approach to explicitly train deep neural networks using the occurrence patterns to boost the accuracy of action unit detection. These experiments validate that action unit patterns directly impact the evaluation metrics.","['Saurabh Hinduja', 'Shaun Canavan', 'Saandeep Aathreya']","https://export.arxiv.org/abs/2010.07982","2020-10-15","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17296,""
"Position paper: A systematic framework for categorising IoT device   fingerprinting mechanisms","The popularity of the Internet of Things (IoT) devices makes it increasingly important to be able to fingerprint them, for example in order to detect if there are misbehaving or even malicious IoT devices in one's network. The aim of this paper is to provide a systematic categorisation of machine learning augmented techniques that can be used for fingerprinting IoT devices. This can serve as a baseline for comparing various IoT fingerprinting mechanisms, so that network administrators can choose one or more mechanisms that are appropriate for monitoring and maintaining their network. We carried out an extensive literature review of existing papers on fingerprinting IoT devices -- paying close attention to those with machine learning features. This is followed by an extraction of important and comparable features among the mechanisms outlined in those papers. As a result, we came up with a key set of terminologies that are relevant both in the fingerprinting context and in the IoT domain. This enabled us to construct a framework called IDWork, which can be used for categorising existing IoT fingerprinting mechanisms in a way that will facilitate a coherent and fair comparison of these mechanisms. We found that the majority of the IoT fingerprinting mechanisms take a passive approach -- mainly through network sniffing -- instead of being intrusive and interactive with the device of interest. Additionally, a significant number of the surveyed mechanisms employ both static and dynamic approaches, in order to benefit from complementary features that can be more robust against certain attacks such as spoofing and replay attacks.","['Poonam Yadav', 'Angelo Feraudo', 'Budi Arief', 'Siamak F. Shahandashti', 'Vassilios G. Vassilakis']","https://export.arxiv.org/abs/2010.08466","2020-10-16","cs.NI cs.CR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17297,""
"Average-reward model-free reinforcement learning: a systematic review   and literature mapping","Model-free reinforcement learning (RL) has been an active area of research and provides a fundamental framework for agent-based learning and decision-making in artificial intelligence. In this paper, we review a specific subset of this literature, namely work that utilizes optimization criteria based on average rewards, in the infinite horizon setting. Average reward RL has the advantage of being the most selective criterion in recurrent (ergodic) Markov decision processes. In comparison to widely-used discounted reward criterion, it also requires no discount factor, which is a critical hyperparameter, and properly aligns the optimization and performance metrics. Motivated by the solo survey by Mahadevan (1996a), we provide an updated review of work in this area and extend it to cover policy-iteration and function approximation methods (in addition to the value-iteration and tabular counterparts). We also identify and discuss opportunities for future work.","['Vektor Dewanto', 'George Dunn', 'Ali Eshragh', 'Marcus Gallagher', 'Fred Roosta']","https://export.arxiv.org/abs/2010.08920","2020-10-18","cs.LG cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17298,""
"Technical Question Answering across Tasks and Domains","Building automatic technical support system is an important yet challenge task. Conceptually, to answer a user question on a technical forum, a human expert has to first retrieve relevant documents, and then read them carefully to identify the answer snippet. Despite huge success the researchers have achieved in coping with general domain question answering (QA), much less attentions have been paid for investigating technical QA. Specifically, existing methods suffer from several unique challenges (i) the question and answer rarely overlaps substantially and (ii) very limited data size. In this paper, we propose a novel framework of deep transfer learning to effectively address technical QA across tasks and domains. To this end, we present an adjustable joint learning approach for document retrieval and reading comprehension tasks. Our experiments on the TechQA demonstrates superior performance compared with state-of-the-art methods.","['Wenhao Yu', 'Lingfei Wu', 'Yu Deng', 'Qingkai Zeng', 'Ruchi Mahindru', 'Sinem Guven', 'Meng Jiang']","https://export.arxiv.org/abs/2010.09780","2020-10-19","cs.CL cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17299,""
"PROP: Pre-training with Representative Words Prediction for Ad-hoc   Retrieval","Recently pre-trained language representation models such as BERT have shown great success when fine-tuned on downstream tasks including information retrieval (IR). However, pre-training objectives tailored for ad-hoc retrieval have not been well explored. In this paper, we propose Pre-training with Representative wOrds Prediction (PROP) for ad-hoc retrieval. PROP is inspired by the classical statistical language model for IR, specifically the query likelihood model, which assumes that the query is generated as the piece of text representative of the ""ideal"" document. Based on this idea, we construct the representative words prediction (ROP) task for pre-training. Given an input document, we sample a pair of word sets according to the document language model, where the set with higher likelihood is deemed as more representative of the document. We then pre-train the Transformer model to predict the pairwise preference between the two word sets, jointly with the Masked Language Model (MLM) objective. By further fine-tuning on a variety of representative downstream ad-hoc retrieval tasks, PROP achieves significant improvements over baselines without pre-training or with other pre-training methods. We also show that PROP can achieve exciting performance under both the zero- and low-resource IR settings. The code and pre-trained models are available at https://github.com/Albert-Ma/PROP.","['Xinyu Ma', 'Jiafeng Guo', 'Ruqing Zhang', 'Yixing Fan', 'Xiang Ji', 'Xueqi Cheng']","https://export.arxiv.org/abs/2010.10137","2020-10-20","cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17300,""
"ICFHR 2020 Competition on Image Retrieval for Historical Handwritten   Fragments","This competition succeeds upon a line of competitions for writer and style analysis of historical document images. In particular, we investigate the performance of large-scale retrieval of historical document fragments in terms of style and writer identification. The analysis of historic fragments is a difficult challenge commonly solved by trained humanists. In comparison to previous competitions, we make the results more meaningful by addressing the issue of sample granularity and moving from writer to page fragment retrieval. The two approaches, style and author identification, provide information on what kind of information each method makes better use of and indirectly contribute to the interpretability of the participating method. Therefore, we created a large dataset consisting of more than 120 000 fragments. Although the most teams submitted methods based on convolutional neural networks, the winning entry achieves an mAP below 40%.","['Mathias Seuret', 'Anguelos Nicolaou', 'Dominique Stutzmann', 'Andreas Maier', 'Vincent Christlein']","https://export.arxiv.org/abs/2010.10197","2020-10-20","cs.CV","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17301,""
"Learning To Retrieve: How to Train a Dense Retrieval Model Effectively   and Efficiently","Ranking has always been one of the top concerns in information retrieval research. For decades, lexical matching signal has dominated the ad-hoc retrieval process, but it also has inherent defects, such as the vocabulary mismatch problem. Recently, Dense Retrieval (DR) technique has been proposed to alleviate these limitations by capturing the deep semantic relationship between queries and documents. The training of most existing Dense Retrieval models relies on sampling negative instances from the corpus to optimize a pairwise loss function. Through investigation, we find that this kind of training strategy is biased and fails to optimize full retrieval performance effectively and efficiently. To solve this problem, we propose a Learning To Retrieve (LTRe) training technique. LTRe constructs the document index beforehand. At each training iteration, it performs full retrieval without negative sampling and then updates the query representation model parameters. Through this process, it teaches the DR model how to retrieve relevant documents from the entire corpus instead of how to rerank a potentially biased sample of documents. Experiments in both passage retrieval and document retrieval tasks show that: 1) in terms of effectiveness, LTRe significantly outperforms all competitive sparse and dense baselines. It even gains better performance than the BM25-BERT cascade system under reasonable latency constraints. 2) in terms of training efficiency, compared with the previous state-of-the-art DR method, LTRe provides more than 170x speed-up in the training process. Training with a compressed index further saves computing resources with minor performance loss.","['Jingtao Zhan', 'Jiaxin Mao', 'Yiqun Liu', 'Min Zhang', 'Shaoping Ma']","https://export.arxiv.org/abs/2010.10469","2020-10-20","cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17302,""
"Using the Full-text Content of Academic Articles to Identify and   Evaluate Algorithm Entities in the Domain of Natural Language Processing","In the era of big data, the advancement, improvement, and application of algorithms in academic research have played an important role in promoting the development of different disciplines. Academic papers in various disciplines, especially computer science, contain a large number of algorithms. Identifying the algorithms from the full-text content of papers can determine popular or classical algorithms in a specific field and help scholars gain a comprehensive understanding of the algorithms and even the field. To this end, this article takes the field of natural language processing (NLP) as an example and identifies algorithms from academic papers in the field. A dictionary of algorithms is constructed by manually annotating the contents of papers, and sentences containing algorithms in the dictionary are extracted through dictionary-based matching. The number of articles mentioning an algorithm is used as an indicator to analyze the influence of that algorithm. Our results reveal the algorithm with the highest influence in NLP papers and show that classification algorithms represent the largest proportion among the high-impact algorithms. In addition, the evolution of the influence of algorithms reflects the changes in research tasks and topics in the field, and the changes in the influence of different algorithms show different trends. As a preliminary exploration, this paper conducts an analysis of the impact of algorithms mentioned in the academic text, and the results can be used as training data for the automatic extraction of large-scale algorithms in the future. The methodology in this paper is domain-independent and can be applied to other domains.","['Yuzhuo Wang', 'Chengzhi Zhang']","https://export.arxiv.org/abs/2010.10817","2020-10-21","cs.CL cs.IR cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17303,""
"Neural Networks for Entity Matching","Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years we have seen new methods based upon deep learning techniques for natural language processing emerge.   In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.","['Nils Barlaug', 'Jon Atle Gulla']","https://export.arxiv.org/abs/2010.11075","2020-10-21","cs.DB cs.CL cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17304,""
"XOR QA: Cross-lingual Open-Retrieval Question Answering","Multilingual question answering tasks typically assume answers exist in the same language as the question. Yet in practice, many languages face both information scarcity---where languages have few reference articles---and information asymmetry---where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on questions from TyDi QA lacking same-language answers. Our task formulation, called Cross-lingual Open Retrieval Question Answering (XOR QA), includes 40k information-seeking questions from across 7 diverse non-English languages. Based on this dataset, we introduce three new tasks that involve cross-lingual document retrieval using multi-lingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington.edu/xorqa.","['Akari Asai', 'Jungo Kasai', 'Jonathan H. Clark', 'Kenton Lee', 'Eunsol Choi', 'Hannaneh Hajishirzi']","https://export.arxiv.org/abs/2010.11856","2020-10-22","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17305,""
"On the impact of publicly available news and information transfer to   financial markets","We quantify the propagation and absorption of large-scale publicly available news articles from the World Wide Web to financial markets. To extract publicly available information, we use the news archives from the Common Crawl, a nonprofit organization that crawls a large part of the web. We develop a processing pipeline to identify news articles associated with the constituent companies in the S\&P 500 index, an equity market index that measures the stock performance of U.S. companies. Using machine learning techniques, we extract sentiment scores from the Common Crawl News data and employ tools from information theory to quantify the information transfer from public news articles to the U.S. stock market. Furthermore, we analyze and quantify the economic significance of the news-based information with a simple sentiment-based portfolio trading strategy. Our findings provides support for that information in publicly available news on the World Wide Web has a statistically and economically significant impact on events in financial markets.","['Metod Jazbec', 'Barna PÃ¡sztor', 'Felix Faltings', 'Nino Antulov-Fantulin', 'Petter N. Kolm']","https://export.arxiv.org/abs/2010.12002","2020-10-22","q-fin.ST cs.LG physics.soc-ph q-fin.TR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17306,""
"Neural Passage Retrieval with Improved Negative Contrast","In this paper we explore the effects of negative sampling in dual encoder models used to retrieve passages for automatic question answering. We explore four negative sampling strategies that complement the straightforward random sampling of negatives, typically used to train dual encoder models. Out of the four strategies, three are based on retrieval and one on heuristics. Our retrieval-based strategies are based on the semantic similarity and the lexical overlap between questions and passages. We train the dual encoder models in two stages: pre-training with synthetic data and fine tuning with domain-specific data. We apply negative sampling to both stages. The approach is evaluated in two passage retrieval tasks. Even though it is not evident that there is one single sampling strategy that works best in all the tasks, it is clear that our strategies contribute to improving the contrast between the response and all the other passages. Furthermore, mixing the negatives from different strategies achieve performance on par with the best performing strategy in all tasks. Our results establish a new state-of-the-art level of performance on two of the open-domain question answering datasets that we evaluated.","['Jing Lu', 'Gustavo Hernandez Abrego', 'Ji Ma', 'Jianmo Ni', 'Yinfei Yang']","https://export.arxiv.org/abs/2010.12523","2020-10-23","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17307,""
"Retrieve, Rerank, Read, then Iterate: Answering Open-Domain Questions of   Arbitrary Complexity from Text","Current approaches to open-domain question answering often make crucial assumptions that prevent them from generalizing to real-world settings, including the access to parameterized retrieval systems well-tuned for the task, access to structured metadata like knowledge bases and web links, or a priori knowledge of the complexity of questions to be answered (e.g., single-hop or multi-hop). To address these limitations, we propose a unified system to answer open-domain questions of arbitrary complexity directly from text that works with off-the-shelf retrieval systems on arbitrary text collections. We employ a single multi-task model to perform all the necessary subtasks---retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents---in an iterative fashion. To emulate a more realistic setting, we also constructed a new unified benchmark by collecting about 200 multi-hop questions that require three Wikipedia pages to answer, and combining them with existing datasets. We show that our model not only outperforms state-of-the-art systems on several existing benchmarks that exclusively feature single-hop or multi-hop open-domain questions, but also achieves strong performance on the new benchmark.","['Peng Qi', 'Haejun Lee', 'Oghenetegiri ""TG"" Sido', 'Christopher D. Manning']","https://export.arxiv.org/abs/2010.12527","2020-10-23","cs.CL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17308,""
"Deep Learning for Radio-based Human Sensing: Recent Advances and Future   Directions","While decade-long research has clearly demonstrated the vast potential of radio frequency (RF) for many human sensing tasks, scaling this technology to large scenarios remained problematic with conventional approaches. Recently, researchers have successfully applied deep learning to take radio-based sensing to a new level. Many different types of deep learning models have been proposed to achieve high sensing accuracy over a large population and activity set, as well as in unseen environments. Deep learning has also enabled detection of novel human sensing phenomena that were previously not possible. In this survey, we provide a comprehensive review and taxonomy of recent research efforts on deep learning based RF sensing. We also identify and compare several publicly released labeled RF sensing datasets that can facilitate such deep learning research. Finally, we summarize the lessons learned and discuss the current limitations and future directions of deep learning based RF sensing.","['Isura Nirmal', 'Abdelwahed Khamis', 'Mahbub Hassan', 'Wen Hu', 'Xiaoqing Zhu']","https://export.arxiv.org/abs/2010.12717","2020-10-23","eess.SP cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17309,""
"Constraint Translation Candidates: A Bridge between Neural Query   Translation and Cross-lingual Information Retrieval","Query translation (QT) is a key component in cross-lingual information retrieval system (CLIR). With the help of deep learning, neural machine translation (NMT) has shown promising results on various tasks. However, NMT is generally trained with large-scale out-of-domain data rather than in-domain query translation pairs. Besides, the translation model lacks a mechanism at the inference time to guarantee the generated words to match the search index. The two shortages of QT result in readable texts for human but inadequate candidates for the downstream retrieval task. In this paper, we propose a novel approach to alleviate these problems by limiting the open target vocabulary search space of QT to a set of important words mined from search index database. The constraint translation candidates are employed at both of training and inference time, thus guiding the translation model to learn and generate well performing target queries. The proposed methods are exploited and examined in a real-word CLIR system--Aliexpress e-Commerce search engine. Experimental results demonstrate that our approach yields better performance on both translation quality and retrieval accuracy than the strong NMT baseline.","['Tianchi Bi', 'Liang Yao', 'Baosong Yang', 'Haibo Zhang', 'Weihua Luo', 'Boxing Chen']","https://export.arxiv.org/abs/2010.13658","2020-10-26","cs.CL cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17310,""
"Affordance as general value function: A computational model","General value functions (GVFs) in the reinforcement learning (RL) literature are long-term predictive summaries of the outcomes of agents following specific policies in the environment. Affordances as perceived valences of action possibilities may be cast into predicted policy-relative goodness and modelled as GVFs. A systematic explication of this connection shows that GVFs and especially their deep learning embodiments (1) realize affordance prediction as a form of direct perception, (2) illuminate the fundamental connection between action and perception in affordance, and (3) offer a scalable way to learn affordances using RL methods. Through a comprehensive review of existing literature on recent successes of GVF applications in robotics, rehabilitation, industrial automation, and autonomous driving, we demonstrate that GVFs provide the right framework for learning affordances in real-world applications. In addition, we highlight a few new avenues of research opened up by the perspective of ""affordance as GVF"", including using GVFs for orchestrating complex behaviors.","['Daniel Graves', 'Johannes GÃ¼nther', 'Jun Luo']","https://export.arxiv.org/abs/2010.14289","2020-10-27","cs.AI","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17311,""
"Dynamic Boundary Time Warping for Sub-sequence Matching with Few   Examples","The paper presents a novel method of finding a fragment in a long temporal sequence similar to the set of shorter sequences. We are the first to propose an algorithm for such a search that does not rely on computing the average sequence from query examples. Instead, we use query examples as is, utilizing all of them simultaneously. The introduced method based on the Dynamic Time Warping (DTW) technique is suited explicitly for few-shot query-by-example retrieval tasks. We evaluate it on two different few-shot problems from the field of Natural Language Processing. The results show it either outperforms baselines and previous approaches or achieves comparable results when a low number of examples is available.","['Lukasz Borchmann', 'Dawid Jurkiewicz', 'Filip Gralinski', 'Tomasz GÃ³recki']","https://export.arxiv.org/abs/2010.14464","2020-10-27","cs.DS cs.CL cs.IR","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17312,""
"Speech-Image Semantic Alignment Does Not Depend on Any Prior   Classification Tasks","Semantically-aligned $(speech, image)$ datasets can be used to explore ""visually-grounded speech"". In a majority of existing investigations, features of an image signal are extracted using neural networks ""pre-trained"" on other tasks (e.g., classification on ImageNet). In still others, pre-trained networks are used to extract audio features prior to semantic embedding. Without ""transfer learning"" through pre-trained initialization or pre-trained feature extraction, previous results have tended to show low rates of recall in $speech \rightarrow image$ and $image \rightarrow speech$ queries.   Choosing appropriate neural architectures for encoders in the speech and image branches and using large datasets, one can obtain competitive recall rates without any reliance on any pre-trained initialization or feature extraction: $(speech,image)$ semantic alignment and $speech \rightarrow image$ and $image \rightarrow speech$ retrieval are canonical tasks worthy of independent investigation of their own and allow one to explore other questions---e.g., the size of the audio embedder can be reduced significantly with little loss of recall rates in $speech \rightarrow image$ and $image \rightarrow speech$ queries.","['Masood S. Mortazavi']","https://export.arxiv.org/abs/2010.15288","2020-10-28","cs.LG cs.CV cs.IT cs.MM math.IT","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17313,""
"Uncovering Latent Biases in Text: Method and Application to Peer Review","Quantifying systematic disparities in numerical quantities such as employment rates and wages between population subgroups provides compelling evidence for the existence of societal biases. However, biases in the text written for members of different subgroups (such as in recommendation letters for male and non-male candidates), though widely reported anecdotally, remain challenging to quantify. In this work, we introduce a novel framework to quantify bias in text caused by the visibility of subgroup membership indicators. We develop a nonparametric estimation and inference procedure to estimate this bias. We then formalize an identification strategy to causally link the estimated bias to the visibility of subgroup membership indicators, provided observations from time periods both before and after an identity-hiding policy change. We identify an application wherein ""ground truth"" bias can be inferred to evaluate our framework, instead of relying on synthetic or secondary data. Specifically, we apply our framework to quantify biases in the text of peer reviews from a reputed machine learning conference before and after the conference adopted a double-blind reviewing policy. We show evidence of biases in the review ratings that serves as ""ground truth"", and show that our proposed framework accurately detects these biases from the review text without having access to the review ratings.","['Emaad Manzoor', 'Nihar B. Shah']","https://export.arxiv.org/abs/2010.15300","2020-10-28","cs.CL cs.CY cs.LG","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17314,""
"Typable Fragments of Polynomial Automatic Amortized Resource Analysis","Being a fully automated technique for resource analysis, automatic amortized resource analysis (AARA) can fail in returning worst-case cost bounds of programs, fundamentally due to the undecidability of resource analysis. For programmers who are unfamiliar with the technical details of AARA, it is difficult to predict whether a program can be successfully analyzed in AARA. Motivated by this problem, this article identifies classes of programs that can be analyzed in type-based polynomial AARA. Firstly, it is shown that the set of functions that are typable in univariate polynomial AARA coincides with the complexity class PTIME. Secondly, the article presents a sufficient condition for typability that axiomatically requires every sub-expression of a given program to be polynomial-time. It is proved that this condition implies typability in multivariate polynomial AARA under some syntactic restrictions.","['Long Pham', 'Jan Hoffmann']","https://export.arxiv.org/abs/2010.16353","2020-10-30","cs.PL","arXiv","Undecided","","","","","","","","","","","","","False","False","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020-11-06","",17315,""
