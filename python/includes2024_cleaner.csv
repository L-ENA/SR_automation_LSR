q7,title,abstract,authors,link,date,datasource,initial_decision,q12,extraction_date,expert_decision,ID,q13,subject,q47,exclusion_reason,q0,q1,q3,q5,q6,q15,q27,q42,q39,q25,q16,q26,q24,q29,q34,q17,q40,q43,q45,q46,q14,q20,q28,q44,q4,q18,q2,q19,q33,q32,q31,q35,q22,q10,q36,q37,q21,q23,q8,q9,q11,q30,q38,q41,q48,q49,o1,q50,q51,q52,q53,q54,q55,q56,q57,q58,q59,q60,LLM Prompt,LLM Runs,Unnamed: 76,LLM notes,LLM application,SWAR?,Column1
Text or probably text file,Sentence retrieval for abstracts of randomized controlled trials,"BACKGROUND: The practice of evidence-based medicine (EBM) requires clinicians to integrate their expertise with the latest scientific research. But this is becoming increasingly difficult with the growing numbers of published articles. There is a clear need for better tools to improve clinician's ability to search the primary literature. Randomized clinical trials (RCTs) are the most reliable source of evidence documenting the efficacy of treatment options. This paper describes the retrieval of key sentences from abstracts of RCTs as a step towards helping users find relevant facts about the experimental design of clinical studies. METHOD: Using Conditional Random Fields (CRFs), a popular and successful method for natural language processing problems, sentences referring to Intervention, Participants and Outcome Measures are automatically categorized. This is done by extending a previous approach for labeling sentences in an abstract for general categories associated with scientific argumentation or rhetorical roles: Aim, Method, Results and Conclusion. Methods are tested on several corpora of RCT abstracts. First structured abstracts with headings specifically indicating Intervention, Participant and Outcome Measures are used. Also a manually annotated corpus of structured and unstructured abstracts is prepared for testing a classifier that identifies sentences belonging to each category. RESULTS: Using CRFs, sentences can be labeled for the four rhetorical roles with F-scores from 0.93-0.98. This outperforms the use of Support Vector Machines. Furthermore, sentences can be automatically labeled for Intervention, Participant and Outcome Measures, in unstructured and structured abstracts where the section headings do not specifically indicate these three topics. F-scores of up to 0.83 and 0.84 are obtained for Intervention and Outcome Measure sentences. CONCLUSION: Results indicate that some of the methodological elements of RCTs are identifiable at the sentence level in both structured and unstructured abstract reports. This is promising in that sentences labeled automatically could potentially form concise summaries, assist in information retrieval and finer-grained extraction.
","Chung, G. Y.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,668,"Grace Y Chung: Centre for Health Informatics, University of New South Wales, Sydney, NSW, 2052, Australia

#Ask code, data",,"""Results indicate that some of the methodological elements of RCTs are identifiable at the sentence level in both structured and unstructured abstract reports. This is promising in that sentences labeled automatically could potentially form concise summaries, assist in information retrieval and finer-grained extraction.""",,LS,"CRF, SVM, Multi-layer perceptron","Precision, Recall, F1, Accuracy","Abstracts, Titles",RCT,"PubMed export, published between 1998 and 2006, keywords: asthma, diabetes, breast cancer, prostate cancer, erectile dysfunction, heart failure, cardiovascular, angina",,Yes,Yes,NR,No,No,No,"Error analysis and examples given, full scores reported.","Mallet, SVMlight, GENIA tagger",Yes,cross-validation 15 fold,"Evaluation on separate datasets, tested structured and unstructured abstracts. ",Yes,No,Yes,"normaization: int, real number, measurement, range, statistical entity, n, time, date money replaced with semantic class: ""normalization in which a script using regular expressions replaces complex numerical and mathematical notation into a canonical form or the semantic class. All integers and real numbers are mapped to symbols INT and REAL. All entities that represent measurements are normalized. For instance, a surface form of ""200 mg/d"" maps to MEASUREMENT. Ranges such as ""200â€“300 mg/d"" map to MEASUREMENT_RANGE. Statistical expressions such as p-values, confidence intervals, risk ratios, are mapped to a generic class STATISTICAL_EXPRESSION. Another common notation is population counts such as ""n = 100"" which has a semantic form POPULATION. Similarly, time and date and monetary expressions are also reduced to canonical form"". "" better performance was ascertained from using simple unigram bag-of-words, without further processing. Higher order n-gram features, stemming or removal of stop words did not improve performance""",Yes,"Noisy labels: poorer performance for P element since its often mixed into I sentences, and labelled differently which created noise. Participant sentences include different information, such as age/gender/diagnosis",,"Gold standard: 318 abstracts with 107 unstructured and 211 structured (344 Intervention sentences, 341 Outcome Measure sentences, and 144 Participant sentences). 13.6 k abstracts and 156 k sentences from structured abstracts with automatically mapped classes. ""Set I contains 1575 abstracts, 21.2 k sentences; Set P contains 2280 abstracts, 29.8 k sentences; Set O contains 1740 abstracts and 22.9 k sentences""",,Yes,Yes,NR,No,Yes,"CRF described, default parameters. Binary SVM. No forther detail on hyperparameters.",,"common entities, general-topic text, using range of keywords for RCT retrieval",No,Yes,No,Spans or annotations,"P, IC, O, Sections (Aim; Method etc.)",Sentences,None reported,NR,Yes; random splits and ratio given,2009,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Evaluation of a rule-based method for epidemiological document classification towards the automation of systematic reviews,"INTRODUCTION: Most data extraction efforts in epidemiology are focused on obtaining targeted information from clinical trials. In contrast, limited research has been conducted on the identification of information from observational studies, a major source for human evidence in many fields, including environmental health. The recognition of key epidemiological information (e.g., exposures) through text mining techniques can assist in the automation of systematic reviews and other evidence summaries. METHOD: We designed and applied a knowledge-driven, rule-based approach to identify targeted information (study design, participant population, exposure, outcome, confounding factors, and the country where the study was conducted) from abstracts of epidemiological studies included in several systematic reviews of environmental health exposures. The rules were based on common syntactical patterns observed in text and are thus not specific to any systematic review. To validate the general applicability of our approach, we compared the data extracted using our approach versus hand curation for 35 epidemiological study abstracts manually selected for inclusion in two systematic reviews. RESULTS: The returned F-score, precision, and recall ranged from 70% to 98%, 81% to 100%, and 54% to 97%, respectively. The highest precision was observed for exposure, outcome and population (100%) while recall was best for exposure and study design with 97% and 89%, respectively. The lowest recall was observed for the population (54%), which also had the lowest F-score (70%). CONCLUSION: The generated performance of our text-mining approach demonstrated encouraging results for the identification of targeted information from observational epidemiological study abstracts related to environmental exposures. We have demonstrated that rules based on generic syntactic patterns in one corpus can be applied to other observational study design by simple interchanging the dictionaries aiming to identify certain characteristics (i.e., outcomes, exposures). At the document level, the recognised information can assist in the selection and categorization of studies included in a systematic review.
","Karystianis, G.
 and Thayer, K.
 and Wolfe, M.
 and Tsafnat, G.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,1789," george.karystianis@mq.edu.au
Guy Tsafnat


#Ask: I saw that the names of the studies you used are available, and that you shared the dictionaries. Are the labelled train/dev/evaluation data available? is the source code available?",,"""We have demonstrated that rules based on generic syntactic patterns in one corpus can be applied to other observational study design by simple interchanging the dictionaries aiming to identify certain characteristics (i.e., outcomes, exposures). At the document level, the recognised information can assist in the selection and categorization of studies included in a systematic review."" Multiple factors led to lower recall, so authors decided to maximise precision, eg. in population entity. ",,LS,Rule-based,"Precision, Recall, F1","Abstracts, Titles","Cohort, Cross sectional survey, Case control",5 already completed systematic reviews of environmental health exposure reviews in human population,NR,No,No,NR,No,No,No,"Error analysis, explanation of iterative process (first deriving rules from train data, then further development on a dev set)",General Architecture for Text Engineering: for creating rules,Yes,,"Discussion of changing exposure (or outcomes) terms in a dictionary, no changes to rules themselves are needed. ",No,Yes,Yes,"No, or probably no, not mentioned explicitly. ",Yes,"Unseen/ unexpected, complex (ie. more than 1 entity in a sentence), or generic/common lexical patterns cause false negatives (ie. lower recall). Their small set of studies from 5 SR might not be representative.","micro averages across different document-level mentions, macro averages across different characteristics.","number of reviews, documents per review, nr of positive instances. ",,No,Yes,"A discussion mentions that generic lexical patterns were not formulated as rules (which could have increased recall), and that a high-precision model was chosen instead. No alternative high-recall metrics are given.  ",Yes,Yes,process of deriving rules is described. Rules and dictionaries available as supplementary data., Confounders,"common entities, rules developed based on studies contained in 3 SR, tested on 2 unrelated reviews. The dataset might be to small to generalise.",No,Yes,No,Spans or annotations,"P, O, Country, Exposure",Entities,"TP, FP, FN","No hidden variables in manually crafted rules, all rules are explicitly mentioned and visible.",Yes; compl. different datasets (ie. diferent SRs; journals..),2017,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Combination of conditional random field with a rule based method in the extraction of PICO elements,"BACKGROUND: Extracting primary care information in terms of Patient/Problem, Intervention, Comparison and Outcome, known as PICO elements, is difficult as the volume of medical information expands and the health semantics is complex to capture it from unstructured information. The combination of the machine learning methods (MLMs) with rule based methods (RBMs) could facilitate and improve the PICO extraction. This paper studies the PICO elements extraction methods. The goal is to combine the MLMs with the RBMs to extract PICO elements in medical papers to facilitate answering clinical questions formulated with the PICO framework. METHODS: First, we analyze the aspects of the MLM model that influence the quality of the PICO elements extraction. Secondly, we combine the MLM approach with the RBMs in order to improve the PICO elements retrieval process. To conduct our experiments, we use a corpus of 1000 abstracts. RESULTS: We obtain an F-score of 80% for P element, 64% for the I element and 92% for the O element. Given the nature of the used training corpus where P and I elements represent respectively only 6.5 and 5.8% of total sentences, the results are competitive with previously published ones. CONCLUSIONS: Our study of the PICO element extraction shows that the task is very challenging. The MLMs tend to have an acceptable precision rate but they have a low recall rate when the corpus is not representative. The RBMs backed up the MLMs to increase the recall rate and consequently the combination of the two methods gave better results.
","Chabou, S.
 and Iglewski, M.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,563,"Target design likely RCTs, mix and other types. Very similar to the model presented in 6659 by same author, although features and CRF implementation vary, as well as testing data. 
#Ask for code",,"""The MLMs [Machine Learning Models] tend to have an acceptable precision rate but they have a low recall rate when the corpus is not representative. The RBMs [Rule-based Models] backed up the MLMs to increase the recall rate and consequently the combination of the two methods gave better results."", ""Our study of the PICO element extraction shows that the task is very challenging.""",,LS,"Rule-based, CRF, Other","Precision, Recall, F1","Abstracts, Titles","RCT, Mix","as described for ALTA challenge, NICTA-PIBOSO corpus (see Di 9695)",,No,Yes,,No,No,No,assesed multiple combinations of rule-based and ML systems,Mallet,Yes,"Cross-validation 5-fold for machine learning component: ""We executed a 5-fold cross validation to assess overfitting and robustness of the model""; parts of this system are rule-based",,Yes,No,Yes,"replace end-of-sentence characters, invalid decimal points, standardise section headers, and other",Yes,"""most of the training corpora used in the literature are either not representative in terms of size [8, 10, 13] or not well balanced in terms of:   the distribution of PICO elements [11, 12, 14] or  the abstract types (structured, unstructured) [5,6,7, 9]"", recall lower than precision due to frequency of entities, overlap of entities in sentences. Hyperparameter settings depend on quality of training corpus, noisy labels, potentially underestimated performance of the model",,"abstracts (1000), total and per-class nr of sentences described elsewhere",cTAKES,Yes,Yes,NR,No,Yes,"some CRF parameters described, eg. gaussioan prior 0.1, 1, 10, and 100",,"common entity/methods, same dataset was used by other authors becasue it is a shared task dataset",No,Yes,No,Spans or annotations,"P, IC, O",Sentences,None reported,,Yes; other description given,2018,,,,,,,,,,,,,,,,,,,,
Text or probably text file,An Ontology-Enabled Natural Language Processing Pipeline for Provenance Metadata Extraction from Biomedical Text (Short Paper),"Extraction of structured information from biomedical literature is a complex and challenging problem due to the complexity of biomedical domain and lack of appropriate natural language processing (NLP) techniques. High quality domain ontologies model both data and metadata information at a fine level of granularity, which can be effectively used to accurately extract structured information from biomedical text. Extraction of provenance metadata, which describes the history or source of information, from published articles is an important task to support scientific reproducibility. Reproducibility of results reported by previous research studies is a foundational component of scientific advancement. This is highlighted by the recent initiative by the US National Institutes of Health called 'Principles of Rigor and Reproducibility'. In this paper, we describe an effective approach to extract provenance metadata from published biomedical research literature using an ontology-enabled NLP platform as part of the Provenance for Clinical and Healthcare Research (ProvCaRe). The ProvCaRe-NLP tool extends the clinical Text Analysis and Knowledge Extraction System (cTAKES) platform using both provenance and biomedical domain ontologies. We demonstrate the effectiveness of ProvCaRe-NLP tool using a corpus of 20 peer-reviewed publications. The results of our evaluation demonstrate that the ProvCaRe-NLP tool has significantly higher recall in extracting provenance metadata as compared to existing NLP pipelines such as MetaMap.
","Valdez, J.
 and Rueschman, M.
 and Kim, M.
 and Redline, S.
 and Sahoo, S. S.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,3905,"#Ask: Code, labelled data? The links given in the publication seem to be broken unfortunately. 

Joshua Valdez  Satya S. Sahoo satya.sahoo@case.edu
Division of Medical Informatics and Electrical Engineering and Computer Science Department, Case Western Reserve University, Cleveland, OH, USA
",,'We demonstrate the effectiveness of ProvCaRe-NLP tool using a corpus of 20 peer-reviewed publications. The results of our evaluation demonstrate that the ProvCaRe-NLP tool has significantly higher recall in extracting provenance metadata as compared to existing NLP pipelines such as MetaMap.',,LS,Ontology pipeline,Recall,"Abstracts, Full texts, Titles","RCT, Animal studies, Mix",PubMed,,No,No,NR,No,No,No,"No individual scores for entities, no discussion or error analysis (as this is only a short paper)","cTAKES for tokenisation and baseline system, as well as ontologies integrated into the pipeline",No,NR,"Evaluation limited to sleep-disorder based corpus, no evaluation on external corpus. ",Yes,No,Yes,"Tokenisation, POS-tagging, shallow parsing. Post-processing",No,NR,Results are aggregated- no results per entity type,"Size (20 articles), unclear if any development data were used, no descriptions of entity chracteristics or frequency.",,Yes,Yes,NR,No,Yes,"Systema rchitecture and ontologies are described on a high level, but only few details are provided","O as ""Instruments"" or ""Study Variables""",Used several other tools in a comparative evaluation on their corpus. Evaluation is limited to a sleep-research-based corpus. ,No,Yes,No,"Ontology, Spans or annotations","P, IC, O, O (measurement instrument), Sections (Aim; Method etc.)",Entities,None reported,NR,One dataset/ NA,2016,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Towards Evidence-based Precision Medicine: Extracting Population Information from Biomedical Text using Binary Classifiers and Syntactic Patterns,"Precision Medicine is an emerging approach for prevention and treatment of disease that considers individual variability in genes, environment, and lifestyle for each person. The dissemination of individualized evidence by automatically identifying population information in literature is a key for evidence-based precision medicine at the point-of-care. We propose a hybrid approach using natural language processing techniques to automatically extract the population information from biomedical literature. Our approach first implements a binary classifier to classify sentences with or without population information. A rule-based system based on syntactic-tree regular expressions is then applied to sentences containing population information to extract the population named entities. The proposed two-stage approach achieved an F-score of 0.81 using a MaxEnt classifier and the rule- based system, and an F-score of 0.87 using a Nai've-Bayes classifier and the rule-based system, and performed relatively well compared to many existing systems. The system and evaluation dataset is being released as open source.
","Raja, K.
 and Dasot, N.
 and Goyal, P.
 and Jonnalagadda, S. R.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,3102,"Kalpana Raja,  Siddhartha R Jonnalagadda
Division of Health and Biomedical Informatics, Department of Preventive Medicine, Northwestern University Feinberg School of Medicine, Chicago, IL

mines some other study types besides RCT, eg. some SR. General-domain test on non-RCT data?

#ASK:  Are data and code available? I saw that the text mentioned some supplementary material and a Git repository (reference 13) but I was not able to find/download the supplements, and the git repository is not visible to me.",,"""Our work aimed to extract population information pertaining to evidence for supporting the retrieval of citations and ultimately evidence-based precision medicine. We used three different methods: rule-based system, MaxEnt classifier with rule-based system, and NaÃ¯ve-Bayes classifier with rule-based system. In all the three methods, we used a rule-based system to extract the population named entities. F-score of the best classifier is 90% and that of whole system is 87%. We are optimistic about the use of our system to advance precision medicine, especially in being able to deliver individualized evidence summaries at the point-of-care.""",,LS,"Rule-based, Other ML classifier","Precision, Recall, F1","Abstracts, Titles","RCT, Other",medline,,Yes,Yes,NR,No,No,No,error analysis,"Stanford lexical parser, Stanford Tregex (Tree regular expressions) parser,  MALLET (Machine Learning for LanguagE Toolkit)",Yes,crossvalidation (5-fold) for ML systems,evaluation on completely different dataset and domain: UpToDate clinical knowledge system data,Yes,No,Yes,"No pre-processing before parsing. ""removal of non-ASCII or Unicode characters, conversion to lower case, removal of stop words, and lemmatization."" Uni- and bigram features for MaxEnt classifier, or 50 terms (uni- or bigram) with most information gain for NB.",Yes," For rule-based system: Complex sentence structure, eg. higher distance between patient and disease terms. Incorrect parsing of sentence structue; Comparability: Accuracy of model is not comparable with results from similar systems because datasets differ.",,"size, distribution of some entities",,Yes,Yes,"higher amount of Tregex patterns for higher recall, causing lower precision. They decided to limit Tregex patterns to decrease the percentage of false positives. ",Yes,Yes,parameters for parsing sentence structure are described,,"They published a comparison with another tool. However, the dataset was restricted to the cardiovascular domain, and is not available.",No,Yes,No,"Spans or annotations, Structured text and summary","P, IC, Age, Gender, P (Condition or disease)","Entities, Sentences",None reported,"Error analysis don, but only for regular expression patterns. ",Yes; just ratio given,2016,,,,,,,,,,,,,,,,,,,,
"XML, HTML",Automated information extraction of key trial design elements from clinical trial publications,"Clinical trials are one of the most valuable sources of scientific evidence for improving the practice of medicine. The Trial Bank project aims to improve structured access to trial findings by including formalized trial information into a knowledge base. Manually extracting trial information from published articles is costly, but automated information extraction techniques can assist. The current study highlights a single architecture to extract a wide array of information elements from full-text publications of randomized clinical trials (RCTs). This architecture combines a text classifier with a weak regular expression matcher. We tested this two-stage architecture on 88 RCT reports from 5 leading medical journals, extracting 23 elements of key trial information such as eligibility rules, sample size, intervention, and outcome names. Results prove this to be a promising avenue to help critical appraisers, systematic reviewers, and curators quickly identify key information elements in published RCT articles.
","de Bruijn, B.
 and Carini, S.
 and Kiritchenko, S.
 and Martin, J.
 and Sim, I.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,834,"Berry de Bruijn,  Ida Sim
Institute for Information Technology, National Research Council, Ottawa, Ontario;
2University of California San Francisco, San Francisco, CA

#ASK: data, code",,"""These preliminary results demonstrate the systemâ€™s ability to assist critical appraisers, systematic reviewers, and curators in extracting essential information from RCT reports."", ""weak rules can be used to extract the exact snippets from candidate sentences"". ""Since the system is not intended to replace human annotators, but rather to assist them by proposing possible values for required information elements, we consider the achieved performance as promising.""",,LS,"Rule-based, SVM","Precision, Recall, Other","Full texts, Titles, Abstracts",RCT,"five journals: PLoS Clinical Trials, NEJM, Lancet, JAMA, and Annals of Internal Medicine, used random sample",NR,No,Yes,NR,No,No,No,Error analysis and description of challenging information elements,NR,Yes,"cross-validation on training set (leave one out), further evaluation on held-out set of 10 unseen articles",NR,Yes,No,Yes,"sentence splitting, post-processing, normalisation: integers, â€˜unitsâ€™, â€˜measurementsâ€™, and â€˜datesâ€™. ""Each sentence is annotated according to the (sub) section in which it occurred (â€˜Abstractâ€™, â€˜Methodsâ€™, etc.), special characters and symbols are dealt with, and, for a few concepts, occurrences are tagged as such. E.g., â€œ17 women participatedâ€ gets tagged into â€œ<integer>17</integer> <person>women</person> participatedâ€. Other general concepts that are tagged include â€˜unitsâ€™, â€˜measurementsâ€™, and â€˜datesâ€™.""",Yes,"'funding organization name' and n'ame of the experimental treatment' elements were not extracted to the author's satisfaction. This is due to variations in natural language, and variations of interventions: eg. drugs, therapies, differences in procedures.","partial matching for recall and precision, using top 5 sentences.","88 articles, distribution of entities",,Yes,No,NR,No,Yes,"SVM parameters or kernel unclear, rules only described on a very high level."," start and end date of enrolment, first author's name, publication date, DOI, make of device, manufacturer of device, primary and secondary outcome time points","common entities, general-topic text",No,No,No,Structured text and summary,"IC (per arm), IC (dose; duration and others), O (time point), O (primary or secondary outcome), N (total), Eligibility criteria, Enrolment dates, Funding org, Grant number, Early stopping, Trial registration, Other","Entities, Sentences",None reported,NR,Yes; random splits and ratio given,2008,,,,,,,,,,,,,,,,,,,,
PDF,A Novel Framework to Expedite Systematic Reviews by Automatically   Building Information Extraction Training Corpora,"A systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question. The process is manual and involves lot of time. A tool to automate this process is lacking. The aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication, without having to go through the expensive task of manual annotation to build gold standards for each data element type. The system is developed in two stages. Initially, it uses information contained in existing systematic reviews to identify the sentences from the PDF files of the included references that contain specific data elements of interest using a modified Jaccard similarity measure. These sentences have been treated as labeled data.A Support Vector Machine (SVM) classifier is trained on this labeled data to extract data elements of interests from a new article. We conducted experiments on Cochrane Database systematic reviews related to congestive heart failure using inclusion criteria as an example data element. The empirical results show that the proposed system automatically identifies sentences containing the data element of interest with a high recall (93.75%) and reasonable precision (27.05% - which means the reviewers have to read only 3.7 sentences on average). The empirical results suggest that the tool is retrieving valuable information from the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction from biomedical research publications. The future scope of this work is to generalize this information framework for all types of systematic reviews.
","['Tanmay Basu', 'Shraman Kumar', 'Abhishek Kalyan', 'Priyanka Jayaswal', 'Pawan Goyal', 'Stephen Pettifer', 'Siddhartha R. Jonnalagadda']
 ","https://export.arxiv.org/abs/1606.06424
",01/05/2020,,Include,TRUE,01/05/2020,Include,9471,"Tanmay Basu, Siddhartha R. Jonnalagadda
#Ask: Code, data
Division of Biomedical Informatics, Feinberg School of Medicine, Northwestern University, USA
",,"""The  empirical  results  suggest  that  the  tool  is  retrieving  valuable  information  from  the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction from biomedical research publications. The future scope of this work is to generalize this information framework for all types of systematic reviews""",,LS,"SVM, PDF extraction","Precision, Recall, Other","Full texts, Titles, Abstracts",RCT,"Cochrane SRs on heart failure, PubMed",,Yes,Yes,NR,No,No,No,"parameters for creation of gold standard and classifier discussed, SVM parameters discussed in detail. ","PDFx tool, scikit-learn",No,cross-validation,"Describes caveat about this classifer's performance on unrelated domain literature. No comparison was made, although a completely different data set, random RCTs from Cochrane library, was used for evaluation after training w. cross-validation. 
",No,No,Yes,"text conversion to features described, nofurther info",Yes,"No guarantee that classifier works in all contexts (ie medical domains such as heart, neurology, psychology, and data types. Performance also strongly depends on some hyperparameters, which could easily lead to models that include many false-positives. ",Sentences needed to screen per article in order to find one relevant sentence.,"69 full-texts from 31 SR as goldstandard train set with 122 positive instances and 12651 negative instances.  24 full texts as test set, manually labelled.",,No,Yes,discussion about hyperparameters and influence on false-negative retrieval,Yes,No,"Hyperparameters for creation of gold standard and classifier discussed, implementation details discussed",,"Trained on specific subset of Cochrane library SR- included RCTs, tested on random other RCTs ""mostly related to heart disease"". ",Yes,Yes,No,Spans or annotations,Eligibility criteria,Sentences,"TP, FP, FN","discusses that prevalence of very common phrases could bias the clasification, and shows examples that the system also classifed positive instances in the absence of these common phrases
","Yes; random splits and ratio given, Yes; compl. different datasets (ie. diferent SRs; journals..)",2016,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Data Mining in Clinical Trial Text: Transformers for Classification and   Question Answering Tasks,"This research on data extraction methods applies recent advances in natural language processing to evidence synthesis based on medical texts. Texts of interest include abstracts of clinical trials in English and in multilingual contexts. The main focus is on information characterized via the Population, Intervention, Comparator, and Outcome (PICO) framework, but data extraction is not limited to these fields. Recent neural network architectures based on transformers show capacities for transfer learning and increased performance on downstream natural language processing tasks such as universal reading comprehension, brought forward by this architecture's use of contextualized word embeddings and self-attention mechanisms. This paper contributes to solving problems related to ambiguity in PICO sentence prediction tasks, as well as highlighting how annotations for training named entity recognition systems are used to train a high-performing, but nevertheless flexible architecture for question answering in systematic review automation. Additionally, it demonstrates how the problem of insufficient amounts of training annotations for PICO entity extraction is tackled by augmentation. All models in this paper were created with the aim to support systematic review (semi)automation. They achieve high F1 scores, and demonstrate the feasibility of applying transformer-based classification methods to support data mining in the biomedical literature.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs https://arxiv.org/abs/2001.11268; Years 2020; Authors Lena Schmidt; Julie Weeds; Julian P. T. Higgins; Deduplication_Notes ; X Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 1; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 1] CHECK DUPLICATE [Source dblp; Title Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks.; Abstract NaN; Keywords NaN; DOIs 10.5220/0008945700830094; DOI_original 10.5220/0008945700830094; URLs https://doi.org/10.5220/0008945700830094; Years 2020; Authors Lena Schmidt; Julie Weeds; Julian P. T. Higgins; Deduplication_Notes ; X Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 1; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 1]
","['Lena Schmidt', 'Julie Weeds', 'Julian P. T. Higgins']
 ","https://export.arxiv.org/abs/2001.11268
",01/05/2020,,Include,TRUE,01/05/2020,Include,9964,,,"Ambiguity in sentence prediction tasks can be tackled by using multi-label, multi-class classifiers. Augmentation helps to avoid problems related to insufficient training data: training data from different domains can be used to improve classifier perfomance and robustness.",,LS,BERT incl. biomedical versions,"Precision, Recall, F1","Abstracts, Titles",RCT,"SQuAD, EBM-NLP (Nye 2018), 10346-Jin and Szolovits (2018) PICO sentence corpus. All open-source and available online ",,Yes,No,"https://github.com/L-ENA/HealthINF2020 for original code

https://www.kaggle.com/lenaschmidt0493/qa-integrated-biomedical-ner-classifier-for-pico 
latter is more accessible and code can be executed in online notebook and visualised",Yes,No,Yes,"Error analysis and examples for misclassifications given. A practical test was carried out later (Using a COVID-19 dataset, not described in original paper)","BERT tensorflow, Transformers, Google Colab",Yes,NR,"Small test on multilingual (non-english) inputs for two abstracts. Later, a big test was conducted on a COVID-19 dataset using the described classifiers (not described in present publication): https://www.kaggle.com/lenaschmidt0493/qa-integrated-biomedical-ner-classifier-for-pico 
",Yes,No,Yes,"As described in EBM-NLP (Nye 2018), 10346 Jin and Szolovits (2018) PICO sentence corpus",Yes,"Noisy sentence labels make classifier scores less reliable. Projection into multi-label domain adds additional noise. Ambiguity is a problem, especially with sentence-level data from medical abstracts. The transfer-lerning capabilities of the transformer model were only evaluated in short qualitative test.",,"distribution of entities, size of corpora",,Yes,Yes,plots of trade-off and impact of different classification thresholds.,Yes,Yes,Parameters described in paper and available along with the code on git,Single and multilabel comparison,"common entities, using multiple benchmark datasets",No,Yes,Yes,Spans or annotations,"P, IC, O","Entities, Sentences",None reported,NR,Yes; random splits and ratio given,2020,,,,,,,,,,,,,,,,,,,,
PDF,Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision,"Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations. To this end, we propose a novel method - supervised distant supervision (SDS) - that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by learning to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction.
","Wallace, B. C.
 and Kuiper, J.
 and Sharma, A.
 and Zhu, M. B.
 and Marshall, I. J.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,4020,"Output format: add Word. 

ASK: Is source code available?
Is there a runnable version of this software (or is it fully integrated into RobotReviewer? By looking at the repo I assume that the scibert model now does the PICO mining in full?)
Data: Is the dataset available somewhere? I found this, but was unsure if it is the train/test data: https://raw.githubusercontent.com/bwallace/CDSR_NN/master/ctg_pico_cuis.csv

byron@ccs.neu.edu; iain.marshall@kcl.ac.uk 
",,"""We demonstrated that [supervised distant supervision] consistently improves performance compared to baseline models that exploit either distant or direct supervision only, and generally also outperforms a previously proposed approach to combining direct and distant supervision""",,LS,"Rule-based, CRF, SVM, Other","F1, Other","Abstracts, Titles",RCT,"â€œOperationally, we ranked all sentences in a given article with respect to the raw number of word (unigram) tokens shared with the CDSR summary, excluding stop words. The top 10 sentences that shared at least 4 tokens with the summary were considered positiveâ€. Did some manual labelling as gold standard â€œUltimately, we acquired a set of 2,821 labels on sentences from 133 unique articles; these comprise 1009, 1006 and 806 sentences corresponding to â€˜participantsâ€™, â€˜interventionsâ€™ and â€˜outcomesâ€™, respectively.â€",https://www.robotreviewer.net/,Yes,Yes,"This code might be integrated into RobotReviewer to some extent: https://github.com/ijmarshall/robotreviewer but no source code related to this publication specifically was found. 
 ",No,Yes,No,"SDS tested on completely unrelated twenty newsgroup corpus, crossvalidation (5-fold) on 133 articles and additional test on 50 manually labeled held-out articles.",scikit-learn,Yes,"crossvalidation, testing on held-out gold-standard data","Distant supervision experiment on unrelated datasets (eg twenty newsgroup corpus), RobotReviewer software can be used on any PDF and validation studies exist, although it is unclear if the code from this publication is used for those validations.",Yes,No,Yes,excluded stopwords,Yes,Potentially noisy labels due to the distant supervision approach ,"NDCG  (normalized Discounted Cumulative Gain). ""Metrics are calculated for each article separately and we then report mean and standard deviations over these. NDCG and only among top 20 sentences, precision among top 3,10,20 sentences""","number of full texts (~12500 per class), number of sentences per full text (~335), number of held-out articles annotated: 50 and 133 for different evaluations",,Yes,Yes,Precision is measured wrt. different number of top n sentences. Recall is not reported. ,Yes,Yes,"Novel distant supervision, classifier descriebed (similar to SVM?). For scikit-learn they used default parameters and described changes in footnotes. ",Training data from Cochrane Database RCT data (free text summaries of PICOs from reviews),"common entities, RCTs of different topics, large dataset size",No,Yes,No,"JSON, HTML, Other","P, IC, O",Sentences,None reported,NR,Yes; compl. different datasets (ie. diferent SRs; journals..),2016,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Classification of PICO elements by text features systematically extracted from PubMed abstracts,"We propose and evaluate a systematic approach to detect and classify Patient/Problem, Intervention, Comparison and Outcome (PICO) from the medical literature. The training and test corpora were generated systematically and automatically from structured PubMed abstracts. 23,472 sentences by exact pattern match of head words of P-I-O categories. Afterward, the terms with top frequencies were used as the features of NaÃ¯ve Bayesian classifier. This approach achieves F-measure values of 0.91 for Patient/Problem, 0.75 for Intervention and 0.88 for Outcome, comparable to previous studied based on mixed textural, paragraphical, and semantic features. In conclusion, we show that by stricter pattern matching criteria of training set, detection and classification of PICO elements can be reproducible with minimal expert intervention. The results of this work are higher than previous studies.
","K. Huang
 and C. C. Liu
 and S. Yang
 and F. Xiao
 and J. Wong
 and C. Liao
 and I. Chiang
 ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6122608
",01/05/2020,,Include,TRUE,01/05/2020,Include,4564,"K. Huang kimikohuang@ntu.edu.tw, chliu@ntu.edu.tw ; ijchiang@tmu.edu.tw
#Ask: Code, data, Is this data set overlapping with the one used in this publication: ""PICO element detection in medical text without metadata: are first sentences enough? (2013)""",,"""In conclusion, we show that by stricter pattern matching criteria of training set, detection and classification of PICO elements can be  reproducible  with  minimal  expert  intervention.  The  results  of  this work are higher than previous studies""",,LS,"Other ML classifier, Other, Multi-layer perceptron","Precision, Recall, F1","Abstracts, Titles","RCT, Mix",PubMed,,No,Yes,NR,No,No,No,Discussion of different feature selection methods is given.,"NLTK WordNet stemmer, unclear how classifiers were implemented",Yes,cross-validation (10-fold),NR,No,No,Yes,stemming,Yes,"They discuss difficulty to compare methods due to: different datasets, different pre-processing and feature selection methods",,"size: 23,472 sentences from structured abstracts, 8448 P-sentences, 5615 I-sentences and 9409 O-sentences obtained by exact matching of headings",ensemble of classifiers,Yes,No,"effect of number of selected features on P, R, F1 given in table and plots. ",Yes,Yes,No implementation details or hyperparameters given. Feature selection strategies are discussed. ,,"common entities, general-topic text. A comparison with results from other publications is given, but caveats wrt. comparability of these results are discussed. ",No,No,No,Spans or annotations,"P, IC, O",Sentences,None reported,NR,Yes; random splits and ratio given,2011,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Improving reference prioritisation with PICO recognition,"BACKGROUND: Machine learning can assist with multiple tasks during systematic reviews to facilitate the rapid retrieval of relevant references during screening and to identify and extract information relevant to the study characteristics, which include the PICO elements of patient/population, intervention, comparator, and outcomes. The latter requires techniques for identifying and categorising fragments of text, known as named entity recognition. METHODS: A publicly available corpus of PICO annotations on biomedical abstracts is used to train a named entity recognition model, which is implemented as a recurrent neural network. This model is then applied to a separate collection of abstracts for references from systematic reviews within biomedical and health domains. The occurrences of words tagged in the context of specific PICO contexts are used as additional features for a relevancy classification model. Simulations of the machine learning-assisted screening are used to evaluate the work saved by the relevancy model with and without the PICO features. Chi-squared and statistical significance of positive predicted values are used to identify words that are more indicative of relevancy within PICO contexts. RESULTS: Inclusion of PICO features improves the performance metric on 15 of the 20 collections, with substantial gains on certain systematic reviews. Examples of words whose PICO context are more precise can explain this increase. CONCLUSIONS: Words within PICO tagged segments in abstracts are predictive features for determining inclusion. Combining PICO annotation model into the relevancy classification pipeline is a promising approach. The annotations may be useful on their own to aid users in pinpointing necessary information for data extraction, or to facilitate semantic search.
","Brockmeier, A. J.
 and Ju, M.
 and Przybyla, P.
 and Ananiadou, S.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,456,"Paper mines PICOs specifically for using them as clssification features while supporting the screening phase of a review. Ignoring the screening classifier for the purpose of this review.

#Ask: Code
",,"""Words within PICO tagged segments in abstracts are predictive features for determining inclusion. Combining PICO annotation model into the relevancy classification pipeline is a promising approach. The annotations may be useful on their own to aid users in pinpointing necessary information for data extraction, or to facilitate semantic search.""",,LS,"CRF, Word embedding, LSTM","Precision, Recall, F1, WSS@95","Abstracts, Titles",RCT,EBM-NLP (Nye 2018),,Yes,Yes,NR,Yes,No,No,Detailled assessment of PICOs with strongest influence.,"GENIA tagger and sentence splitter. Neural network implementation is unclear, MALLET for LDA",Yes,"early stopping, dropout, L2-regularisation, weight decay in NER network. ",tested on a variety of different review data sets,Yes,Yes,Yes,"See Nye 2018 for PICO training data. Describes lowercasing, lemmatisation, digit normalisation to 0.",Yes,Challenging to find best hyperparameters,,See Nye 2018,,Yes,Yes,NR,No,Yes,Neural architecture and dimensions described in detail.,,used benchmark EBM-NLP (Nye 2018),No,Yes,No,Spans or annotations,"P, IC, O",Entities,None reported,NR,Yes; random splits and ratio given,2019,,,,,,,,,,,,,,,,,,,,
PDF,Extractive text summarization system to aid data extraction from full text in systematic review development,"OBJECTIVES: Extracting data from publication reports is a standard process in systematic review (SR) development. However, the data extraction process still relies too much on manual effort which is slow, costly, and subject to human error. In this study, we developed a text summarization system aimed at enhancing productivity and reducing errors in the traditional data extraction process. METHODS: We developed a computer system that used machine learning and natural language processing approaches to automatically generate summaries of full-text scientific publications. The summaries at the sentence and fragment levels were evaluated in finding common clinical SR data elements such as sample size, group size, and PICO values. We compared the computer-generated summaries with human written summaries (title and abstract) in terms of the presence of necessary information for the data extraction as presented in the Cochrane review's study characteristics tables. RESULTS: At the sentence level, the computer-generated summaries covered more information than humans do for systematic reviews (recall 91.2% vs. 83.8%, p<0.001). They also had a better density of relevant sentences (precision 59% vs. 39%, p<0.001). At the fragment level, the ensemble approach combining rule-based, concept mapping, and dictionary-based methods performed better than individual methods alone, achieving an 84.7% F-measure. CONCLUSION: Computer-generated summaries are potential alternative information sources for data extraction in systematic review development. Machine learning and natural language processing are promising approaches to the development of such an extractive summarization system.
","Bui, D. D. A.
 and Del Fiol, G.
 and Hurdle, J. F.
 and Jonnalagadda, S.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,473,#ASK: Data (annotated RCTs) and code available?,Autom. other SR task,"""Computer-generated summaries are potential alternative information sources for data extraction in systematic review development. Machine learning and natural language processing are promising approaches to the development of such an extractive summarization system.""",,LS,"Rule-based, SVM, PDF extraction","Precision, Recall, F1","Full texts, Titles, Abstracts",RCT,RCTs included in 8 Cochrane systematic reviews,,No,No,NR,No,No,No,Error analysis,"PDFBox, Stanford CoreNLP, Weka, MetaMap",Yes,NR,NR,Yes,Yes,Yes,"Yes, number normalisation, acronyms replaced with expanded form,  filter table, figures, metadata, introduction, citation and sentences relating to other studies.",Yes,Human involvment still needed (semi-automation),,number of articles (48) and dirstribution of entities,,Yes,Yes,NR,No,No,"hyperparameters (default parameters from toolkit), description of rules","Some phrase level/ sentence level mining, alhough recall, precision, F1 are reported at entity level.",The system was developed and tested on RCT data limited to the general topic of 'heart and circulation',No,Yes,No,Structured text and summary,"P, IC, O, N (per arm), N (total)","Entities, Sentences",None reported,NR,Yes; just ratio given,2016,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"Developing a fully automated evidence synthesis tool for identifying, assessing and collating the evidence","Evidence synthesis is a key element of evidence-based medicine. However, it is currently hampered by being labour intensive meaning that many trials are not incorporated into robust evidence syntheses and that many are out of date. To overcome this, a variety of techniques are being explored, including using automation technology. Here, we describe a fully automated evidence synthesis system for intervention studies, one that identifies all the relevant evidence, assesses the evidence for reliability and collates it to estimate the relative effectiveness of an intervention. Techniques used include machine learning, natural language processing and rule-based systems. Results are visualised using modern visualisation techniques. We believe this to be the first, publicly available, automated evidence synthesis system: an evidence mapping tool that synthesises evidence on the fly.
","Brassey, J.
 and Price, C.
 and Edwards, J.
 and Zlabinger, M.
 and Bampoulidis, A.
 and Hanbury, A.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,444,"Did sentiment analysis, RoB assessment
Data-mining for PICOs focused on titles
#ASK: data, code?

support@tripdatabase.com
jon.brassey@tripdatabase.com

#Ask for data, code (ie. implementation of rules)",,"P, IC, N mining was not main focus of paper, but a conclusion from this section was : ""Due  to  the  complex  text  structure  of  abstracts,  we  decided  to  focus  on  the  titles  of  RCTs.  In  most  cases,  the  title  already  contained all of the desired PIC information""",,LS,Rule-based,"Precision, Recall, Accuracy","Abstracts, Titles",RCT,"sampled randomly from PubMed (restricted to human medicine, RCT, English)",https://www.tripdatabase.com/#pico,No,No,NR,No,Yes,No,"Assessed P, I, C in titles, N in abstracts. Error analysis done usually as part of rule-development, although not explicitly described",NR,Yes,NA for rule-base,NR,Yes,Yes,Yes,NR,Yes,NR,"Accuracy reported for N-mining, P/R for P, IC mining","number of articles (1750), 241 used to develop N-mining rules, PIC mined on titels mostly",,No,No,NR,No,Yes,rules not reported,,"large dataset, not restricted to specific topic, usable via app",No,No,No,Spans or annotations,"P, IC (per arm), N (total)",Entities,None reported,"NA fur rule-based system, all rules are explicitly implemented and therefore visible",Yes; random splits and ratio given,2019,,,,,,,,,,,,,,,,,,,,
PDF,A Neural Candidate-Selector Architecture for Automatic Structured Clinical Text Annotation,"We consider the task of automatically annotating free texts describing clinical trials with concepts from a controlled, structured medical vocabulary. Specifically we aim to build a model to infer distinct sets of (ontological) concepts describing complementary clinically salient aspects of the underlying trials: the populations enrolled, the interventions administered and the outcomes measured, i.e., the PICO elements. This important practical problem poses a few key challenges. One issue is that the output space is vast, because the vocabulary comprises many unique concepts. Compounding this problem, annotated data in this domain is expensive to collect and hence sparse. Furthermore, the outputs (sets of concepts for each PICO element) are correlated: specific populations (e.g., diabetics) will render certain intervention concepts likely (insulin therapy) while effectively precluding others (radiation therapy). Such correlations should be exploited. We propose a novel neural model that addresses these challenges. We introduce a Candidate-Selector architecture in which the model considers setes of candidate concepts for PICO elements, and assesses their plausibility conditioned on the input text to be annotated. This relies on a 'candidate set' generator, which may be learned or relies on heuristics. A conditional discriminative neural model then jointly selects candidate concepts, given the input text. We compare the predictive performance of our approach to strong baselines, and show that it outperforms them. Finally, we perform a qualitative evaluation of the generated annotations by asking domain experts to assess their quality.
","Singh, G.
 and Marshall, I. J.
 and Thomas, J.
 and Shawe-Taylor, J.
 and Wallace, B. C.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,3546,#Ask: Is Code/labelled data free to share? ,,"""Our model defines a novel Candidate-Selector architecture composed of two parts: candidate generation and then (possibly joint) selection and assignment of these candidates to constituent PICO elements. In our CS-Joint model the selection model is a Convolutional Neural Network jointly conditioned on a triplet of structured PICO UMLS terms and the free-text to be annotated, thus realizing a fully joint approach. This model achieved consistently strong empirical results, besting alternative approaches.""",,LS,"Word embedding, CNN, Other","Precision, Recall, F1, Other","Full texts, Titles, Abstracts",RCT,Dataset from Cochrane,,Yes,Yes,NR,No,No,No,description of different models and different relaxation techniques to influence precision and recall. A qualitative analysis of model output is described. ,MetaMap. ,Yes,"Final model tested on completely new dataset, unclear if this dataset's origin is also Cochrane. ",Detailled analysis how model reacts to previously unseen data. No evaluation on any benchmarking dataset. ,Yes,No,Yes,MetaMap for parsing input text and generating input. ,Yes,"Unseen concepts are a big challenge for data mining in healthcare, because a CNN model can not predict previously unseen concepts. Therefore, a different approach needed to be integated in the architecture. The large output label space is a further challenge for the task of assigning specific concepts (which goes one step further than just identifying spans). Short source texts are source of errors. ",micro-scores for all metrics; recall at â€˜2-hopsâ€™ or 'k-hops',"Distribution of entities reported, dataset size unclear. Held-out data: '88 instances, annotated in total with 76, 87, and 139 unique concepts corresponding to population, intervention/comparator and outcomes, respectively'",DeepWalk concept embeddings,Yes,Yes,"Influence of n-hops on precision and recall is plotted. Not a traditional trade-off, but it shows how close many predictions are to original concepts. ",Yes,Yes,"Description of the selector and candidate generation architectures, as well as network dimensions and hyperparameters for sampling and training. ",assigns mined fields to specific UMLS concepts. ,"'common entities, general-topic text'",Yes,Yes,No,"JSON, HTML, Other","P, IC, O",Entities,None reported,"Length of input text is discussed as source for errors. Previously unseen entities, and their classification results, are discussed. ","Yes; random splits and ratio given, Yes; compl. different datasets (ie. diferent SRs; journals..)",2017,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Finding medication doses in the liteature,"Medication doses, one of the determining factors in medication safety and effectiveness, are present in the literature, but only in free-text form. We set out to determine if the systems developed for extracting drug prescription information from clinical text would yield comparable results on scientific literature and if sequence-to-sequence learning with neural networks could improve over the current state-of-the-art. We developed a collection of 694 PubMed Central documents annotated with drug dose information using the i2b2 schema. We found that less than half of the drug doses are present in the MEDLINE/PubMed abstracts, and full-text is needed to identify the other half. We identified the differences in the scope and formatting of drug dose information in the literature and clinical text, which require developing new dose extraction approaches. Finally, we achieved 83.9% recall, 87.2% precision and 85.5% F<sub>1</sub> score in extracting complete drug prescription information from the literature.
","Demner-Fushman, D.
 and Mork, J. G.
 and Rogers, W. J.
 and Shooshan, S. E.
 and Rodriguez, L.
 and Aronson, A. R.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,876,"Dina Demner-Fushman, National Library of Medicine, National Institutes of Health, HHS, Bethesda, MD, USA

#Ask: Thank you for sharing the data and implementation details! Are the classifiers developed for this paper currently used in practice (eg. in a tool, or by anyone in the author team). Secondly, are you aware of anyone who has used your data set and trained different classifiers?",,"""We identified the differences in the scope and formatting of drug dose information in the literature and clinical text, which require developing new dose extraction approaches. Finally, we achieved 83.9% recall, 87.2% precision and 85.5% F1 score in extracting complete drug prescription information from the literature.""",,LS,"CRF, Word embedding, Character embedding, LSTM","Precision, Recall, F1","Abstracts, Full texts, Titles","RCT, Animal studies, Mix",PMC documents,,No,No,https://ii.nlm.nih.gov/DataSets/index.shtml,Yes,No,Yes,"error analysis, discussing idiosyncrasy","Tensorflow, MedEx UIMA 1.3.7",Yes,NR,NR,Yes,No,Yes,"masking certain entities such as medication names and units, code available",Yes,"Variations in natural language when describing drugs, doses, and, non-numerical descriptions such as ""low-dose aspirin"" poses challenge. Also, descriptions of cell medium and bacterical cultures can be confused with dosages due to identical patterns. Character conversions to ASCII can introduce errors and reduced readability. Annotator disagreements are time-intensive if reconciled.",given only for Dose & Strength entities," 694 documents, entity distribution",,Yes,Yes,NR,No,Yes,implementation (code) available,drug form; drug reason,"availability of data should lead to future comparisons. Due to small/selective dataset and fine-grained entities, a direct comparison to previous work is probably not possible",No,Yes,No,Spans or annotations,"IC (dose; duration and others), Other",Entities,None reported,NR,Yes; compl. different datasets (ie. diferent SRs; journals..),2018,,,,,,,,,,,,,,,,,,,,
"Text or probably text file, PDF",Automating Biomedical Evidence Synthesis: RobotReviewer,"We present RobotReviewer, an open-source web-based system that uses machine learning and NLP to semi-automate biomedical evidence synthesis, to aid the practice of Evidence-Based Medicine. RobotReviewer processes full-text journal articles (PDFs) describing randomized controlled trials (RCTs). It appraises the reliability of RCTs and extracts text describing key trial characteristics (e.g., descriptions of the population) using novel NLP methods. RobotReviewer then automatically generates a report synthesising this information. Our goal is for RobotReviewer to automatically extract and synthesise the full-range of structured data needed to inform evidence-based practice.
","Marshall, I. J.
 and Kuiper, J.
 and Banner, E.
 and Wallace, B. C.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,2392,"Paper on high-level system that does data extraction. Some related papers describe the machine-learning approaches in more detail, see https://www.robotreviewer.net/publications. Some of these papers describe current or outdated RobotReviewer classifiers. Here, we try to give an overview of the system as a whole, please see related publications:  4020 for more info,
",,"'RR used in a fully automatic workflow (without manual checks) might improve upon relying on the source articles alone, particularly given those in clinical practice are unlikely to have time to read the full texts. To explore how automation should be used in practice, we plan to experimentally evaluate RR in real-world use: in terms of time saved, user experience, and the resultant review quality.'",,LS,"Word embedding, PICO embedding, CNN, SVM, PDF extraction, Other",AUC-ROC,"Abstracts, Full texts, Titles",RCT,see 4020 (Wallace),https://www.robotreviewer.net,Yes,Yes,"https://github.com/ijmarshall/robotreviewer
https://figshare.com/articles/Spa/997707
",No,Yes,Yes,see 4020 (Wallace) and related papers,see 4020 (Wallace),Yes,"'common entities, general-topic text'",Psychology-domain example is given on the project's website. ,Yes,No,Yes,tokenization,Yes,NR,,see 4020 (Wallace),supervised distant supervision,Yes,Yes,NR,No,Yes,"High-level description, no hyperparameters given. See: 4020 (Wallace et. al), 
SpÃ¡ J. Kuiper I. J. Marshall B. C. Wallace M. A. Swertz (2014),
Marshall, I. J., Kuiper, J., & Wallace, B. C. (2014). Automating Risk of Bias Assessment for Clinical Trials. In Proceedings of the ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB) (pp. 88â€“95). ACM","Socio-economic status, ","'common entities, general-topic text'",No,Yes,No,Structured text and summary,"P, P (Condition or disease), IC, O, Age, Gender, Randomisation, Blinding, Design, Eligibility criteria, Race, Other","Entities, Sentences",None reported,NR,Yes; random splits and ratio given,2017,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Pretraining to Recognize PICO Elements from Randomized Controlled Trial Literature,"PICO (Population/problem, Intervention, Comparison, and Outcome) is widely adopted for formulating clinical questions to retrieve evidence from the literature. It plays a crucial role in Evidence-Based Medicine (EBM). This paper contributes a scalable deep learning method to extract PICO statements from RCT articles. It was trained on a small set of richly annotated PubMed abstracts using an LSTM-CRF model. By initializing our model with pretrained parameters from a large related corpus, we improved the model performance significantly with a minimal feature set. Our method has advantages in minimizing the need for laborious feature handcrafting and in avoiding the need for large shared annotated data by reusing related corpora in pretraining with a deep neural network.
","Kang, T.
 and Zou, S.
 and Weng, C.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,1771,"2019;  Chunhua Weng ude.aibmuloc@auhnuhc Tian Kang tk2624@cumc.columbia.edu ; 
Annotator agreement:0.916 for P, 0.72 for O, overall agreement 0.83;

#ASK: You also added a BERT model after publication (saw this in GitHub code repository), is there an updated version of this paper, or an evaluation for using the BERT model? 2nd: I saw some test files in the git repository, but I was wondering if the full data for the 170 RCTs that you annotated yourself is available? ask just confirming: you did not apply any pre-processing or normalization to the text in your corpus?",,"""By initializing our model with pretrained parameters from a large related corpus, we improved the model performance significantly with a minimal feature set. Our method has advantages in minimizing the need for laborious feature handcrafting and in avoiding the need for large shared annotated data by reusing related corpora in pretraining with a deep neural network.""",,LS,"CRF, Word embedding, Character embedding, LSTM, Other","Precision, Recall, F1","Abstracts, Titles",RCT,Medline and EBM-NLP,"Code and usage examples are given: https://github.com/Tian312/PICO_Parser
",Yes,Yes,https://github.com/Tian312/PICO_Parser,No,No,Yes,"Short error analysis done, results discussed for different model/training inputs.","Tensorflow, QuickUMLS",Yes,cross-validation (6-fold),tested and compared with EBM-NLP benchmark,Yes,No,Yes,"EBM-NLP as described in Nye 2018, any further pre-processing is not reported. Post-processing and entity normalisation is described. ",Yes,"Variance in annotated data leads to noisy labels, due to inter-annotator disagreements. If multiple PICO terms appear in conjunction, multiple basic PICOs should be classified, but sometimes they are classified as one entity. 

Evaluation on very small test sets (because of cross-validation) can be unreliable because not many entities are available for prediction. They give example of a recall of 0.5, in a case where only 2 entities were present in their test set during a cross-validatopn fold. 

Different annotation standards can impact the measured performance on a model (ie. EBM-NLP annotates largest span, while Kang would have annotated multiple separate spans)",,"170 RCTs, distribution of entities for their annotated data, also used EBM-NLP",Entity Normalization,No,Yes,NR,No,Yes,Parameters described and code is available. ,"Measure (eg. N, duration); Qualifier (eg. improving, lower)","common entities, tested on random RCTs",No,Yes,No,"XML, JSON","P, IC, O",Entities,None reported,NR,Yes; random splits and ratio given,2019,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Mining characteristics of epidemiological studies from Medline: a case study in obesity,"BACKGROUND: The health sciences literature incorporates a relatively large subset of epidemiological studies that focus on population-level findings, including various determinants, outcomes and correlations. Extracting structured information about those characteristics would be useful for more complete understanding of diseases and for meta-analyses and systematic reviews. RESULTS: We present an information extraction approach that enables users to identify key characteristics of epidemiological studies from MEDLINE abstracts. It extracts six types of epidemiological characteristic: design of the study, population that has been studied, exposure, outcome, covariates and effect size. We have developed a generic rule-based approach that has been designed according to semantic patterns observed in text, and tested it in the domain of obesity. Identified exposure, outcome and covariate concepts are clustered into health-related groups of interest. On a manually annotated test corpus of 60 epidemiological abstracts, the system achieved precision, recall and F-score between 79-100%, 80-100% and 82-96% respectively. We report the results of applying the method to a large scale epidemiological corpus related to obesity. CONCLUSIONS: The experiments suggest that the proposed approach could identify key epidemiological characteristics associated with a complex clinical problem from related abstracts. When integrated over the literature, the extracted data can be used to provide a more complete picture of epidemiological efforts, and thus support understanding via meta-analysis and systematic reviews.
","Karystianis, G.
 and Buchan, I.
 and Nenadic, G.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,1788,"George Karystianis, karystig@cs.man.ac.uk, contact: Goran Nenadic g.nenadic@manchester.ac.uk

includes some meta-analysis publication texts and is relatively domain-specific.

#Ask: is the source code available? I found the data and the rules, thank you for sharing it.",,"The experiments suggest that the proposed [rule-based] approach could identify key epidemiological characteristics associated with a complex clinical problem from related abstracts. When integrated over the literature, the extracted data can be used to provide a more complete picture of epidemiological efforts, and thus support understanding via meta-analysis and systematic reviews.",,LS,"Rule-based, Other","Precision, Recall, F1","Abstracts, Titles","Cohort, Case series, Case control, Other",medline,,Yes,No,NR,Yes,No,No,"Detailled error-analysis on eval set, incl. description of source for false-positives. applied to a much larger (unlabelled) dataset of the same domain, and discussed results. Further, limited error analysis on larger unlabelled data set wrt. some entities","LINNAEUS, the Specialist lexicon, MinorThird",Yes,,"tested on a larger, but unlabelled dataset in the same domain. ",Yes,Yes,Yes,stopword removal,Yes,"Recall and precision scores for infrequent entities should be treated with caution, if frequency is too low then a system's results are only indicative.False positives can appear if more than one entity (such as design) is reported.different ways to describe P entity or exposure. Identification of covariates is challenging because they are not mentioned often explicitly. "" task of recognizing these epidemiological elements (e.g., outcomes, exposures) through a rule based approach is not an easy task and requires a number of rules to accommodate different types of expression""","""Micro averages are calculated across all different document level mentions; macro averages are calculated across different characteristics.""","size: 60 abstracts for designing, 30 more to optimise performance.
http://gnteam.cs.manchester.ac.uk/old/epidemiology/data.html
",entity normalisation,Yes,Yes,NR,No,No,"rules are described, eg. examples and number of rules per entity. Dictionaries given on http://gnteam.cs.manchester.ac.uk/old/epidemiology/data/vocabularies.txt
",effect size; covariates,"developed and tested within one domain (obesity, but some entities are not domain specific",No,Yes,Yes,Spans or annotations,"P, O, Design, Exposure, Other",Entities,"TP, FP, FN",,Yes; mentions random splitting,2014,,,,,,,,,,,,,,,,,,,,
"Text or probably text file, XML","A Hybrid Citation Retrieval Algorithm for Evidence-based Clinical   Knowledge Summarization: Combining Concept Extraction, Vector Similarity and   Query Expansion for High Precision","Novel information retrieval methods to identify citations relevant to a clinical topic can overcome the knowledge gap existing between the primary literature (MEDLINE) and online clinical knowledge resources such as UpToDate. Searching the MEDLINE database directly or with query expansion methods returns a large number of citations that are not relevant to the query. The current study presents a citation retrieval system that retrieves citations for evidence-based clinical knowledge summarization. This approach combines query expansion, concept-based screening algorithm, and concept-based vector similarity. We also propose an information extraction framework for automated concept (Population, Intervention, Comparison, and Disease) extraction. We evaluated our proposed system on all topics (as queries) available from UpToDate for two diseases, heart failure (HF) and atrial fibrillation (AFib). The system achieved an overall F-score of 41.2% on HF topics and 42.4% on AFib topics on a gold standard of citations available in UpToDate. This is significantly high when compared to a query-expansion based baseline (F-score of 1.3% on HF and 2.2% on AFib) and a system that uses query expansion with disease hyponyms and journal names, concept-based screening, and term-based vector similarity system (F-score of 37.5% on HF and 39.5% on AFib). Evaluating the system with top K relevant citations, where K is the number of citations in the gold standard achieved a much higher overall F-score of 69.9% on HF topics and 75.1% on AFib topics. In addition, the system retrieved up to 18 new relevant citations per topic when tested on ten HF and six AFib clinical topics.
","['Kalpana Raja', 'Andrew J Sauer', 'Ravi P Garg', 'Melanie R Klerer', 'Siddhartha R Jonnalagadda']
 ","https://export.arxiv.org/abs/1609.01597
",01/05/2020,,Include,TRUE,01/05/2020,Include,9483,"Briefly describes P, IC sentence classifiers on p. 18, and p.40-41. 
#ASK: Code, data, is used in oractice?  For the Intervention, comparison, disease entity you describe on p. 18 that you used a combination of rule-based and machine learning methods. 

For the machine-learning, the paper ""Jonnalagadda SR[..], Automatically extracting sentences from Medline citations to support clinicians' information needs"" is cited. In this paper, I found that the text-rank algorithm was used. Was ther any other machine-learning classifier used? ",,"""The proposed  system  differs  from  the  existing  approaches  by  focusing  on  obtaining  a  high precision while maintaining the recall to obtain the most relevant citations for evidence-based clinical knowledge summarization. We showed that a hybrid combination of query-expansion,  concept-basedscreening andconcept-based  vector  similarity outperformed other approaches.""",,LS,"Rule-based, APIs and metadata retrieval, Other","Precision, Recall, F1","Full texts, Titles, Abstracts",RCT,"Medline, Up to Date database, and CHLEF 2013 (benchmark) corpus",,Yes,No,NR,No,No,No,Only numbers are given. As entity/ sentence classification was not main focus of this paper. ,"MedTagger, UpToDate  database, Stanford  lexical  parser, open NLP suite",Yes,NR/ NA for rule-base,Evaluated on two different data sets for each entity type. ,Yes,No,Yes,"tokenization, expanding abbreviations and lexical normalization",No,"""PICO  framework  based  filtration  at  abstract  level  is  limited  to  citations having abstracts. However, there are citations(e.g. practice guidelines)where the abstract is not available. Therefore, searching for relevant information should be extended to full text rather than at abstract level. """,evaluation as part of a wider screening system is discussed,"size and distribution: 4,824 sentences  from  18  UpToDate  documents  and 714  sentences from  MEDLINE  citations for P. For I: CLEF  2013  shared task, and 852 MEDLINE citations related to HF and AFib",entity normalisation,Yes,Yes,NR,No,Yes,"Tregex rule-based patterns are described for P, the IC machine-learning implementation is unclear. ",,"text is mostly related to heart conditions, but evaluations were done, in part, on benchmark datasets such as CHLEF 2013. ",No,Yes,No,Spans or annotations,"P, IC, P (Condition or disease)","Entities, Sentences",None reported,NR,Yes; compl. different datasets (ie. diferent SRs; journals..),2016,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Mining Biomedical Literature for Terms related to Epidemiologic Exposures,"Epidemiologic studies contribute greatly to evidence-based medicine by identifying risk factors for diseases and determining optimal treatments for clinical practice. However, there is very limited effort on automatic extraction of knowledge from epidemiologic articles, such as exposures, outcomes, and their relations. In this initial study, we developed a system that consists of a natural language processing (NLP) engine and a rule-based classifier, to automatically extract exposure-related terms from titles of epidemiologic articles. The evaluation using 450 titles annotated by an epidemiologist showed the highest F-measure of 0.646 (Precision 0.610 and Recall 0.688) using in-exact matching, which indicated the feasibility of automated methods on mining epidemiologic literature. Further analysis of terms related to epidemiologic exposures suggested that although UMLS would have reasonable coverage, more appropriate semantic classifications of epidemiologic exposures would be required.
","Xu, H.
 and Lu, Y.
 and Jiang, M.
 and Liu, M.
 and Denny, J. C.
 and Dai, Q.
 and Peterson, N. B.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,4236,"Hua Xu

Ask: Code, manually labelled titles available? ",,"""The evaluation using 450 titles annotated by an epidemiologist showed the highest F-measure of 0.646 (Precision 0.610 and Recall 0.688) using in-exact matching, which indicated the feasibility of automated methods on mining epidemiologic literature. Further analysis of terms related to epidemiologic exposures suggested that although UMLS would have reasonable coverage, more appropriate semantic classifications of epidemiologic exposures would be required""",,LS,Rule-based,"Precision, Recall, F1",Titles,"Cohort, Case series, Case control, Other","American Journal of Epidemiology, titles of articles",,No,No,NR,No,No,No,"error analysis, ",KnowledgeMap Concept Indexer (KMCI) for extracting noun phrases,Yes,rule-base,Only tested on one dataset,Yes,No,Yes,Parsed by KMCI,Yes,"Term and concept ambiguity are challenges that cause errors in concept-mapping (eg. mapping to UMLS), boundaries of noun-phrases can be misclassified. The manual annotator might miss concepts to annotate, variation in language, environmental concepts are not covered well enough in UMLS",,"1000 titles for development, 450 titles for testing which were manually annotated",,Yes,Yes,"Impact of different score cut-off values on precision, recall, and F1 is given. ",Yes,No,"Description of rule-based scoring classifier, assigning a heuristic score to noun phrases. Example for rules is given",,Tested only on titles from one journal. ,No,Yes,No,Spans or annotations,Exposure,Entities,None reported,NR,Yes; random splits and ratio given,2010,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Automatic endpoint detection to support the systematic review process,"Preparing a systematic review can take hundreds of hours to complete, but the process of reconciling different results from multiple studies is the bedrock of evidence-based medicine. We introduce a two-step approach to automatically extract three facets - two entities (the agent and object) and the way in which the entities are compared (the endpoint) - from direct comparative sentences in full-text articles. The system does not require a user to predefine entities in advance and thus can be used in domains where entity recognition is difficult or unavailable. As with a systematic review, the tabular summary produced using the automatically extracted facets shows how experimental results differ between studies. Experiments were conducted using a collection of more than 2million sentences from three journals Diabetes, Carcinogenesis and Endocrinology and two machine learning algorithms, support vector machines (SVM) and a general linear model (GLM). F1 and accuracy measures for the SVM and GLM differed by only 0.01 across all three comparison facets in a randomly selected set of test sentences. The system achieved the best performance of 92% for objects, whereas the accuracy for both agent and endpoints was 73%. F1 scores were higher for objects (0.77) than for endpoints (0.51) or agents (0.47). A situated evaluation of Metformin, a drug to treat diabetes, showed system accuracy of 95%, 83% and 79% for the object, endpoint and agent respectively. The situated evaluation had higher F1 scores of 0.88, 0.64 and 0.62 for object, endpoint, and agent respectively. On average, only 5.31% of the sentences in a full-text article are direct comparisons, but the tabular summaries suggest that these sentences provide a rich source of currently underutilized information that can be used to accelerate the systematic review process and identify gaps where future research should be focused.
","Blake, C.
 and Lucic, A.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,369,"related to 2288, which is an updated version of this paper. 

Ana Lucic School of Information Sciences, University of Illinois, Champaign, IL and Catherine L. Blake clblake@illinois.edu
",,"""On average, only 5.31% of the sentences in a full-text article are direct comparisons, but the tabular summaries suggest that these sentences provide a rich source of currently underutilized information that can be used to accelerate the systematic review process and identify gaps where future research should be focused.""",,LS,"Rule-based, SVM, Other ML classifier","Precision, Recall, F1, Accuracy","Full texts, Titles, Abstracts","RCT, Animal studies, Mix","comparison sentences from full-text articles in the journals Diabetes, Carcinogenesis, and Endocrinology, TREC Genomicscollection",,No,No,NR,No,No,No,"Error analysis, discussion wrt. different classifiers and entities given. In-depth analysis of sentence classifications given, with explanations for mis-classifications. Qualitative analysis on large set of sentences is described. ","Oracle Data Miner, version 3.2; Stanford dependency parser",Yes,NR,Not tested on different/larger set of labelled data. Qualitative analysis done on text from same journals. ,Yes,Yes,Yes,"okenisation, conversion to features, dependency parsing.",Yes,"complex sentence structures, nuanced language, multiple entities in the same sentence present challenges to models. ","qualitative evaluation on large, unlabelled dataset","size: 100 sentences with 656 noun phrases for train, test set of 132 sentences w. 936 noun phrases","general linear model,",Yes,Yes,precision-recall plots given for different classification thresholds,Yes,No,"Some hyperparameters for SVM and GLM are descriebed, eg. kernel type, complexity factor, and confidence level for GLM ","â€˜agentâ€™ and 'object' class which can be both patient and Intervention/control, always 2 entities that are being compared wrt. endpoint (here O)","specific dataset from selected journal, small in size",Yes,Yes,No,"Spans or annotations, Structured text and summary","O, Other",Entities,"TP, TN, FP, FN",Discussed errors with respect to proximity of entities and impact of position on classification scores. ,Yes; compl. different datasets (ie. diferent SRs; journals..),2015,,,,,,,,,,,,,,,,,,,,
"XML, HTML",ExaCT: automatic extraction of clinical trial characteristics from journal publications,"BACKGROUND: Clinical trials are one of the most important sources of evidence for guiding evidence-based practice and the design of new trials. However, most of this information is available only in free text - e.g., in journal publications - which is labour intensive to process for systematic reviews, meta-analyses, and other evidence synthesis studies. This paper presents an automatic information extraction system, called ExaCT, that assists users with locating and extracting key trial characteristics (e.g., eligibility criteria, sample size, drug dosage, primary outcomes) from full-text journal articles reporting on randomized controlled trials (RCTs). METHODS: ExaCT consists of two parts: an information extraction (IE) engine that searches the article for text fragments that best describe the trial characteristics, and a web browser-based user interface that allows human reviewers to assess and modify the suggested selections. The IE engine uses a statistical text classifier to locate those sentences that have the highest probability of describing a trial characteristic. Then, the IE engine's second stage applies simple rules to these sentences to extract text fragments containing the target answer. The same approach is used for all 21 trial characteristics selected for this study. RESULTS: We evaluated ExaCT using 50 previously unseen articles describing RCTs. The text classifier (first stage) was able to recover 88% of relevant sentences among its top five candidates (top5 recall) with the topmost candidate being relevant in 80% of cases (top1 precision). Precision and recall of the extraction rules (second stage) were 93% and 91%, respectively. Together, the two stages of the extraction engine were able to provide (partially) correct solutions in 992 out of 1050 test tasks (94%), with a majority of these (696) representing fully correct and complete answers. CONCLUSIONS: Our experiments confirmed the applicability and efficacy of ExaCT. Furthermore, they demonstrated that combining a statistical method with 'weak' extraction rules can identify a variety of study characteristics. The system is flexible and can be extended to handle other characteristics and document types (e.g., study protocols).
","Kiritchenko, S.
 and de Bruijn, B.
 and Carini, S.
 and Martin, J.
 and Sim, I.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,1894,"Is refinement of De Bruijn B, Carini S, Kiritchenko S, Martin J, Sim I: Automated information extraction of key trial design elements from clinical trial publications. Proceedings of the AMIA Annual Symposium: 8-12 November 2008; Washington, DC. 2008, American Medical Informatics Association, 141-145.

Svetlana.Kiritchenko@nrc-cnrc.gc.ca

#ASK: Access (send my name, affiliation), Code, data available? Are any of the implementation details (ie. hyperparameter settings for the classifiers) available?",,"The system is flexible and can be extended to handle other characteristics and document types (e.g., study
protocols).",,LS,"Rule-based, SVM","Precision, Recall, Other","Abstracts, Full texts, Titles",RCT,medline,https://exact.cluster.gctools.nrc.ca/ExactDemo/,Yes,No,NR,No,Yes,No,error analysis and practial tests on SRs.,NR,Yes,No implementation details published.,"evaluated on different dataset from different journals, discussed effects of idiosyncrasy",Yes,Yes,Yes,"sentence splitting, automatic annotation of common entities, section heading identification + irrelevant section removal",Yes,"Entity classification for long information elements such as I, O often contains irrelevant information in the end because it is harder to find the coreect end-boundary. Risk of idiosyncrasy when training on a small train set from journals with specific formats. Challenging to classify data elements such as in/ exclusion criteria becasue they can span multiple sentences, having one ""key"" sentence and sentences with additional information. Another challenge is presented when distinguishing primary and secondary outcomes, as they are often not described clearly and therefore there was insufficient information to learn this. Also, measures are not always explicitly defined as outcome, they vary throughout trials and dicipline so there was insufficient training material to train a comprehensive classifier. Similar to outcomes, a clear distinction between experimental and control intervention was chellenging due to non-explicit descriptions (intervention and control sometimes, but not always, separated by ""or"", ""versus"", etc .. . Finally, linking each intervention with its exactly corresponding entities such as dose/ duration/ route is described as further work. Comparability is limited due to lack of benchmark evaluation datasets",Percentage of articles needing human adjustment or addition to system output; Average time required and time saved for extracting an article; top5 recall for sentences; overall micro + macro scores for sentence and entities,"distribution of entities is described, 132 articles for training, 50 for testing. ",post-processing,Yes,No, impact of top1 and topN method on precision and recall is described,Yes,Yes,"no parameters and implementation details given for SVM, regex patterns not public",,"common entities, general-topic text",Yes,No,Yes," Spans or annotations, Structured text and summary","P, IC (per arm), IC (dose; duration and others), O (time point), O (primary or secondary outcome), N (total), Eligibility criteria, Enrolment dates, Funding org, Grant number, Early stopping, Trial registration","Entities, Sentences",None reported,"Uncovered that location of funding sorces had impact on classification scores, the location varied between train and test data.",Yes; compl. different datasets (ie. diferent SRs; journals..),2010,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Advancing PICO Element Detection in Biomedical Text via Deep Neural   Networks,"In evidence-based medicine (EBM), defining a clinical question in terms of the specific patient problem aids the physicians to efficiently identify appropriate resources and search for the best available evidence for medical treatment. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components typically reported in clinical trials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). In this work, we propose a novel deep learning model for recognizing PICO elements in biomedical abstracts. Based on the previous state-of-the-art bidirectional long-short term memory (biLSTM) plus conditional random field (CRF) architecture, we add another layer of biLSTM upon the sentence representation vectors so that the contextual information from surrounding sentences can be gathered to help infer the interpretation of the current one. In addition, we propose two methods to further generalize and improve the model: adversarial training and unsupervised pre-training over large corpora. We tested our proposed approach over two benchmark datasets. One is the PubMed-PICO dataset, where our best results outperform the previous best by 5.5%, 7.9%, and 5.8% for P, I, and O elements in terms of F1 score, respectively. And for the other dataset named NICTA-PIBOSO, the improvements for P/I/O elements are 2.4%, 13.6%, and 1.0% in F1 score, respectively. Overall, our proposed deep learning model can obtain unprecedented PICO element detection accuracy while avoiding the need for any manual feature selection.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Advancing PICO Element Detection in Medical Text via Deep Neural Networks.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs http://arxiv.org/abs/1810.12780; Years 2018; Authors Di Jin; Peter Szolovits; Deduplication_Notes ; X Advancing PICO Element Detection in Medical Text via Deep Neural Networks.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]
","['Di Jin', 'Peter Szolovits']
 ",https://export.arxiv.org/abs/1810.12780,01/05/2020,,Include,TRUE,01/05/2020,Include,9695,"#No questions, all reported and open source. ",,"'Overall, our proposed deeplearning model can obtain unprecedented PICO element detection accuracy while avoiding the need for any manual feature selection.'",,LS,"Word embedding, CNN, RNN, BERT incl. biomedical versions","Precision, Recall, F1","Abstracts, Titles",RCT,same as Di (10346) and NICTA-PIBOSO (made available by this author),,Yes,Yes,"https://github.com/jind11/Deep-PICO-Detection
",Yes,No,Yes,Error assessment given in the appendix. ,"Deep-learning with tensorflow, other dependencies are listed in the code repository",Yes,Cross-validation (10-fold),"Evaluated on two different benchark datasets, described that different model architectures yield the respective best results/ ",No,No,Yes,As described for each original corpus. This work ignored section headings in NICTA-PIBOSO. ,Yes,"For small datasets such as NICTA-PIBOSO, a high variance of results is seen when using 10-fold cross-validation. Noise in the dataset is caused by the class 'Other', as this class often carries confusing information from other classes. ",micro-scores,"https://github.com/jind11/NICTA-PIBOSO-Dataset, see Di (10346) for second corpus. ",adversarial training for the neural networks;,Yes,Yes,NR,No,Yes,"Detailled escription, incl. hyperparameters, given in appendix. Code is open-source",,"'common entities, general-topic text' ",No,Yes,No,Spans or annotations,"P, IC, O, Design, Sections (Aim; Method etc.)",Sentences,"TP, TN, FP, FN",NR,Yes; random splits and ratio given,2018,,,,,,,,,,,,,,,,,,,,
Text or probably text file,PICO Element Detection in Medical Text via Long Short-Term Memory Neural Networks.,"not available
","Di Jin
 and Peter Szolovits
 ",https://doi.org/10.18653/v1/w18-2308 ,01/05/2020,,Include,TRUE,01/05/2020,Include,10346,"jindi15@mit.edu
psz@mit.edu

#No questions! Thanks for making everything open-source.",," ""By jointly classifying subsequent sentences  in  the  given  text,  we  achievestate-of-the-art  results  on  PICO  element classification  compared  to  several  strongbaseline   models.We   also   make   ourcurated  data  public  as  a  benchmarking dataset so that the community can benefit from it.""",,LS,"CRF, Word embedding, LSTM, Other ML classifier, Multi-layer perceptron","Precision, Recall, F1","Abstracts, Titles",RCT,medline,,No,Yes,"https://github.com/jind11/LSTM-PICO-Detection
",Yes,No,Yes,Error analysis and discussion of common mistakes, Stanford CoreNLP toolkit for tokenization,Yes,"Cross-validation (10-fold), neural-net techniques suchas regularization, stopping. ",Not evaluated or described wrt. different datasets,Yes,No,Yes,"number normalisation, sentence tokenization",Yes,Ambiguity in sentence labels can lead to mis-classifications. ,,"Size and distribution, '24,668  abstracts  contain  at least  one  of  the  P/I/O  labels.   There  are  21,198 abstracts  with  P-labels,  13,712  with  I-labels  and 20,473 with O-labels', https://github.com/jind11/PubMed-PICO-Detection
",,Yes,Yes,NR,No,Yes,"Detailled description of neural net, hyperparameters and implementation in text, fully open-source.",Methods,"comparison with other models is presented, although not on the same dataset. Common entities, large and general text, availability as benchmarking dataset",No,Yes,No,Spans or annotations,"P, IC, O, Sections (Aim; Method etc.)",Sentences,"TP, TN, FP, FN",NR,Yes; random splits and ratio given,2018,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Evaluating automated entity extraction with respect to drug and non-drug treatment strategies,"OBJECTIVES: Treatment used in a randomized clinical trial is a critical data element both for physicians at the point of care and reviewers who are evaluating different interventions. Much of existing work on treatment extraction from the biomedical literature has focused on the extraction of pharmacological interventions. However, non-pharmacological interventions (e.g., exercise, diet, etc.) that are frequently used to address chronic conditions are less well studied. The goal of this study is to compare knowledge-based and machine learning strategies for the extraction of both drug and non-drug treatments. METHODS: We collected 800 randomized clinical trial abstracts each for breast cancer and diabetes from PubMed. The treatments in the result/conclusion sentences of the abstracts were manually annotated and marked as drug/non-drug treatments. We then designed three methods to identify the treatments and evaluated the systems with respect to drug/non-drug treatments. The first method is solely based on knowledge base (here we used MetaMap). The second method is based on a machine learning model trained mainly on contextual features (ML_only). The third method is a combination approach that integrates the previous two approaches. RESULTS/DISCUSSION: Results show that MetaMap, when used with high precision semantic types, has better performance for drug compared to non-drug treatments (F1=0.77 vs. 0.64). The ML_only approach has smaller performance difference between drug and non-drug treatments compared with the KB-based approach (F1=0.02 vs. 0.05, 0.07, and 0.13). The combination approach achieves significantly better performance than all MetaMap approaches alone for total treatments (F1=0.76 vs. 0.72, p<0.001). The performance gain mainly comes from the non-drug treatments (0.03-0.08 improvement in F1), while the drug treatments do not benefit much from the combination approach (0-0.03 improvement in F1). CONCLUSION: These results suggest that a knowledge-based approach should be employed for medical conditions that are primarily treated with drugs whereas conditions that are treated with either a combination of drug and non-drug interventions or primarily non-drug interventions should use automated tools that combine machine learning and a knowledge-based approach to achieve optimal performance.
","Guo, J.
 and Blake, C.
 and Guan, Y.
 ","https://dx.doi.org/10.1016/j.jbi.2019.103177
",01/05/2020,,Include,TRUE,01/05/2020,Include,1376,"Jinlong Guo jguo24@illinois.edu; Yingjun Guan

not 100% applicable due to restriction od diabetes/cancer, but does mine RCT abstracts

#ASK: Code, data",,""" These results suggest that a knowledge-based approach should be employed for medical conditions that are primarily treated with drugs whereas conditions that are treated with either a combination of drug and non-drug interventions or primarily non-drug interventions should use automated tools that combine machine learning and a knowledge-based approach to achieve optimal performance. """,,LS,SVM,"Precision, Recall, F1, Other","Abstracts, Titles",RCT,"medline (pubmed), searched for breast cancer and diabetes",,No,Yes,NR,No,No,No,discussed effects of different settings and system combinations for each entity type.,"MetaMap, Stanford part of speech (POS) tagger (version 3.7.0), OpenNLP tool (version 1.7.2), Oracle Data Miner (Version 17.4.1.054) implementation of the SVM algorithm",Yes,cross-validation (10-fold),"Discussed that their dataset might not contain enough variation for the non-drug treatment, impact of using only structured abstracts",Yes,Yes,Yes,"tokenization, some normalisation (eg. ""some expressions common in RCTs such as â€œ(nâ€¯=â€¯48)â€, â€œpâ€¯<â€¯0.05â€ complicate the NP chunker, we therefore removed them""), splitting phrases at some conjunctions (eg. ""and"", ""vs""), replaced abbreviations, removed treatments of no interest (eg. ""experimental group"" and standard units, eg. ""0.25â€¯mg""); stemming, lower-casing",Yes,"Applicability of a system developed on structured abstracts to unstructured is not guaranteed. Noisy labels due to inter-annotator disagreement might impact scores negatively, especially in classes with lower agreement. ","""binomial proportion test to see whether the proportion of correct prediction is significantly different"", McNemar test","size (800 RCT abstracts of breast cancer and diabetes each, total 1600 structured abstracts),  treatments in the result/conclusion sentences were annotated. Class distribution described, BRAT tool used",,Yes,Yes,Discussed and evaluated influence of number of metaMap concepts and their impact on precision and recall. ,Yes,No,linear SVM,annotated drug and non-drug intervention entities,"unclear, breast cancer and diabetes specific dataset and only structured abstracts. Extracted the commonly used intervention entity.",No,Yes,No,Spans or annotations,"IC, IC (Drug name), Other",Entities,None reported,NR,Yes; random splits and ratio given,2019,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"Automatic extracting of patient-related attributes: disease, age, gender and race","In the Evidence-based Medicine (EBM), PICO format is designed to easily and correctly search for the best available evidence. As the main element of PICO, the Patient/Problem (P) represents the attributes of patient in the clinical question and studies. In order to better understand the clinical problems, patient attribute identification is crucial and indispensable. Due to the richness of the human nature language, many issues like various term representations, grammar structures and abbreviations present challenges for automatically extracting the patient-related attributes from the unstructured data. In this paper, we employed the nature language processing (NLP) technologies to deeply analyze the linguistic characteristics of the attributes. Based on the NLP analysis results, we built the rule sets for different attributes and applied the rule-based approach to extract the patient-related attributes.
","Zhu, H.
 and Ni, Y.
 and Cai, P.
 and Qiu, Z.
 and Cao, F.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,4462,"Huijia ZHU zhuhuij@cn.ibm.com IBM  Research  â€“  China,

#ASK: Data, code available? Did you use abstracts, or full texts? Is the finished app available somewhere? ",,"'In  this  paper,  we  employed  the nature language processing (NLP) technologies to deeply analyze the linguistic characteristics  of  the  attributes.  Based  on  the  NLP  analysis  results,  we  built  the  rule sets for different attributes and applied the rule-based approach to extract the patient-related attributes. '",,LS,Rule-based,"Precision, Recall, F1","Abstracts, Titles",RCT,PubMed ,,No,No,NR,No,No,No,"Evaluation was done based on 50 questions from the Trip Answers website. Only high-level results are given, no results for separate entities are reported. ","MetaMap,Stanford  Parser  ",Yes,rule-based,NR,No,No,Yes,"Processed and taged with MetaMap, stanford parser, abbreviations and concepts are capitalised",Yes,NR,,"size (50 articles), number of disease terms given. No indication about the age, gender, race entities. ",,Yes,Yes,NR,No,No,Rule-base explained for each entity type,,"Evaluation based on patient-characteristics related questions, not reported for the separate entities. ",No,Yes,No,Spans or annotations,"P, Age, Gender, Race",Entities,None reported,NR,Yes; compl. different datasets (ie. diferent SRs; journals..),2012,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Extracting PICO elements from RCT abstracts using 1-2gram analysis and   multitask classification,"The core of evidence-based medicine is to read and analyze numerous papers in the medical literature on a specific clinical problem and summarize the authoritative answers to that problem. Currently, to formulate a clear and focused clinical problem, the popular PICO framework is usually adopted, in which each clinical problem is considered to consist of four parts: patient/problem (P), intervention (I), comparison (C) and outcome (O). In this study, we compared several classification models that are commonly used in traditional machine learning. Next, we developed a multitask classification model based on a soft-margin SVM with a specialized feature engineering method that combines 1-2gram analysis with TF-IDF analysis. Finally, we trained and tested several generic models on an open-source data set from BioNLP 2018. The results show that the proposed multitask SVM classification model based on 1-2gram TF-IDF features exhibits the best performance among the tested models.
","['Xia Yuan', 'Liao xiaoli', 'Li Shilei', 'Shi Qinwen', 'Wu Jinfa', 'Li Ke']
 ",https://export.arxiv.org/abs/1901.08351,01/05/2020,,Include,TRUE,01/05/2020,Include,9742,"bruce_xia6116@163.com
colinlike@163.com

#ASK: is the code available? Are the weights for your word2vec model available? ",,"""The results show that theproposedmultitask SVM classification model based on 1-2gram TF-IDFfeatures exhibits the best performance among the tested models""",,LS,SVM,"Precision, Recall, F1, Accuracy","Abstracts, Titles",RCT,10346 Jin corpus,,No,Yes,NR,Yes,No,No,"Discussed errors, influence of hyperparameters",gensim for word vectors,Yes,"crossvalidation, testing on held-out gold-standard data",Evaluated only on one benchmark set of structured abstracts. ,Yes,No,Yes,see 10346 Jin corpus,Yes,PICO information is more complete in full texts. ,,see 10346 Jin corpus,,Yes,Yes,ROC curve given for different TF-IDF an embedding experiments,Yes,Yes,"SVM, along with hyperparameters is described",,"used benchmarking dataset, commoon entities and general-topic text. Comparison to different methods on same dataset was done.",No,Yes,No,Spans or annotations,"P, IC, O",Sentences,None reported,NR,Yes; random splits and ratio given,2019,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Identifying scientific artefacts in biomedical literature: the Evidence Based Medicine use case,"Evidence Based Medicine (EBM) provides a framework that makes use of the current best evidence in the domain to support clinicians in the decision making process. In most cases, the underlying foundational knowledge is captured in scientific publications that detail specific clinical studies or randomised controlled trials. Over the course of the last two decades, research has been performed on modelling key aspects described within publications (e.g., aims, methods, results), to enable the successful realisation of the goals of EBM. A significant outcome of this research has been the PICO (Population/Problem-Intervention-Comparison-Outcome) structure, and its refined version PIBOSO (Population-Intervention-Background-Outcome-Study Design-Other), both of which provide a formalisation of these scientific artefacts. Subsequently, using these schemes, diverse automatic extraction techniques have been proposed to streamline the knowledge discovery and exploration process in EBM. In this paper, we present a Machine Learning approach that aims to classify sentences according to the PIBOSO scheme. We use a discriminative set of features that do not rely on any external resources to achieve results comparable to the state of the art. A corpus of 1000 structured and unstructured abstracts - i.e., the NICTA-PIBOSO corpus - is used for training and testing. Our best CRF classifier achieves a micro-average F-score of 90.74% and 87.21%, respectively, over structured and unstructured abstracts, which represents an increase of 25.48 percentage points and 26.6 percentage points in F-score when compared to the best existing approaches.
","Hassanzadeh, H.
 and Groza, T.
 and Hunter, J.
 ","https://dx.doi.org/10.1016/j.jbi.2014.02.006
",01/05/2020,,Include,TRUE,01/05/2020,Include,1454,"Hamed Hassanzadeh, Jane Hunter
h.hassanzadeh@uq.edu.au jane@itee.uq.edu.au

#Ask: Code available?",,"""At the same time, unlike some of the previous research, our approach did not use any external resources (e.g., domain-specific concepts) as classification features. Experimental results have shown a significant improvement over the state of the art, leading to a micro-average F-score of 90.74% and 87.21%, respectively, over structured and unstructured abstracts, i.e., an increase of 25.48 percentage points and 26.6 percentage points in F-score when compared to best existing approaches""",,LS,"CRF, SVM, Other ML classifier","Precision, Recall, F1","Abstracts, Titles",RCT,"NICTA-PIBOSO corpus, MEDLINE and 'The Global Evidence Mapping Initiative (GEM), and The Agency for Healthcare Research and Quality (AHRQ)'",,No,Yes,NR,Yes,No,No,"Detailled description of scores, different clsasifiers and error-analysis. ","GATE Natural Language Processing toolkit for feature extraction, MALLET for CRF, WEKA for remaining classifiers.",Yes,"cross-validation (stratified by class, 10-fold)",Not discussed.,Yes,No,Yes,"'linguistic pre-processing' using GATE toolkit, lemmatisation but they retained original orthographic case",Yes,"ambiguity between sentence classes, and correlation of low classification results with high inter-annotator disagreement is discussed. ",micro (class-based) scores,described size (1000 abstracts) and distribution of entities. It is a multi-label corpus. They clearly described how they retrieved the data. ,,Yes,Yes,NR,No,Yes,"Described feature extraction pipeline (Tokeniser, POS Tagger, Verb Phrase Chunker) and usage of statistical, positional, sequential and token-based classification features. For CRF/SVM, hyperparameters (default MALLET/WEKA settings and further description) are given. ","Background, Other","evaluated on benchmark NICTA-PIBOSO, using common entities/metrics. Comparison to 3 other papers is given. ",Yes,Yes,No,Spans or annotations,"P, IC, O, Design",Sentences,"TP, TN, FP, FN","Discussed the influence of 'trigger' tokens that determine classification of a specific class, eg. the population class. ",Yes; random splits and ratio given,2014,,,,,,,,,,,,,,,,,,,,
Text or probably text file,PICO element detection in medical text without metadata: are first sentences enough?,"Efficient identification of patient, intervention, comparison, and outcome (PICO) components in medical articles is helpful in evidence-based medicine. The purpose of this study is to clarify whether first sentences of these components are good enough to train naive Bayes classifiers for sentence-level PICO element detection. We extracted 19,854 structured abstracts of randomized controlled trials with any P/I/O label from PubMed for naive Bayes classifiers training. Performances of classifiers trained by first sentences of each section (CF) and those trained by all sentences (CA) were compared using all sentences by ten-fold cross-validation. The results measured by recall, precision, and F-measures show that there are no significant differences in performance between CF and CA for detection of O-element (F-measure=0.731+/-0.009 vs. 0.738+/-0.010, p=0.123). However, CA perform better for I-elements, in terms of recall (0.752+/-0.012 vs. 0.620+/-0.007, p<0.001) and F-measures (0.728+/-0.006 vs. 0.662+/-0.007, p<0.001). For P-elements, CF have higher precision (0.714+/-0.009 vs. 0.665+/-0.010, p<0.001), but lower recall (0.766+/-0.013 vs. 0.811+/-0.012, p<0.001). CF are not always better than CA in sentence-level PICO element detection. Their performance varies in detecting different elements.
","Huang, K. C.
 and Chiang, I. J.
 and Xiao, F.
 and Liao, C. C.
 and Liu, C. C.
 and Wong, J. M.
 ","https://dx.doi.org/10.1016/j.jbi.2013.07.009
",01/05/2020,,Include,TRUE,01/05/2020,Include,1583,"Ke-ChunHuang, Jau-Min Wong jmwong@ntu.edu.tw;

#ASK: Thank you for uploading the data set, but I think the link broken, I am getting a server error: http://kimiko.biome.tk/2013_PICO/
Is the data set, and possibly the code, available somewhere? 
 ",,"""Our study shows that the very first sentence in a section does not always contain all the essential information for information retrieval."", ""Based on a combined precision and recall measure (i.e. F-measures), can be used for P detection and can be used for I detection."" ,""Our results suggest some terms that commonly occur on stopword lists are useful distinguishing features, therefore stoplists seem to be less relevant than they are believed to be.""",,LS,Other ML classifier,"Precision, Recall, F1","Full texts, Titles, Abstracts",RCT,"medline, structured RCT abstracts",,No,Yes,NR,No,No,No,"Analysis of different feature sets, pre-processing techniques, and most relevant features. ",NLTK WordNet stemmer and naive bayes classifier,Yes,cross-validation (10-fold),NR,No,No,Yes,"stemming, feature selection of top-n words",Yes,"Noisy datasets, especially if non-pico labels are used. This can be a cause for under-performing classifiers. No manual review of classification results, labels are relying on the abstract structure. ",,"19,854 abstracts, reported distribution of classes, number of sentences, words.",,Yes,Yes,"graphs show impact of feature number on precision, recall, and F1. Discussion which classifier should be used for high-recall scenarios.",Yes,Yes,description of architecture and different naive bayes classifiers. ,,"'common entities, general-topic text' ",Yes,Yes,No,Spans or annotations,"P, IC, O",Sentences,None reported,Included table with words that had the most impact on each class classification. ,Yes; random splits and ratio given,2013,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Indonesian medical question classification with pattern matching,"Indonesian medical question answering system requires the extraction of named entity recognition process. This research aims to propose and evaluate a systematic approach to classify Problem, Intervention, Comparison and Outcome (PICO) from the Indonesian medical sentences. We here declare that the extraction using the PICO frames for Indonesian medical sentences is the first. The advantage of PICO frame is to accelerate the classification process based on Problem Intervention, Comparison, and Outcome criteria. Our strategy here was to build a combining question term with multiple classifiers and repetition. The training and test data were generated automatically from Indonesia medical literature with 200 sentences by the exact pattern match of head words of P-I-C-O categories. This approach achieved F-measure values of 0.90 for Problem and Intervention; 0.89 for Problem, Intervention, and Comparison; 0.91 for Problem, Comparison and Outcome. It then can be concluded that by the pattern in matching criteria of the training set and the classification of PICO elements is reproducible with minimal expert intervention.
","W. Suwarningsih
 and A. Purwarianti
 and I. Supriana
 ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440185,01/05/2020,,Include,TRUE,01/05/2020,Include,6575,"wiwin.suwarningsih@students.itb.ac.id, ayu@stei.itb.ac.id, iping@stei.itb.ac.id 

#Ask: It is very interesting to see a paper that looks at non-English text. Question about classifier: you mentioned that machine-learning was used, could you please clarify which kind of algorithm was used? It seems like there is a description in the paper you cite: '""Transitive Strategy on  CLQA  system  for  low  resources  language  pair  -  via  development  of  indonesian-japanese  CLQA,""' but unfortunately I was not able to access it. Other: Code, dataset. ",,"'This   paper   proposes   the   evaluation   of   a   systematic   approach  to  classify  Problem,  Intervention,  Comparison  and  Outcome  (PICO)  from  the  Indonesian  medical.  An  alignment  method  was  able  to  detect  the  best  performing  pattern  based  on the difference combination of PICO that here could be used to  determine  an  incorrect  pattern  in  view  of  the  calculation  precision of pattern combination '",,LS,"Rule-based, Other ML classifier","Precision, Recall, F1, Other","Abstracts, Titles",RCT,"sentences from abstracts indonesian medical literature, source not discussed. ",,No,Yes,NR,No,No,No,error analysis and influence of sentence order is discussed. ,Implementation of the system is unclear.,Yes,Cross-validation (10-fold),NR,No,No,No,"suffix-tree conversion, normalisation of sentences, ",Yes,NR,Odds ratio,"200 sentences (or abstracts, likely typo in paper), distribution of classes given (198  abstracts  were  with  P-labels,  129  with  I-labels,  190  with  C-labels  and  178  with O-labels). Automatic assignment of labels.","Paper mentions machine-learning, and cross-validation, but exact approach is unclear. ",Yes,No,"Text mentions that between 1 and 10% of terms were selected for classification, but no scores wrt. that are reported. ",No,Yes,"machine-learning approach unclear, unclear how exactly the suffix trees were obtained. ",,"common entities, unclear if the topic is general. ",Yes,No,No,Spans or annotations,"P, IC (per arm), O",Sentences,None reported,"Discusses influence of sentence structure and sentence order on precision, recall, and F1, showing that some sentence combinations up to 10% lower classification scores. . ",Yes; random splits and ratio given,2015,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Tool for filtering PubMed search results by sample size,"Objective: The most used search engine for scientific literature, PubMed, provides tools to filter results by several fields. When searching for reports on clinical trials, sample size can be among the most important factors to consider. However, PubMed does not currently provide any means of filtering search results by sample size. Such a filtering tool would be useful in a variety of situations, including meta-analyses or state-of-the-art analyses to support experimental therapies. In this work, a tool was developed to filter articles identified by PubMed based on their reported sample sizes. Materials and Methods: A search engine was designed to send queries to PubMed, retrieve results, and compute estimates of reported sample sizes using a combination of syntactical and machine learning methods. The sample size search tool is publicly available for download at http://ihealth.uemc.es. Its accuracy was assessed against a manually annotated database of 750 random clinical trials returned by PubMed. Results: Validation tests show that the sample size search tool is able to accurately (1) estimate sample size for 70% of abstracts and (2) classify 85% of abstracts into sample size quartiles. Conclusions: The proposed tool was validated as useful for advanced PubMed searches of clinical trials when the user is interested in identifying trials of a given sample size.
","Baladron, C.
 and Santos-Lozano, A.
 and Aguiar, J. M.
 and Lucia, A.
 and Martin-Hernandez, J.
 ","https://dx.doi.org/10.1093/jamia/ocx155
",01/05/2020,,Include,TRUE,01/05/2020,Include,248,"Meanwhile, if you want to use the SSS (Sample Size Search) Tool for PubMed, please send an email to professor Carlos Baladron at cbaladron@uemc.es

#ASK: Code, data? Data were not pre-processed?",,"""The proposed tool was validated as useful for advanced PubMed searches of clinical trials when the user is interested in identifying trials of a given sample size.""",,LS,"Rule-based, APIs and metadata retrieval, SVM","Precision, Recall, Accuracy, AUC-ROC","Abstracts, Titles",RCT,"Pubmed, random RCTs","A link was given, but tool is not yet online: https://ihealth.uemc.es/
",No,Yes,NR,No,No,No,"Error analysis, ROC curve",NR,Yes,cross-validation (5-fold),NR,Yes,Yes,Yes,NR,Yes,"Indirect reporting, variation in language causes errors for rule-base, variation of describing participant types induces errors in the rule-base. ",,"648 abstracts, manually labelled",tested other classifiers such as SVM but no results reported. ,No,No,ROC curve given for false/true-positive rates wrt. the number of abstracts used in training.  ,Yes,Yes,"Low confidence detection: ""boosted tree ensemble classifier"", no implementation detail or hyperparameters described. Rules for sample-size estimation are discussed. ",,common entity and general-topic text,No,No,No,Spans or annotations,"N (per arm), N (total)",Entities,"TP, TN, FP, FN",,Yes; random splits and ratio given,2018,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature","We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the 'PICO' elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.
","Nye, B.
 and Jessy Li, J.
 and Patel, R.
 and Yang, Y.
 and Marshall, I. J.
 and Nenkova, A.
 and Wallace, B. C.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,2748,FALSE,,"""We acquired annotations from a di-verse set of workers with varying levels ofexpertise and cost.  We describe our datacollection process and the corpus itself indetail.  We then outline a set of challenging NLP tasks that would aid searching ofthe medical literature and the practice of evidence-based medicine.""",,LS,"CRF, Word embedding, Character embedding, LSTM, Other ML classifier","Precision, Recall, F1","Abstracts, Titles",RCT,"MEDLINE, focus on cardiovascular diseases, cancer, and autism articles. ",,No,Yes,"https://github.com/bepnye/EBM-NLP

https://colab.research.google.com/drive/1Ir52OmkJ2C_Iy9V_eS-_KFVLircJ4MXp

https://colab.research.google.com/drive/1YbbQojM147Ybt1nEcyoXTqlvefmwMg-q",Yes,No,Yes,"Models are only intended as baseline, no detailled discussion or analysis of model outputs, or error analysis. ", Stanford CoreNLP,Yes,"Described in code, not in publication. Early stopping and evaluation of different dataset, by expert annotators. ",This is a benchmark dataset. No comparison was made wrt. another benchmark dataset.,Yes,No,Yes,"POS tagging is described for CRF model, unclear if any other pre-processing was applied before embedding. Code is available. ",No,"labelled data can be noisy, crowd-sourced annotators have low agreement but when aggregated, multiple crowd-sourced annotations ' compare favorably against the expert labels'",,"size: 5000 abstracts, entity distribution. https://ebm-nlp.herokuapp.com/
https://github.com/bepnye/EBM-NLP
",,Yes,Yes,NR,No,Yes,"CRF, LogReg, LSTM-CRF hyperparameters and code are open-source. ","Detailled annotations include:  condition, 9 different intervention types, and 6 outcome types. ","'common entities, general-topic text', project website contains a comparison with one other model, and after publication this dataset was used by others. ",No,Yes,No,Spans or annotations,"P, P (Condition or disease), IC, O, N (total), Age, Gender, Race",Entities,None reported,NR,Yes; random splits and ratio given,2018,,,,,,,,,,,,,,,,,,,,
Text or probably text file,PICO extraction by combining the robustness of machine-learning methods with the rule-based methods,"Machine-learning methods (MLMs) are robust methods in the extraction of the information; they have been also used in the extraction of PICO elements in order to answer clinical questions; MLMs are only used at coarse-grained level in PICO extraction, because of lack of training corpora for PICO at the fine-grained level. Coarse-grained level cannot explore the semantics within the sentence for use as a means of relevance between different answers. We propose a hybrid approach combining the robustness of MLMs and the fine grained level of RBMs to enhance PICO extraction process and facilitate the validity and the pertinence of the answers to clinic questions formulated with the PICO framework.
","S. Chabou
 and M. Iglewski
 ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367038,01/05/2020,,Include,TRUE,01/05/2020,Include,6659,"S. Chabou*,  M. Iglewski*
computer Science and Engineering Department,  UniversitÃ© du QuÃ©bec en Outaouais, Gatineau, Canada, J8Y 3G5

#ASK: Data? Did you use the NICTA-PIBOSO corpus as part of your dataset for this publication? We saw your 2018 publication of a very similar model, using different CRF implementation and features. However, with respect to this publication here, did you conduct an evaluation? The text says 'The  gold  standard  corpus  counts  1000  abstracts  for  the  blind  test.  The  different  statistical  measures  (f,  recall,  precision, etc.) will be assessed on the blind test results.', but I was not able to find the scores in this short publication. For now, we included both this paper, and your 2018 paper as two different data-extraction approaches, but were wondering if you think it is more appropriate to merge them, and just take the 2018 results?",,'We propose a hybrid approach combining the robustness of MLMs and the fine grained level of RBMs to enhance PICO extraction process and facilitate the validity and the pertinence of the answers to clinic questions formulated with the PICO framework.',,LM,"Rule-based, CRF","Precision, Recall, F1","Abstracts, Titles",RCT,"Unclear, text only mentions 3000 abstracts, some structured. NICTA-PIBOSO size is only 1000, but possibly used here as part of the data. ",,No,No,NR,No,No,No,NR,"CRF++, c-Takes",No,NR,NR,No,No,No,"c-Takes tagging for feature generation, no pre-processing described.",No,NR,,size: 3000 abstracts,,No,Yes,NR,No,No,"CRF hyperparameters not given, but features are described. For rule-base, only two examples for rules are given. ",,Dataset description and model assessment are unclear.,No,No,No,Spans or annotations,"P, IC, O","Entities, Sentences",None reported,NR,Yes; random splits and ratio given,2015,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Towards identifying intervention arms in randomized controlled trials: Extracting coordinating constructions,"Background: Large numbers of reports of randomized controlled trials (RCTs) are published each year, and it is becoming increasingly difficult for clinicians practicing evidence-based medicine to find answers to clinical questions. The automatic machine extraction of RCT experimental details, including design methodology and outcomes, could help clinicians and reviewers locate relevant studies more rapidly and easily. Aim: This paper investigates how the comparison of interventions is documented in the abstracts of published RCTs. The ultimate goal is to use automated text mining to locate each intervention arm of a trial. This preliminary work aims to identify coordinating constructions, which are prevalent in the expression of intervention comparisons. Methods and results: An analysis of the types of constructs that describe the allocation of intervention arms is conducted, revealing that the compared interventions are predominantly embedded in coordinating constructions. A method is developed for identifying the descriptions of the assignment of treatment arms in clinical trials, using a full sentence parser to locate coordinating constructions and a statistical classifier for labeling positive examples. Predicate-argument structures are used along with other linguistic features with a maximum entropy classifier. An F-score of 0.78 is obtained for labeling relevant coordinating constructions in an independent test set. Conclusions: The intervention arms of a randomized controlled trials can be identified by machine extraction incorporating syntactic features derived from full sentence parsing. (C) 2008 Elsevier Inc. All rights reserved.
","Chung, G. Y. C.
 ",<Go to ISI>://WOS:000270870500005,01/05/2020,,Include,TRUE,01/05/2020,Include,9760,#Ask: Is code and labelled data available? ,,'The intervention arms of a randomized controlled trials can be identified by machine extraction incorporating syntactic features derived from full sentence parsing.',,LS,"Rule-based, CRF","Precision, Recall, F1, Accuracy","Abstracts, Titles",RCT,"PubMed, same original search as Chung 2009 (#668 in this review), with random selection of RCTs for this study. For tagging method sections, the whole corpus was used.",,Yes,Yes,NR,No,No,No,Detailled discussion and error analysis with respect to major error sources. ,"Enju parser for parsing grammatical structure, GENIA tagger",Yes,Cross-validation,"The evaluation data were obtained as completely different dataset, from different search and time period. Both datasets looked at pharmacological interventions only. ",Yes,No,Yes,"Sentence tokenization, normalisation (numbers, currencies, dates/times, measurements, statistical and mathematical symbols, some patient-related words such as 'women, adults' mapped to word 'patient')",Yes,"Implied information: the phenomenon of ellipsis, where information in a sentence is ommitted, but implied, eg '10 or 20mg', where 10mg is implied. Detecting this and correctly classifying it is important when mining interventions, but it is a challenge. When parsing grammatical structure, some neighbouring concepts can be included in the identified chunks. Variation in wording, described as 'novel conjunction expressions', eg. '1 mg terbutaline (or placebo)' '",,"size: 206 RCTs for training, 124 RCT abstracts for independent evaluation, using different search. All RCTs had pharmacological interventions.",,Yes,Yes,NR,No,No,All parts of they system are described,,"Focus is only on pharmaceutical interventions, this is discussed as limitation in text. ",Yes,Yes,No,Spans or annotations,"IC, Sections (Aim; Method etc.)",Entities,None reported,Describes negative influence of abbreviations or adjectival and prepositional constructs as sources of misclassification. ,Yes; random splits and ratio given,2009,,,,,,,,,,,,,,,,,,,,
"PDF, XML",A distantly supervised dataset for automated data extraction from diagnostic studies.,"not available
","Christopher R. Norman
 and Mariska M. G. Leeflang
 and RenÃ© Spijker
 and Evangelos Kanoulas
 and AurÃ©lie NÃ©vÃ©ol
 ",https://doi.org/10.18653/v1/w19-5012 ,01/05/2020,,Include,TRUE,01/05/2020,Include,10348,#Ask: code,,"â€œOur results suggest that distant supervision can work as well as, or better than direct supervision on this problem, and that distantly trained models can perform as well as, or better than human annotators.â€",,LS,"BERT incl. biomedical versions,  Other ML classifier, PDF extraction",Precision,"Abstracts, Full texts, Titles",Diagnostic test,"Cochrane reviews, LIMSI-Dataset: Norman, Christopher, Leeflang, Mariska, & NÃ©vÃ©ol, AurÃ©lie. (2018). Cochrane DTA Reference Dataset (Version 20181101) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.1303259. Available are: entities, forest plots, bias judgements, some study characteristics. Not availiable (copyright) full original texts, manually annotated sentences.",,No,Yes,Part of code is accessible through google's tensorflow implementation of BERT/ Glue task. ,Yes,No,No,"Different algorithms, pre-processing strategies and comparison with human annotators is discussed. Evaluation resuts given wrt. every review in the dataset and discussed for low-performing articles.","Tensorflow and BIOBERT model, sci-kit learn, https://metamap.nlm.nih.gov/ (Metamap), Grobid (Lopez, 2009)
",Yes,"Clear for BioBERT as standard implementation was used, Weighting for data imbalance, cross-validation","No evaluation on different dataset/benchmark set, at this point no benchmark set is available for these
entities.",Yes,No,Yes,evaluated non-pre-provessed vs UMLS semantic-type replacement,Yes,"""small size is further compounded byproblems with convertingPDFto text, which mayalso bias the training and evaluation in favor of ar-ticles where the conversion works better (mainly articles from big publishers)"", non-relevant articles included noise in the dataset",average precision across dataset,"http://doi.org/10.5281/zenodo.1303259
Training data derived from Cochrane Database LIMSI-data (free text summaries of PICOs from 48 reviews). â€œ90,996 sentenceswere distantly labeled for target condition, 94,290 sentences were distantly labeled for index test, and 79,504 sentences were distantly labeled for reference standardâ€. 981 + 1,031 sentences manually annotated. 
",Supervised Distant Supervision; logistic regression for ranking sentences,Yes,Yes,NR,No,Yes,"BIOBERT source and fine-tuning described and default parameters, logistic regression description and
hyperparameters discussed.",Training data from Cochrane Database RCT data (free text summaries of PICOs from reviews),"generic entities of interest to any DTA-mining algorithm, general-topic text",No,Yes,No,Spans or annotations,"Diagnostic tests, P (Condition or disease)","Entities, Sentences",None reported,NR,Yes; compl. different datasets (ie. diferent SRs; journals..),2019,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Improving Endpoint Detection to Support Automated Systematic Reviews,"Authors of biomedical articles use comparison sentences to communicate the findings of a study, and to compare the results of the current study with earlier studies. The Claim Framework defines a comparison claim as a sentence that includes at least two entities that are being compared, and an endpoint that captures the way in which the entities are compared. Although automated methods have been developed to identify comparison sentences from the text, identifying the role that a specific noun plays (i.e. entity or endpoint) is much more difficult. Automated methods have been successful at identifying the second entity, but classification models were unable to clearly differentiate between the first entity and the endpoint. We show empirically that establishing if head noun is an amount or measure provides a statistically significant improvement that increases the endpoint precision from 0.42 to 0.56 on longer and from 0.51 to 0.58 on shorter sentences and recall from 0.64 to 0.71 on longer and from 0.69 to 0.74 on shorter sentences. The differences were not statistically significant for the second compared entity.
","Lucic, A.
 and Blake, C. L.
 ","not available
",01/05/2020,,Include,TRUE,01/05/2020,Include,2288,"Ana Lucic School of Information Sciences, University of Illinois, Champaign, IL and Catherine L. Blake

#ASK: code, data",,"""The results from this study suggest that establishing if the head noun is an amount or measure enables the Support Vector Machine to differentiate nouns that play an endpoint role from other candidate noun phrases in a comparison sentence more effectively. Thus treating endpoints as an activity and seeing them through the lens of measurement provided a boost in performance with reference to their identification and retrieval""",Not globally applicable,LS,"Rule-based, SVM","Precision, Recall, F1, Accuracy, Other","Full texts, Titles, Abstracts","RCT, Animal studies, Mix","comparison sentences from the journals Diabetes, Carcinogenesis, and Endocrinology, TREC Genomics collection",,No,No,NR,No,No,No,"analysis of sentence length and impact on system performance, error analysis","Oracle Data Miner, version 3.2; Stanford dependency parser, version 3.5.1.",Yes,NR,"Not tested on different/larger set, but the limitation is noted in the text",No,No,Yes,"Tokenisation, conversion to features, dependency parsing. ",Yes,"Matching detected endpoints to ontological classes using UMLS is not straightforward, especially if the population group consists of animals. Discuss ambiguity (eg. 'concentration' as quantitative or as mental concept).  Precision of grammatical and semantic parsers needs to improve in order to improve results.",statistical significance and confidence intervals for each metric,"size: 100 sentences with 641 noun phrases for train, test set of 132 sentences w. 939 noun phrases",,Yes,Yes,NR,No,No,"SVM kernels described: gaussian/linear, binary vs. multi-class. Lists of terms in order to identify copparisons or descriptions of change. No hyperparameters or implementation details given","'entity' class which can be both patient and Intervention/control, always 2 entities that are being compared wrt. endpoint (here O)","specific dataset from selected journal, small in size",No,Yes,No,Spans or annotations,O,Entities,None reported,NR,Yes; compl. different datasets (ie. diferent SRs; journals..),2017,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Automatic classification of sentences to support evidence based medicine,"Aim: Given a set of pre-defined medical categories used in Evidence Based Medicine, we aim to automatically annotate sentences in medical abstracts with these labels.
Method

We constructed a corpus of 1,000 medical abstracts annotated by hand with specified medical categories (e.g. Intervention, Outcome). We explored the use of various features based on lexical, semantic, structural, and sequential information in the data, using Conditional Random Fields (CRF) for classification.
Results

For the classification tasks over all labels, our systems achieved micro-averaged f-scores of 80.9% and 66.9% over datasets of structured and unstructured abstracts respectively, using sequential features. In labeling only the key sentences, our systems produced f-scores of 89.3% and 74.0% over structured and unstructured abstracts respectively, using the same sequential features. The results over an external dataset were lower (f-scores of 63.1% for all labels, and 83.8% for key sentences).
Conclusions

Of the features we used, the best for classifying any given sentence in an abstract were based on unigrams, section headings, and sequential information from preceding sentences. These features resulted in improved performance over a simple bag-of-words approach, and outperformed feature sets used in previous work.","S.N. Kim, D. Martinez, L. Cavedon, L. Yencken ",https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-S2-S5,,,Include,TRUE,9/24/2020,Include,15918," David Martinez NICTA VRL, The University of Melbourne, 3010, Australia

#ASK: Code? The supplementary data are not available anymore (Error on BMC website when trying to access)
Data not available through original publication but provided through other publications. ",,"'Of the features we used, the best for classifying any given sentence in an abstract were based on unigrams, section headings, and sequential information from preceding sentences. These features resulted in improved performance over a simple bag-of-words approach, and outperformed feature sets used in previous work.'",,LS,CRF,"Precision, Recall, F1","Abstracts, Titles",RCT,MEDLINE,,Yes,Yes,NR,No,No,No,error analysis and discussion wrt. structured vs unstructured abstracts and results. ,"Mallet for CRF, CPAN module Lingua::EN::Tagger, MetaMap analyser",Yes,Cross-validation (10-fold),Evaluatd on external corpus for benchmarkiing (15932),Yes,Yes,Yes,"Tokenization, POS-tagging. ",Yes,"Sparseness of terms in bigram-representations and ambiguity in output from MetaMap. Ambiguity in sentence-labels, especially the label 'Other' becasue it can include information from other classes. ",micro scores. ,"size (1000 abstracts), distribution of classes given. Text topics include traumatic brain injury and spinal cord injury, and other randomly sampled articles from general-topic text. 376/1000 are structured abstracts. ",,Yes,Yes,NR,No,Yes,"hyperparameters described as default setting, used gaussian prior. Features (lexical, semantic, structurral and sequential) described. ","multi-class, multi-label. ","Common entities, general-topic text. The authors replicated the system from 668 (small changes) and tested it on their own dataset. ",No,Yes,No,Spans or annotations,"P, IC, O, Design",Sentences,"TP, TN, FP, FN",NR,Yes; random splits and ratio given,2011,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Combining Classifiers for robust PICO element detection,"Background

Formulating a clinical information need in terms of the four atomic parts which are Population/Problem, Intervention, Comparison and Outcome (known as PICO elements) facilitates searching for a precise answer within a large medical citation database. However, using PICO defined items in the information retrieval process requires a search engine to be able to detect and index PICO elements in the collection in order for the system to retrieve relevant documents.
Methods

In this study, we tested multiple supervised classification algorithms and their combinations for detecting PICO elements within medical abstracts. Using the structural descriptors that are embedded in some medical abstracts, we have automatically gathered large training/testing data sets for each PICO element.
Results

Combining multiple classifiers using a weighted linear combination of their prediction scores achieves promising results with an f-measure score of 86.3% for P, 67% for I and 56.6% for O.
Conclusions

Our experiments on the identification of PICO elements showed that the task is very challenging. Nevertheless, the performance achieved by our identification method is competitive with previously published results and shows that this task can be achieved with a high accuracy for the P element but lower ones for I and O elements.","Boudin F, Nie JY, Bartlett JC, Grad R, Pluye P, Dawes M",https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/1472-6947-10-29,,,Include,TRUE,9/24/2020,Include,15919,"#ASK: Code, data",,"'Our experiments on the identification of PICO elements showed that the task is very challenging. Nevertheless, the performance achieved by our identification method is competitive with previously published results and shows that this task can be achieved with a high accuracy for the P element but lower ones for I and O elements.'",,LS,"SVM, Other ML classifier,  Multi-layer perceptron","Precision, Recall, F1","Abstracts, Titles",RCT,PubMed,,No,Yes,NR,No,No,No,brief error analysis and discussion of each element. ,WEKA,Yes,Cross-validation (10-fold),no evaluation on external data,Yes,Yes,Yes,"Normalisation to canonical form, alphanumeric conversted to numeric and tagging: cue-words/word-lists determined manually and derived from MeSH/UMLS for semantic types (P, IC, and O).",Yes,"Risk of lower performance if abstract includes sequences with unexpected/missing rhetoric role. Removal or unavailability of those sentences (ie Aim, method, result, conclusion) introduces bias. Further challenges are: errors committed by POS tagging and high computational cost for high-dimentional feature vectors. testing data sets are different and therefore not directly comparable. I and O are harder to classify than P, due to higher complexity, eg. in drug names or multiple outcomes in one abstract. ",,"14279 abstracts for P, 9095 abstracts for I and 2394 abstracts for O, auto-annotation of structured abstracts, human RCTs 1999-2009. Nr of sentences for each class is given. ",positron classifyer; ensemble with different voting strategies. ,Yes,Yes,"Describe influence of using top predictied sentence only, versus using top 2 and 3 sentences and resulting F1 score. ",Yes,Yes,Kernel-settings and hyperparameters described for SVM,,"common entities, general-topic text, but no evaluation done on benchmarking dataset. ",No,Yes,No,Spans or annotations,"P, IC, O",Sentences,None reported,NR,Yes; random splits and ratio given,2010,,,,,,,,,,,,,,,,,,,,
Text or probably text file,A statistical relational learning approach to identifying evidence based medicine categories,"Evidence-based medicineis an approachwhereby clinical decisions are supported bythe best available findings gained from scien-tific research. This requires efficient accessto such evidence. To this end, abstracts inevidence-based medicine can be labeled usinga set of predefined medical categories, the so-calledPICOcriteria. This paper presents anapproach to automatically annotate sentencesin medical abstracts with these labels. Sinceboth structural and sequential information areimportant for this classification task, we usekLog, a new language for statistical relationallearning with kernels. Our results show a clearimprovement with respect to state-of-the-artsystems.","Verbeke M, Van Asch V, Morante R, Frasconi P, Daelemans W, De Raedt L. A",https://www.aclweb.org/anthology/D12-1053/,,,Include,TRUE,9/24/2020,Include,15920,"mathias.verbeke@cs.kuleuven.be

Code? was error analysis done, or was this system implemented and used after the publication of this paper? ",,"'This paper presents anapproach to automatically annotate sentencesin medical abstracts with these labels.  Sinceboth structural and sequential information areimportant  for  this  classification  task,  we  usekLog, a new language for statistical relationallearning with kernels. Our results show a clearimprovement  with  respect  to  state-of-the-artsystems.'",,LS,"SVM, Other, Hidden markov model",F1,"Abstracts, Titles",RCT,15918 NICTA-PIBOSO and 15932,,Yes,Yes,NR,No,No,No,Description for difference between structured and unstructured abstracts,"BiogaphTA named entity module, BiogaphTA   named   entity   module, memory-based tagger MBT",Yes,cross-validation (10-fold),used benchmarking datasets (15918 NICTA-PIBOSO and 15932),Yes,No,Yes,"tagging and dependency parsing, lemmatisation",Yes,NR,micro average,15918 NICTA-PIBOSO and 15932,kLog,Yes,Yes,NR,No,Yes,Method and some hyperparameters are described. ,,"common entities, general-topic text",No,Yes,No,Spans or annotations,"P, IC, O, Design",Sentences,None reported,NR,Yes; random splits and ratio given,2012,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Extracting clinical trial design information from MEDLINE abstracts,"Evidence-based medicine (EBM) requires medical practitioners to select appropriate treatments for individual patients based on the current best evidence, and the results of phase III clinical trials are the major source of such evidence. In this paper, we report results of experiment in extracting important information for EBM from the abstracts of phase III clinical trials, in an effort to investigate how far the existing natural language processing (NLP) techniques could support EBM using MEDLINE database.","Hara K, Matsumoto Y. Extracting clinical trial design information from MEDLINE abstracts",https://link.springer.com/article/10.1007/s00354-007-0017-5,,,Include,TRUE,9/24/2020,Include,15921,"Kazuo HARA and Yuji MATSUMOTO
kazuo-h@is.naist.jp, matsu@is.naist.jp
 
#ASK: Data for chunks, lNP labels, and sentence labels, code? ",,"' In this paper, we report results of experiment inextracting important information for EBM from the abstracts of phase IIIclinical trials, in an effort to investigate how far the existing natural languageprocessing (NLP) techniques could support EBM using MEDLINE database.'",,LS,"Rule-based, CRF, SVM, Tree classifier","Precision, Recall, Other","Abstracts, Titles",RCT,PubMed,,No,Yes,NR,No,No,No,"Error analysis, assessment wrt. to entities (labelling NP chunks) and sentences done. ","YamCha, CRF++, TinySVM,  BACT (http://tahoo.org/taku/software/bact),",Yes,"Cross-validation (10-fold) for the CRF and SVM, leave-one-out training for another SVM,NA for the rule-based regex pattern matching. ",Not tested on external or general-topic benchmark dataset. ,No,No,Yes,"chunking into noun phrases, POS tagging",Yes,"Complicated sentences with many information elements are a challenge. Noise in the dataset, as some abstracts did not contain any information of interest. ",number of abstracts with correct classifications. ,"RCTs and neoplasms as topic, size (200, 140 contain relevant information), distribution of classes. ",,Yes,Yes,NR,No,No,"Multiple classifiers and approaches are described, hyperparameters not always clear. ",,Dataset restricted to neoplasms as topic. ,No,Yes,No,Spans or annotations,"P, IC","Entities, Sentences",TP,NR,Yes; random splits and ratio given,2006,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Exploiting classification correlations for the extraction of evidence-based practice information,"Crucial study data in research articles, such as patient details, study design and results, need to be extracted and presented explicitly for the ease of applicability and validity judgment in evidence-based practice. To perform this extraction, we propose to use two soft classifications, one at the sentence level and the other at the word level, and exploit the correlations between them for better accuracy. Our evaluation results show that propagating the results from the first classification to second improves performance of the second and vice versa. Moreover, the two classifications may benefit each other and help improve performance through joint inference algorithms. Another key finding of our work is that irrelevant sentences in the training data need to be properly filtered out; otherwise they compromise system accuracy and make joint inference models less scalable and more expensive to train.","Zhao J, Bysani P, Kan MY",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3540431/,,,Include,TRUE,9/24/2020,Include,15922,"Note: some data elements are extracted as entities, as sentences, or as both. 
#ASK: Data, code? ",,"' Our evaluation results show that propagating the results from the first classification to second improves performance of the second and vice versa. Moreover, the two classifications may benefit each other and help improve performance through joint inference algorithms. Another key finding of our work is that irrelevant sentences in the training data need to be properly filtered out; otherwise they compromise system accuracy and make joint inference models less scalable and more expensive to train.'",,LS,CRF,"Precision, Recall, F1","Abstracts, Titles",RCT,"corpus from the 'Evidence-Based Nursing Unit in National University Hospital', comprised of publications from 17 journals. ",,No,Yes,NR,No,No,No,"error analysis, effect of sentences that can contain multiple class-labels, and discussion of different model performance wrt. the sentence or entity classes. ","MALLET for CRF, GRMM for joint models. ",Yes,Cross-validation (5-fold),Evaluated only on one corpus,No,No,Yes,"Tokenization and tagging (MEsH, entity and wordlists) for feature creation",Yes,"Variety of linguistic expressions, short descriptions, large vocabulary size (eg. for medical condition and interventions). The system has low recall. Errors are produced when a sentence has multiple true class labels. Scalability is mentioned as drawback. ",,"Size (2000 sentences), distribution of classes",,Yes,Yes,NR,No,Yes,Features for CRF described,,"common entities, general-topic text",No,Yes,No,Spans or annotations,"P, P (Condition or disease), IC, Age, Gender, Design, Race","Entities, Sentences",None reported,NR,Yes; random splits and ratio given,2012,,,,,,,,,,,,,,,,,,,,
Text or probably text file,A  method  of  extracting  thenumber  of  trial participants   from   abstracts   describing   randomized   controlled   trials,"We have developed a method for extracting the number of trial participants from abstracts describing randomized controlled trials (RCTs); the number of trial participants may be an indication of the reliability of the trial. The method depends on statistical natural language processing. The number of interest was determined by a binary supervised classification based on a support vector machine algorithm. The method was trialled on 223 abstracts in which the number of trial participants was identified manually to act as a gold standard. Automatic extraction resulted in 2 false-positive and 19 false-negative classifications. The algorithm was capable of extracting the number of trial participants with an accuracy of 97% and an F-measure of 0.84. The algorithm may improve the selection of relevant articles in regard to question-answering, and hence may assist in decision-making.","Hansen  MJ,  Rasmussen  NO,  Chung  G",https://pubmed.ncbi.nlm.nih.gov/18852316/,,,Include,TRUE,9/24/2020,Include,15924,"Marie J Hansen*, Nana Ã˜ Rasmussen*and Grace Chung mjha03@hst.aau.dk 

#ASK: Is the labelled corpus available? Are the labels for sample size part of the original corpus, or did you label sample size additionally, in the scope of this paper? Code?",,"' The algorithm was capable of extracting the number of trial participants with an accuracyof 97% and an F-measure of 0.84. The algorithm may improve the selection of relevant articles in regard toquestion-answering, and hence may assist in decision-making.'",,LS,"Rule-based, SVM, Other ML classifier","Precision, Recall, F1, Accuracy","Abstracts, Titles",RCT,Uses corpus from 15933 ,,No,No,NR,No,No,No,Error analysis for misclassifications. ,"Genia Tagger for POS, WEKA for classifiers",Yes,"Reports random splits of data, but no implementation details are given. ",Tested on small corpus that was limited to RCTs of pharmaceutical interventions. ,No,No,Yes,"Normalisation of alpha-numeric numbers, removal of sentences that do not contain integers, POS tagging. Post-processing was done. ",Yes,"Total population number can be confused with numbers for each arm, or with other numbers (such as dosages, where the unit is not directly adjacent). Alternatively, if total N is not reported, a single N from an arm is chosen, leading to errors. ",,size ( 223 abstracts) and distribution of entities,"NB and K-nearest neighbour algorithms implemented, but no results reported. ",Yes,Yes,NR,No,Yes,"Feature extraction, word-lists for feature creation, kernel description for SVM",,Authors implemented a previously published algorithm (10036) by another team and tested it on their data,No,Yes,No,Spans or annotations,N (total),Entities,"TP, TN, FP, FN",NR,Yes; random splits and ratio given,2008,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Extracting Subject Demographic Information from Abstracts of Randomized Clinical Trial Reports,"In order to make more informed healthcare decisions, consumers need information systems that deliver accurate and reliable information about their illnesses and potential treatments. Reports of randomized clinical trials (RCTs) provide reliable medical evidence about the efficacy of treatments. Current methods to access, search for, and retrieve RCTs are keyword-based, time-consuming, and suffer from poor precision. Personalized semantic search and medical evidence summarization aim to solve this problem. The performance of these approaches may improve if they have access to study subject descriptors (e.g. age, gender, and ethnicity), trial sizes, and diseases/symptoms studied.","Xu R, Garten Y, Supekar KS, Das AK, Altman RB, Garber AM",https://pubmed.ncbi.nlm.nih.gov/17911777/,,,Include,TRUE,9/24/2020,Include,15925,#ASK: Is code and labelled data (esp. the 250 unstructured abstracts with the entity labels) available? There was a link given in the text (http://www.stanford.edu/~xurong/medinfo2007) but the site is not available anymore. ,,"'We  have  developed  a  novel  method  to  automaticallyextract  such  subject  demographic  information  from  RCTabstracts.  We  used  text  classification  augmented  with  aHidden  Markov  Model  to  identify  sentences  containingsubject  demographics,  and  subsequently  these  sentenceswere  parsed  using  Natural  Language  Processing  tech-niques  to  extract  relevant  information.  Our  results  showaccuracy levels of 82.5%, 92.5%, and 92.0% for extractionof  subject  descriptors,  trial  sizes,  and  diseases/symptomsdescriptors respectively.'",,LS,"Rule-based, Other ML classifier, Hidden markov model","F1, Accuracy","Abstracts, Titles",RCT,Unclear,,No,No,NR,No,No,No,Error analysis for each entity,"Stanford parser,  UMLS MetaMap",Yes,"NA for rule-base, not described for ML models",No evaluation on external benchmarking data,Yes,No,No,POS parseing,Yes,"Writing styles between structured and unstructured abstracts vary, which leads to classificaion errors. Further variations in style and complexity are challenging. Parse-tree errors can lead to errors. ",,"3,896 structured RCT abstracts with 46370 sentences, and 4018  sentences  from 250 unstructured abstracts. Fine-grained extraction of subject descriptors, N, and disease/symptom was done based on the subset of  250 unstructured abstracts.",MaxEnt classifier,Yes,Yes,NR,No,Yes,"HMM for sentence classification has no description of hyperparameters, rule-based approach for identifying N entity and disease descriptors is described. Limited examples for rules are given, word lists are not available online anymore. ","'(1)  subject  descriptors  (such  as  â€œmenâ€,â€œhealthyâ€,  â€œelderlyâ€),  (2)  number  of  participants  (or  allsubgroups,  when  total  number  is  not  available),  and  (3) diseases/symptoms and their descriptors'","common entities, general-topic text",No,No,No,Spans or annotations,"P, P (Condition or disease), N (total), Other","Entities, Sentences",None reported,NA for rule-base,Yes; compl. different datasets (ie. diferent SRs; journals..),2007,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"Identifying treatments, groups, and outcomes in medical abstracts","Detecting and extracting treatments, treatmentgroups and outcomes is a key step in gener-ating summaries of medical research papers.We describe initial results in applying named-entity recognition methods to the task of ex-tracting such entities from BMJ abstracts. Re-sults are promising, showing that a conditionalrandom field approach using word and seman-tic features appears to be more useful for rec-ognizing treatments and outcomes than fea-tures based on word shape.","Summerscales RL, Argamon S, Hupert J, Schwartz A",http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.216.2540&rep=rep1&type=pdf,,,Include,TRUE,9/24/2020,Include,15926,"rsummers@iit.edu Rodney L. Summerscales
alansz@uic.edu Alan Schwartz
#ASK: data (same as 15927?), code",,"'We have presented here an initial approach to thenovel task of recognizing treatments, groups, andoutcomes in medical abstracts. Our results suggestthat features that include information about a wordare more useful for recognizing these entities thanfeatures based on word shape. This result is espe-cially true for recognizing outcome entities.'",,LS,CRF,"Precision, Recall, F1","Abstracts, Titles",RCT,"BMJ articles, obtained from PubMed",,Yes,Yes,NR,No,No,No,Error analysis wrt. misclassifications from each entity type and discussion of exact vs. partial match implications. ,"MALLET SimpleTagger for CRF, OpenNLP tokenizer,  Stanford Named EntityRecognizer for feature generation. ",Yes,Cross-validation (10-fold),Evaluated their system on a different corpus that contains the IC entity (Barbara Rosario and Marti A. Hearst. 2004. Classifyingsemantic relations in bioscience texts). This corpus includes intervention and diease entities from the biomedical literature.  ,No,No,Yes,"Tokenization, POS tagging",Yes,"Multiple entity-lengths are possible, depending on the amount of context that is accepable. Therefore, exact, and different partial matches are explored as options. ",,"Size (100 abstracts), 1344 sentences and entity distribution given. ",,Yes,Yes,NR,No,Yes,"CRF features described, sources for wordlists and feature generation are given. ",,"common entities, general-topic text, although the corpus is small",Yes,Yes,No,Spans or annotations,"IC (per arm), O",Entities,None reported,"Describes that the system has problems identifying the 'Group' entity if neither the words 'group' nor 'arm' are missing. Since group entity can often be identified through those words, this entity performs best when compared to the other entities. ",Yes; random splits and ratio given,2009,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Automatic Summarization of Results from Clinical Trials,"A central concern in Evidence Based Medicine (EBM) is how to convey research results effectively to practitioners. One important idea is to summarize results by key summary statistics that describe the effectiveness (or lack thereof) of a given intervention, specifically the absolute risk reduction (ARR) and number needed to treat (NNT). Manual summarization is slow and expensive, thus, with the exponential growth of the biomedical research literature, automated solutions are needed. In this paper, we present a novel method for automatically creating EBM-oriented summaries from research abstracts of randomly-controlled trials (RCTs). The system extracts descriptions of the treatment groups and outcomes, as well as various associated quantities, and then calculates summary statistics. Results on a hand-annotated corpus of research abstracts show promising, and potentially useful, results.","Summerscales Rodney L, Argamon Shlomo, Bai Shangda, Hupert Jordan, Schwartz Alan",https://ieeexplore.ieee.org/document/6120468,,,Include,TRUE,9/24/2020,Include,15927,"rsummers@iit.edu Rodney L. Summerscales
alansz@uic.edu Alan Schwartz
#ASK: data (same as 15926?), code",,"'We have presented a method for accomplishing the noveltask of automatically extracting information and calculatingsummary statistics from peer-reviewed medical research arti-cles describing randomized controlled trials. Such structuredsummaries are needed to support effective evidence-basedmedicine. To our knowledge, this is the first attempt atextracting outcome numbers and associating mentions withquantities for the purpose of calculating summary statistics.'",,LS,"Rule-based, CRF, Other ML classifier","Precision, Recall, F1","Abstracts, Titles",RCT,"Similar to 15926, BMJ abstracts of RCTS 2005-2009 obtained via PubMed",,No,Yes,NR,No,No,No,Error analysis and evaluation of the correctnes of statistical summaries produced by this method. ,"As in 15926, plus MegaM v0.92 for MaxEnt classifier. ",Yes,Cross-validation (10-fold),NR,No,No,Yes,"As in 15926, but using UMLS features. Post-processing done. ",Yes,"Ways of reporting demographic information (eg. N of male/female) and total/group N is similar, and challenging to distinguish in an automated fashion. Referring to the same group  with multiple names throughout is noted as another challenge. ","Evaluated the quality of automatically generated statistical summaries wrt. the extracted data (in terms of P, R, F1 scores)",size (263 abstracts),MaxEnt classifier,Yes,Yes,NR,No,Yes,"CRF for classifying integers as N, number related to outcome, or other. No hyperparameers given for any classifier.  Other classification is same as 15926, but this paper employs an additional Maximum Entropy Classifier to associate group with size, outcome with group, outcome with number. Rule-base is used to determine if a group is experimental or control. ",,"common entities, general-topic text, Compared with the BANNER system on this dataset",No,Yes,No,Structured text and summary,"IC (per arm), O, N (per arm), N (total)","Entities, Sentences",None reported,NR,Yes; random splits and ratio given,2011,,,,,,,,,,,,,,,,,,,,
HTML,Extracting formulaic and free text clinical research articles metadata using conditional random fields,"We explore the use of conditional randomfields (CRFs) to automatically extract impor-tant metadata from clinical research articles.These metadata fields include formulaic meta-data about the authors, extracted from the titlepage, as well as free text fields concerning thestudyÃ¢â‚¬â„¢s critical parameters, such as longitudi-nal variables and medical intervention meth-ods, extracted from the body text of the arti-cle. Extracting such information can help bothreaders conduct deep semantic search of arti-cles and policy makers and sociologists trackmacro level trends in research. Preliminary re-sults show an acceptable level of performancefor formulaic metadata and a high precisionfor those found in the free text.","Lin S, Ng J-P, Pradhan S, Shah J, Pietrobon R, Kan M-Y",https://www.aclweb.org/anthology/W10-1114/,,,Include,TRUE,9/24/2020,Include,15928,"justin@seinlin.com, {junping,kanmy}@comp.nus.edu.sg
Sein Lin

#ASK: Data, code, was the dataset fully labelled, and were there any unpublished or published algorthms or practical usage of the data extraction described here? ",,"'We  have  developed  a  CRF-based  information  ex-traction  system  that  targets  two  different  types  ofmetadata  present  in  clinical  articles.   Our  work  inprogress  demonstrates  that  formulaic  author  meta-data  can  be  effectively  extracted  using  the  CRFmethodology.   By further performing feature engi-neering, we were able to extract key study parame-ters with a moderate level of success'",,LS,"Rule-based, CRF","Precision, Recall, F1","Full texts, Titles, Abstracts","RCT, Cohort",PMC,,No,Yes,NR,No,No,No,"Very short description of errors, this work is described as being in-progress, so a more throrough evaluation ",CRF++,Yes,Cross-validation (3-fold),NR,No,No,Yes,"Stemming, case normalisation, keyword lists for feature generation",No,Comprehensive annotation guidelines are needed to avoid missed data by annotators. ,,"Size (185 full texts, of which 93 are manually labelled at time of publication) Entity distribution. Full-text cardiological and oncological studies from Asia-Pacific region. Labelled data are limited to relevant secions, eg. Methods and Demographics,",,Yes,Yes,NR,No,No,"No description of hyperparameters or CRF architecture other than the library used. Description of feature generation is given, but the work itself is described to be still in progress. ",,Dataset text limited to two common conditions and Asia-Pacific region,No,No,No,Spans or annotations,"IC, N (total), Age, Setting, Design, Enrolment dates, Funding org",Entities,None reported,NR,Yes; random splits and ratio given,2010,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Knowledge  Extraction  for  Clinical  Question  Answering:   Preliminary   Results,"The combination of recent developments in question an-swering research and the unparalleled resources devel-oped specifically for automatic semantic processing oftext in the medical domain provides a unique opportu-nity to explore complex question answering in the clin-ical domain. In this paper, we attempt to operationalizemajor aspects of evidence-based medicine in the formof knowledge extractors that serve as the fundamentalbuilding blocks of a clinical question answering sys-tem. Our evaluations demonstrate that domain-specificknowledge can be effectively leveraged to extract PICOframe elements from MEDLINE abstracts. Clinical in-formation systems in support of physiciansÃ¢â‚¬â„¢ decision-making process have the potential to improve the qual-ity of patient care in real-world settings.",D.  Demner-Fushman,http://users.umiacs.umd.edu/~jimmylin/publications/Demner-Fushman_Lin_AAAI2005_workshop.pdf,,,Include,TRUE,9/24/2020,Include,15930,"#ASK: Data, code (rules?). ",,'Our evaluations demonstrate that domain-specificknowledge can be effectively leveraged to extract PICOframe elements from MEDLINE abstracts. Clinical in-formation systems in support of physiciansâ€™ decision-making process have the potential to improve the qual-ity of patient care in real-world settings.',,LS,"Rule-based, Other ML classifier, Other",Other,"Abstracts, Titles","RCT, Cohort, Case series, Case control, Diagnostic test, Other","Abstracts from MEDLINE, retrieved via PubMed. ",,No,No,NR,No,No,No,Error analysis wrt. every class. ,"MALLET for classifiers, MetaMap",Yes,NR,Only evaluated on this one dataset.,Yes,No,Yes,"POS tagging, chunking. ",Yes,"Numbers of patients are not always explicitly reported, not all populations are actually humans, ambiguity for intervention terms, POS and chunking errors can have a negative impact on model performane. Extracting the health condition depends on UMLS coverage of the disease. ","Number of correct, unknown and wrong sentences or entities. ",100 abstracts annotated fully with all entities for testing. Topics range from immunisation to different diseases (eg. diabetes). 633 abstracts annotated with outcomes. ,document length classifier,Yes,Yes,"For outcome extraction, the correct results when extracting top 2 or 3 sentences are decribed. ",Yes,Yes,"Rule-base described, implementation of classifiers is described, no hyperparameters are given. ",Condition (problem),"common entities, general-topic text",No,Yes,No,Spans or annotations,"P, P (Condition or disease), IC, O, N (total), Design","Entities, Sentences",TP,NR,Yes; other description given,2005,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Interpreting comparative constructions in biomedical text,"We propose a methodology using underspecified semantic interpretation to process comparative constructions in MEDLINE citations, concentrating on two structures that are prevalent in the research literature reporting on clinical trials for drug therapies. The method exploits an existing semantic processor, SemRep, which constructs predications based on the Unified Medical Language System. Results of a preliminary evaluation were recall of 70%, precision of 96%, and F-score of 81%. We discuss the generalization of the methodology to other entities such as therapeutic and diagnostic procedures. The available structures in computable format are potentially useful for interpreting outcome statements in MEDLINE citations.","M. Fiszman, D. Demner-Fushman, F.M. Lang, P. Goetz, T.C. Rindflesch",https://www.aclweb.org/anthology/W07-1018.pdf,,,Include,TRUE,9/24/2020,Include,15931,"mfiszman@utmck.edu; {ddemner|goetzp|flang|trindflesch}@mail.nih.gov

More recent paper on SemRepsystem. Linked paper: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-486 and https://www.sciencedirect.com/science/article/pii/S1532046403001175?via%3Dihub

#ASK: On the SemRep website I saw that you made available a dataset of 500 annotated sentences, including compartive constructions. Do these sentences include the 300 sentences used in this paper here? Are there any further annotations specific to the  B",,"'Although we restricted the method to comparisons of   drug   therapies,   the   method   can   be   easily   generalized to other entities such as diagnostic and therapeutic   procedures.   The   availability   of   this   information  in  computable  format  can  support  the  identification  of  outcome  sentences  in  MEDLINE,  which  in  turn  supports  translation  of  biomedical  research  into  improvements  in  quality  of  patient  care. '",Not fitting target entities or data,LS,Rule-based,"Precision, Recall, F1","Abstracts, Titles",RCT,"MEDLINE, RCTs comparing drug interventions.","https://semrep.nlm.nih.gov/SemRep.v1.8_Installation.html, SemMed is a web-based application published after this paper was released: https://skr3.nlm.nih.gov/SemMed/semmed.html",No,No,"https://semrep.nlm.nih.gov/SemRep.v1.8_Installation.html#Download Code might be accessible via download, but account is needed. ",Yes,Yes,No,error analysis for false negatives and positives.,"Xerox Part-of-Speech Tagger, UMLS",Yes,,NR,Yes,No,Yes,"Tokenization, POS tagging and parsing grammatical structure via SemRep",Yes,"False negatives are caused by 'empty heads' phenomenon where 'syntactic head of a noun phrase does not reflect semantic thrust', this apples eg. where drug dosage and formulations are described in the text. Ambiguity is causing false-positives.",,"Size, distribution of entities reported. Sentences extracted from 10,000 abstracts for development, and 300 annotated sentences for evaluation. The dataset (published later, but available via website) contains 500 sentences.",,Yes,Yes,NR,No,Yes,"Rules, lexica and usage of system previously described. The Cpmparative pattern and decision process which nouns are extracted as entities is clear.","comp1 patterns for interventions, comp2 patterns for outcomes/effectiveness. Limited to drug interventions. ","general-topic text, although limited to drug interventions (common across literature)",No,Yes,Yes,"Structured text and summary, XML",IC (per arm),Entities,None reported,NR,Yes; compl. different datasets (ie. diferent SRs; journals..),2007,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Automatically Identifying Health Outcome Information in MEDLINE Records,"Objective: Understanding the effect of a given intervention on the patient's health outcome is one of the key elements in providing optimal patient care. This study presents a methodology for automatic identification of outcomes-related information in medical text and evaluates its potential in satisfying clinical information needs related to health care outcomes.

Design: An annotation scheme based on an evidence-based medicine model for critical appraisal of evidence was developed and used to annotate 633 MEDLINE citations. Textual, structural, and meta-information features essential to outcome identification were learned from the created collection and used to develop an automatic system. Accuracy of automatic outcome identification was assessed in an intrinsic evaluation and in an extrinsic evaluation, in which ranking of MEDLINE search results obtained using PubMed Clinical Queries relied on identified outcome statements.

Measurements: The accuracy and positive predictive value of outcome identification were calculated. Effectiveness of the outcome-based ranking was measured using mean average precision and precision at rank 10.

Results: Automatic outcome identification achieved 88% to 93% accuracy. The positive predictive value of individual sentences identified as outcomes ranged from 30% to 37%. Outcome-based ranking improved retrieval accuracy, tripling mean average precision and achieving 389% improvement in precision at rank 10.

Conclusion: Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicineÃ¢â‚¬â€œmodel approach in timely delivery of information critical to clinical decision support at the point of service.","Demner-Fushman D, Few B, Hauser SE, Thoma GR",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380197/,,,Include,TRUE,9/24/2020,Include,15932,"ddemner@mail.nih.gov

Note: Is extension of 15930 (study from same authors, original corpus)
#ASK: Code, data, mentioned that a system was planned to be implemented at the time",,'Preliminary results in outcome-based document ranking show potential validity of the evidence-based medicineâ€“model approach in timely delivery of information critical to clinical decision support at the point of service.',,LS,"Rule-based, SVM, Other ML classifier, Other","Precision, Recall, Other","Abstracts, Titles",RCT,PubMed/MEDLINE,Article mentions that an app is being implemented. ,Yes,No,NR,No,No,No,Error analysis and real-life task evaluation. ,"WEKA, MALLET",Yes,"Random splits of training and testing data are described, but overfitting is not discussed. ","1,312 MEDLINE citations used as real-life task evaluation ranking task (topics back pain, obesity, osteoporosis, panic disorder and warts). Generalizability is discussed. ",No,No,Yes,"Tokenization, no further information. ",Yes,"Length of outcome statements and vague language caused inter-annotator disagreement, annotators should be trained and carefully instructed. ","precision at rank 10 (for ranking sentences as relevant), kappa","Distribution: 1.9 sentences per abstrat are outcomes, 592 references contain outcome sentences. Topics: rheumatoid arthritis, migraine, breast cancer, pulmonary tuberculosis, renal hypertension, and asthma.","n-gramâ€“based classifier, position classifier, a document length classifier, and a semantic classifier",Yes,Yes,NR,No,Yes,"Algorithms and toolkits are described, but no hyperparameters are given. ",,"common entities, general-topic text",No,No,No,Spans or annotations,O,Sentences,TP,NR,Yes; random splits and ratio given,2006,,,,,,,,,,,,,,,,,,,,
Text or probably text file,A study of structured clinical abstracts and the semantic classification of sentences,"This paper describes experiments in classi-fying sentences of medical abstracts into anumber of semantic classes given by sectionheadings in structured abstracts. Using con-ditional random fields, we obtainF-scoresranging from 0.72 to 0.97. By using a smallset of sentences that appear under thePAR-TICPANTSheading, we demonstrate that it ispossible to recognize sentences that describepopulation characteristics of a study. Wepresent a detailed study of the structure ofabstracts of randomized clinical trials, andexamine how sentences labeled underPAR-TICIPANTScould be used to summarize thepopulation group.","Chung GY, Coiera E",https://www.aclweb.org/anthology/W07-1016.pdf,,,Include,TRUE,9/24/2020,Include,15933,"note: related to 668, dataset might be subset
{graceyc,e.coiera}@unsw.edu.au
#ASK: Code, data? ",,"'By using a smallset of sentences that appear under thePAR-TICPANTSheading, we demonstrate that it ispossible to recognize sentences that describepopulation  characteristics  of  a  study.    Wepresent  a  detailed  study  of the  structure  ofabstracts  of  randomized  clinical  trials,  andexamine how sentences labeled underPAR-TICIPANTS could be used to summarize thepopulation group.'",,LS,"CRF, SVM","Precision, Recall, F1, Accuracy","Abstracts, Titles",RCT,MEDLINE,,No,No,NR,No,No,No,Error analysis,"MetaMap, MALLET",Yes,"Data splits are described, but overfitting is not described. ","No evaluation on external dataset, although generalizability is briefly discussed. ",Yes,No,Yes,"tokenization, stemming and stop-word removal did not improve results. ",Yes,"Sentence labels can be ambiguous. Extracting total N from abstracts is harder for unstructured abstracts, as it is harder to predict the position where information will appear, or, the information might not appear at all in the abstract. Therefore, it is not straightforward to apply classifiers trained on structured abstracts to unstructured abstracts. ",,"size (3657 structured abstracts with sentence tags, 204 abstracts with N (total) labels). topics: asthma, diabetes, breast cancer, prostate cancer, erectile dysfunction, heart failure, cardiovascular, angina",,Yes,Yes,NR,No,Yes,"Implementation and features of CRF described, kernel described for SVM. Implementation of 2-stage system described. ",,"common entities, general-topic text. Discussion of related work and classification performance, but no direct comparison on this dataset. ",No,Yes,No,Spans or annotations,"P, N (total), Sections (Aim; Method etc.)",Sentences,None reported,NR,Yes; random splits and ratio given,2007,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Improving Medical Information Retrieval with PICO Element Detection,"Without a well formulated and structured question, it can be very difficult and time consuming for physicians to identify appropriate resources and search for the best available evidence for medical treatment in evidence-based medicine (EBM). In EBM, clinical studies and questions involve four aspects: Population/Problem (P), Intervention (I), Comparison (C) and Outcome (O), which are known as PICO elements. It is intuitively more advantageous to use these elements in Information Retrieval (IR). In this paper, we first propose an approach to automatically identify the PICO elements in documents and queries. We test several possible approaches to use the identified elements in IR. Experiments show that it is a challenging task to determine accurately PICO elements. However, even with noisy tagging results, we can still take advantage of some PICO elements, namely I and P elements, to enhance the retrieval process, and this allows us to obtain significantly better retrieval effectiveness than the state-of-the-art methods.","Boudin F, Shi L, Nie JY",https://link.springer.com/content/pdf/10.1007%2F978-3-642-12275-0.pdf,,,Include,TRUE,9/24/2020,Include,15935,"{boudinfl,shilixin,nie}@iro.umontreal.ca 
FlorianBoudin, Lixin Shi,andJian-YunNie

Sentence data, or the 378 tagged documents for the IR task? Code?",,"'Experiments show that it is a challenging task to determine accuratelyPICO elements. However, even with noisy tagging results, we can stilltake advantage of some PICO elements, namely I and P elements, toenhance the retrieval process, and this allows us to obtain significantlybetter retrieval effectiveness than the state-of-the-art methods.'",,LS,"Rule-based, SVM, Other ML classifier, Multi-layer perceptron, Other","Precision, Recall, F1, Other","Abstracts, Titles",RCT,"PubMed for sentnce corpus, and a Cochrane review for entity-level data. ",,No,Yes,NR,No,No,No,Brief error analysis,WEKA for classification,Yes,Cross-validation (10-fold),"Evaluation on internal corpus, entity-level retrieval is evaluated on diabetes-topic reviews. ",No,No,Yes,"Tokenization, removal of sentence headings",Yes,"By using structured abstracts as automatic labels, it is possible that information is missed. O and I elements are more complex than P, eg. mentioned more often in different places throughout an abstract. ","MAP (mean average precision), Precision at rank 10 (P@10)","Paragraphs: Population/Problem (14279), Intervention/Comparison (9 095) and Outcome (2394) from structured abstracts. 378 entity-level tagged documents. ","J48, position classifier, 'automatic query tagging'",Yes,Yes,NR,No,No,Word-lists for knowledge-based feature generation and statistical features are described. Some parameters for the SVM are described. In-depth explanation of the IR algorithm for entity retrieval. ,,"common entities, general-topic text for the sentence models. The entity-level query tagging is less comparable to other entity recognition systems because it does not evaluate the model in terms of the standard precision, recall, or f1 scores. ",Yes,Yes,No,Spans or annotations,"P, IC, O","Entities, Sentences",None reported,Removal of sentence headings in order to avoid creating a system biased towards common terms. ,Yes; random splits and ratio given,2010,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Overview of the ALTA 2012 Shared Task,"The ALTA shared task ran for the third time in2012, with the aim of bringing research studentstogether to work on the same task and data set,and compare their methods in a current researchproblem. The task was based on a recent studyto build classifiers for automatically labeling sen-tences to a pre-defined set of categories, in the do-main of Evidence Based Medicine (EBM). Thepartaking groups demonstrated strong skills thisyear, outperforming our proposed benchmark sys-tems. In this overview paper we explain the pro-cess of building the benchmark classifiers anddata set, and present the submitted systems andtheir performance.","Amini I, Martinez D, Molla D",https://www.aclweb.org/anthology/U12-1017.pdf,,,Include,TRUE,9/24/2020,Include,15936,"This publication summarises results from the ALTA 2012 shared task papers, cite these in review text:
https://www.aclweb.org/anthology/U12-1019/ Marco Lui
https://www.aclweb.org/anthology/U12-1018/ Spandana Gella, Long Duong Thanh
https://www.aclweb.org/anthology/U12-1020/ Diego Molla
15937,",,"'Thepartaking groups demonstrated strong skills thisyear, outperforming our proposed benchmark sys-tems. In this overview paper we explain the pro-cess of building the benchmark classifiers and data set, and present the submitted systems and their performance.'",,LS,"CRF, SVM, Other ML classifier, Other","AUC-ROC, Other","Abstracts, Titles",RCT,"As described in 15918, available under ID 9695",,No,Yes,NR,No,No,No,"No detailled description of error assessment is given, but for some of the participating systems this is described in the cited references. "," NLTK for POS, Mallet for CRF, MetaMap",Yes,Cross-validation (10-fold),Not evaluated on any other benchmarking dataset. ,Yes,No,Yes,"As described in 15918, available under ID 9695",No,"NR, mor information is givien in the linked papers. ",random shuffling for statistical significance between systems compared in this task. ,"As described in 15918, available under ID 9695",MaxEnt,Yes,Yes,NR,No,Yes,"Baseline: variation of the CRF classifier proposed in 15918, features are described. Descriptions and publications of the top-performing systems are given. For more details, see linked papers.  ",Background,"common entities, general-topic text. This is a shared task, comparison between different tools is the main objective of this paper. ",No,Yes,No,Spans or annotations,"P, IC, O, Design",Sentences,None reported,NR,Yes; random splits and ratio given,2012,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"Trialstreamer: a living, automatically updated database of clinical trial reports","Objective Randomized controlled trials (RCTs) are the gold standard method for evaluating whether a treatment works in healthcare, but can be difficult to find and make use of. We describe the development and evaluation of a system to automatically find and categorize all new RCT reports. Materials and Methods Trialstreamer, continuously monitors PubMed and the WHO International Clinical Trials Registry Platform (ICTRP), looking for new RCTs in humans using a validated classifier. We combine machine learning and rule-based methods to extract information from the RCT abstracts, including free-text descriptions of trial populations, interventions and outcomes (the 'PICO') and map these snippets to normalised MeSH vocabulary terms. We additionally identify sample sizes, predict the risk of bias, and extract text conveying key findings. We store all extracted data in a database which we make freely available for download, and via a search portal, which allows users to enter structured clinical queries. Results are ranked automatically to prioritize larger and higher-quality studies. Results As of May 2020, we have indexed 669,895 publications of RCTs, of which 18,485 were published in the first four months of 2020 (144/day). We additionally include 303,319 trial registrations from ICTRP. The median trial sample size in the RCTs was 66. Conclusions We present an automated system for finding and categorising RCTs. This yields a novel resource: A database of structured information automatically extracted for all published RCTs in humans. We make daily updates of this database available on our website (trialstreamer.robotreviewer.net).","Iain J Marshall, Benjamin Nye, JoÃƒÂ«l Kuiper, Anna Noel-Storr, Rachel Marshall, Rory Maclean, Frank Soboczenski, Ani Nenkova, James Thomas, Byron C Wallace","https://www.google.com/search?q=Trialstreamer:+a+living,+automatically+updated+database+of+clinical+trial+reports",,,Include,TRUE,9/24/2020,Include,15939,"System comprises architectures published in
Marshall
IJ
Noel-Storr
A
Kuiper
J
, et al.  Machine learning for identifying randomized controlled trials: an evaluation and practitionerâ€™s guide
. Res Synth Methods
2018
; 9 (4): 602â€“14. doi: 10.1002/jrsm.1287.
and extensions to 2748
#ASK: Labelled data for the sample size model? The labelled data for the RCT classifier? ",,'We present an automated system for finding and categorizing RCTs. This yields a novel resource: a database of structured information automatically extracted for all published RCTs in humans. We make daily updates of this database available on our website (https://trialstreamer.robotreviewer.net).',,LS,"CRF, Word embedding, LSTM, CNN, SVM, Multi-layer perceptron","Precision, Recall, AUC-ROC, Other","Abstracts, Titles",RCT,"PubMed, ICTRP","Database with all extracted data is available online: https://trialstreamer.robotreviewer.net/
",Yes,Yes,"https://trialstreamer.robotreviewer.net/
https://github.com/ijmarshall/trialstreamer
",Yes,Yes,Yes,"Error assessment, and subsequent application to large real-life datasets, such as COVID-19 data and demonstration on the website. ",Reported in associated Git repositories.,Yes,Cross-validation,Real-life application to new datasets and newly published information is described. ,Yes,Yes,Yes,As described in related publications. ,Yes,"Assigning classes, such as PICOs, to unstructured text is a challenging task. The trade-off between precision and recall means that some relevant RCTs might be missed (lower recall), but therefore the screening load is lower (higher precision)","C-statistic (95% CI), Brier score (95% CI)","Some parts of the dataset (EBM-NLP) are available, see 2748. ",,Yes,Yes,"Precision-recall trade-off for RCT classification, no information about trade-offs in PICO classification. ",Yes,Yes,"RCT classifier as described previously, classifier in human studies is an SVM with adaptation and parameters described in the reference. Rule-base and novel model for extracting total N is described in detail, incl. hyperparameters. Extensions to the LSTM model in 2748 and assignment to controlled vocabularies, eg. MEsH, SNOMED CT, UMLS.",,"common entities, general-topic text",No,Yes,Yes,"Spans or annotations, Structured text and summary","P, IC, O, N (total), Design",Entities,None reported,NR,Yes; random splits and ratio given,2020,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Clinical Context-Aware Biomedical Text Summarization Using Deep Neural Network: Model Development and Validation,"Automatic text summarization (ATS) enables users to retrieve meaningful evidence from big data of biomedical repositories to make complex clinical decisions. Deep neural and recurrent networks outperform traditional machine-learning techniques in areas of natural language processing and computer vision; however, they are yet to be explored in the ATS domain, particularly for medical text summarization. Traditional approaches in ATS for biomedical text suffer from fundamental issues such as an inability to capture clinical context, quality of evidence, and purpose-driven selection of passages for the summary. We aimed to circumvent these limitations through achieving precise, succinct, and coherent information extraction from credible published biomedical resources, and to construct a simplified summary containing the most informative content that can offer a review particular to clinical needs. In our proposed approach, we introduce a novel framework, termed Biomed-Summarizer, that provides quality-aware Patient/Problem, Intervention, Comparison, and Outcome (PICO)-based intelligent and context-enabled summarization of biomedical text. Biomed-Summarizer integrates the prognosis quality recognition model with a clinical context-aware model to locate text sequences in the body of a biomedical article for use in the final summary. First, we developed a deep neural network binary classifier for quality recognition to acquire scientifically sound studies and filter out others. Second, we developed a bidirectional long-short term memory recurrent neural network as a clinical context-aware classifier, which was trained on semantically enriched features generated using a word-embedding tokenizer for identification of meaningful sentences representing PICO text sequences. Third, we calculated the similarity between query and PICO text sequences using Jaccard similarity with semantic enrichments, where the semantic enrichments are obtained using medical ontologies. Last, we generated a representative summary from the high-scoring PICO sequences aggregated by study type, publication credibility, and freshness score. Evaluation of the prognosis quality recognition model using a large dataset of biomedical literature related to intracranial aneurysm showed an accuracy of 95.41% (2562/2686) in terms of recognizing quality articles. The clinical context-aware multiclass classifier outperformed the traditional machine-learning algorithms, including support vector machine, gradient boosted tree, linear regression, K-nearest neighbor, and naÃƒÆ’Ã‚Â¯ve Bayes, by achieving 93% (16127/17341) accuracy for classifying five categories: aim, population, intervention, results, and outcome. The semantic similarity algorithm achieved a significant Pearson correlation coefficient of 0.61 (0-1 scale) on a well-known BIOSSES dataset (with 100 pair sentences) after semantic enrichment, representing an improvement of 8.9% over baseline Jaccard similarity. Finally, we found a highly positive correlation among the evaluations performed by three domain experts concerning different metrics, suggesting that the automated summarization is satisfactory. By employing the proposed method Biomed-Summarizer, high accuracy in ATS was achieved, enabling seamless curation of research evidence from the biomedical literature to use for clinical decision-making.","Afzal, Alam, Malik, Malik",https://doi.org/10.2196/19810,20201026,PubMed,Include,TRUE,11/02/2020,Include,15968,They use Jin 2018 dataset for training but extend it with topic-specific data.  Todo: download and add dataset to dataverse,automatic text summarization; biomedical informatics; brain aneurysm; deep neural network; semantic similarity; word embedding,"By designing the proposed Biomed-Summarizer framework, we employed a set of methods for information extraction from credible published biomedical resources to construct a simplified summary that is precise, relevant, and contextually suited to clinical needs. We designed the framework to provide openness for other researchers to use, extend, or even make use of a subpart of it and extend for designing their own systems and services. Alongside the framework, we created a specialized dataset containing PICO elements and a few other text sequences such as Aim, Method, and Result for researchers to use in their experiments in the domain of brain aneurysm. The PICO dataset was extended using a custom data-mining process by incorporating the rigorous text processing techniques on PubMed research documents. This method can be further used to create a PICO dataset in other related biomedical domains by obtaining related research papers from PubMed. The evaluation results signify that the proposed Biomed-Summarizer framework performs significantly better than existing approaches.",,LS,LSTM,"Precision, Recall, F1","Abstracts, Titles",RCT,,,No,Yes,https://github.com/smileslab/Brain_Aneurysm_Research/tree/master/BioMed_Summarizer,Yes,No,Yes,,,Yes,,,Yes,Yes,Yes,descibed,Yes,"For auto summarisation: low recall due to missing info in abstracts, and missing assessments of study-quality. For auto-labelled text: noise",Jaccard Similarity after semantic enrichment for evaluating summarisation and matching to user queries,"Jin data edited to 173k PICO sentences by merging some categories (P+M, O+C) and then adding 42,000 Brain Aneurysm sentences. https://github.com/smileslab/Brain_Aneurysm_Research/tree/master/BioMed_Summarizer ",,Yes,Yes,,No,Yes,,,,No,Yes,Yes,Structured text and summary,"P, IC, O, Sections (Aim; Method etc.)",Sentences,None reported,,Yes; random splits and ratio given,2020, Pearson correlation coefficients of the scores of each evaluator for PICO summaries (0.82-0.92),,,,,,,,,,,,,,,,,,,
XML,"EBM+: Advancing Evidence-Based Medicine via two level automatic identification of Populations, Interventions, Outcomes in medical literature","Evidence-Based Medicine (EBM) has been an important practice for medical practitioners. However, as the number of medical publications increases dramatically, it is becoming extremely difficult for medical experts to review all the contents available and make an informative treatment plan for their patients. A variety of frameworks, including the PICO framework which is named after its elements (Population, Intervention, Comparison, Outcome), have been developed to enable fine-grained searches, as the first step to faster decision making. In this work, we propose a novel entity recognition system that identifies PICO entities within medical publications and achieves state-of-the-art performance in the task. This is achieved by the combination of four 2D Convolutional Neural Networks (CNNs) for character feature extraction, and a Highway Residual connection to facilitate deep Neural Network architectures. We further introduce a PICO Statement classifier, that identifies sentences that not only contain all PICO entities but also answer questions stated in PICO. To facilitate this task we also introduce a high quality dataset, manually annotated by medical practitioners. With the combination of our proposed PICO Entity Recognizer and PICO Statement classifier we aim to advance EBM and enable its faster and more accurate practice.","Stylianou, Razis, Goulis, Vlahavas",https://doi.org/10.1016/j.artmed.2020.101949,20201023,PubMed,Include,TRUE,11/02/2020,Include,16128,FALSE,Evidence Based Medicine; Machine learning; Natural Language Processing; Neural networks; PICO,"Conclusively, we showcased how a robust PICO Entity Recognizer
can benefit the PICO Statement classification task, which is directly
related to extracting evidence from medical publications. With the
ability to identify PICO entities in full medical publications, we have also
enabled other researchers to efficiently use PICO entities in their
respective tasks (e.g. question answering, information retrieval, etc.). With our contributions we aim to enhance the practice of EBM and assist
medical practitioners to faster decisions during treatment procedures.",,LS,"CRF, Word embedding, LSTM, CNN, Other ML classifier, Other","Precision, Recall, F1","Abstracts, Titles","RCT, Non-randomised (intervention) study",own and EBM-NLP,,No,Yes,https://github.com/nstylia/pico_entities/,Yes,No,Yes,,,Yes,,"Own dataset for sentence classification, EBM-NLP for entities. Only one dataset per task",Yes,Yes,Yes,,Yes,,,"from medline;ndocrinology, Gynecology, Neurology, Orthopedic
Surgery, Pediatrics and Thoracic Surgery. Data available in code repo https://github.com/nstylia/pico_entities/ ",ELMO,No,Yes,,No,Yes,,,compared 5 architectures for NER,Yes,Yes,Yes,Spans or annotations,P,"Entities, Sentences",None reported,ablation studies,Yes; mentions random splitting,2020,"Inter
Annotator Agreement (IAA) [35] (78-81) and Cohenâ€™s Kappa [36] (54-61).",,,,,,,,,,,,,,,,,,,
Text or probably text file,Aceso: PICO-guided Evidence Summarization on Medical Literature,"Evidence-Based Medicine (EBM) aims to apply the best available evidence gained from scientific methods to clinical decision making. A generally accepted criterion to formulate evidence is to use the PICO framework, where PICO stands for Problem/Population, Intervention, Comparison, and Outcome. Automatic extraction of PICO-related sentences from medical literature is crucial to the success of many EBM applications. In this work, we present our Aceso system, which automatically generates PICO-based evidence summaries from medical literature. In Aceso <sup>1</sup>, we adopt an active learning paradigm, which helps to minimize the cost of manual labeling and to optimize the quality of summarization with limited labeled data. An UMLS2Vec model is proposed to learn a vector representation of medical concepts in UMLS <sup>2</sup>, and we fuse the embedding of medical knowledge with textual features in summarization. The evaluation shows that our approach is better on identifying PICO sentences against state-of-the-art studies and outperforms baseline methods on producing high-quality evidence summaries.","Zhang, Geng, Zhang, Lu, Gao, Mei",https://doi.org/10.1109/JBHI.2020.2984704,20200805,PubMed,Include,TRUE,11/02/2020,Include,16957,"uses UMLS vector embeddings (metamap) for external knowledge, they make active-learning PICO annotations, and info from completed systematic reviews to speed-up creation of initial dataset",,"We present the Aceso system, which automatically generates evidence summaries from full-text medical literature. We adopt active learning to minimize the cost of human labeling. We also propose an UMLS2Vec model, which learns vector representation of each concept in UMLS semantic network and fuses medical knowledge with linguistic features in evidence summarization. Experiments show that our approach is better on identifying PICO sentences against previous studies and outperforms baseline methods on producing concise and informative evidence summaries.",,LS,"LSTM, CNN, Word embedding","Precision, Recall, F1","Abstracts, Full texts, Titles",RCT,,,Yes,,https://github.com/wds-seu/Aceso,Yes,No,Yes,,,Yes,,"Jin, data from 9742, and EBMNLP used in benchmarking in addition to own data",,,Yes,,Yes,"ambiguity: eg presence of specific words such as patient and age causing a P label in a true O sentence, in that case multi-label classification would have been beter than single-label",,"domain heart diseases, from RCTS included in finished reviews and fulltexts, 5099 sentences, P, IC, O data here: https://github.com/wds-seu/Aceso/tree/master/datasets. Used Jin, data from 9742, and EBMNLP",UMLS network vector embeddings (DeepWalk + word2vec),,Yes,,,Yes,,,,Yes,Yes,,"Structured text and summary, Spans or annotations","P, IC, O",Sentences,None reported,"manual error analysis, presence of specific words that lead to wrong classifications, confusion matrix for mis-classifications given.",Yes; random splits and ratio given,2020,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"Evidence Inference 20: More Data, Better Models","How do we most effectively treat a disease or condition? Ideally, we could consult a database of evidence gleaned from clinical trials to answer such questions. Unfortunately, no such database exists; clinical trial results are instead disseminated primarily via lengthy natural language articles. Perusing all such articles would be prohibitively time-consuming for healthcare practitioners; they instead tend to depend on manually compiled systematic reviews of medical literature to inform care.   NLP may speed this process up, and eventually facilitate immediate consult of published evidence. The Evidence Inference dataset was recently released to facilitate research toward this end. This task entails inferring the comparative performance of two treatments, with respect to a given outcome, from a particular article (describing a clinical trial) and identifying supporting evidence. For instance: Does this article report that chemotherapy performed better than surgery for five-year survival rates of operable cancers? In this paper, we collect additional annotations to expand the Evidence Inference dataset by 25\%, provide stronger baseline models, systematically inspect the errors that these make, and probe dataset quality. We also release an abstract only (as opposed to full-texts) version of the task for rapid model prototyping. The updated corpus, documentation, and code for new baselines and evaluations are available at http://evidence-inference.ebm-nlp.com/.","['Jay DeYoung', 'Eric Lehman', 'Ben Nye', 'Iain J. Marshall', 'Byron C. Wallace']",https://export.arxiv.org/abs/2005.04177,05/08/2020,arXiv,Include,TRUE,11/06/2020,Include,17134,FALSE,cs.CL,"We have introduced an expanded version of the Evidence Inference dataset. We have proposed and evaluated BERT-based models for the evidence inference task (which entails identifying snippets of evidence for particular ICO prompts in long results on this task. With this expanded dataset, we hope to support further development of NLP for assisting Evidence Based Medicine. Our results demonstrate promise for the task of automatically inferring results from Randomized Control Trials, but still leave room for improvement. In our future work, we intend to jointly automate the identification of ICO triplets and inference concerning these. We are also keen to investigate whether pre-training on related scientific â€˜fact verificationâ€™ tasks might improve performance (Wadden et al., 2020). Acknowledgments Wethank the anonymous BioNLP reviewers. This work was supported by the National Science Foundation, CAREER award 1750978. documents and then classifying the reported finding on the basis of these), achieving state of the art results on this task. With this expanded dataset, we hope to support further development of NLP for assisting Evidence Based Medicine. Our results demonstrate promise for the task of automatically inferring results from Randomized Control Trials, but still leave room for improvement.",,LS,"Multi-layer perceptron, BERT incl. biomedical versions, Other Transformer","Precision, Recall, F1","Abstracts, Full texts, Titles",RCT,,,No,Yes,https://github.com/jayded/evidence-inference,Yes,No,Yes,,,Yes,separate splitss for train/test data,,Yes,No,Yes,,Yes,sparsity: wide variety of treatments and outcomes used in the trials leads to much unique data,,"Fulltext: 12,616 prompts stemming from 3,346 articles (prompt is ICO triplet); Abstract-only: 6375 prompts. data here http://evidence-inference.ebm-nlp.com/download/",Biomed RoBERTa,Yes,Yes,,No,Yes,,directionality and strength of effect in outcomes,they provided a leaderboard and website to display results and comparisons between other models and submissions for their benchmark dataset,Yes,Yes,,Spans or annotations,"IC (per arm), O, Other",Sentences,None reported,"ablation studies, error analysis, comparisons between classes (wrt. inference of efffect strength only)",Yes; other description given,2020,"Accuracy of prompt and explanation generation (ie entities):94 &96%; accuracy of prompt and explanation annotation (ie directionality and strength) 90&89. Second task is harder.
overall Krippendorfâ€™s Î± of Î± = 0.854 between annotators generating and annotating prompts; ",,,,,,,,,,,,,,,,,,,
Text or probably text file,Unlocking the Power of Deep PICO Extraction: Step-wise Medical NER   Identification,"The PICO framework (Population, Intervention, Comparison, and Outcome) is usually used to formulate evidence in the medical domain. The major task of PICO extraction is to extract sentences from medical literature and classify them into each class. However, in most circumstances, there will be more than one evidences in an extracted sentence even it has been categorized to a certain class. In order to address this problem, we propose a step-wise disease Named Entity Recognition (DNER) extraction and PICO identification method. With our method, sentences in paper title and abstract are first classified into different classes of PICO, and medical entities are then identified and classified into P and O. Different kinds of deep learning frameworks are used and experimental results show that our method will achieve high performance and fine-grained extraction results comparing with conventional PICO extraction works.","['Tengteng Zhang', 'Yiqin Yu', 'Jing Mei', 'Zefang Tang', 'Xiang Zhang', 'Shaochun Li']",https://export.arxiv.org/abs/2005.06601,4/29/2020,arXiv,Include,TRUE,11/06/2020,Include,17138,also doing UMLS integration via DeepWalk,cs.CL cs.IR,"In this paper, we propose a step-wise method for extracting medical entities based on the PICO framework. Main steps include PICO sentence classification, disease entity recognition and disease mapping. Mainstream deep neural networks such as CNN and Bi-LSTM are used to classify sentences into PICO elements. The disease entity recognition is based on dominated deep learning frameworks. Structural medical knowledge is embedded with probabilistic knowledge mapping model. Experimental results show that our method achieves reasonable performance on precision, recall and F1 score. An online web system is also developed to facilitate clinical researchers to do medical entity based PICO extraction.",,LS,"Rule-based, Word embedding, Character embedding, LSTM, CNN, BERT incl. biomedical versions, Other","Precision, Recall, F1","Abstracts, Titles",RCT,,"AUTOPICO app screenshot in paper, but no further details or link to deployment or install file",Yes,Yes,,No,No,No,,,Yes,,compared against Jin and XIA models and datasets,No,No,Yes,,Yes,PICO sentence annotations are not high quality because they are automatic. There is a lack of annotation standards between datasets,,"Jin data; 500 own expert-labelled papers for PICO sentences and 100 for evaluating P vs O rule-based classifier; Xia 2019, ",DeepWalk for UMLS,No,Yes,,No,Yes,,,,Yes,Yes,Yes,Spans or annotations,"P, IC, O","Entities, Sentences",None reported,"manual error analysis, looking at impact of specific words or presence of multiple entities in one sentence as source of error",,2020,,,,,,,,,,,,,,,,,,,,
Text or probably text file,MS2: Multi-Document Summarization of Medical Studies,"To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and highly manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS^2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20k summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system's generated summaries. Data and models are available at https://github.com/allenai/ms2","['Jay DeYoung', 'Iz Beltagy', 'Madeleine van Zuylen', 'Bailey Kuehl', 'Lucy Lu Wang']",https://export.arxiv.org/abs/2104.06486,4/13/2021,arXiv,Include,TRUE,08/03/2021,Include,17677,"only extracted info about RCT mining, they did also mine text from completed reviews",cs.CL cs.AI cs.LG,"Given increasing rates of publication, multidocument summarization, or the creation of literature reviews, has emerged as an important NLP task in science. The urgency for automation technologies has been magnified by the COVID-19 pandemic, which has led to both an accelerated speed of publication (Horbach, 2020) as well as proliferation of non-peer-reviewed preprints which may be of lower quality (Lachapelle, 2020). By releasing MSË†2, we provide a MDS dataset that can help to address these challenges. Though we demonstrate that our MDS models can produce fluent text, our results show that there are significant outstanding challenges that remain unsolved, such as PICO tuple extraction, co-reference resolution, and evaluation of summary quality and faithfulness in the multi-document setting. We encourage others to use this dataset to better understand the challenges specific to MDS in the domain of biomedical text, and to push the boundaries on the real world task of systematic review automation.",,LS,BERT incl. biomedical versions,"Precision, Recall, F1, Other","Abstracts, Titles","RCT, Cohort",,,No,Yes,https://github.com/allenai/ms2,Yes,No,Yes,"quantitiative based on labelled data, and manual error analysis of summarisation results from 150 selected reviews",,Yes,,"PICO entity classification not comparable, as P R F1 scores are given for sentence-labelling tasks on sentences that were automatically tagged with PICO entities, thus providing only indirect evaluation of NER performance",Yes,No,Yes,,Yes,"Challenges for summarisation are that there is often contradictory text and that the quality of summaries is hard to evaluate, it is unclear how summary scores relate to the practical usefulness of a system. Current models need to perform better, especially with more reliable PICO labels. Their ethical concerns statement mentioned that this kind of system is not ready to be deployed, due to the risk of producing correct-looking, yet factually-incorrect statements.","ROUGE and âˆ†EI for summarisation. PRF1 given for sentence-labelling tasks on sentences that were automatically tagged with PICO entities, thus providing only indirect evaluation","Create MSË†2 dataset: 470 studies from 20k reviews from PubMed; EBMNLP and EvidenceInference (EI, deYoung 2020) dataset for PICOs training and direction of effect mining, Data: https://github.com/allenai/ms2 ",BART transformer for summarisation,No,Yes,,No,No,all hyperparameters described in appendix,direction of effect,"Not for PICO NER, which is focus of this LSR",No,Yes,Yes,Structured text and summary,"P, IC, O","Entities, Sentences",None reported,,Yes; random splits and ratio given,2021,"agreement:86%,Cohenâ€™s Îº:0.76",,,,,,,,,,,,,,,,,,,
Text or probably text file,A neuro-symbolic method for understanding free-text medical evidence,"We introduce Medical evidence Dependency (MD)-informed attention, a novel neuro-symbolic model for understanding free-text clinical trial publications with generalizability and interpretability. We trained one head in the multi-head self-attention model to attend to the Medical evidence Ddependency (MD) and to pass linguistic and domain knowledge on to later layers (MD informed). This MD-informed attention model was integrated into BioBERT and tested on 2 public machine reading comprehension benchmarks for clinical trial publications: Evidence Inference 2.0 and PubMedQA. We also curated a small set of recently published articles reporting randomized controlled trials on COVID-19 (coronavirus disease 2019) following the Evidence Inference 2.0 guidelines to evaluate the model's robustness to unseen data. The integration of MD-informed attention head improves BioBERT substantially in both benchmark tasks-as large as an increase of +30% in the F1 score-and achieves the new state-of-the-art performance on the Evidence Inference 2.0. It achieves 84% and 82% in overall accuracy and F1 score, respectively, on the unseen COVID-19 data. MD-informed attention empowers neural reading comprehension models with interpretability and generalizability via reusable domain knowledge. Its compositionality can benefit any transformer-based architecture for machine reading comprehension of free-text medical evidence.","Kang, Turfah, Kim, Perotte, Weng",https://doi.org/10.1093/jamia/ocab077,20210802,PubMed,Include,FALSE,08/03/2021,Include,18405,relation extraction and reading comprehension,machine reading comprehension; medical evidence computing; natural language understanding; transformer,"MD-informed attention empowers neural reading comprehension models with interpretability
and generalizability via reusable domain knowledge. Its compositionality can benefit any transformer-based architecture for machine reading comprehension of free-text medical evidence.",,LS,BERT incl. biomedical versions,"Precision, Recall, F1, Accuracy","Abstracts, Full texts, Titles",RCT,,,Yes,,https://github.com/Tian312/MD-Attention,Yes,No,Yes,,,Yes,,Curated small independent test set themselves,Yes,Yes,Yes,,Yes,long dependencies for relation extraction and complexity within study,,Evidence Inference 2.0; and own COVID-19 corpus following EI2.0 standard with 10 RCT abstracts available in suppl data (Todo follow up to see if this can be downloaded). Also used PubMedQA by changing/mapping some of the labels,,,,,No,Yes,,directionality and strength of effect in outcomes,used 2 benchmark datasets,Yes,Yes,Yes,Spans or annotations,"IC, O, Other",Sentences,None reported,"visualised and analysed attention values for specific terms, tested how position and length of dependency affects predictions",Yes; random splits and ratio given,2021,,,,,,,,,,,,,,,,,,,,
XML,Toward assessing clinical trial publications for reporting transparency,"To annotate a corpus of randomized controlled trial (RCT) publications with the checklist items of CONSORT reporting guidelines and using the corpus to develop text mining methods for RCT appraisal. We annotated a corpus of 50 RCT articles at the sentence level using 37 fine-grained CONSORT checklist items. A subset (31 articles) was double-annotated and adjudicated, while 19 were annotated by a single annotator and reconciled by another. We calculated inter-annotator agreement at the article and section level using MASI (Measuring Agreement on Set-Valued Items) and at the CONSORT item level using Krippendorff's ÃƒÅ½Ã‚Â±. We experimented with two rule-based methods (phrase-based and section header-based) and two supervised learning approaches (support vector machine and BioBERT-based neural network classifiers), for recognizing 17 methodology-related items in the RCT Methods sections. We created CONSORT-TM consisting of 10,709 sentences, 4,845 (45%) of which were annotated with 5,246 labels. A median of 28 CONSORT items (out of possible 37) were annotated per article. Agreement was moderate at the article and section levels (average MASI: 0.60 and 0.64, respectively). Agreement varied considerably among individual checklist items (Krippendorff's ÃƒÅ½Ã‚Â±= 0.06-0.96). The model based on BioBERT performed best overall for recognizing methodology-related items (micro-precision: 0.82, micro-recall: 0.63, micro-F1: 0.71). Combining models using majority vote and label aggregation further improved precision and recall, respectively. Our annotated corpus, CONSORT-TM, contains more fine-grained information than earlier RCT corpora. Low frequency of some CONSORT items made it difficult to train effective text mining models to recognize them. For the items commonly reported, CONSORT-TM can serve as a testbed for text mining methods that assess RCT transparency, rigor, and reliability, and support methods for peer review and authoring assistance. Minor modifications to the annotation scheme and a larger corpus could facilitate improved text mining models. CONSORT-TM is publicly available at https://github.com/kilicogluh/CONSORT-TM.","Kilicoglu, Rosemblat, Hoang, Wadhwa, Peng, MaliÃƒâ€žÃ‚Âki, Schneider, Ter Riet",https://doi.org/10.1016/j.jbi.2021.103717,20210728,PubMed,Include,TRUE,08/03/2021,Include,18910,FALSE,CONSORT; Corpus annotation; Reporting guidelines; Sentence classification; Text mining; Checklist; Support Vector Machine,"Our annotated corpus, CONSORT-TM, contains more fine-grained information than earlier RCT corpora. Low frequency of some CONSORT items made it difficult to train effective text mining models to recognize them. For the items commonly reported, CONSORT-TM can serve as a testbed for text mining methods that assess RCT transparency, rigor, and reliability, and support methods for peer review and authoring assistance. Minor modifications to the annotation scheme and a larger corpus could facilitate improved text mining models. CONSORT-TM is publicly available at https://github.com/kilicogluh/CONSORT-TM.",,LS,"Rule-based, SVM, BERT incl. biomedical versions","Precision, Recall, F1, AUC-ROC","Full texts, Titles, Abstracts",RCT,,,No,No,https://github.com/kilicogluh/CONSORT-TM,Yes,No,Yes,,,Yes,,,Yes,Yes,Yes,,Yes,"Transformer performs well for common items(IC, O) and poor for rare items, leading to lower score when macro-averaging. Rule-based and SVM performed better in these cases. For annotation this was a complex task, leading to confusion and disagreements between annotators for some classes. This can impact model training and evaluation, but full resolution and discussion is non-feasible.  Overall, most classes don't perform well enough for practical application.","McNemarâ€™s test, micro and macro showing different results over all classes, transformer only favoured in micro-scores because rules/svms did better on rare classes",new corpus:  CONSORT-TM with 50 RCTS annotated with 37 CONSORT items. https://github.com/kilicogluh/CONSORT-TM,,Yes,Yes,,No,Yes,,Eligibility criteria corresponds to P,"small dataset, but published as benchmark corpus to maximise comparability to future models",Yes,Yes,No,Spans or annotations,"P, IC, O, N (total), Withdrawals or exclusions, Randomisation, Blinding, Design, Eligibility criteria, Enrolment dates, Early stopping",Sentences,None reported,"Error analysis for poor-performing classes and confusions in the model, eg for confusing Statistical Methods for Outcomes (12a) with Statistical Methods for Other Analyses (12b) and exploration of merging of classes",Not reported,2021," Article/section level used MASI measure (Measuring Agreement on Set-Valued Items) moderate 0.6 overall, differences between article sections range between 0.5-0.89
class level used Krippendorffâ€™s alpha 0.06-0.96; 0.53 for I, 0.57 for O",,,,,,,,,,,,,,,,,,,
Text or probably text file,A clinical trials corpus annotated with UMLS entities to enhance the access to evidence-based medicine,"The large volume of medical literature makes it difficult for healthcare professionals to keep abreast of the latest studies that support Evidence-Based Medicine. Natural language processing enhances the access to relevant information, and gold standard corpora are required to improve systems. To contribute with a new dataset for this domain, we collected the Clinical Trials for Evidence-Based Medicine in Spanish (CT-EBM-SP) corpus. We annotated 1200 texts about clinical trials with entities from the Unified Medical Language System semantic groups: anatomy (ANAT), pharmacological and chemical substances (CHEM), pathologies (DISO), and lab tests, diagnostic or therapeutic procedures (PROC). We doubly annotated 10% of the corpus and measured inter-annotator agreement (IAA) using F-measure. As use case, we run medical entity recognition experiments with neural network models. This resource contains 500 abstracts of journal articles about clinical trials and 700 announcements of trial protocols (292 173 tokens). We annotated 46 699 entities (13.98% are nested entities). Regarding IAA agreement, we obtained an average F-measure of 85.65% (Ãƒâ€šÃ‚Â±4.79, strict match) and 93.94% (Ãƒâ€šÃ‚Â±3.31, relaxed match). In the use case experiments, we achieved recognition results ranging from 80.28% (Ãƒâ€šÃ‚Â±00.99) to 86.74% (Ãƒâ€šÃ‚Â±00.19) of average F-measure. Our results show that this resource is adequate for experiments with state-of-the-art approaches to biomedical named entity recognition. It is freely distributed at: http://www.lllf.uam.es/ESP/nlpmedterm_en.html . The methods are generalizable to other languages with similar available sources.","Campillos-Llanos, Valverde-Mateos, Capllonch-CarriÃƒÆ’Ã‚Â³n, Moreno-Sandoval",https://doi.org/10.1186/s12911-021-01395-z,20210423,PubMed,Include,TRUE,08/03/2021,Include,18952,"They sy they don't do PICOS, but they do  pathologies (DISO), and lab tests, diagnostic or therapeutic procedures (PROC) which I think fits as they use clinical trial texts!",Clinical Trials; Evidence-Based Medicine; Inter-Annotator Agreement; Natural Language Processing; Semantic Annotation; Evidence-Based Medicine; Humans; Language; Natural Language Processing; Semantics; Unified Medical Language System,"We believe this work contributes to enhancing the access to evidence-based information for both health professionals and patients. We would also be very satisfied if this resource played a beneficial role for developing systems that help patients to understand trial protocols, interventions and procedures better.",,LS,"CRF, Word embedding, LSTM, BERT incl. biomedical versions, Flair","Precision, Recall, F1","Abstracts, Other","RCT, Diagnostic test","PubMed, Eudract",,No,Yes,https://github.com/lcampillos/Medical-NER,Yes,No,Yes,,,Yes,,This is the only dataset in Spanish for this task,Yes,Yes,Yes,,Yes,"When registering a Spanish trial, there can be noisy data such as spelling mistakes or other errors in English titles, due to incorrect translations. Also, especially when gold-standard data are limited it is hard to build a corpus that generalizes well. 

Caveats are ambiguity, sparse data, missing target entity for normalization , acronyms and abbreviations. Modifiers not belonging to entities (grave, severe) or methods (ambulatory) caused errors but these arrors might be valid annotation in other contexts.
Methods would benefit from relation annotation (eg Drug A treats disease B)",," CT-EBM-SP: 1200 texts about clinical trials (500 abstracts and 700 trial records from 2020), annotated and normalised to UMLS, Spanish. They described characteristics such as entity distribution, length of sentences.  Data: http://www.lllf.uam.es/ESP/nlpmedterm_en.html 

Also mentions another corpus that is a subset of NICTA but focussed on semantic similarity from Hassanzadeh H, Nguyen A, Verspoor K. Quantifying semantic similarity of clinical evidence in the biomedical literature to facilitate related evidence synthesis. J Biomed Inform. 2019;100:103321.

Koroleva A, Kamath S, Paroubek P. Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations. J Biomed Inform. 2019;4:100058.
",used Flair and Fasttext,Yes,Yes,,No,Yes,,,They shared benchmark corpus so that other researchers can publish comparisons in the future,Yes,Yes,No,Spans or annotations,"P (Condition or disease), IC (Drug name), Diagnostic tests",Entities,None reported,"Error analysis, analysis of characteristics of dataset, and how the annotation scope could have affected model performance",Yes; random splits and ratio given,2021,"measured IAA with F1 score strict 85.6% and 93.9 relaxed overall. They observed increasing scores between training and end-annotation stage, eg starting at 86 for relaxed score",,,,,,,,,,,,,,,,,,,
Text or probably text file,"Artificial Intelligence Clinical Evidence Engine for Automatic Identification, Prioritization, and Extraction of Relevant Clinical Oncology Research","We developed a system to automate analysis of the clinical oncology scientific literature from bibliographic databases and match articles to specific patient cohorts to answer specific questions regarding the efficacy of a treatment. The approach attempts to replicate a clinician's mental processes when reviewing published literature in the context of a patient case. We describe the system and evaluate its performance. We developed separate ground truth data sets for each of the tasks described in the paper. The first ground truth was used to measure the natural language processing (NLP) accuracy from approximately 1,300 papers covering approximately 3,100 statements and approximately 25 concepts; performance was evaluated using a standard F1 score. The ground truth for the expert classifier model was generated by dividing papers cited in clinical guidelines into a training set and a test set in an 80:20 ratio, and performance was evaluated for accuracy, sensitivity, and specificity. The NLP models were able to identify individual attributes with a 0.7-0.9 F1 score, depending on the attribute of interest. The expert classifier machine learning model was able to classify the individual records with a 0.93 accuracy (95% CI, 0.9 to 0.96, <i>P</i> &lt; .0001), and sensitivity and specificity of 0.95 and 0.91, respectively. Using a decision boundary of 0.5 for the positive (expert) label, the classifier demonstrated an F1 score of 0.92. The system identified and extracted evidence from the oncology literature with a high degree of accuracy, sensitivity, and specificity. This tool enables timely access to the most relevant biomedical literature, providing critical support to evidence-based practice in areas of rapidly evolving science.","Saiz, Sanders, Stevens, Nielsen, Britt, Yuravlivker, Preininger, Jackson",https://doi.org/10.1200/CCI.20.00087,20210721,PubMed,Include,TRUE,08/03/2021,Include,19187,"This paper does report F1 scores for entities of interest (ie N, Outcomes, IC). It is extremely high-level in it's description of mining methods and the eval corpus is possibly biased if I understand correctly what they've done. It presents a very useful and good idea of a whole data-extraction pipeline for the general field of cancer so I included it (would have excluded if it were for a specific type of cancer but it covers a whole field). By including ontologies such as UMLS and rxnorm it is also easily transferable to other medical domains. It is geared towards clinicians to help them filter the best available evidence for any cancer-related PICO statement but therefore I think that it is also applicable to systematic review research. ",,"The system identified and extracted evidence from the oncology literature with a high degree of accuracy, sensitivity, and specificity. This tool enables timely access to the most relevant biomedical literature, providing critical support to evidence-based practice in areas of rapidly evolving science.",,LS,"Rule-based, Other",F1,"Abstracts, Titles","RCT, Non-randomised (intervention) study, Mix",Medline,,No,No,,No,No,No,,,Yes,,,Yes,Yes,Yes,,No,"Outcome detection might be biased towards types of outcomes that are over-represented in training sets (eg chemotherapy-based outcomes for cancer literature). Also, In practice the system predictions might become outdated as the training set ages and new therapy approaches are introduced in rapidly developing medical research fields",,"own corpus, 1,300 abstracts with 3,100 outcome statements across different types of cancer","Unclear what they've done, part of it is rule-based based on my interpretation of the text, but the description is very high-level",No,No,,No,No,,"cancer-specific information such as: cancer stage, histology, mutations, study settings, selected biomarkers",tested on selective dataset with unclear focus and biased towards literature cited in clinical guidelines and high-impact journals,No,No,No,Spans or annotations,"IC, O, N (total), Age, Other",Entities,None reported,,Yes; random splits and ratio given,2021,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Enhancing evidence-based medicine with natural language argumentative analysis of clinical trials,"In the latest years, the healthcare domain has seen an increasing interest in the definition of intelligent systems to support clinicians in their everyday tasks and activities. Among others, also the field of Evidence-Based Medicine is impacted by this twist, with the aim to combine the reasoning frameworks proposed thus far in the field with mining algorithms to extract structured information from clinical trials, clinical guidelines, and Electronic Health Records. In this paper, we go beyond the state of the art by proposing a new end-to-end pipeline to address argumentative outcome analysis on clinical trials. More precisely, our pipeline is composed of (i) an Argument Mining module to extract and classify argumentative components (i.e., evidence and claims of the trial) and their relations (i.e., support, attack), and (ii) an outcome analysis module to identify and classify the effects (i.e., improved, increased, decreased, no difference, no occurrence) of an intervention on the outcome of the trial, based on PICO elements. We annotated a dataset composed of more than 500 abstracts of Randomized Controlled Trials (RCT) from the MEDLINE database, leading to a labeled dataset with 4198 argument components, 2601 argument relations, and 3351 outcomes on five different diseases (i.e., neoplasm, glaucoma, hepatitis, diabetes, hypertension). We experiment with deep bidirectional transformers in combination with different neural architectures (i.e., LSTM, GRU and CRF) and obtain a macro F1-score of.87 for component detection and.68 for relation prediction, outperforming current state-of-the-art end-to-end Argument Mining systems, and a macro F1-score of.80 for outcome classification.","Mayer, Marro, Cabrio, Villata",https://doi.org/10.1016/j.artmed.2021.102098,20210908,PubMed,Include,TRUE,11/03/2021,Include,21316,"relation extraction or argument mining, but they do Outcome analysis for O entity mining. macro-scores. Studify with https://www.ijcai.org/proceedings/2019/0953.pdf ","Argument mining; Biomedical natural language processing; Clinical decision support systems; Information extraction; PICO analysis; Randomized controlled trials; Algorithms; Databases, Factual; Electronic Health Records; Evidence-Based Medicine; Language; Natural Language Processing","As the field of Evidence-Based Medicine is still evolving, and to foster future research in the area of argument mining and outcome analysis on clinical trials, we make available to the research community our annotation guidelines, the annotated data, the source codes for the experiments, as well as the results of our system for error analysis. Moreover, we have integrated the proposed pipeline into ACTA [17], the tool we have developed for automating the argumentative analysis of clinical trials (both the argument component and the relation detection modules are fully integrated, while we are currently working at the integration of the Effect-on-Outcome module). Such tool has been designed to support doctors and clinicians in identifying the document(s) of interest about a certain disease, and in analyzing the main argumentative content and PICO elements.",,LS,"CRF, Word embedding, LSTM, BERT incl. biomedical versions, Other, Flair",F1,"Abstracts, Titles",RCT,,http://ns.inria.fr/acta/ ,No,No,https://gitlab.com/tomaye/ecai2020-transformer_based_am ,Yes,Yes,Yes,,,Yes,,,Yes,Yes,Yes,,Yes,"Incomplete/partially correct detection of outcome labels, eg in 'The levels of VEGF were significantly lower' detecting 'VEGF' instead of 'The levels of VEGF'",,"own dataset called AbstRCT and they extend it, 660 RCT abstracts from medline for relation extraction, 4198 argument components, 2601 argument relations, and 3351 outcomes on five different diseases (i.e., neoplasm, glaucoma, hepatitis, diabetes, hypertension). https://gitlab.com/tomaye/abstrct .Their initial paper describing other PICO elements uses EBMNLP","Gated Recurrent Unit (GRU), fasttext, FlairPM",Yes,Yes,,No,Yes,,Relations and modifiers to outcomes,,No,Yes,No,JSON,"P, P (Condition or disease), O, Other",Entities,None reported,Error analysis and confusion matrix for misclassifications provided,Yes; random splits and ratio given,2021,"Fleissâ€™ kappa 0.81 for outcome annotation on 47 abstracts, 0.62-0.72 for the relation annotations based on 30 abstracts",,,,,,,,,,,,,,,,,,,
Text or probably text file,Understanding Clinical Trial Reports: Extracting Medical Entities and Their Relations,"The best evidence concerning comparative treatment effectiveness comes from clinical trials, the results of which are reported in unstructured articles. Medical experts must manually extract information from articles to inform decision-making, which is time-consuming and expensive. Here we consider the <i>end-to-end</i> task of both (a) extracting treatments and outcomes from full-text articles describing clinical trials (entity identification) and, (b) inferring the reported results for the former with respect to the latter (relation extraction). We introduce new data for this task, and evaluate models that have recently achieved state-of-the-art results on similar tasks in Natural Language Processing. We then propose a new method motivated by how trial results are typically presented that outperforms these purely data-driven baselines. Finally, we run a fielded evaluation of the model with a non-profit seeking to identify existing drugs that might be re-purposed for cancer, showing the potential utility of end-to-end evidence extraction systems.","Nye, DeYoung, Lehman, Nenkova, Marshall, Wallace",https://www.google.com/search?q=Understanding+Clinical+Trial+Reports:+Extracting+Medical+Entities+and+Their+Relations.,20210910,PubMed,Include,TRUE,11/03/2021,Include,21231,"Evidence infreence is a newer research trend that requires identification of I and C seprately. It requires construction of triplets, and the EI 2.0 corpus and data rfom this paper provide the structure.  Main change: From snippet identification or claim-making sentence identification to linking ICs with Os. and to infer the meaning/finding. This is first paper to cite and then to discuss the other papers using EI 2.0 corpus in the review. ",Humans; Natural Language Processing,"We have proposed the end-to-end task of automatically extracting structured evidence â€” interventions, outcomes, and comparative results â€” from trial reports. This differs from prior work which has considered the tasks of data extraction and inferring results separately. Weperformedafieldedevaluation in collaboration with a non-profit (RebootRX) that is interested in identifying results from previously conducted studies on drugs that might be used to treat cancer. This small study suggested that despite low absolute performance metrics, the proposed model can be useful in practice.",,LS,"CRF, LSTM, BERT incl. biomedical versions","Precision, Recall, F1, Other","Abstracts, Titles",RCT,,,Yes,,,No,No,No,,,Yes,,"Used benchmark dataset and multiple independent test, dev, and practical evaluation datasets",Yes,No,Yes,,Yes,"Caveat of error-propagation in joint models. And limited information in abstracts, distance between I-O sentences for the triplets, and complicted nature of the task that is a challenge to humans, mean that performance of the system is expectedly low. Also, treatment arms are only named explicityy in 37% of the time across this dataset, more often implying information (ie, 'Group 1' referring to non-named intervention grop, or implying an effect across a group of outcomes, such as across all adverse events). Also, distinguishing between IC can be challenging, especially since trials can have more than 2 arms. Also, complexity in language and understanding, eg. reporting a positive effect on an adverse outcomeTherefore, qualitative evaluation by domain-experts is needed to assess real-world value, which can differ from metrics.",Likert scale for practical evaluation,"Train dataset based on first Evidence Inference corpus, creating more complete annotations by  using distant supervision for 1772 abstracts with ICO triplet spans, directionality labels, and supporting sentences. Additional dev/test corpora are newly created, consisting of 60 and 100 abstracts each. Corpus for practical evaluation contained 20 cancer RCT abstracts.",,,,,No,Yes,,direction of effect,Direct re-implementation/comparison with other architectures done and described in the paper,,Yes,No,Spans or annotations,"P, P (Condition or disease), IC (per arm), O, O (measurement instrument), Other","Entities, Sentences",None reported,"Error analysis, ablations studies of system compoenents to pinpoint errors",Yes; compl. different datasets (ie. diferent SRs; journals..),2020,"B3, MUC, CEAFe scores of 0.40, 0.46, and 0.42",,,,,,,,,,,,,,,,,,,
Text or probably text file,Automated tabulation of clinical trial results: A joint entity and   relation extraction approach with transformer-based language representations,"Evidence-based medicine, the practice in which healthcare professionals refer to the best available evidence when making decisions, forms the foundation of modern healthcare. However, it relies on labour-intensive systematic reviews, where domain specialists must aggregate and extract information from thousands of publications, primarily of randomised controlled trial (RCT) results, into evidence tables. This paper investigates automating evidence table generation by decomposing the problem across two language processing tasks: \textit{named entity recognition}, which identifies key entities within text, such as drug names, and \textit{relation extraction}, which maps their relationships for separating them into ordered tuples. We focus on the automatic tabulation of sentences from published RCT abstracts that report the results of the study outcomes. Two deep neural net models were developed as part of a joint extraction pipeline, using the principles of transfer learning and transformer-based language representations. To train and test these models, a new gold-standard corpus was developed, comprising almost 600 result sentences from six disease areas. This approach demonstrated significant advantages, with our system performing well across multiple natural language processing tasks and disease areas, as well as in generalising to disease domains unseen during training. Furthermore, we show these results were achievable through training our models on as few as 200 example sentences. The final system is a proof of concept that the generation of evidence tables can be semi-automated, representing a step towards fully automating systematic reviews.","['Jetsun Whitton', 'Anthony Hunter']",https://export.arxiv.org/abs/2112.05596,12/10/2021,arXiv,Include,TRUE,12/06/2022,Include,22861,"relation extraction, systematic reviews and guideline production. Discuss outputs data as tables/csv in relevant section of review",cs.CL cs.AI,"While a great amount of work still remains in automating systematic reviews of clinical evidence, our study has shown that a key barrierâ€“ differentiating interventions, outcomes and their measures into relevant categoriesâ€“ may be overcome with context-based language representations, and decomposing the classification problems across a pipeline approach. In the short term, this technology could be used to semi-automate construction of evidence tables, potentially as a first pass process that allows reviewers to start from a pre-filled baseline. Long term, as language representations evolve, and more innovative methods are developed to classify their outputs, it is conceivable that future systems could play an even greater role in automating the systematic review process, with medical domain experts needed only for oversight. This could potentially result in thousands of hours saved in labour costs across the healthcare industry, which could be redirected to achieve the ultimate goal of improving patient care.",,LS,BERT incl. biomedical versions,"Precision, Recall, F1","Abstracts, Titles",RCT,,,Yes,,https://github.com/jetsunwhitton/RCT-ART,Yes,No,Yes,error analysis with per-class misclassifcation confusion matrix and discussing of errors in the text. ,,Yes,,They used (although combined) 2 different benchmark datasets. They later divided their dataset by medical domains and did a kind of sensitivity analysis by leaving out one domain (ie cardiovascular) from training and then using it for eval. They also retrained based on selected domain groups and also just using the biggest domains separately,No,No,Yes,,Yes,"O is hardest entity to identify due to variation in length and annotator disagreements on the exact boundaries. Also, there can be entity/relation information across multiple sentences, rather than having them named in the same sentence, which is a limitation to their system.","harmonic means of P and R, micro-averaging for F1 due to no priority difference","Own corpus incl. 600 results sentences from 6 disease areas here, it is a re-annotation of data from EBM-NLP and Trenta corpora but only using 2-arm RCTs https://github.com/jetsunwhitton/RCT-ART Note add trenta citation here because this includes all RCTs  (Trenta excluded for being topic-specific on glaucoma https://arxiv.org/pdf/1509.05209.pdf )",,,Yes,,No,No,,"O (numeric measurement, eg X occurred in 17% of patients where 17% is the measurement), relations between measure-ourcome, measure-IC","Dataset only includes 2-arm trials and abstracts that have specific info (ie a results arm), so there might be a bias since these abstracts might be easier to classify.",No,Yes,Yes,Structured text and summary,"IC, O, Other","Entities, Sentences",None reported,,Yes; random splits and ratio given,2021,,,,,,,,,,,,,,,,,,,,
XML,Assessment of contextualised representations in detecting outcome   phrases in clinical trials,"Automating the recognition of outcomes reported in clinical trials using machine learning has a huge potential of speeding up access to evidence necessary in healthcare decision-making. Prior research has however acknowledged inadequate training corpora as a challenge for the Outcome detection (OD) task. Additionally, several contextualized representations like BERT and ELMO have achieved unparalleled success in detecting various diseases, genes, proteins, and chemicals, however, the same cannot be emphatically stated for outcomes, because these models have been relatively under-tested and studied for the OD task. We introduce ""EBM-COMET"", a dataset in which 300 PubMed abstracts are expertly annotated for clinical outcomes. Unlike prior related datasets that use arbitrary outcome classifications, we use labels from a taxonomy recently published to standardize outcome classifications. To extract outcomes, we fine-tune a variety of pre-trained contextualized representations, additionally, we use frozen contextualized and context-independent representations in our custom neural model augmented with clinically informed Part-Of-Speech embeddings and a cost-sensitive loss function. We adopt strict evaluation for the trained models by rewarding them for correctly identifying full outcome phrases rather than words within the entities i.e. given an outcome ""systolic blood pressure"", the models are rewarded a classification score only when they predict all 3 words in sequence, otherwise, they are not rewarded. We observe our best model (BioBERT) achieve 81.5\% F1, 81.3\% sensitivity and 98.0\% specificity. We reach a consensus on which contextualized representations are best suited for detecting outcomes from clinical-trial abstracts. Furthermore, our best model outperforms scores published on the original EBM-NLP dataset leader-board scores.","['Micheal Abaho', 'Danushka Bollegala', 'Paula R Williamson', 'Susanna Dodd']",https://export.arxiv.org/abs/2203.03547,2/13/2022,arXiv,Include,TRUE,12/06/2022,Include,23005,"{m.abaho,danushka,prw,shinds}@liverpool.ac.uk",cs.CL cs.LG,"We observe our best model (BioBERT) achieve 81.5% F1, 81.3% sensitivity and 98.0% specificity. We reach a consensus on which contextualised representations are best suited for detecting outcome phrases from clinical trial abstracts. Furthermore, our best model outperforms scores published on the original EBM-NLP dataset leader-board scores.",,LS,"CRF, Word embedding, LSTM, BERT incl. biomedical versions, Other, Flair","Precision, Recall, F1, Specificity","Abstracts, Titles",RCT,,,Yes,,https://github.com/LivNLP/ODP-tagger,Yes,No,Yes,,,Yes,,"Evaluated on own and external benchmark, although they did revise the external benchmark to fit their own annotation guidelines more closely",No,No,Yes,,Yes,Mentions that gold-standard corpora use different annotation conventions (eg annotating measurement instrument in the scope of outcome or including outcome measurements such as MEAN blood pressure or including multiple entities as one). Also that strict vs relaxed evaluation of partially-matched entities can make a big different of up to 11 percent points: LS encourage for papers to report that. ,they mention that specificity scores are high due to imbalance in dataset (ie many more TNs and TNs are single-words while a TP in their context was always a strict full entity.),"EBM-COMET dataset 300 PubMed abstracts with O https://github.com/LivNLP/ODP-tagger ; EBM-NLP (revised version, standardised outcome categories to core outcome measures in effectiveness Trials COMET; making 38 domains and 5 core areas)",BioELMo; BioFLAIR,,Yes,,No,Yes,,,comparisons with EBMNLP leaderboard,Yes,Yes,Yes,Spans or annotations,"P, IC, O",Entities,None reported,error analyses on long entities; multi-entity phrases; common words eg 'of'; and analysis of contribution of different parts of the architecture.,Yes; random splits and ratio given,2022,Created their own dataset but did not duplicate work,,,,,,,,,,,,,,,,,,,
Text or probably text file,Improvement of intervention information detection for automated clinical literature screening during systematic review,"Systematic literature review (SLR) is a crucial method for clinicians and policymakers to make their decisions in a flood of new clinical studies. Because manual literature screening in SLR is a highly laborious task, its automation by natural language processing (NLP) has been welcomed. Although intervention is a key information for literature screening, NLP models for its detection in previous works have not shown adequate performance. In this work, we first design an algorithm for automated construction of high-quality intervention labels by utilizing information retrieved from a clinical trial database. We then design another algorithm for improving model's recall and F1 score by imposing adaptive weights on training instances in the loss function. The intervention detection model trained on the weighted datasets is tested with the Evidence-Based Medicine NLP (EBM-NLP) corpus, and shows 9.7% and 4.0% improvements respectively in recall and F1 score compared to the previous state-of-the-art model on the corpus. The proposed algorithms can boost automation of literature screening during SLR in the clinical domain. Copyright Ã‚Â© 2022 The Author(s). Published by Elsevier Inc. All rights reserved.","Tsubota, Tadashi

Bollegala, Danushka

Zhao, Yang

Jin, Yingzi

Kozu, Tomotake",https://dx.doi.org/10.1016/j.jbi.2022.104185,20220826,Medline,Include,TRUE,12/06/2022,Include,23572,v good paper. they do abbreviation expansion and an interesting adaption to the loss function with additional weighing to improve scores on EBM-NLP and their expanded version of EBM-NLP,"Journal of Biomedical Informatics. 134:104185, 2022 10.","Since intervention information detection was the main bottleneck for ML-based automation of literature screening, our proposed approach can help overcome it, and eventually contribute to alleviate human effort during SLR. The intervention detection model trained on the weighted datasets is tested with the Evidence-Based Medicine NLP (EBM-NLP) corpus, and shows 9.7% and 4.0% improvements respectively in recall and F1 score compared to the previous state-of-the-art model on the corpus.",,LS,"Rule-based, BERT incl. biomedical versions, Other Transformer","Precision, Recall, F1","Abstracts, Titles",RCT,,,Yes,,https://data.mendeley.com/datasets/ccfnn3jb2x/1 ,Yes,No,Yes,,,Yes,,"They constructed an own dataset, did separate and merged training/eval. They did several 'ablation' studies with versions of the dataset processed and filtered differently such that the datasets would have different characteristics.",Yes,Yes,Yes,,Yes,"Evaluation of models can be influenced by random seed, leading to a SD of 0.5 in F1 score on the popular Scibert model (similar discussion as Wallalce paper). Also, automatically-labelled data can lead to models underperforming in recall because of incorrectly-missed labels that were not picked up by the rule-based approach. Also, EBM-NLP has only <2018 data and does not include up-to-date records. ",,"EBM-NLP + automatically-labelled intervention labels derived from clinical trial registrations and matching with PubMed records, 1807 abstracts . https://data.mendeley.com/datasets/ccfnn3jb2x/1 ",BioELECTRA,Yes,Yes,,No,Yes,,,direct comparison to Nye LSTMs and other EBM-NLP papers reported,Yes,Yes,Yes,Spans or annotations,"IC, Design","Entities, Sentences",None reported,"Discussed impact of different words such as 'versus' or 'plus' when detecting intervention names, and impact especially on automatically-derived datasets where labelling can be incomplete. ","Yes; random splits and ratio given, Yes; compl. different datasets (ie. diferent SRs; journals..)",2022,,,,,,,,,,,,,,,,,,,,
Text or probably text file,PICO entity extraction for preclinical animal literature,"BACKGROUND: Natural language processing could assist multiple tasks in systematic reviews to reduce workflow, including the extraction of PICO elements such as study populations, interventions, comparators and outcomes. The PICO framework provides a basis for the retrieval and selection for inclusion of evidence relevant to a specific systematic review question, and automatic approaches to PICO extraction have been developed particularly for reviews of clinical trial findings. Considering the difference between preclinical animal studies and clinical trials, developing separate approaches is necessary. Facilitating preclinical systematic reviews will inform the translation from preclinical to clinical research.

METHODS: We randomly selected 400 abstracts from the PubMed Central Open Access database which described in vivo animal research and manually annotated these with PICO phrases for Species, Strain, methods of Induction of disease model, Intervention, Comparator and Outcome. We developed a two-stage workflow for preclinical PICO extraction. Firstly we fine-tuned BERT with different pre-trained modules for PICO sentence classification. Then, after removing the text irrelevant to PICO features, we explored LSTM-, CRF- and BERT-based models for PICO entity recognition. We also explored a self-training approach because of the small training corpus.

RESULTS: For PICO sentence classification, BERT models using all pre-trained modules achieved an F1 score of over 80%, and models pre-trained on PubMed abstracts achieved the highest F1 of 85%. For PICO entity recognition, fine-tuning BERT pre-trained on PubMed abstracts achieved an overall F1 of 71% and satisfactory F1 for Species (98%), Strain (70%), Intervention (70%) and Outcome (67%). The score of Induction and Comparator is less satisfactory, but F1 of Comparator can be improved to 50% by applying self-training.

CONCLUSIONS: Our study indicates that of the approaches tested, BERT pre-trained on PubMed abstracts is the best for both PICO sentence classification and PICO entity recognition in the preclinical abstracts. Self-training yields better performance for identifying comparators and strains. Copyright Ã‚Â© 2022. The Author(s).","Wang, Qianying

Liao, Jing

Lapata, Mirella

Macleod, Malcolm",https://dx.doi.org/10.1186/s13643-022-02074-4,20220930,Medline,Include,TRUE,12/06/2022,Include,23753,malcolm.macleod@ed.ac.uk,"Systematic Reviews. 11(1):209, 2022 09 30.","Our study indicates that of the approaches tested, BERT pre-trained on PubMed abstracts is the best for both PICO sentence classification and PICO entity recognition in the preclinical abstracts. Self-training yields better performance for identifying comparators and strains.",,LS,"CRF, Word embedding, LSTM, BERT incl. biomedical versions","Precision, Recall, F1","Abstracts, Titles",Animal studies,,They mention a streamlit app but do not provide a deployed version (although the code is available from the OSF repo),No,,https://osf.io/2dqcg/,Yes,No,Yes,,,Yes,,,Yes,Yes,Yes,,Yes,"Background information in abstracts can lead to false-positive PICO-classifications. Outcome-detection on entity-level has complex boundaries or tends to group multiple outcomes separated by commas. Also, abstract-level NER leads to incomplete info due to lack of full text. ",,"Own corpus of PICO-labelled animal RCTs, 400 abstracts from PMC; 10k abstracts for secondary corpus, where they used models trained on corpus 1 to retrieve most likely entity annotations (>95% probability threshold) to expand the training data. https://osf.io/2dqcg/ ",,,Yes,,No,Yes,,"Species, strains, induction method",common entities and no topic-filters,Yes,Yes,Yes,Spans or annotations,"IC (per arm), O, Other","Entities, Sentences",None reported,implications of commas in the correct classification of outcome spans is described,Yes; random splits and ratio given,2022,,,,,,,,,,,,,,,,,,,,
Text or probably text file,An annotated corpus of clinical trial publications supporting schema-based relational information extraction,"BACKGROUND: The evidence-based medicine paradigm requires the ability to aggregate and compare outcomes of interventions across different trials. This can be facilitated and partially automatized by information extraction systems. In order to support the development of systems that can extract information from published clinical trials at a fine-grained and comprehensive level to populate a knowledge base, we present a richly annotated corpus at two levels. At the first level, entities that describe components of the PICO elements (e.g., population's age and pre-conditions, dosage of a treatment, etc.) are annotated. The second level comprises schema-level (i.e., slot-filling templates) annotations corresponding to complex PICO elements and other concepts related to a clinical trial (e.g. the relation between an intervention and an arm, the relation between an outcome and an intervention, etc.).

RESULTS: The final corpus includes 211 annotated clinical trial abstracts with substantial agreement between annotators at the entity and scheme level. The mean Kappa value for the glaucoma and T2DM corpora was 0.74 and 0.68, respectively, for single entities. The micro-averaged F1 score to measure inter-annotator agreement for complex entities (i.e. slot-filling templates) was 0.81. The BERT-base baseline method for entity recognition achieved average micro- F1 scores of 0.76 for glaucoma and 0.77 for diabetes with exact matching.

CONCLUSIONS: In this work, we have created a corpus that goes beyond the existing clinical trial corpora, since it is annotated in a schematic way that represents the classes and properties defined in an ontology. Although the corpus is small, it has fine-grained annotations and could be used to fine-tune pre-trained machine learning models and transformers to the specific task of extracting information about clinical trial abstracts.For future work, we will use the corpus for training information extraction systems that extract single entities, and predict template slot-fillers (i.e., class data/object properties) to populate a knowledge base that relies on the C-TrO ontology for the description of clinical trials. The resulting corpus and the code to measure inter-annotation agreement and the baseline method are publicly available at https://zenodo.org/record/6365890. Copyright Ã‚Â© 2022. The Author(s).","Sanchez-Graillet, Olivia

Witte, Christian

Grimm, Frank

Cimiano, Philipp",https://dx.doi.org/10.1186/s13326-022-00271-7,20220523,Medline,Include,TRUE,12/06/2022,Include,24105,relation extraction; olivia.sanchez@uni-bielefeld.de,"Journal of biomedical semantics. 13(1):14, 2022 05 23.","In this work, we have created a corpus that goes beyond the existing clinical trial corpora, since it is annotated in a schematic way that represents the classes and properties defined in an ontology. Although the corpus is small, it has fine-grained annotations and could be used to fine-tune pre-trained machine learning models and transformers to the specific task of extracting information about clinical trial abstracts.For future work, we will use the corpus for training information extraction systems that extract single entities, and predict template slot-fillers (i.e., class data/object properties) to populate a knowledge base that relies on the C-TrO ontology for the description of clinical trials.",,LS,BERT incl. biomedical versions,"F1, Precision, Recall","Abstracts, Titles",RCT,,,Yes,,https://zenodo.org/record/6365890,Yes,No,Yes,,,Yes,,They used two different topics within their dataset (glaucoma and diabetes) and report results for each separate dataset. ,Yes,Yes,Yes,,Yes,"information from tables when mining full text can be missed, and transfer for models trained on abstracts and predicting on full texts may not perform well","micro-F1 within the complex entities (ie the sub-entities age, country below population)","own corpus, 211 RCT abstracts on glaucoma and diabetes with at least 2 drug intervention arms, for PICO entities and more complex relations in RDF format. They created an additional test set from 20 full texts
https://zenodo.org/record/6365890",,,Yes,,No,Yes,,"change scores, baseline values for groups, statistical information, and others","Very small and specific dataset, but available as benchmark corpus so comparability is given if someone re-uses it. ",Yes,Yes,No,Ontology,"P, IC (per arm), O, Other, IC (dose; duration and others), Country, Age, Gender, IC (Drug name), N (per arm), O (time point), P (Condition or disease), IC, Race",Entities,None reported,"Some discussions about extracting entity names such as author names, where order matters. Also discussed domain transfer abilities between abstracts and full texts",Yes; other description given,2022,Kappa for entity annotation between 0.74 and 0.68; F1 for complex entities (containing sub-classes and relations) is 0.81,,,,,,,,,,,,,,,,,,,
Text or probably text file,Constructing Artificial Data for Fine-tuning for Low-Resource Biomedical Text Tagging with Applications in PICO Annotation,"Biomedical text tagging systems are plagued by the dearth of labeled training data. There have been recent attempts at using pre-trained encoders to deal with this issue. Pre-trained encoder provides representation of the input text which is then fed to task-specific layers for classification. The entire network is fine-tuned on the labeled data from the target task. Unfortunately, a low-resource biomedical task often has too few labeled instances for satisfactory fine-tuning. Also, if the label space is large, it contains few or no labeled instances for majority of the labels. Most biomedical tagging systems treat labels as indexes, ignoring the fact that these labels are often concepts expressed in natural language e.g. `Appearance of lesion on brain imaging'. To address these issues, we propose constructing extra labeled instances using label-text (i.e. label's name) as input for the corresponding label-index (i.e. label's index). In fact, we propose a number of strategies for manufacturing multiple artificial labeled instances from a single label. The network is then fine-tuned on a combination of real and these newly constructed artificial labeled instances. We evaluate the proposed approach on an important low-resource biomedical task called \textit{PICO annotation}, which requires tagging raw text describing clinical trials with labels corresponding to different aspects of the trial i.e. PICO (Population, Intervention/Control, Outcome) characteristics of the trial. Our empirical results show that the proposed method achieves a new state-of-the-art performance for PICO annotation with very significant improvements over competitive baselines.",Gaurav Singh; Zahra Sabet; John Shawe-Taylor; James Thomas,https://doi.org/10.48550/arXiv.1910.09255,2019,Other,Include,TRUE,3/21/2023,Include,25157,g.singh@cs.ucl.ac.uk,,"We proposed a new method for biomedical text annotation that can work with limited
training data. More specifically, our model uses pre-trained bidirectional encoders for
mapping clinical texts to output concepts, Our empirical results show that the proposed method achieves
a new state-of-the-art performance for PICO annotation with very significant improvements over competitive baselines.",,LS,"CNN, BERT incl. biomedical versions","Precision, Recall, F1","Abstracts, Titles",RCT,,,No,,https://github.com/gauravsc/pico-tagging,No,No,Yes,,,Yes,,,,,Yes,,,,"reported macro-and micro scores, but separately for each class. Also, they report both micro and macro-F1 to average across P IC O","they used a Cochrane-provided dataset with P, IC, O (10137 abstracts) but it is unclear if it was used elsewhere","A MESH-pre-trained version of BERT, described in the paper",,,,,,,,,No,,,Spans or annotations,"P, IC, O",Entities,None reported,,,2020,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Detect and Classify Ã¢â‚¬â€œ Joint Span Detection and Classification for Health Outcomes,"A health outcome is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a predefined set of categories depending on an outcome that is mentioned somewhere in that text. However, this decoupling of span detection and classification is problematic from a modelling perspective and ignores global structural correspondences between sentence-level and word-level information present in a given text. To address this, we propose a method that uses both word-level and sentence-level information to simultaneously perform outcome span detection and outcome type classification. In addition to injecting contextual information to hidden vectors, we use label attention to appropriately weight both word and sentence level information. Experimental results on several benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results.",Abaho Michael; Bollegala Danushka; Williamson Paula R; Dodd Susanna,https://www.google.com/search?q=Detect+and+Classify+Ã¢â‚¬â€œ+Joint+Span+Detection+and+Classification+for+Health+Outcomes,2021,EPPIOpenAlex,Include,TRUE,3/21/2023,Include,25158,FALSE,,"Experimental results on several
benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results.",,LS,"Word embedding, LSTM, BERT incl. biomedical versions","Precision, Recall, F1, Other","Abstracts, Titles",RCT,,,Yes,,https://github.com/MichealAbaho/Label-Context-Aware-Attention-Model,Yes,No,Yes,,,Yes,,using 2 different datasets + combination of both,,,,,,error cascading when using a pipeline of entity recognition - normalization into categories. They state that there are biases within the training texts (such as the ones in ROB tool) that might be learned by the model,"relaxation methods precision@ 1 , 3, 5 and Normalized Discounted Cumulated Gain nDCG@", EBM-NLP and EBM-COMET; and a merged version. https://github.com/MichealAbaho/Label-Context-Aware-Attention-Model,,,,,,,,includes normalisation to outcome categories ,,Yes,,,Spans or annotations,O,Entities,None reported,error analysis done,,2021,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations,"The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which requires finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then normalising Population, Intervention, Comparator, and Outcome (PICO) information from clinical trial articles may be an effective way to automatically assign trials to systematic reviews and avoid searching and screening - the two most time-consuming systematic review processes. We propose and test a novel approach to PICO span detection. The major difference between our proposed method and previous approaches comes from detecting spans without needing annotated span data and using only crowdsourced sentence-level annotations. Experiments on two datasets show that PICO span detection results achieve much higher results for recall when compared to fully supervised methods with PICO sentence detection at least as good as human annotations. By removing the reliance on expert annotations for span detection, this work could be used in human-machine pipeline for turning low-quality crowdsourced, and sentence-level PICO annotations into structured information that can be used to quickly assign trials to relevant systematic reviews.",Liu Shifeng; Sun Yifang; Li Bing; Wang Wei; Bourgeois Florence T; Dunn Adam G,https://doi.org/10.48550/arxiv.2109.02254,2021,EPPIOpenAlex,Include,TRUE,3/21/2023,Include,25174,"{shifeng.liu,adam.dunn}@sydney.edu.au",,"The difference between Sent2Span and previous approaches to PICO detection include the use of only
low-quality sentence-level annotations as training
data and the results demonstrating achieve high
recall in span detection, which is an important requirement for systematic review processes.",,LS,BERT incl. biomedical versions,"Precision, Recall, F1, Accuracy","Abstracts, Titles",RCT,,,Yes,,https://github.com/evidence-surveillance/sent2span ,Yes,No,Yes,,,Yes,,multiple datasets,,,,,,,,"EBMNLP; plus 4240  RCTs labelled with P: https://github.com/yinfeiy/PICO-data  An Thanh Nguyen, Byron C. Wallace, Junyi Jessy Li,
Ani Nenkova, and Matthew Lease. 2017. Aggregating and predicting sequence labels from crowd annotations. In ACL (1), pages 299â€“309. Association for
Computational Linguistics.",,,,,,,,,,Yes,,,Spans or annotations,"P, IC, O","Sentences, Entities","FP, FN",error analysis done,,2021,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"CCS Explorer: Relevance Prediction, Extractive Summarization, and Named Entity Recognition from Clinical Cohort Studies","Clinical Cohort Studies (CCS), such as randomized clinical trials, are a great source of documented clinical research. Ideally, a clinical expert inspects these articles for exploratory analysis ranging from drug discovery for evaluating the efficacy of existing drugs in tackling emerging diseases to the first test of newly developed drugs. However, more than 100 articles are published daily on a single prevalent disease like COVID-19 in PubMed. As a result, it can take days for a physician to find articles and extract relevant information. Can we develop a system to sift through the long list of these articles faster and document the crucial takeaways from each of these articles? In this work, we propose CCS Explorer, an end-to-end system for relevance prediction of sentences, extractive summarization, and patient, outcome, and intervention entity detection from CCS. CCS Explorer is packaged in a web-based graphical user interface where the user can provide any disease name. CCS Explorer then extracts and aggregates all relevant information from articles on PubMed based on the results of an automatically generated query produced on the back-end. For each task, CCS Explorer fine-tunes pre-trained language representation models based on transformers with additional layers. The models are evaluated using two publicly available datasets. CCS Explorer obtains a recall of 80.2%, AUC-ROC of 0.843, and an accuracy of 88.3% on sentence relevance prediction using BioBERT and achieves an average Micro F1-Score of 77.8% on Patient, Intervention, Outcome detection (PIO) using PubMedBERT. Thus, CCS Explorer can reliably extract relevant information to summarize articles, saving time by $\sim \text{660}\times$.",Al-Hussaini Irfan; An Davi Nakajima; Lee Albert J; Bi Sarah; Mitchell Cassie S,https://doi.org/10.1109/bigdata55660.2022.10020807,2022,EPPIOpenAlex,Include,TRUE,3/21/2023,Include,25189,relation extraction and summarisation using EI dataset,,"CCS Explorer obtains a recall of 80.2%, AUC-ROC of 0.843, and an accuracy of 88.3% on sentence relevance prediction using BioBERT and achieves an average Micro F1-Score of 77.8% on Patient, Intervention, Outcome detection (PIO) using PubMedBERT. Thus, CCS Explorer can reliably extract relevant information to summarize articles, saving time by ~660Ã—.",,LS,"BERT incl. biomedical versions, Other Transformer","Precision, Recall, F1, Accuracy, AUC-ROC","Abstracts, Titles",RCT,,,Yes,,,Yes,No,No,,,Yes,,Small test on real-world data to measure time-saved,,,,,,,"micro F1, time-saved using CSS explorer",EI dataset and EBM-NLP,"BioELECTRA
SapBERT [39]: Pre-trained a BERT model on the biomedical knowledge graph of UMLS
KRISSBERT [43]: Initialized with PubMedBERT [38] parameters, and then pre-trained using biomedical entity names from the UMLS ontology ",,,,,,,,,No,,,"Spans or annotations, Structured text and summary","P, IC, O","Entities, Sentences",None reported,,,2022,,,,,,,,,,,,,,,,,,,,
XML,Automated detection of over- and under-dispersion in baseline tables in randomised controlled trials,"<ns3:p><ns3:bold>Background</ns3:bold>: Papers describing the results of a randomised trial should include a baseline table that compares the characteristics of randomised groups. Researchers who fraudulently generate trials often unwittingly create baseline tables that are implausibly similar (under-dispersed) or have large differences between groups (over-dispersed). I aimed to create an automated algorithm to screen for under- and over-dispersion in the baseline tables of randomised trials.</ns3:p><ns3:p> <ns3:bold>Methods</ns3:bold>: Using a cross-sectional study I examined 2,245 randomised controlled trials published in health and medical journals on <ns3:italic>PubMed Central</ns3:italic>. I estimated the probability that a trial's baseline summary statistics were under- or over-dispersed using a Bayesian model that examined the distribution of t-statistics for the between-group differences, and compared this with an expected distribution without dispersion. I used a simulation study to test the ability of the model to find under- or over-dispersion and compared its performance with an existing test of dispersion based on a uniform test of p-values. My model combined categorical and continuous summary statistics, whereas the uniform uniform test used only continuous statistics.</ns3:p><ns3:p> <ns3:bold>Results</ns3:bold>: The algorithm had a relatively good accuracy for extracting the data from baseline tables, matching well on the size of the tables and sample size. Using t-statistics in the Bayesian model out-performed the uniform test of p-values, which had many false positives for skewed, categorical and rounded data that were not under- or over-dispersed. For trials published on <ns3:italic>PubMed Central</ns3:italic>, some tables appeared under- or over-dispersed because they had an atypical presentation or had reporting errors. Some trials flagged as under-dispersed had groups with strikingly similar summary statistics.</ns3:p><ns3:p> <ns3:bold>Conclusions</ns3:bold>: Automated screening for fraud of all submitted trials is challenging due to the widely varying presentation of baseline tables. The Bayesian model could be useful in targeted checks of suspected trials or authors.</ns3:p>",Barnett Adrian G,https://doi.org/10.12688/f1000research.123002.1,2022,EPPIOpenAlex,Include,TRUE,3/21/2023,Include,25193,only extracts N from tables,,,,LS,Rule-based,Other,"Full texts, Titles, Abstracts",RCT,,,No,,https://zenodo.org/record/6647853#.ZBnpLXbP2Uk,Yes,No,Yes,,,Yes,,,,,,,,"Description of baseline characteristics varies, as they can be described as mean/percentage/quartiles",mean difference between extracted sample size and actual size,"200 RCT fulltexts from PMC, annotated N from baseline tables",,,,,,,,,,No,,,Spans or annotations,"N (per arm), N (total)",Entities,None reported,"NA, it is a rule-based approach",,2022,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"Not so weak PICO: leveraging weak supervision for participants, interventions, and outcomes recognition for systematic review automation","Abstract Objective The aim of this study was to test the feasibility of PICO (participants, interventions, comparators, outcomes) entity extraction using weak supervision and natural language processing. Methodology We re-purpose more than 127 medical and nonmedical ontologies and expert-generated rules to obtain multiple noisy labels for PICO entities in the evidence-based medicine (EBM)-PICO corpus. These noisy labels are aggregated using simple majority voting and generative modeling to get consensus labels. The resulting probabilistic labels are used as weak signals to train a weakly supervised (WS) discriminative model and observe performance changes. We explore mistakes in the EBM-PICO that could have led to inaccurate evaluation of previous automation methods. Results In total, 4081 randomized clinical trials were weakly labeled to train the WS models and compared against full supervision. The models were separately trained for PICO entities and evaluated on the EBM-PICO test set. A WS approach combining ontologies and expert-generated rules outperformed full supervision for the participant entity by 1.71% macro-F1. Error analysis on the EBM-PICO subset revealed 18Ã¢â‚¬â€œ23% erroneous token classifications. Discussion Automatic PICO entity extraction accelerates the writing of clinical systematic reviews that commonly use PICO information to filter health evidence. However, PICO extends to more entitiesÃ¢â‚¬â€PICOS (SÃ¢â‚¬â€study type and design), PICOC (CÃ¢â‚¬â€context), and PICOT (TÃ¢â‚¬â€timeframe) for which labelled datasets are unavailable. In such cases, the ability to use weak supervision overcomes the expensive annotation bottleneck. Conclusions We show the feasibility of WS PICO entity extraction using freely available ontologies and heuristics without manually annotated data. Weak supervision has encouraging performance compared to full supervision but requires careful design to outperform it.",Dhrangadhariya Anjani; MÃƒÂ¼ller Henning,https://doi.org/10.1093/jamiaopen/ooac107,2023,EPPIOpenAlex,Include,TRUE,3/21/2023,Include,25244,They say that 1% of EBMNLP tokens in the eval set are errors,,We show the feasibility of WS PICO entity extraction using freely available ontologies and heuristics without manually annotated data. Weak supervision has encouraging performance compared to full supervision but requires careful design to outperform it.,,LS,"BERT incl. biomedical versions, Rule-based","F1, Recall","Abstracts, Titles",RCT,,,No,,https://github.com/anjani-dhrangadhariya/distant-PICO,Yes,No,Yes,,,Yes,,,,,,,,"Annotation of gold standards by humans can be imperfect, and contain errors",macro-averaged ,"EBM-NLP, using rule-based UMLS tagging to re-label it. Data here: https://datadryad.org/stash/dataset/doi:10.5061/dryad.ncjsxkszr ",,,,,,,,,,No,,,Spans or annotations,"P, IC, O",Entities,None reported,error analysis only done on ebmnlp labelling,,2023,cohens k versus the original EBM-NLP annotations between 0.53 for P -0.69 for O,,,,,,,,,,,,,,,,,,,
Text or probably text file,SciBERT: A pretrained language model for scientific text,,"Beltagy, Iz, Lo, Kyle, Cohan, Arman ",http://dx.doi.org/10.18653/v1/d19-1371,,,Include,,10/10/2024,Include,15192596,,,"We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains We demonstrate statistically significant improvements over BERT and achieve new state-of-the-ar",,,BERT incl. biomedical versions,F1,"Abstracts, Full texts, Titles",RCT,"EBM-NLP for eval, Semantic scolar 14 m for pretrain",,Yes,,https://githubcom/allenai/scibert/,Yes,No,Yes,,,Yes,,variety of tasks and banchmark datasets used,No,No,Yes,,,,macro,using benchmark,,,Yes,,No,Yes,,"unclear which version of EBM-NLP was used, but assuming the higher-level PICOs only",,No,,Yes,Spans or annotations,"O, P, IC",Entities,None reported,,,2019,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Enhancing PICOS Information Extraction with UIE and ERNIE-Health,"In the vast sea of medical papers lie numerous crucial pieces of information. Faced with the challenge of unstructured medical literature, it becomes a formidable task to automatically extract the necessary key information. In this paper, we delve into the CHIP2023 evaluation task 5, utilizing UIE and ERNIE-Health to accomplish the extraction of PICOS key information from medical papers. Our results indicate that the framework we propose has achieved commendable performance.","Zhang, Lei, Tian, Wei, Zheng, Yuan  et al.",https://link.springer.com/chapter/10.1007/978-981-97-1717-0_17,,,Include,,10/10/2024,Include,15192595,,,"In this paper, we delve into the CHIP2023 evaluation task 5, utilizing UIE and ERNIE-Health to accomplish the extraction of PICOS key information from medical papers Our results indicate that the framework we propose has achieved commendable performance",,LS,Other Transformer,F1,"Abstracts, Titles",RCT,"CHIP 2023 Task 5, chinese",,No,,,No,No,No,,,No,,,No,No,Yes,,,"varying targets, heterogeneous structures, and demand-specific schemas make task difficult",macro,,"ERNIE-Health (by Baidu, similar to ELECTRA architecture) Universal Information Extraction (UIE) framework, adversarial training, ensemble",,Yes,,No,Yes,,,Used shared task dataset,No,Yes,,Spans or annotations,"P, IC, O, Design","Entities, Sentences",None reported,,,2024,,,,,,,,,,,,,,,,,,,,
Text or probably text file,LLM Collaboration PLM Improves Critical Information Extraction Tasks in Medical Articles,"With the development of modern medical informatics and databases, medical professionals are increasingly inclined to use evidence-based medicine to guide their learning and work. Evidence-based medicine requires a large amount of data and literature information, where most search processes are keyword retrieval. Therefore, anticipating these key information through the model can play an important role in optimizing the query. In the past, the PLM (Pre-trained Language Model) model was mainly used for information extraction, but due to the complexity of the sequence semantic structure and task diversity, it is difficult for traditional PLM to achieve the desired effect. With the advancement of LLM (Large Language Model) technology, these issues can now be well managed. In this paper, we discuss the information extraction evaluation task CHIP-PICOS, and finally decompose it into classification and information extraction sub-problems, applying PLM and LLM respectively, and analyzing the advantages and disadvantages and differences between PLM and LLM. The results show that our framework has achieved significant performance.","Cao, Mengyuan, Wang, Hang, Liu, Xiaoming  et al.",https://link.springer.com/chapter/10.1007/978-981-97-1717-0_16,,,Include,,10/10/2024,Include,15192594,,,"It was found that traditional pre-training models still outperform big models in classification tasks, so the information extraction task was detailed and split into two subtasks The results showthat our framework has achieved significant performance",,LS,"Other Transformer, LLM",F1,"Abstracts, Titles",RCT,CHIP 2023 Task 5,,No,,,No,No,No,,,No,,,No,No,Yes,,,traditional pre-training models still outperform big models in classification tasks,macro,,"ChatGLM, RoBERTa",,Yes,,,Yes,,macro,,No,Yes,,Spans or annotations,"Design, P, IC, O","Entities, Sentences",None reported,,,2024,,,,,,,,,,,,,,Not applicable,No,,ChatGLM is fine-tuned by using P-turning ,Fine-tuning,,
Text or probably text file,An extensive benchmark study on biomedical text generation and mining with ChatGPT,"In recent years, the development of natural language process (NLP) technologies and deep learning hardware has led to significant improvement in large language models (LLMs). The ChatGPT, the state-of-the-art LLM built on GPT-3.5 and GPT-4, shows excellent capabilities in general language understanding and reasoning. Researchers also tested the GPTs on a variety of NLP-related tasks and benchmarks and got excellent results. With exciting performance on daily chat, researchers began to explore the capacity of ChatGPT on expertise that requires professional education for human and we are interested in the biomedical domain.To evaluate the performance of ChatGPT on biomedical-related tasks, this article presents a comprehensive benchmark study on the use of ChatGPT for biomedical corpus, including article abstracts, clinical trials description, biomedical questions, and so on. Typical NLP tasks like named entity recognization, relation extraction, sentence similarity, question and answering, and document classification are included. Overall, ChatGPT got a BLURB score of 58.50 while the state-of-the-art model had a score of 84.30. Through a series of experiments, we demonstrated the effectiveness and versatility of ChatGPT in biomedical text understanding, reasoning and generation, and the limitation of ChatGPT build on GPT-3.5.All the datasets are available from BLURB benchmark https://microsoft.github.io/BLURB/index.html. The prompts are described in the article.","Chen, Qijie, Sun, Haotong, Liu, Haoyang  et al.",https://openalex.org/works/W4386530347,,,Include,,10/10/2024,Include,15188615,,,ChatGPT built on the early version of GPT-35 performed poorly on several biomedical NLP benchmark datasets The biomedical domain is clearly a challenging professional field to deal with for a general LLM running in the zero or few shot scenario,,LS,"LLM, BERT incl. biomedical versions","Precision, Recall, F1","Abstracts, Titles",RCT,EBM-NLP https://microsoftgithubio/BLURB/indexhtml,,Yes,,,Yes,No,No,description how GPT output differs from NER output and challenges for evaluation,,Yes,,BLURB datasets consists of multiple benchmark datasets,Yes,Yes,Yes,,Yes,"It fails to recognise all instances of an entity and returns only one GPT is generative model, while NLP tasks require structured predictions GPT does not follow structured format",macro,,"GPT-35, PubmedBERT, BioLinkBERT-Base, and BioLinkBERTLarge",,,,No,Yes,,,,,,,Spans or annotations,"P, IC, O",Entities,None reported,,,2023,,,,,,,,,,,,,,No,No,,"overall significantly worse scores for ChatGPT, 55 vs 74 Fscore on ebm-nlp Prompt given but no description of dev process",Zero-shot (prompt only),,
Text or probably text file,PICO to PICOS: Weak Supervision to Extend Datasets with New Labels,"Hand-labelling clinical corpora can be costly and inflexible, requiring re-annotation every time new classes need to be extracted. PICO (Participant, Intervention, Comparator, Outcome) information extraction can expedite conducting systematic reviews to answer clinical questions. However, PICO frequently extends to other entities such as Study type and design, trial context, and timeframe, requiring manual re-annotation of existing corpora. In this paper, we adapt Snorkelâ€™s weak supervision methodology to extend clinical corpora to new entities without extensive hand labelling. Specifically, we enrich the EBM-PICO corpus with new entities through an example of â€œStudy type and designâ€ extraction. Using weak supervision, we obtain programmatic labels on 4,081 EBM-PICO documents, achieving an F1-score of 85.02% on the test set.","Dhrangadhariya, Anjani, Manzo, Gaetano, MÃ¼ller, Henning ",https://openalex.org/works/W4401821073,,,Include,,10/10/2024,Include,15188613,,,"Specifically, we enrich the EBM-PICO corpus with new entities through an example of Study type and design extraction Using weak supervision, we obtain programmatic labels on 4,081 EBM-PICO documents, achieving an F1-score of 8502% on the test set",,,Rule-based,"Precision, Recall, F1","Abstracts, Titles",RCT,EBM-NLP adaptation,,No,,link found via manual searching https://githubcom/anjani-dhrangadhariya/distant-studytype/tree/master,No,No,Yes,,,No,,,No,No,Yes,,,,macro average,,,,,,No,,,,,No,Yes,No,Spans or annotations,Design,Entities,None reported,,,2024,pairwise F1 78%,,,,,,,,,,,,,,,,,,,
Text or probably text file,BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER,"Biomedical Named Entity Recognition (BioNER) is the fundamental task of identifying named entities from biomedical text. However, BioNER suffers from severe data scarcity and lacks high-quality labeled data due to the highly specialized and expert knowledge required for annotation. Though data augmentation has shown to be highly effective for low-resource NER in general, existing data augmentation techniques fail to produce factual and diverse augmentations for BioNER. In this paper, we present BioAug, a novel data augmentation framework for low-resource BioNER. BioAug, built on BART, is trained to solve a novel text reconstruction task based on selective masking and knowledge augmentation. Post training, we perform conditional generation and generate diverse augmentations conditioning BioAug on selectively corrupted text similar to the training stage. We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a significant margin (1.5%-21.5% absolute improvement) and is able to generate augmentations that are both more factual and diverse. Code: https://github.com/Sreyan88/BioAug.","Ghosh, Sreyan, Tyagi, Utkarsh, Kumar, Sonal  et al.",https://openalex.org/works/W4384656653,,,Include,,10/10/2024,Include,15188612,,,We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a significant margin (15%-215% absolute improvement) and is able to generate augmentations that are both more factual and div,,LS,Other Transformer,F1,"Abstracts, Titles",RCT,ebmnlp,,Yes,,https://githubcom/Sreyan88/BioAug,Yes,No,Yes,,,Yes,,multiple benchmarks,No,No,Yes,,Yes,,"micro, although description is not 100% clear if micro was applied to average between PICO classes or on a higher level between datasets ",,BioBART,,,,No,Yes,,,benchmark,No,Yes,,Spans or annotations,"P, IC, O",Entities,None reported,,,2023,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Biomedical Abstract Sentence Classification by BERT-Based Reading Comprehension,,"Jiang, Chengyang, Fan, Yao-Chung ",https://openalex.org/works/W4376877121,,,Include,,10/10/2024,Include,15188611,,,"our models outperform the best performing models by 4â€“5% in terms of f1 scores Moreover, we conduct ablations to reveal that the model benefits from the altered training goal of reading comprehension ",,,BERT incl. biomedical versions,F1,"Abstracts, Titles",RCT,NICTA-PIBOSO,,Yes,,https://githubcom/UDICatNCHU/Scientific-Literature-Sentence-Classification-by-BERT-based-Reading-Comprehension,Yes,No,Yes,,,Yes,,"two datasets used, the second one has some overlapping classes but not relevant to this review",Yes,Yes,Yes,,,,weighed average,,BERT base and large,,,,,Yes,,,,,Yes,Yes,Spans or annotations,"O, Sections (Aim; Method etc.), P, IC",Sentences,None reported,,,2023,,,,,,,,,,,,,,,,,,,,
XML,Text classification models for assessing the completeness of randomized controlled trial publications based on CONSORT reporting guidelines,"Complete and transparent reporting of randomized controlled trial publications (RCTs) is essential for assessing their credibility. We aimed to develop text classification models for determining whether RCT publications report CONSORT checklist items. Using a corpus annotated with 37 fine-grained CONSORT items, we trained sentence classification models (PubMedBERT fine-tuning, BioGPT fine-tuning, and in-context learning with GPT-4) and compared their performance. We assessed the impact of data augmentation methods (Easy Data Augmentation (EDA), UMLS-EDA, text generation and rephrasing with GPT-4) on model performance. We also fine-tuned section-specific PubMedBERT models (e.g., Methods) to evaluate whether they could improve performance compared to the single full model. We performed 5-fold cross-validation and report precision, recall, F","Jiang, Lan, Lan, Ming-Shong, Menke, Jan  et al.",https://openalex.org/works/W4402620497,,,Include,,10/10/2024,Include,15188610,,,Most CONSORT checklist items can be recognized reasonably well with the fine-tuned PubMedBERT model but there is room for improvement Improved models can underpin the journal editorial workflows and CONSORT adherence checks ,,LS,"Rule-based, Other, BERT incl. biomedical versions, LLM","F1, AUC-ROC, Precision, Recall","Titles, Abstracts, Full texts",RCT,CONSORT-TM described elsewhere,,No,,https://githubcom/ScienceNLP-Lab/RCT-Transparency,Yes,No,Yes,,,Yes,,,Yes,Yes,Yes,,Yes,model confuses similar items LS cost of LLM leads to smaller train/developmetn sets and poss less good downstream performance,"mic and  mac (show diff models and their impact on rare or common classes), mcnemar",,PubMedBERT AugGPT augmentation GPT-4 ,,,,Yes,Yes,,CONSORT items,,,Yes,No,Spans or annotations,"IC, P, N (total), O, Other","Binary for each document, Sentences",None reported,,,2024,,,,,,,,,,,,,,No,No,,"prompts given, dev split unclear","Fine-tuning, Zero-shot (prompt only), K-shot (context in prompt)",,
Text or probably text file,Automated information extraction model enhancing traditional Chinese medicine RCT evidence extraction (Evi-BERT): algorithm development and validation,"Background In the field of evidence-based medicine, randomized controlled trials (RCTs) are of critical importance for writing clinical guidelines and providing guidance to practicing physicians. Currently, RCTs rely heavily on manual extraction, but this method has data breadth limitations and is less efficient. Objectives To expand the breadth of data and improve the efficiency of obtaining clinical evidence, here, we introduce an automated information extraction model for traditional Chinese medicine (TCM) RCT evidence extraction. Methods We adopt the Evidence-Bidirectional Encoder Representation from Transformers (Evi-BERT) for automated information extraction, which is combined with rule extraction. Eleven disease types and 48,523 research articles from the China National Knowledge Infrastructure (CNKI), WanFang Data, and VIP databases were selected as the data source for extraction. We then constructed a manually annotated dataset of TCM clinical literature to train the model, including ten evidence elements and 24,244 datapoints. We chose two models, BERT-CRF and BiLSTM-CRF, as the baseline, and compared the training effects with Evi-BERT and Evi-BERT combined with rule expression (RE). Results We found that Evi-BERT combined with RE achieved the best performance (precision score = 0.926, Recall = 0.952, F1 score = 0.938) and had the best robustness. We totally summarized 113 pieces of rule datasets in the regulation extraction procedure. Our model dramatically expands the amount of data that can be searched and greatly improves efficiency without losing accuracy. Conclusion Our work provided an intelligent approach to extracting clinical evidence for TCM RCT data. Our model can help physicians reduce the time spent reading journals and rapidly speed up the screening of clinical trial evidence to help generate accurate clinical reference guidelines. Additionally, we hope the structured clinical evidence and structured knowledge extracted from this study will help other researchers build large language models in TCM.","Li, Yizhen, Luan, Zhongzhi, Liu, Yixing  et al.",https://openalex.org/works/W4401622044,,,Include,,10/10/2024,Include,15188608,,,Our work provided an intelligent approach to extracting clinical evidence for TCM RCT data Our model can help physicians reduce the time spent reading journals and rapidly speed up the screening of clinical trial evidence ,,LS,"Rule-based, CRF, LSTM, BERT incl. biomedical versions","Precision, Recall, F1","Abstracts, Titles",RCT,"Eleven disease types and 10,266 Chinese RCT paragraphs China National Knowledge Infrastructure (CNKI), WanFang Data, and VIP database",,No,,"partly, RE patterns in python code for some elements available: https://wwwfrontiersinorg/journals/artificial-intelligence/articles/103389/frai20241454945/full#supplementary-material  ",No,No,Yes,,,Yes,,,Yes,Yes,Yes,,No,"need more verification data, corpus may not represent distributions within TCM literature, noisy annotations",,,ensemble,,,,No,,,dropouts,,No,,,Spans or annotations,"Age, Randomisation, Blinding, Design, P, IC, O, N (total)","Entities, Sentences",None reported,,,2024,Cohenâ€™s kappa 08,,,,,,,,,,,,,,,,,,,
PDF,AI-Driven Evidence Synthesis: Data Extraction of Randomized Controlled Trials with Large Language Models,,"Liu, Jiayi, Ge, Long, Lai, Honghao  et al.",https://openalex.org/works/W4399857258,,,Include,,10/10/2024,Include,15188607,,,"Using structured prompts, Claude can extract data efficiently and accurately, showing the application feasibility and value of LLM in evidence synthesis",,,LLM,"Accuracy, Other","Abstracts, Titles, Full texts",RCT,10 RCTs from Cochrane reviews + 3 for prompt development,,No,,,No,No,No,,,Yes,,"very small dataset, taken from Cochrane reviews",Yes,Yes,Yes,,,extractions around numbers of participants at various timepoints in the study had most mistakes Token-limit and assessment in english texts only,time taken,"source, size, numbers",Claude-2,,,,No,No,,"58 items related to â€œMethods,â€ â€œParticipants,â€ â€œbaseline characteristics,â€ â€œOutcomes,â€ â€œData and analysis,â€ and â€œOthersâ€",,No,,Yes,Spans or annotations,"Country, Race, Other, Exclusion criteria, Enrolment dates, Funding org, P (Condition or disease), Withdrawals or exclusions, Setting, Design, Eligibility criteria, O (measurement instrument), N (total), Age, Gender, P, IC, O",Entities,None reported,,"Yes, compl different datasets",2024,,,,,,,,,,,,,,Yes,No,,no prompts shared,Zero-shot (prompt only),,
Text or probably text file,Evaluation of SURUS: a Named Entity Recognition System to Extract Knowledge from Interventional Study Records,"BACKGROUND Medical decision-making is commonly guided by systematic analysis of peer-reviewed scientific literature, published as systematic literature reviews (SLRs). These analyses are cumbersome to conduct as they require large amounts of time and subject matter expertise to be available. Automated extraction of key datapoints from clinical publications could speed up the process of systematic literature review assembly. To this end, we built, trained and validated SURUS, a named entity recognition (NER) system comprised of a Bidirectional Encoder Representations from Transformers (BERT) model trained on a highly granular dataset. The aim of this study was to assess the quality of classification of critical elements in clinical study abstracts by SURUS, in particular the patient, intervention, comparator and outcome (PICO) elements and elements of study design. DATASET &amp; METHODS The PubMedBERT-based model was trained and evaluated using a dataset of 400 interventional study abstracts, manually annotated by experts using 25 labels with a total of 39,531 annotations according to a strict annotation guideline, with Cohenâ€™s Îº inter-annotator agreement of 0.81. We evaluated in-domain quality, and assessed out-of-domain quality of the system by testing it on out-of-domain abstracts of other disease areas and observational study types. Finally, we tested the utility of SURUS by comparing its predictions to expert-assigned PICO and study design (PICOS) classifications. RESULTS The SURUS NER system achieved an overall F1 score of 0.95, with minor deviation between labels. In addition, SURUS achieved a NER F1 of 0.90 for out-of-domain therapeutic area abstracts and 0.84 for observational study abstracts. Finally, SURUS showed considerable utility when compared to expert-assigned PICOS classifications of interventional studies, with an F1 of 0.89 and a recall of 0.96. CONCLUSION To our knowledge, with an F1 score of 0.95, SURUS ranks among the best-performing models available to date for conducting exhaustive systematic literature analyses. A strict guideline and high inter-annotation agreement resulted in high-quality in-domain medical entity of a finetuned BERT-based model, which was largely preserved during extensive out-of-domain evaluation, indicating its utility across other indication areas and study types. Current approaches in the field lack the granularity in training data and versatility demonstrated by the SURUS approach, thereby making the latter the preferred choice for automated extraction and classification tasks in the clinical literature domain. We think that this approach sets a new standard in medical literature analysis and paves the way for creating highly granular datasets of labelled entities that can be used for downstream analysis outside of traditional SLRs.","Peeters, Casper, Vijverberg, Koen, Pouwer,,  et al.",https://openalex.org/works/W4399273570,,,Include,,10/10/2024,Include,15188605,,,SURUS ranks among the best-performing models available to date for conducting exhaustive systematic literature analyses A strict guideline and high inter-annotation agreement resulted in high-quality in-domain medical entity of a finetuned BERT-based mod,,,BERT incl. biomedical versions,"F1, Recall, Precision","Abstracts, Titles","RCT, Cohort, Mix, Case series, Case control, Diagnostic test, Non-randomised (intervention) study","400 rct abstracts, PubMed, cv , endocrine diseases, neoplasms and resp diseases Ind test set 123 abs including other medical domains and study types  practical eval using included studies from 8 cochrane reviews",,Yes,,,No,No,No,,,Yes,,"3 datasets, with different domain data curated on purpose",Yes,Yes,Yes,,Yes,,weighted mean F1,"size, distribution","PubMedBERT, BioBERT, SciBERT ",,,,No,Yes,,25 entities including results data,multiple datasets and models,No,Yes,No,Spans or annotations,"Other, Diagnostic tests, N (total), Design, Eligibility criteria, Funding org, P, IC, O",,"TP, FP, FN",,"Yes, random splits and ratio given",2024,"cohens kappa 081, token-level F1 088",,,,,,,,,,,,,,,,,,,
Text or probably text file,"Automated Mass Extraction of Over 680,000 PICOs from Clinical Study Abstracts Using Generative AI: A Proof-of-Concept Study","Generative artificial intelligence (GenAI) shows promise in automating key tasks involved in conducting systematic literature reviews (SLRs), including screening, bias assessment and data extraction. This potential automation is increasingly relevant as pharmaceutical developers face challenging requirements for timely and precise SLRs using the population, intervention, comparator and outcome (PICO) framework, such as those under the impending European Union (EU) Health Technology Assessment Regulation 2021/2282 (HTAR). This proof-of-concept study aimed to evaluate the feasibility, accuracy and efficiency of using GenAI for mass extraction of PICOs from PubMed abstracts.","Reason, Tim, Langham, Julia, Gimblett, Andy ",https://openalex.org/works/W4402858850,,,Include,,10/10/2024,Include,15188604,,,"Using GenAI to extract PICOs from clinical study abstracts could fundamentally transform the way SLRs are conducted By enabling pharmaceutical developers to anticipate PICO requirements, this approach allows for proactive preparation for the EU HTAR proc",,,LLM,Accuracy,"Abstracts, Titles",RCT,"682,667 abstracts from PubMed, 350 abstracts evaluated manually",,No,No,,No,No,No,,,Yes,,,Yes,Yes,Yes,,,"unknown how this method would perform with non-English-language abstracts, outcomes extraction most incomplete",,,GPT-4o via OpenAI Batch API,,Yes,,,No,,CSV and PostgreSQL output,,No,Yes,Yes,Other,"O, P, IC",Entities,None reported,,"Yes, random splits and ratio given",2024,,,,,,,,,,,,,,No,No,,"no prompts, no description of prompt development, example prompt does not seem to match the final output ",Zero-shot (prompt only),,
Text or probably text file,EvidenceTriangulator: A Large Language Model Approach to Synthesizing Causal Evidence across Study Designs,"Abstract In managing chronic diseases, the role of social determinants like lifestyle and diet is crucial. A comprehensive strategy combining biomedical and lifestyle changes is necessary for optimal health. However, the complexity of evidence from varied study designs on lifestyle interventions poses a challenge to decision-making. To tackle this challenge, our work focused on leveraging large language model to construct a dataset primed for evidence triangulation. This approach automates the process of gathering and preparing evidence for analysis, thereby simplifying the integration of reliable insights and reducing the dependency on labor-intensive manual curation. Our approach, validated by expert evaluations, demonstrates significant utility, especially illustrated through a case study on reduced salt intake and its effect on blood pressure. This highlights the potential of leveraging large language models to enhance evidence-based decision-making in health care.","Shi, Xuanyu, Zhao, Wenjing, Wang, Jing  et al.",https://openalex.org/works/W4392939106,,,Include,,10/10/2024,Include,15188603,,,"This approach complements traditional metaanalyses by integrating evidence across study designs, thereby facilitating more comprehensive assessments of public health recommendations",,LS,"LLM, Other","Precision, Recall, F1, Other","Abstracts, Titles","RCT, Cohort, Mix","DS1 100 trials, reviews, observational studies, meednelian randomisation studies in cardiovascular disease and nutrition domain practical evaluation DS: salt and hypertension,  1,488 primary research articles",,Yes,,,No,No,No,,,Yes,,separate dataset for practical evaluation,Yes,Yes,Yes,,,"more objective evaluation methods to be developed for llm, difficulties in accurately classifying study designs and normalizing extracted data ",BERTScore,,"deepseek-chat, glm-4-airx, qwenplus, GPT-4-mini,  evidence triangulation algorithm",,No,,No,No,,"relations/evidence inference, for lit reviews N included studies",,No,,No,"Structured text and summary, Spans or annotations","O (primary or secondary outcome), N (total), Exposure, Design, P, IC (per arm), O",Entities,None reported,,,2024,,,,,,,,,,,,,,No,No,,They said prompts are available in supplementary material but this wasn't found attached to the preprint,Zero-shot (prompt only),,
Text or probably text file,PICO Classification Using Domain-Specific Features,"This Paper proposes technique for extracting important information from clinical trial data. In order to extract important information PICO framework is used. PICO framework identifies the sentences in any given medical literature that fall under any one of the four categories: Participants/Problem (P), Intervention (I), Comparison (C), and Outcome (O). In this paper, machine learning classifiers are implemented to automatically detect PICO elements. The classification task is performed on the annotated clinical trial dataset. Instead of using a predefined word embedding like tf-idf, bag-of-word, or any other embedding domain-specific features are used as input to the machine learning classifiers. Domain-specific features were obtained using multiple entity extractors. These entity extractors extract entities related to a particular domain i.e., the biomedical domain. Extracted entities are converted into binary vectors and treated as features for the model. Our Proposed models perform better when compared to various strong baseline models.","Singh,,, Sharan, Aditi ",https://openalex.org/works/W4392126278,,,Include,,10/10/2024,Include,15188602,,,"Domain-specific features were obtained using multiple entity extractors These entity extractors extract entities related to a particular domain ie, the biomedical domain Our Proposed models perform better when compared to various strong baseline models",,LS,"LSTM, SVM, Other ML classifier, Other Transformer, Multi-layer perceptron, CRF","Accuracy, F1, Recall, Precision","Abstracts, Titles",RCT,adaptation of Jin dataset,,No,,,No,No,No,,,Yes,,,No,No,Yes,,,,'average f1 measure',"size, distribution",BioELECTRA fine-tuned on PICO extraction to generate features,Yes,No,,No,No,,,the benchmark was adapted and no comparable eval split,No,Yes,No,Spans or annotations,"P, IC, O",Sentences,None reported,,"Yes, random splits and ratio given",2024,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Fine-tuning large neural language models for biomedical natural language processing,"Large neural language models have transformed modern natural language processing (NLP) applications. However, fine-tuning such models for specific tasks remains challenging as model size increases, especially with small labeled datasets, which are common in biomedical NLP. We conduct a systematic study on fine-tuning stability in biomedical NLP. We show that fine-tuning performance may be sensitive to pretraining settings and conduct an exploration of techniques for addressing fine-tuning instability. We show that these techniques can substantially improve fine-tuning performance for low-resource biomedical NLP applications. Specifically, freezing lower layers is helpful for standard BERT- BASE models, while layerwise decay is more effective for BERT- LARGE and ELECTRA models. For low-resource text similarity tasks, such as BIOSSES, reinitializing the top layers is the optimal strategy. Overall, domain-specific vocabulary and pretraining facilitate robust models for fine-tuning. Based on these findings, we establish a new state of the art on a wide range of biomedical NLP applications.","Tinn, Robert, Cheng, Hao, æ± è°·, è£•äºŒ  et al.",https://openalex.org/works/W4365511667,,,Include,,10/10/2024,Include,15188601,,,We show that fine-tuning performance may be sensitive to pretraining settings and conduct an exploration of techniques for addressing fine-tuning instability We show that these techniques can substantially improve fine-tuning performance ,,LS,"BERT incl. biomedical versions, Other Transformer",F1,"Abstracts, Titles",RCT,"BLURB benchmark, including EBM-NLP",,Yes,,"model weights available, no code: https://akams/huggingface PubMedBERT: https://akams/pubmedbert PubMedBERT-LARGE: https://akams/pubmedbert-large PubMedELECTRA: https://akams/pubmedelectra PubMedELECTRA-LARGE: https://akams/pubmedelectra-large",Yes,No,No,,,Yes,,multiple benchmarks in dataset,No,Yes,Yes,,Yes,"Models are sensitive to changes in pre-training and fine-tuning, which impacts downstream performance",macro,,"ELECTRA, PubMedBERT, BioBERT BlueBERT PubMedELECTRA",,,,No,Yes,,,,No,Yes,No,Spans or annotations,"IC, O, P",Entities,None reported,,,2023,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Extracting the Sample Size From Randomized Controlled Trials in Explainable Fashion Using Natural Language Processing,"Background Extracting the sample size from randomized controlled trials (RCTs) remains a challenge to developing better search functionalities or automating systematic reviews. Most current approaches rely on the sample size being explicitly mentioned in the abstract. Methods 847 RCTs from high-impact medical journals were tagged with six different entities that could indicate the sample size. A named entity recognition (NER) model was trained to extract the entities and then deployed on a test set of 150 RCTs. The entitiesâ€™ performance in predicting the actual number of trial participants who were randomized was assessed and possible combinations of the entities were evaluated to create predictive models. Results The most accurate model could make predictions for 64.7% of trials in the test set, and the resulting predictions were within 10% of the ground truth in 96.9% of cases. A less strict model could make a prediction for 96.0% of trials, and its predictions were within 10% of the ground truth in 88.2% of cases. Conclusion Training a named entity recognition model to predict the sample size from randomized controlled trials is feasible, not only if the sample size is explicitly mentioned but also if the sample size can be calculated, e.g., by adding up the number of patients in each arm.","Windisch, Paul, DennstÃ¤dt, Fabio, Koechli, Carole  et al.",https://openalex.org/works/W4400491757,,,Include,,10/10/2024,Include,15188599,,,"Training a named entity recognition model to predict the sample size from randomized controlled trials is feasible, not only if the sample size is explicitly mentioned but also if the sample size can be calculated, eg, by adding up the number of patient",,LS,"Rule-based, Other Transformer",Other,"Abstracts, Titles",RCT,"847 RCTs train, test 150 RCTs tiabs from high quality journals",,No,,https://wwwscantrialscom/,Yes,Yes,Yes,,,Yes,,,Yes,Yes,Yes,,Yes,variations in descriptions can cause N total to be higher and arms can get confused or added to produce bigger event numbers,"mean absolute error in percent, percent of entities within 1 and 10% of true value","distribution of entites, size",RoBERTa,No,Yes,closely related: Different models of combining N entities led to varying degrees of completeness (ie able to predict a number) and correctness of said number,Yes,No,,"N total, n analyzed, n completed trial, n other, n per arm, n events per arm Ground truth is number randomized and there were different models to infer this from the entities","the 3 papers focusing on N extracion all use different metrics, the current paper introduced own continuous measures while discussing one other paper that reported recall",Yes,Yes,No,Spans or annotations,"N (per arm), N (total), Other",Entities,None reported,discussing which wordings mislead the model,"Yes, random splits and ratio given",2024,,,,,,,,,,,,,,,,,,,,
JSON,A pipeline for medical literature search and its evaluation,"One database commonly used by clinicians for searching the medical literature and practicing evidence-based medicine is PubMed. As the literature grows, it has become challenging for users to find the relevant material quickly because most of the time the relevant results are not on the top. In this article, we propose a search and ranking pipeline to improve the search results based on relevancy. We first propose an ensemble model consisting of three classifiers: bidirectional long-short-term memory conditional random field (bi-LSTM-CRF), support vector machine and naive Bayes to recognise PICO (patient, intervention, comparison, outcome) elements from abstracts. The ensemble was trained on an annotated corpus consisting of 5000 abstracts split into 4000 and 1000 training and testing data, respectively. The ensemble recorded an accuracy of 93%. We then retrieved around 927,000 articles from PubMed for the years 2017â€“2021 (access date 16 April 2021). For every abstract, we extracted and grouped all P, I and O terms, and stored these groups along with the article ID in a separate database. During the search, every P, I and O term of the query is searched only in its corresponding group of every abstract. The scoring method simply counts the number of matches between the queryâ€™s P, I and O elements and the words in P, I and O groups, respectively. The abstracts are sorted by the number of matches and the top five abstracts are listed using their pre-stored abstract IDs. A comprehensive user study was conducted where 60 different queries were formulated and used to generate ranked search results using both PubMed and our proposed model. Five medical professionals assessed the ranked search results and marked every item as relevant or non-relevant. Both models were compared using precision@K and mean-average-precision@K metrics where K is 5. For most of the queries, our model produced higher precision@K values than PubMed. The mean-average-precision@K value of our model is also higher than PubMed (0.83 versus 0.67).","Zafar, Imamah, Wali, Aamir, Kunwar,,  et al.",https://openalex.org/works/W4365454127,,,Include,,10/10/2024,Include,15188597,,,"A comprehensive user study was conducted where 60 different queries were formulated and used to generate ranked search results using both PubMed and our proposed model For most of the queries, our model produced higher precision@K values than PubMed",,LS,"CRF, LSTM, SVM, Other ML classifier","Precision, Recall, F1, Accuracy","Abstracts, Titles",RCT,"EBM-NLP; 927,000 articles from PubMed automatically PIO labelled and evaluated for search result precision",,Yes,,,No,No,No,,,Yes,,practical evaluation applying classifiers to 927k PubMed tiabs,Yes,Yes,Yes,,,,"weighed average scores, precision@K and mean-average-precision@K, qualitative (real-world) evaluation, rel@i",,ensemble model,,,,No,Yes,,,,No,Yes,No,Spans or annotations,"P, IC, O",Entities,None reported,,,2023,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Task-Specific Model Allocation Medical Papers PICOS Information Extraction,"In the field of medical research, extracting PICOS information which includes population, intervention, Comparison, outcomes, and study design from medical papers, plays a significant role in improving search efficiency and guiding clinical practice. Based on the PICOS key information extraction task released from the China Health Information Processing Conference (CHIP 2023), we proposed a method based on Task-Specific Model Allocation, which selects different models according to the characteristics of the abstract and title data. Additionally, incorporating techniques such as multi-task learning and model fusion. Our method achieved first place on the leaderboard with an F1 score of 0.78 on List A and 0.81 on List B.","Zhang, Qi, Qu, Jing, Zhao, Qingbo  et al.",https://openalex.org/works/W4392951372,,,Include,,10/10/2024,Include,15188596,,,"we adopted a multi-task learning approach to address sentence classification and PICOS elements extraction in title tasks and employed optimization methods such as adversarial learningOur method achieved the first rank, demonstrating its effectiveness",,LS,"Other Transformer, LLM",F1,"Abstracts, Titles",RCT,"CHIP 2023 Task 5 4500 tiabs (2k train, 500 val, 2k eval), PICO, design Sentence-level on abs, entity and sentence level on ti Chinese",,No,,,No,No,No,,,No,,,No,No,Yes,,,,macro,just total numbers given,"Chinese roberta, global pointer network, Qwen LLM",,,,,Yes,,,Used shared task dataset,No,Yes,No,Spans or annotations,"Design, P, IC (per arm), O","Entities, Sentences",None reported,,"Yes, random splits and ratio given",2024,,,,,,,,,,,,,,Yes,No,,LoRA training and prompt given,Fine-tuning,,
Text or probably text file,Boundary-aware Dual Biaffine Model for Sequential Sentence Classification in Biomedical Documents,"Assigning appropriate rhetorical roles, such as background, intervention, and outcome, to sentences in biomedical documents can streamline the process for physicians to locate evidence and resources for medical treatment and decision-making. While sequence labeling and span-based methods are frequently employed for this task, the former disregards a document's semantic structure, resulting in a lack of semantic coherence across continuous sentences. Span-based approaches, on the other hand, either necessitate the enumeration of all potential spans, which can be time-consuming, or may lead to the misclassification of sentences over extended spans. Consequently, an approach is required that models the semantic structure of documents explicitly and captures boundary information to achieve precise and effective sentence labeling in biomedical documents. To address these challenges, we propose a new approach, the boundary-aware dual biaffine model, which explicitly models the semantic structure of documents and incorporates boundary information via a dual biaffine layer. We introduce a dynamic programming algorithm to minimize missing labels and overlapping predictions, and achieve globally optimal decoding results. We evaluate our approach on three benchmark datasets, namely PubMed 20k RCT, PubMed-PICO and NICTA-PIBOSO. The experimental results demonstrate that our approach outperforms strong baselines and achieves state-of-the-art performance on PubMed 20k RCT and PubMed-PICO. Additionally, our method also achieves competitive results on NICTA-PIBOSO. Availability: Our codes and data will be available at: https://github.com/CSU-NLP-Group/Sequential-Sentence-Classification.","Duan, J., Guo, H., Jiang, H.  et al.",https://doi.org/10.1109/tcbb.2024.3376566,,,Include,,10/10/2024,Include,14800913,,,"We evaluate our approach on three benchmark datasets, namely PubMed 20k RCT, PubMed-PICO and NICTA-PIBOSO The experimental results demonstrate that our approach outperforms strong baselines and achieves state-of-the-art performance ",,LS,"BERT incl. biomedical versions, LSTM","F1, Recall, Precision","Abstracts, Titles",RCT,"PubMed 20k RCT Dernoncourt, PubMed-PICO (jin) and NICTA-PIBOSO",,Yes,,https://githubcom/CSU-NLP-Group/Sequential-Sentence-Classification ,Yes,No,Yes,,,Yes,,multi benachmark,Yes,Yes,Yes,,Yes,"error analysis on auto-generated gold-standard showed that 56% of misclassifications of the model are due to improper gold label, while 32% were actual misclassifications",,,,,Yes,,No,Yes,,,,No,Yes,Yes,Spans or annotations,"P, Sections (Aim; Method etc.), O, IC",Sentences,None reported,,"Yes, random splits and ratio given",2021,,,,,,,,,,,,,,,,,,,,
Text or probably text file,AlpaPICO: Extraction of PICO frames from clinical trial documents using LLMs,"In recent years, there has been a surge in the publication of clinical trial reports, making it challenging to conduct systematic reviews. Automatically extracting Population, Intervention, Comparator, and Outcome (PICO) from clinical trial studies can alleviate the traditionally time-consuming process of manually scrutinizing systematic reviews. Existing approaches of PICO frame extraction involves supervised approach that relies on the existence of manually annotated data points in the form of BIO label tagging. Recent approaches, such as In-Context Learning (ICL), which has been shown to be effective for a number of downstream NLP tasks, require the use of labeled examples. In this work, we adopt ICL strategy by employing the pretrained knowledge of Large Language Models (LLMs), gathered during the pretraining phase of an LLM, to automatically extract the PICO-related terminologies from clinical trial documents in unsupervised set up to bypass the availability of large number of annotated data instances. Additionally, to showcase the highest effectiveness of LLM in oracle scenario where large number of annotated samples are available, we adopt the instruction tuning strategy by employing Low Rank Adaptation (LORA) to conduct the training of gigantic model in low resource environment for the PICO frame extraction task. More specifically, both of the proposed frameworks utilize AlpaCare as base LLM which employs both few-shot in-context learning and instruction tuning techniques to extract PICO-related terms from the clinical trial reports. We applied these approaches to the widely used coarse-grained datasets such as EBM-NLP, EBM-COMET and fine-grained datasets such as EBM-NLP(rev) and EBM-NLP(h). Our empirical results show that our proposed ICL-based framework produces comparable results on all the version of EBM-NLP datasets and the proposed instruction tuned version of our framework produces state-of-the-art results on all the different EBM-NLP datasets. Our project is available at https://github.com/shrimonmuke0202/AlpaPICO.git.","Ghosh, M., Mukherjee, S., Ganguly, A.  et al.",https://doi.org/10.1016/j.ymeth.2024.04.005,,,Include,,10/10/2024,Include,14800911,,,Our empirical results show that our proposed ICL-based framework produces comparable results on all the version of EBM-NLP datasets and the proposed instruction tuned version of our framework produces state-of-the-art results on all the different EBM-NLP,,,LLM,"F1, Accuracy, Precision, Recall","Abstracts, Titles",RCT,"EBM-NLP, adapted EBM-NLP, EBM-Comet",,Yes,,https://githubcom/shrimonmuke0202/AlpaPICOgit,Yes,No,Yes,error analysis,,Yes,,two datasets,Yes,Yes,Yes,,Yes,Optimal number of training examples in k-shot varies across datasets,,,"few-shot prompt-based learning, and LoRA (freezing LLM and fine-tuning additional parameters)",,,,No,,,,,,Yes,Yes,Spans or annotations,"Gender, O, N (total), Age, P (Condition or disease), P, IC",Entities,None reported,,"Yes, random splits and ratio given",2024,,,,,,,,,,,,,,Yes,No,,"Zero-shot, K-shot, and fine-tuning","Fine-tuning, Zero-shot (prompt only), K-shot (context in prompt)",,
Text or probably text file,Methodological information extraction from randomized controlled trial publications: a pilot study,"Most biomedical information extraction (IE) approaches focus on entity types such as diseases, drugs, and genes, and relations such as gene-disease associations. In this paper, we introduce the task of methodological IE to support fine-grained quality assessment of randomized controlled trial (RCT) publications. We draw from the Ontology of Clinical Research (OCRe) and the CONSORT reporting guidelines for RCTs to create a categorization of relevant methodological characteristics. In a pilot annotation study, we annotate a corpus of 70 full-text publications with these characteristics. We also train baseline named entity recognition (NER) models to recognize these items in RCT publications using several training sets with different negative sampling strategies. We evaluate the models at span and document levels. Our results show that it is feasible to use natural language processing (NLP) and machine learning for fine-grained extraction of methodological information. We propose that our models, after improvements, can support assessment of methodological quality in RCT publications. Our annotated corpus, models, and code are publicly available at https://github.com/kellyhoang0610/RCTMethodologyIE.","Hoang, L., Guan, Y., Kilicoglu, H. ",https://pubmed.ncbi.nlm.nih.gov/37128457/,,,Include,,10/10/2024,Include,14800908,,,"Our results show that it is feasible to use NLP and machine learning for fine-grained extraction of methodological information We propose that our models, after improvements, can support assessment of methodological quality in RCT publications",,LS,"CRF, BERT incl. biomedical versions","Precision, Recall, F1","Full texts, Abstracts, Titles",RCT,"70 full texts (25 from CONSORT-TM and 45 PMC OA, foc on ti-abs-methods-results) https://githubcom/kellyhoang0610/RCTMethodologyIE",,No,,https://githubcom/kellyhoang0610/RCTMethodologyIE,Yes,No,Yes,,,Yes,,,No,No,Yes,,,models fail on entities with low annotation counts Implied information leads to less consistent annotation,strict and relaxed,,PubMedBERT,,,Impact of sampling method on recall/precision discussed,Yes,,,"Planned Sample Size and Actual Sample Size, drop-out, comparative intent, allocation Concealment, many fine-grained methods items",,No,Yes,No,Spans or annotations,"Withdrawals or exclusions, Randomisation, Setting, Blinding, N (per arm), N (total), Design, Other","Entities, Binary for each document",None reported,,"Yes, random splits and ratio given",2022,"document-level Cohenâ€™s Îº pairwise between 3 annotators range 074-083, span-F1 092-095 Span-level F1 09-094",,,,,,,,,,,,,,,,,,,
Text or probably text file,Towards precise PICO extraction from abstracts of randomized controlled trials using a section-specific learning approach,"MOTIVATION: Automated extraction of participants, intervention, comparison/control, and outcome (PICO) from the randomized controlled trial (RCT) abstracts is important for evidence synthesis. Previous studies have demonstrated the feasibility of applying natural language processing (NLP) for PICO extraction. However, the performance is not optimal due to the complexity of PICO information in RCT abstracts and the challenges involved in their annotation. RESULTS: We propose a two-step NLP pipeline to extract PICO elements from RCT abstracts: (i) sentence classification using a prompt-based learning model and (ii) PICO extraction using a named entity recognition (NER) model. First, the sentences in abstracts were categorized into four sections namely background, methods, results, and conclusions. Next, the NER model was applied to extract the PICO elements from the sentences within the title and methods sections that include &gt;96% of PICO information. We evaluated our proposed NLP pipeline on three datasets, the EBM-NLPmoddataset, a randomly selected and reannotated dataset of 500 RCT abstracts from the EBM-NLP corpus, a dataset of 150 COVID-19 RCT abstracts, and a dataset of 150 Alzheimer's disease (AD) RCT abstracts. The end-to-end evaluation reveals that our proposed approach achieved an overall micro F1 score of 0.833 on the EBM-NLPmod dataset, 0.928 on the COVID-19 dataset, and 0.899 on the AD dataset when measured at the token-level and an overall micro F1 score of 0.712 on EBM-NLPmod dataset, 0.850 on the COVID-19 dataset, and 0.805 on the AD dataset when measured at the entity-level. AVAILABILITY: Our codes and datasets are publicly available at https://github.com/BIDS-Xu-Lab/section_specific_annotation_of_PICO. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.","Hu, Y., Keloth, V. K., Raja, K.  et al.",https://doi.org/10.1093/bioinformatics/btad542,,,Include,,10/10/2024,Include,14800907,,,"By annotating the method section and title alone, we not only reduce annotation complexity for PICO extraction but were able to achieve a much higher performance on retrieving unique PICO elements without much loss of information ",,LS,"LSTM, BERT incl. biomedical versions, CRF","Precision, F1, Recall","Method sections, Titles, Abstracts",RCT,"NER: 1EBM-NLPmod dataset, a randomly selected and re-annotated dataset of 500 RCT abstracts 2 a dataset of 150 Covid RCT abstracts, 3 a dataset of 150 Alzheimerâ€™s disease (AD) RCT abstracts  Sentences: PubMed 20k methods sect + 200 covid and AD each",,Yes,No,code and data: https://githubcom/BIDS-Xu-Lab/section_specific_annotation_of_PICO/blob/main/data/COVID-19/fold2/testtxt,Yes,No,Yes,,,Yes,,3 corpora for sent/entity tasks each,Yes,Yes,Yes,,Yes,"lexical variations of the same PICO concept in abstracts, overfitting on word 'placebo' and confusion between I and C, requiring context information and understanding of implied information",micro scores and sentence+NER end-to-end eval,entity distributions,,No,,,No,Yes,,used title+methods sentences from structured abstracts,common entities although benchmark dataset was reannotated and text scope was changed,No,Yes,No, Spans or annotations,"Sections (Aim; Method etc.),  O, IC (per arm), P","Sentences, Entities",None reported,,"Yes, random splits and ratio given",2023,Cohenâ€™s Kappa coefficient was only 03 on ebm-nlp reannotation but increased to 074 when redefining guidelines and restricting to ti+methods sections of abstracts,,,,,,,,,,,,,,,,,,,
Text or probably text file,EvidenceMap: a three-level knowledge representation for medical evidence computation and comprehension,"OBJECTIVE: To develop a computable representation for medical evidence and to contribute a gold standard dataset of annotated randomized controlled trial (RCT) abstracts, along with a natural language processing (NLP) pipeline for transforming free-text RCT evidence in PubMed into the structured representation. MATERIALS AND METHODS: Our representation, EvidenceMap, consists of 3 levels of abstraction: Medical Evidence Entity, Proposition and Map, to represent the hierarchical structure of medical evidence composition. Randomly selected RCT abstracts were annotated following EvidenceMap based on the consensus of 2 independent annotators to train an NLP pipeline. Via a user study, we measured how the EvidenceMap improved evidence comprehension and analyzed its representative capacity by comparing the evidence annotation with EvidenceMap representation and without following any specific guidelines. RESULTS: Two corpora including 229 disease-agnostic and 80 COVID-19 RCT abstracts were annotated, yielding 12 725 entities and 1602 propositions. EvidenceMap saves users 51.9% of the time compared to reading raw-text abstracts. Most evidence elements identified during the freeform annotation were successfully represented by EvidenceMap, and users gave the enrollment, study design, and study Results sections mean 5-scale Likert ratings of 4.85, 4.70, and 4.20, respectively. The end-to-end evaluations of the pipeline show that the evidence proposition formulation achieves F1 scores of 0.84 and 0.86 in the adjusted random index score. CONCLUSIONS: EvidenceMap extends the participant, intervention, comparator, and outcome framework into 3 levels of abstraction for transforming free-text evidence from the clinical literature into a computable structure. It can be used as an interoperable format for better evidence retrieval and synthesis and an interpretable representation to efficiently comprehend RCT findings.","Kang, T., Sun, Y., Kim, J. H.  et al.",https://doi.org/10.1093/jamia/ocad036,,,Include,,10/10/2024,Include,14800904,,,"EvidenceMap extends the participant, intervention, comparator, and outcome framework into 3 levels of abstraction for transforming free-text evidence from the clinical literature into a computable structure It can be used as an interoperable format ",,,"BERT incl. biomedical versions, LSTM, CRF","Precision, Recall, F1","Titles, Abstracts",RCT,"medline  80 randomly selected COVID-19 RCT abstracts,    229 general RCT article abstracts,  10 for cross-tool comparison",,Yes,,code nd data here and in previous Kang paper  https:// githubcom/WengLab-InformaticsResearch/EvidenceMap_Model,Yes,No,Yes,,,Yes,,3 corpora,Yes,Yes,Yes,,Yes,methods may fail on rct types such as crossover or factorial NER is technical bottleneck,"weighed average  likert scale practical eval: helpfulness  time-taken to comprehend abstract with their approach, full manual, trialstreamer, and EvidenceInference 20 models",distributions,Blue Bert,,,,,Yes,,measure (ie treatment effect) and participant count experienceng effect,4-way practical comparison to to other tool or approaches output described,,Yes,Yes,"Spans or annotations, Structured text and summary","O, IC (per arm), P",Entities,None reported,,"Yes, random splits and ratio given",2023,IRR f1 for entities mean 0861 (072-092) for entities and 069 for dependency annotations,,,,,,,,,,,,,,,,,,,
Text or probably text file,SEETrials: Leveraging Large Language Models for Safety and Efficacy Extraction in Oncology Clinical Trials,"BACKGROUND: Initial insights into oncology clinical trial outcomes are often gleaned manually from conference abstracts. We aimed to develop an automated system to extract safety and efficacy information from study abstracts with high precision and fine granularity, transforming them into computable data for timely clinical decision-making. METHODS: We collected clinical trial abstracts from key conferences and PubMed (2012-2023). The SEETrials system was developed with four modules: preprocessing, prompt modeling, knowledge ingestion and postprocessing. We evaluated the system's performance qualitatively and quantitatively and assessed its generalizability across different cancer types- multiple myeloma (MM), breast, lung, lymphoma, and leukemia. Furthermore, the efficacy and safety of innovative therapies, including CAR-T, bispecific antibodies, and antibody-drug conjugates (ADC), in MM were analyzed across a large scale of clinical trial studies. RESULTS: SEETrials achieved high precision (0.958), recall (sensitivity) (0.944), and F1 score (0.951) across 70 data elements present in the MM trial studies Generalizability tests on four additional cancers yielded precision, recall, and F1 scores within the 0.966-0.986 range. Variation in the distribution of safety and efficacy-related entities was observed across diverse therapies, with certain adverse events more common in specific treatments. Comparative performance analysis using overall response rate (ORR) and complete response (CR) highlighted differences among therapies: CAR-T (ORR: 88%, 95% CI: 84-92%; CR: 95%, 95% CI: 53-66%), bispecific antibodies (ORR: 64%, 95% CI: 55-73%; CR: 27%, 95% CI: 16-37%), and ADC (ORR: 51%, 95% CI: 37-65%; CR: 26%, 95% CI: 1-51%). Notable study heterogeneity was identified (&gt;75% I (2) heterogeneity index scores) across several outcome entities analyzed within therapy subgroups. CONCLUSION: SEETrials demonstrated highly accurate data extraction and versatility across different therapeutics and various cancer domains. Its automated processing of large datasets facilitates nuanced data comparisons, promoting the swift and effective dissemination of clinical insights.","Lee, K., Paek, H., Huang, L. C.  et al.",https://doi.org/10.1101/2024.01.18.24301502,,,Include,,10/10/2024,Include,14800898,,,SEETrials demonstrated highly accurate data extraction and versatility across different therapeutics and various cancer domains Its automated processing of large datasets facilitates nuanced data comparisons ,,,LLM,"Precision, Recall, F1","Abstracts, Titles",RCT,"Dev on multiple myeloma 245 abstracts, eval on 115 abstracts across four other cancersâ€”breast, lung, lymphoma, and leukemia",,Yes,,https://githubcom/applebyboy/SEEtrials no prompts given,No,No,Yes,,OPenai API,Yes,,Limited generalisability of prompts tested across different cancer types,Yes,Yes,Yes,,Yes,"lack of explicit descriptions of groups, doses, subgroups Inconsistent performance of extraction and allocation of information (eg linking dosage with arm) Conference abstracts",,"distributions, size",GPT-4,,Yes,,No,No,,"trial phase, length of follow-up ",,No,,No, Spans or annotations,"P, IC, IC (Drug name), IC (dose; duration and others), Country, O, N (per arm), N (total), Design",Entities,None reported,,"Yes, compl different datasets",2024,,,,,,,,,,,,,,Yes,No,,"No prompts shared, but python scripts to interact with OpenAI API",Zero-shot (prompt only),,
Text or probably text file,A Sample Size Extractor for RCT Reports,"Sample size is an important indicator of the power of randomized controlled trials (RCTs). In this paper, we designed a total sample size extractor using a combination of syntactic and machine learning methods, and evaluated it on 300 Covid-19 abstracts (Covid-Set) and 100 generic RCT abstracts (General-Set). To improve the performance, we applied transfer learning from a large public corpus of annotated abstracts. We achieved an average F1 score of 0.73 on the Covid-Set testing set, and 0.60 on the General-Set using exact matches. The F1 scores for loose matches on both datasets were over 0.74. Compared with the state-of-the-art tool, our extractor reports total sample sizes directly and improved F1 scores by at least 4% without transfer learning. We demonstrated that transfer learning improved the sample size extraction accuracy and minimized human labor on annotations.","Lin, F., Liu, H., Moon, P.  et al.",https://doi.org/10.3233/shti220151,,,Include,,10/10/2024,Include,14800896,,,"Compared with the state-of-the-art tool, our extractor reports total sample sizes directly and improved F1 scores by at least 4% without transfer learning We demonstrated that transfer learning improved the sample size extraction accuracy ",,LS,"Word embedding, Multi-layer perceptron, Rule-based","Recall, Precision, Accuracy, F1","Titles, Abstracts",RCT,300 Covid19 RCT abstracts  100 generic RCT abstracts EBM-NLP for pre-training,,Yes,No,,No,No,No,,,Yes,,"multiple eval datasets, one on covid and one general, tested how well covid-trained model generalises",Yes,No,Yes,"text-to int, ex sixty-six to 66",Yes,"LS: Not a single N (total and per arm), algorithm needs to deal with loss-of follow-up ",relaxed metric counting results as true within 10% margin,distributions,,Yes,Yes,,No,Yes,,"preferred N analysed as opposed to N randomised, but information was not always complete",compared performance to trialstreamer open-source model,No,Yes,No,Spans or annotations,"N (per arm), N (total)",Entities,None reported,,"Yes, random splits and ratio given",2021,,,,,,,,,,,,,,,,,,,,
"PDF, XML",Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews,"BACKGROUND: Evidence-based medicine requires synthesis of research through rigorous and time-intensive systematic literature reviews (SLRs), with significant resource expenditure for data extraction from scientific publications. Machine learning may enable the timely completion of SLRs and reduce errors by automating data identification and extraction. METHODS: We evaluated the use of machine learning to extract data from publications related to SLRs in oncology (SLR 1) and Fabry disease (SLR 2). SLR 1 predominantly contained interventional studies and SLR 2 observational studies. Predefined key terms and data were manually annotated to train and test bidirectional encoder representations from transformers (BERT) and bidirectional long-short-term memory machine learning models. Using human annotation as a reference, we assessed the ability of the models to identify biomedical terms of interest (entities) and their relations. We also pretrained BERT on a corpus of 100,000 open access clinical publications and/or enhanced context-dependent entity classification with a conditional random field (CRF) model. Performance was measured using the F(1) score, a metric that combines precision and recall. We defined successful matches as partial overlap of entities of the same type. RESULTS: For entity recognition, the pretrained BERT+CRF model had the best performance, with an F(1) score of 73% in SLR 1 and 70% in SLR 2. Entity types identified with the highest accuracy were metrics for progression-free survival (SLR 1, F(1) score 88%) or for patient age (SLR 2, F(1) score 82%). Treatment arm dosage was identified less successfully (F(1) scores 60% [SLR 1] and 49% [SLR 2]). The best-performing model for relation extraction, pretrained BERT relation classification, exhibited F(1) scores higher than 90% in cases with at least 80 relation examples for a pair of related entity types. CONCLUSIONS: The performance of BERT is enhanced by pretraining with biomedical literature and by combining with a CRF model. With refinement, machine learning may assist with manual data extraction for SLRs.","Panayi, A., Ward, K., Benhadji-Schaff, A.  et al.",https://doi.org/10.1186/s13643-023-02351-w,,,Include,,10/10/2024,Include,14800894,,,"The performance of BERT is enhanced by pretraining with biomedical literature and by combining with a CRF model With refinement, machine learning may assist with manual data extraction for SLRs",,LS,"CRF, BERT incl. biomedical versions, LSTM","Recall, F1, Other, Precision","Titles, Abstracts, Full texts",RCT,"cancer and Fabry disease RCT and observational (prognostic) studies from 2 finshed reviews plus additional references retrieved from PubMed (cancer 16+70; Fabry 26+150) , fulltext",,Yes,,https://githubcom/TakedaGME/MedTrialExtractor/,No,No,Yes,,,Yes,,different datasets,Yes,Yes,Yes,,,human expensive and non-trivial Fulltext XML better than PDF Figure and table data difficult to access Copyright fulltext dataset,relaxed score counting partial entity matches as correct,"size, distributions",own BERT pre-training,,Yes,,No,,,,,No,Yes,Yes,Spans or annotations,"IC, IC (dose; duration and others), Age, Design",Entities,None reported,,"Yes, random splits and ratio given",2023,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Artificial Intelligence to Automate Network Meta-Analyses: Four Case Studies to Evaluate the Potential Application of Large Language Models,"BACKGROUND: The emergence of artificial intelligence, capable of human-level performance on some tasks, presents an opportunity to revolutionise development of systematic reviews and network meta-analyses (NMAs). In this pilot study, we aim to assess use of a large-language model (LLM, Generative Pre-trained Transformer 4 [GPT-4]) to automatically extract data from publications, write an R script to conduct an NMA and interpret the results. METHODS: We considered four case studies involving binary and time-to-event outcomes in two disease areas, for which an NMA had previously been conducted manually. For each case study, a Python script was developed that communicated with the LLM via application programming interface (API) calls. The LLM was prompted to extract relevant data from publications, to create an R script to be used to run the NMA and then to produce a small report describing the analysis. RESULTS: The LLM had a &gt; 99% success rate of accurately extracting data across 20 runs for each case study and could generate R scripts that could be run end-to-end without human input. It also produced good quality reports describing the disease area, analysis conducted, results obtained and a correct interpretation of the results. CONCLUSIONS: This study provides a promising indication of the feasibility of using current generation LLMs to automate data extraction, code generation and NMA result interpretation, which could result in significant time savings and reduce human error. This is provided that routine technical checks are performed, as recommend for human-conducted analyses. Whilst not currently 100% consistent, LLMs are likely to improve with time.","Reason, T., Benbow, E., Langham, J.  et al.",https://doi.org/10.1007/s41669-024-00476-9,,,Include,,10/10/2024,Include,14800893,,,"This study provides a promising indication of the feasibility of using current generation LLMs to automate data extraction, code generation and NMA result interpretation, which could result in significant time savings and reduce human error ",,,LLM,Accuracy,"Titles, Full texts, Abstracts",RCT,"4 NMA reviews with 29 RCTS, skin disorder, cancer, binary and survival outcomes (prognistic reviews)",,Yes,,,No,No,,,,Yes,,Described that prompt needs to be changed across reviews,Yes,Yes,Yes,,,"general errors, eg not understand concept of statistical significance Hallucinations, misinterpretations when asked to describe results Reproducibility issues on re-run may get worse over time",,,GPT-4,,,,No,,,outcome data incl survival data,,No,,,Spans or annotations,"N (per arm), N (total), Randomisation, Other, IC (per arm)",Entities,None reported,,Not reported,2024,,,,,,,,,,,,,,No,Yes,,"differences on re-run (20 iterations) Prompt development described, but no dev splits reported",Zero-shot (prompt only),Yes,
Text or probably text file,Comparing generative and extractive approaches to information extraction from abstracts describing randomized clinical trials,"BACKGROUND: Systematic reviews of Randomized Controlled Trials (RCTs) are an important part of the evidence-based medicine paradigm. However, the creation of such systematic reviews by clinical experts is costly as well as time-consuming, and results can get quickly outdated after publication. Most RCTs are structured based on the Patient, Intervention, Comparison, Outcomes (PICO) framework and there exist many approaches which aim to extract PICO elements automatically. The automatic extraction of PICO information from RCTs has the potential to significantly speed up the creation process of systematic reviews and this way also benefit the field of evidence-based medicine. RESULTS: Previous work has addressed the extraction of PICO elements as the task of identifying relevant text spans or sentences, but without populating a structured representation of a trial. In contrast, in this work, we treat PICO elements as structured templates with slots to do justice to the complex nature of the information they represent. We present two different approaches to extract this structured information from the abstracts of RCTs. The first approach is an extractive approach based on our previous work that is extended to capture full document representations as well as by a clustering step to infer the number of instances of each template type. The second approach is a generative approach based on a seq2seq model that encodes the abstract describing the RCT and uses a decoder to infer a structured representation of a trial including its arms, treatments, endpoints and outcomes. Both approaches are evaluated with different base models on a manually annotated dataset consisting of RCT abstracts on an existing dataset comprising 211 annotated clinical trial abstracts for Type 2 Diabetes and Glaucoma. For both diseases, the extractive approach (with flan-t5-base) reached the best F1 score, i.e. 0.547 ( Â±0.006 ) for type 2 diabetes and 0.636 ( Â±0.006 ) for glaucoma. Generally, the F1 scores were higher for glaucoma than for type 2 diabetes and the standard deviation was higher for the generative approach. CONCLUSION: In our experiments, both approaches show promising performance extracting structured PICO information from RCTs, especially considering that most related work focuses on the far easier task of predicting less structured objects. In our experimental results, the extractive approach performs best in both cases, although the lead is greater for glaucoma than for type 2 diabetes. For future work, it remains to be investigated how the base model size affects the performance of both approaches in comparison. Although the extractive approach currently leaves more room for direct improvements, the generative approach might benefit from larger models.","Witte, C., Schmidt, D. M., Cimiano, P. ",https://doi.org/10.1186/s13326-024-00305-2,,,Include,,10/10/2024,Include,14800892,,,"both approaches show promising performance extracting structured PICO information from RCTs, the extractive approach performs best in both cases, although the lead is greater for glaucoma than for type 2 diabetes",,LS,"LLM, Other Transformer, Other",F1,"Titles, Abstracts, Full texts",RCT,T2DM and glaucoma sanchez-graillet corpus from last update https://zenodoorg/records/10419786  ,,Yes,,https://zenodoorg/records/10419786 ,Yes,No,Yes,,,Yes,,"2 different datasets, discussing separate and merged training",Yes,Yes,Yes,,Yes,"model performance differs between different disease areas, model good to acellerate process, but not to completely automate",micro,,"longformer, FLAN-T5, template-filling algorithm",,,,No,Yes,,,,Yes,Yes,Yes,"Spans or annotations, Structured text and summary","Gender, Country, P (Condition or disease), Trial registration, O (measurement instrument), N (per arm), N (total), Age, IC (dose; duration and others), O, O (time point), O (primary or secondary outcome), P, IC (per arm), IC (Drug name)",Entities,None reported,Theyy discuss that it is not possible to pinpoint properties betwwen the datasets that cause performance difference betweend isease areas,,2024,,,,,,,,,,,,,,Not applicable,Yes,,"averaged across 10 model runs, but mode of application differs to GPT-like models, the present approach is much easier to evaluate quantitatively",Fine-tuning,,
Text or probably text file,Zero-shot information extraction for clinical meta-analysis using large language models,"Meta-analysis of randomized clinical trials (rcts) plays a crucial role in evidence-based medicine but can be labor-intensive and error-prone. this study explores the use of large language models to enhance the efficiency of aggregating results from randomized clinical trials (rcts) at scale. we perform a detailed comparison of the performance of these models in zero-shot prompt-based information extraction from a diverse set of rcts to traditional manual annotation methods. we analyze the results for two different meta-analyses aimed at drug repurposing in cancer therapy pharmacovigilience in chronic myeloid leukemia. our findings reveal that the best model for the two demonstrated tasks, chatgpt can generally extract correct information and identify when the desired information is missing from an article. we additionally conduct a systematic error analysis, documenting the prevalence of diverse error types encountered during the process of prompt-based information extraction.","Kartchner, David, Ramalingam, Selvi, Al-Hussaini, Irfan  et al.",https://aclanthology.org/2023.bionlp-1.37,,,Include,,10/10/2024,Include,14800884,,,"The results of our research indicate that LLMs can contribute to more streamlined, transparent, and reproducible results in clinical research It also reveals that they still make significant errors and should be used cautiously ",,,LLM,"Other, ROUGE, Accuracy","Abstracts, Titles",RCT,ReMedy database (cancer) and own curated leukemia dataset,,No,No,remedy data: https://remedycancerappemoryedu/multi-search? ,Yes,No,No,error analysis,,Yes,,Prompts will not generalise well due to answer options,No,No,Yes,,Yes,"verbosity: LLM over-explaining, adding and answering own qurstions, need for post-processing Halluc: N in grous or inv control group Assum even splits in grps or guess design if info n/a Eval: hard for summaries due to imp human answers",jaccard,entity distributions,CHat-GPT and GPT-JT,No,,,No,No,GPT parameters,,selective and small dataset,No,Yes,No,Spans or annotations,"O, Design, N (total), N (per arm), IC (dose; duration and others), P (Condition or disease)",Entities,None reported,,,2023,,,,,,,,,,,,,,Yes,No,,"No N of prompt development abstracts given, no info on how selected",Zero-shot (prompt only),Yes,
Text or probably text file,Intra-template entity compatibility based slot-filling for clinical trial information extraction,"We present a deep learning based information extraction system that can extract the design and results of a published abstract describing a randomized controlled trial (rct). in contrast to other approaches, our system does not regard the pico elements as flat objects or labels but as structured objects. we thus model the task as the one of filling a set of templates and slots; our two-step approach recognizes relevant slot candidates as a first step and assigns them to a corresponding template as second step, relying on a learned pairwise scoring function that models the compatibility of the different slot values. we evaluate the approach on a dataset of 211 manually annotated abstracts for type 2 diabetes and glaucoma, showing the positive impact of modelling intra-template entity compatibility. as main benefit, our approach yields a structured object for every rct abstract that supports the aggregation and summarization of clinical trial results across published studies and can facilitate the task of creating a systematic review or meta-analysis.","Witte, Christian, Cimiano, Philipp ",https://aclanthology.org/2022.bionlp-1.18,,,Include,,10/10/2024,Include,14800882,,," As main benefit, our approach yields a structured object for every RCT abstract that supports the aggregation and summarization of clinical trial results across published studies and can facilitate the task of creating a systematic review or metaanalysis",,LS,"Other, BERT incl. biomedical versions","Recall, Precision, F1","Full texts, Titles, Abstracts",RCT,T2DM and glaucoma sanchez-graillet corpus from last update,,No,,,Yes,No,No,,,Yes,,,No,No,Yes,,Yes,,micro,distribution of entites,template-filling algorithm,,,,No,Yes,,outcome data per group,,No,Yes,No,"Structured text and summary, Spans or annotations","O, IC (dose; duration and others), IC (Drug name), IC (per arm), N (per arm), O (primary or secondary outcome), O (measurement instrument), O (time point), P, Trial registration, Gender, Age, N (total), Country, P (Condition or disease)",Entities,None reported,,,2022,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"Distant-cto: a zero cost, distantly supervised approach to improve low-resource entity extraction using clinical trials literature","Pico recognition is an information extraction task for identifying participant, intervention, comparator, and outcome information from clinical literature. manually identifying pico information is the most time-consuming step for conducting systematic reviews (sr), which is already labor-intensive. a lack of diversified and large, annotated corpora restricts innovation and adoption of automated pico recognition systems. the largest-available pico entity/span corpus is manually annotated which is too expensive for a majority of the scientific community. to break through the bottleneck, we propose distant-cto, a novel distantly supervised pico entity extraction approach using the clinical trials literature, to generate a massive weakly-labeled dataset with more than a million `intervention' and `comparator' entity annotations. we train distant ner (named-entity recognition) models using this weakly-labeled dataset and demonstrate that it outperforms even the sophisticated models trained on the manually annotated dataset with a 2% f1 improvement over the intervention entity of the pico benchmark and more than 5% improvement when combined with the manually annotated dataset. we investigate the generalizability of our approach and gain an impressive f1 score on another domain-specific pico benchmark. the approach is not only zero-cost but is also scalable for a constant stream of pico entity annotations.","Dhrangadhariya, Anjani, M""uller, Henning ",https://aclanthology.org/2022.bionlp-1.34,,,Include,,10/10/2024,Include,14800881,,,We train distant NER (named-entity recognition) models using this weakly labeled dataset and demonstrate that it outperforms even the sophisticated models trained on the manually annotated dataset with  more than 5% improvement,,LS,"CRF, BERT incl. biomedical versions, LSTM","F1, Recall, Precision","Other, Full texts, Abstracts",RCT,DS labelled dataset of interventions in clinical trial records: 200 manual gold standard and 940k matched interventions for training  153 Physio/rehabilitation PICO entities and spans https://linkspringercom/chapter/101007/978-3-030-85251-1_6  EBM-NLP,,Yes,,code: https://githubcom/anjani-dhrangadhariya/distant-cto,No,No,Yes,error analysis,,Yes,,multiple benchmark datasets and effects of combining/removing datasets,No,No,Yes,,Yes,boundary errors caused by different annotation styles between corpora,,distributions,,,No,,No,Yes,,,reported performance of other models on EBM-NLP,No,Yes,Yes,Spans or annotations,IC,Entities,"FN, FP",,,2022,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Sectioning of biomedical abstracts: a sequence of sequence classification task,"Rapid growth of the biomedical literature has led to many advances in the biomedical text mining field. among the vast amount of information, biomedical article abstracts are the easily accessible sources. however, the number of the structured abstracts, describing the rhetorical sections with one of background, objective, method, result and conclusion categories is still not considerable. exploration of valuable information in the biomedical abstracts can be expedited with the improvements in the sequential sentence classification task. deep learning based models has great performance/potential in achieving significant results in this task. however, they can often be overly complex and overfit to specific data. in this project, we study a state-of-the-art deep learning model, which we called ssn-4 model here. we investigate different components of the ssn-4 model to study the trade-off between the performance and complexity. we explore how well this model generalizes to a new data set beyond randomized controlled trials (rct) dataset. we address the question that whether word embeddings can be adjusted to the task to improve the performance. furthermore, we develop a second model that addresses the confusion pairs in the first model. results show that ssn-4 model does not appear to generalize well beyond rct dataset.","Karabulut, Mehmet Efruz, Vijay-Shanker, K. ",https://export.arxiv.org/abs/2201.07112,,,Include,,10/10/2024,Include,14800880,,,"we demonstrate that RCT is a restricted data set and models trained on RCT have limited performance on the general biomedical domain We introduce a new data set we called GMED, that allows our model to generalize better in the general biomedical domain ",,LS,"BERT incl. biomedical versions, LSTM",F1,"Titles, Abstracts","Non-randomised (intervention) study, Diagnostic test, RCT, Case control, Case series, Cross sectional survey, Cohort, Mix","medline, RCT 20k and RCT 200k created GMED (520k abs)",,Yes,,"data via Git, no code yet: https://githubcom/udel-biotm-lab/ASUP",Yes,No,No,,,Yes,,,No,No,Yes,same as Jin 2018,Yes,models trained on RCT datasets don't generalise well for sentence classification,micro-averaging,"size, distruution of classes",pointer entwork and triplet loss,Yes,Yes,,No,Yes,,,,No,Yes,Yes,Spans or annotations,"Sections (Aim; Method etc.), ",Sentences,None reported,,"Yes, random splits and ratio given",2023,,,,,,,,,,,,,,,,,,,,
Text or probably text file,"Jointly extracting interventions, outcomes, and findings from rct reports with llms","Results from randomized controlled trials (rcts) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. however, results from rcts are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. this onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. in this work we propose and evaluate a text-to-text model built on instruction-tuned large language models (llms) to jointly extract interventions, outcomes, and comparators (ico elements) from clinical abstracts, and infer the associated results reported. manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning llms for this purpose realizes considerable ($\sim$20 point absolute f1 score) gains over the previous sota. we perform ablations and error analyses to assess aspects that contribute to model performance, and to highlight potential directions for further improvements. we apply our model to a collection of published rcts through mid-2022, and release a searchable database of structured findings: http://ico-relations.ebm-nlp.com","Wadhwa, Somin, DeYoung, Jay, Nye, Benjamin  et al.",https://export.arxiv.org/abs/2305.03642,,,Include,,10/10/2024,Include,14800878,,,Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable (âˆ¼20 point absolute F1 score) gains over the previous SOTA,,,LLM,"F1, Precision, Recall","Abstracts, Titles",RCT,"EvidenceInference (deYoung), Nye 2022 160 rcts, applied to Trialstreamer database RCTs for prediction for error analysis",http://ico-relations.ebm-nlp.com/,Yes,No,,Yes,Yes,No,error analysis,,Yes,,,Yes,No,Yes,,Yes,"Incorrectly structured outputs: ICO parts missing, irrelevant or duplicate outputs Incorrect inference label: swapping de- or increase and I/C reteining correct labels Grouping of outputs: grouping multiple ourcomes Missing annotations halluc w non-rcts",,"size, distribution",Flan-T5-large,No,Yes,,No,Yes,hyperparameters described,relation extraction,,No,Yes,Yes,Spans or annotations,"IC (per arm), O, P",Entities,None reported,,"Yes, compl different datasets",2023,"For evaluation by experts: Fleiss kappa, Îº = 077, same label chosen 924% of the time",,,,,,,,,,,,,Yes,No,,Fine-tuning LLMs,Fine-tuning,,
Text or probably text file,A span-based model for extracting overlapping pico entities from rct publications,"Objectives extraction of pico (populations, interventions, comparison, and outcomes) entities is fundamental to evidence retrieval. we present a novel method picox to extract overlapping pico entities. materials and methods picox first identifies entities by assessing whether a word marks the beginning or conclusion of an entity. then it uses a multi-label classifier to assign one or more pico labels to a span candidate. picox was evaluated using one of the best-performing baselines, ebm-nlp, and three more datasets, i.e., pico-corpus, and rct publications on alzheimer's disease or covid-19, using entity-level precision, recall, and f1 scores. results picox achieved superior precision, recall, and f1 scores across the board, with the micro f1 score improving from 45.05 to 50.87 (p &lt;&lt; 0.01). on the pico-corpus, picox obtained higher recall and f1 scores than the baseline and improved the micro recall score from 56.66 to 67.33. on the covid-19 dataset, picox also outperformed the baseline and improved the micro f1 score from 77.10 to 80.32. on the ad dataset, picox demonstrated comparable f1 scores with higher precision when compared to the baseline. conclusion picox excels in identifying overlapping entities and consistently surpasses a leading baseline across multiple datasets. ablation studies reveal that its data augmentation strategy effectively minimizes false positives and improves precision.","Zhang, Gongbo, Zhou, Yiliang, Hu, Yan  et al.",https://export.arxiv.org/abs/2401.06791,,,Include,,10/10/2024,Include,14800873,,,PICOX excels in identifying overlapping entities and consistently surpasses a leading baseline across multiple datasets Ablation studies reveal that its data augmentation strategy effectively minimizes false positives and improves precision,,LS,"BERT incl. biomedical versions, Other Transformer,  ","F1, Recall, Precision","Method sections, Titles, Abstracts",RCT,"mutinda PICO-corpus, EBM-NLP, own corpora 150 each for Covid-19 and Alzheimers using only methods sentences",,No,,https://githubcom/WengLab-InformaticsResearch/PICOX,Yes,No,Yes,,,Yes,,"multiple datasets, although training data was shared between datasets",Yes,Yes,Yes,,Yes,LS: same model architecture can obtain very different scores on different corpora (46 vs 73 f1 on interventions between benchmark and own-created) but oc was easier ,micro and macro,entity distributions,bioelectra,,,span classification thresholds have p/rec impact,Yes,Yes,,,,No,Yes,Yes,Spans or annotations,"O, IC, P",Entities,None reported,,,2024,"IRR combined 0746, range 07-08",,,,,,,,,,,,,,,,,,,
Text or probably text file,Lstm-based deep neural network with a focus on sentence representation for sequential sentence classification in medical scientific abstracts,"The sequential sentence classification task within the domain of medical abstracts, termed as ssc, involves the categorization of sentences into pre-defined headings based on their roles in conveying critical information in the abstract. in the ssc task, sentences are sequentially related to each other. for this reason, the role of sentence embeddings is crucial for capturing both the semantic information between words in the sentence and the contextual relationship of sentences within the abstract, which then enhances the ssc system performance. in this paper, we propose a lstm-based deep learning network with a focus on creating comprehensive sentence representation at the sentence level. to demonstrate the efficacy of the created sentence representation, a system utilizing these sentence embeddings is also developed, which consists of a convolutional-recurrent neural network (c-rnn) at the abstract level and a multi-layer perception network (mlp) at the segment level. our proposed system yields highly competitive results compared to state-of-the-art systems and further enhances the f1 scores of the baseline by 1.0%, 2.8%, and 2.6% on the benchmark datasets pudmed 200k rct, pudmed 20k rct and nicta-piboso, respectively. this indicates the significant impact of improving sentence representation on boosting model performance.","Lam, Phat, Pham, Lam, Nguyen, Tin  et al.",https://export.arxiv.org/abs/2401.15854,,,Include,,10/10/2024,Include,14800872,,,"Our proposed system yields highly competitive results compared to state-of-the-art systems and further enhances the F1 scores of the baseline by 10%, 28%, and 26% on the benchmark datasets PudMed 200K RCT, PudMed 20K RCT and NICTA-PIBOSO, respectively",,LS,"Multi-layer perceptron, LSTM, CNN, BERT incl. biomedical versions, Character embedding, Word embedding, Other ML classifier","Precision, Recall, F1","Titles, Abstracts",RCT,NICTA,,Yes,No,,Yes,No,No,"No details other than eval scores, unclear how class scores were combined, no error analysis",,Yes,,Evaluated on two benchmark datasets with different classes,Yes,No,Yes,,No,,"no info which combination strategy was used, although combined p/r/f1 were reported across all classes",Used public benchmark described elsewhere,,No,Yes,,No,Yes,visually and in text description,,usage of public benchmark dataset,No,Yes,Yes, Spans or annotations,"O, IC, P, Design",Sentences,None reported,,"Yes, random splits and ratio given",2024,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Factpico: factuality evaluation for plain language summarization of medical evidence,"Plain language summarization with llms can be useful for improving textual accessibility of technical content. but how factual are these summaries in a high-stakes domain like medicine? this paper presents factpico, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (rcts), which are the basis of evidence-based medicine and can directly inform patient treatment. factpico consists of 345 plain language summaries of rct abstracts generated from three llms (i.e., gpt-4, llama-2, and alpaca), with fine-grained evaluation and natural language rationales from experts. we assess the factuality of critical elements of rcts in those summaries: populations, interventions, comparators, outcomes (pico), as well as the reported findings concerning these. we also evaluate the correctness of the extra information (e.g., explanations) added by llms. using factpico, we benchmark a range of existing factuality metrics, including the newly devised ones based on llms. we find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.","Joseph, Sebastian Antony, Chen, Lily, Trienes, Jan  et al.",https://export.arxiv.org/abs/2402.11456,,,Include,,10/10/2024,Include,14800871,,,"We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level",,,LLM,"ROUGE, Other","Titles, Abstracts",RCT,"45 plain language summaries of 115 RCT abstracts generated from three LLMs (GPT-4, Llama-2, and Alpaca) Abstracts from EI20 Data: https://utexasappboxcom/s/mpe5idxrqrzs1wcakphng7xfi7h4g83j",,No,,https://githubcom/lilywchen/FactPICO,Yes,No,Yes,error analysis,,Yes,,,Yes,No,Yes,,Yes,"PT-4 and Llama-2 hallucinate more than Alpaca, causing misleading results IO conveyed well; P, C and EI more failed Overgeneralization and omissions, safety-concerns in medical domain Time-consuming","Readibility: Flesch-Kincaid Grade Level and ROUGE-L Dependency-Arc Entailment, QuestEval, QAFactEval, AlignScore, LLM Evaluators (self-evaluation) on abs or added ft PICO-R Extraction Pipeline, BERTScore","size, distributions","GPT-4, Llama-2, and Alpaca",No,Yes,,No,Yes,,Evidence Inference,,No,Yes,Yes,Structured text and summary,"P, IC (per arm), O, Other",Other,None reported,,"Yes, random splits and ratio given",2024,Randolphâ€™s kappa PICO range wrt 15 texts: 056 (P) - 08 (I); EvidenceInference 047,,,,,,,,,,,,,Yes,No,,LLM,Zero-shot (prompt only),,
XML,Automatically extracting numerical results from randomized controlled trials with large language models,"Meta-analyses statistically aggregate the findings of different randomized controlled trials (rcts) to assess treatment effectiveness. because this yields robust estimates of treatment effectiveness, results from meta-analyses are considered the strongest form of evidence. however, rigorous evidence syntheses are time-consuming and labor-intensive, requiring manual extraction of data from individual trials to be synthesized. ideally, language technologies would permit fully automatic meta-analysis, on demand. this requires accurately extracting numerical results from individual trials, which has been beyond the capabilities of natural language processing (nlp) models to date. in this work, we evaluate whether modern large language models (llms) can reliably perform this task. we annotate (and release) a modest but granular evaluation dataset of clinical trial reports with numerical findings attached to interventions, comparators, and outcomes. using this dataset, we evaluate the performance of seven llms applied zero-shot for the task of conditionally extracting numerical findings from trial reports. we find that massive llms that can accommodate lengthy inputs are tantalizingly close to realizing fully automatic meta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality). however, llms -- including ones trained on biomedical texts -- perform poorly when the outcome measures are complex and tallying the results requires inference. this work charts a path toward fully automatic meta-analysis of rcts via llms, while also highlighting the limitations of existing models for this aim.","Yun, Hye Sun, Pogrebitskiy, David, Marshall, Iain J.  et al.",https://export.arxiv.org/abs/2405.01686,,,Include,,10/10/2024,Include,14800869,,,"We find that massive LLMs  are tantalizingly close to realizing fully automatic meta-analysis, especially for dichotomous (binary) outcomes (eg, mortality) However, LLMs  perform poorly when the outcome measures are complex",,,LLM,"Accuracy, F1, Other","Full texts, Titles, Abstracts",RCT,120 ti/abs/result section ICO triplets and numerical outcome data Derived from EvidenceInference corpus https://githubcom/hyesunyun/llm-meta-analysis,,Yes,No,https://githubcom/hyesunyun/llm-meta-analysis,Yes,No,Yes,,,Yes,,practical case study replicating MA of an independent Cochrane review,Yes,No,Yes,"number normalization, chunking of text",Yes,"hallucinations ignoring negative numbers, perform poorly on complex outcome measures ,tallying the results requires inference, better at dichotomous than continuous, length of articles means zero-shot prompting to reduce length and cost","accuracy (binary outcomes), exact and relaxed evaluation for results data MSE and variace for numerical data differencers between pred and gold F1 practical evaluation within a finished SR","size, distribution","GPT-4, 35 turbo, Alpaca, Mistral Instruct 20 Gemma instruct, OLMo Instruct, PMC LLaMA, BioMistral",Yes,,,No,,,"numerical findings, outcome types, events, continuous numbers",,Yes,Yes,,"Spans or annotations, Other","N (per arm), Other, IC (per arm), O","Entities, Binary for each document",None reported,influence of certain words or implied/missing information discussed,"Yes, random splits and ratio given",2024,,,,,,,,,,,,,,Yes,No,,complete data-extraction to MA ,Zero-shot (prompt only),Yes,
Text or probably text file,Exploring the use of a large language model for data extraction in systematic reviews: a rapid feasibility study,"This paper describes a rapid feasibility study of using gpt-4, a large language model (llm), to (semi)automate data extraction in systematic reviews. despite the recent surge of interest in llms there is still a lack of understanding of how to design llm-based automation tools and how to robustly evaluate their performance. during the 2023 evidence synthesis hackathon we conducted two feasibility studies. firstly, to automatically extract study characteristics from human clinical, animal, and social science domain studies. we used two studies from each category for prompt-development; and ten for evaluation. secondly, we used the llm to predict participants, interventions, controls and outcomes (picos) labelled within 100 abstracts in the ebm-nlp dataset. overall, results indicated an accuracy of around 80%, with some variability between domains (82% for human clinical, 80% for animal, and 72% for studies of human social sciences). causal inference methods and study design were the data extraction items with the most errors. in the pico study, participants and intervention/control showed high accuracy (&gt;80%), outcomes were more challenging. evaluation was done manually; scoring methods such as bleu and rouge showed limited value. we observed variability in the llms predictions and changes in response quality. this paper presents a template for future evaluations of llms in the context of data extraction for systematic review automation. our results show that there might be value in using llms, for example as second or third reviewers. however, caution is advised when integrating models such as gpt-4 into tools. further research on stability and reliability in practical settings is warranted for each type of data that is processed by the llm.","Schmidt, Lena, Hair, Kaitlyn, Graziozi, Sergio  et al.",https://export.arxiv.org/abs/2405.14445,,,Include,,10/10/2024,Include,14800868,,,"Our results show that there might be value in using LLMs, for example as second or third reviewers However, caution is advised when integrating models such as GPT-4 into tools Further research on stability and reliability in practical settings is warran",,,LLM,"BLEU, Accuracy, ROUGE","Titles, Abstracts","Mix, Animal studies, RCT","EBM-NLP and own curated corpus from PubMed (30 tiabs Animal, social science, clinical)",,Yes,No,https://githubcom/L-ENA/ES-hackathon-GPT-evaluation,No,No,Yes,,GPT-4 Openai API,Yes,,Evaluated prompts on EBM-NLP,Yes,Yes,Yes,,Yes,"Variability in LLM answers, hallucination, robustness of evaluation of gen AI is questioned due to problems with using goldstandard ebm-nlp benchmark",,"domains, size",GPT-4 API,No,Yes,,No,Yes,,"number of arms, causal inference method, is human, is animal",,Yes,Yes,Yes, Spans or annotations,"O, IC, P, N (total), Country, Design",Entities,TP,Some discussion on results variability influenced by prompt order,"Yes, other description given",2024,,,,,,,,,,,,,,Yes,Yes,,"Paper describes difficulty in evaluation with objective scores such as recall, precision, F1 due to the generative nature",Zero-shot (prompt only),,
"PDF, XML",Accelerating clinical evidence synthesis with large language models,"Automatic medical discovery by ai is a dream of many. one step toward that goal is to create an ai model to understand clinical studies and synthesize clinical evidence from the literature. clinical evidence synthesis currently relies on systematic reviews of clinical trials and retrospective analyses from medical literature. however, the rapid expansion of publications presents challenges in efficiently identifying, summarizing, and updating evidence. we introduce trialmind, a generative ai-based pipeline for conducting medical systematic reviews, encompassing study search, screening, and data extraction phases. we utilize large language models (llms) to drive each pipeline component while incorporating human expert oversight to minimize errors. to facilitate evaluation, we also create a benchmark dataset trialreviewbench, a custom dataset with 870 annotated clinical studies from 25 meta-analysis papers across various medical treatments. our results demonstrate that trialmind significantly improves the literature review process, achieving high recall rates (0.897-1.000) in study searching from over 20 million pubmed studies and outperforming traditional language model embeddings-based methods in screening (recall@20 of 0.227-0.246 vs. 0.000-0.102). furthermore, our approach surpasses direct gpt-4 performance in result extraction, with accuracy ranging from 0.65 to 0.84. we also support clinical evidence synthesis in forest plots, as validated by eight human annotators who preferred trialmind over the gpt-4 baseline with a winning rate of 62.5%-100% across the involved reviews. our findings suggest that an llm-based clinical evidence synthesis approach, such as trialmind, can enable reliable and high-quality clinical evidence synthesis to improve clinical research efficiency.","Wang, Zifeng, Cao, Lang, Danek, Benjamin  et al.",https://export.arxiv.org/abs/2406.17755,,,Include,,10/10/2024,Include,14800864,,,"Our findings suggest that an LLM-based clinical evidence synthesis approach, such as TrialMind, can enable reliable and high-quality clinical evidence synthesis to improve clinical research efficiency ",,,LLM,Accuracy,"Titles, Full texts, Abstracts",RCT,"TrialReviewBench dataset 870 involved clinical studies from 25 meta-analyses, cancer domain",,No,,,No,No,No,,,Yes,,,No,No,Yes,,,"hallucinations more frequent when extracting study results Challenges include finding the location of information in paper, extracting numerical values Results-extraction pipeline is separate to charact data ex Cost of llm",,Parent reviews from which studies are sourced,GPT-4 and Sonnet,,Yes,,No,No,,,,No,,,"Spans or annotations, Structured text and summary","IC (per arm), P, O, Design",Entities,"TN, FP, FN, TP",,Not reported,2024,,,,,,,,,,,,,,No,No,,,Zero-shot (prompt only),,
Text or probably text file,Automated clinical knowledge graph generation framework for evidence based medicine.,,"Fakhare, Alam, Hamed,,, Khalid,, ",https://doi.org/10.1016/j.eswa.2023.120964,,,Include,,10/10/2024,Include,14800861,,,"this paper proposes a topic specific, PICO enabled, and fully automated framework to curate information and create KG of different clinical domains The evaluation shows that the proposed framework achieves significant improvement over baseline models",,LS,"CNN, LSTM, BERT incl. biomedical versions, RNN","Recall, Precision, Accuracy, F1","Titles, Abstracts",RCT,"aneurism dataset from afzal 15968 who expanded jin dataset  separate PICO covid dataset, size unclear",,Yes,,https://githubcom/smileslab/EBM_Automated_KG/tree/main,No,No,Yes,,,No,,multiple datasets,Yes,Yes,Yes,,Yes,,,"Data only available for previously published benchmark dataset from Afzal, they did not make the own-curated data accessible","clustering knn, topic-models",,,,No,,,,,No,Yes,No,"Ontology, Spans or annotations","P, O, IC, IC (per arm)",Sentences,None reported,,,2023,,,,,,,,,,,,,,,,,,,,
Text or probably text file,Blinktextsubscriptlstm: biolinkbert and lstm based approach for extraction of pico frame from clinical trial text.,,"Madhusudan, Ghosh, Shrimon, Mukherjee, Payel, Santra  et al.",https://doi.org/10.1145/3632410.3632442,,,Include,,10/10/2024,Include,14800860,,,"We have also shown with our framework that ensembling technique can be highly effective for the token span prediction task and empirical findings demonstrate that our ensemble approach, when applied on EBM-NLP dataset, achieves new state-of-the-art",,LS,"CRF, LSTM, BERT incl. biomedical versions",F1,"Titles, Abstracts",RCT,"EBM-NLP, EBM-COMET, and RedHOT (social media) EBM-NLP adaptation by abaho 2022",,Yes,Yes,https://githubcom/shrimonmuke0202/EBM-PICO,Yes,No,Yes,ablation study,Flair,Yes,CV with multiple runs and seeds,"3 different datasets, including one with social media (REDDIT) posts",No,No,Yes,,Yes,"PICO terms evolve over time, which requires usage of newer datasets such as data derived from Cochrane reviews",macro,using public benchmarks,"BioLinkBERT, also ensembling models w majority voting",,Yes,,No,Yes,,,,No,Yes,Yes,Spans or annotations,"Age, Gender, N (total), P (Condition or disease), P, IC, O, IC (Drug name)",Entities,None reported,,"Yes, random splits and ratio given",2024,,,,,,,,,,,,,,,,,,,,
