ID,source original,source new,title original,title new,abstract original,abstract new,author original,author new,link original,link new,date added
15939.0,,pubmed,"Trialstreamer: a living, automatically updated database of clinical trial reports","Trialstreamer: A living, automatically updated database of clinical trial reports","Objective Randomized controlled trials (RCTs) are the gold standard method for evaluating whether a treatment works in healthcare, but can be difficult to find and make use of. We describe the development and evaluation of a system to automatically find and categorize all new RCT reports. Materials and Methods Trialstreamer, continuously monitors PubMed and the WHO International Clinical Trials Registry Platform (ICTRP), looking for new RCTs in humans using a validated classifier. We combine machine learning and rule-based methods to extract information from the RCT abstracts, including free-text descriptions of trial populations, interventions and outcomes (the 'PICO') and map these snippets to normalised MeSH vocabulary terms. We additionally identify sample sizes, predict the risk of bias, and extract text conveying key findings. We store all extracted data in a database which we make freely available for download, and via a search portal, which allows users to enter structured clinical queries. Results are ranked automatically to prioritize larger and higher-quality studies. Results As of May 2020, we have indexed 669,895 publications of RCTs, of which 18,485 were published in the first four months of 2020 (144/day). We additionally include 303,319 trial registrations from ICTRP. The median trial sample size in the RCTs was 66. Conclusions We present an automated system for finding and categorising RCTs. This yields a novel resource: A database of structured information automatically extracted for all published RCTs in humans. We make daily updates of this database available on our website (trialstreamer.robotreviewer.net).","Randomized controlled trials (RCTs) are the gold standard method for evaluating whether a treatment works in health care but can be difficult to find and make use of. We describe the development and evaluation of a system to automatically find and categorize all new RCT reports. Trialstreamer continuously monitors PubMed and the World Health Organization International Clinical Trials Registry Platform, looking for new RCTs in humans using a validated classifier. We combine machine learning and rule-based methods to extract information from the RCT abstracts, including free-text descriptions of trial PICO (populations, interventions/comparators, and outcomes) elements and map these snippets to normalized MeSH (Medical Subject Headings) vocabulary terms. We additionally identify sample sizes, predict the risk of bias, and extract text conveying key findings. We store all extracted data in a database, which we make freely available for download, and via a search portal, which allows users to enter structured clinical queries. Results are ranked automatically to prioritize larger and higher-quality studies. As of early June 2020, we have indexed 673Â 191 publications of RCTs, of which 22Â 363 were published in the first 5 months of 2020 (142 per day). We additionally include 304Â 111 trial registrations from the International Clinical Trials Registry Platform. The median trial sample size was 66. We present an automated system for finding and categorizing RCTs. This yields a novel resource: a database of structured information automatically extracted for all published RCTs in humans. We make daily updates of this database available on our website (https://trialstreamer.robotreviewer.net).","Iain J Marshall, Benjamin Nye, JoÃ«l Kuiper, Anna Noel-Storr, Rachel Marshall, Rory Maclean, Frank Soboczenski, Ani Nenkova, James Thomas, Byron C Wallace","Marshall, Nye, Kuiper, Noel-Storr, Marshall, Maclean, Soboczenski, Nenkova, Thomas, Wallace","https://www.google.com/search?q=Trialstreamer:+a+living,+automatically+updated+database+of+clinical+trial+reports",https://doi.org/10.1093/jamia/ocaa163,2020-11-02
9949.0,,pubmed,Opportunities and Challenges in Deep Learning Methods on   Electrocardiogram Data: A Systematic Review,Opportunities and challenges of deep learning methods for electrocardiogram data: A systematic review,"Background: Electrocardiogram (ECG) is one of the most commonly used diagnostic tools in medicine and healthcare. Deep learning methods have achieved promising results predictive health-care tasks using ECG signals. Objective: This paper conducts a systematic review of deep learning methods on ECG data from both model and application perspectives. Methods: We extracted papers that deploy deep learning (deep neural networks) models on ECG data that published between January 1st 2010 and February 29th 2020 from Google Scholar, PubMed, andDBLP. We then analyze them in three aspects, including task, model, and data. Last we discuss open challenges and unsolved problems in this area. Results: The total number of papers is 191; among them, 108 papers are published after the year 2019. Almost all kinds of common deep learning architectures have been used in ECG analytics tasks like disease detection/classification, annotation/localization, sleep staging, biometric human identification, denoising, and so on. Conclusion: The number of works about deep learning on Electrocardiogram data is growing explosively in recent years. Indeed, these works have achieved a far better performance in terms of accuracy. However, there are some new challenges and problems like interpretability, scalability, efficiency, which need to be addressed and paid more attention. Moreover, it is also worth investigating by discovering new interesting applications from both the dataset view and the method view. Significance: This paper summarizes existing deep learning methods on modeling ECG data from multiple views while also point out existing challenges and problems while it can become a potential research direction in the future.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Opportunities and Challenges in Deep Learning Methods on Electrocardiogram Data: A Systematic Review.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs http://arxiv.org/abs/2001.01550; Years 2020; Authors Shenda Hong; Yuxi Zhou; Junyuan Shang; Cao Xiao; Jimeng Sun; Deduplication_Notes ; X Opportunities and Challenges in Deep Learning Methods on Electrocardiogram Data: A Systematic Review.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]","The electrocardiogram (ECG) is one of the most commonly used diagnostic tools in medicine and healthcare. Deep learning methods have achieved promising results on predictive healthcare tasks using ECG signals. This paper presents a systematic review of deep learning methods for ECG data from both modeling and application perspectives. We extracted papers that applied deep learning (deep neural network) models to ECG data that were published between January 1st of 2010 and February 29th of 2020 from Google Scholar, PubMed, and the Digital Bibliography &amp; Library Project. We then analyzed each article according to three factors: tasks, models, and data. Finally, we discuss open challenges and unsolved problems in this area. The total number of papers extracted was 191. Among these papers, 108 were published after 2019. Different deep learning architectures have been used in various ECG analytics tasks, such as disease detection/classification, annotation/localization, sleep staging, biometric human identification, and denoising. The number of works on deep learning for ECG data has grown explosively in recent years. Such works have achieved accuracy comparable to that of traditional feature-based approaches and ensembles of multiple approaches can achieve even better results. Specifically, we found that a hybrid architecture of a convolutional neural network and recurrent neural network ensemble using expert features yields the best results. However, there are some new challenges and problems related to interpretability, scalability, and efficiency that must be addressed. Furthermore, it is also worth investigating new applications from the perspectives of datasets and methods. This paper summarizes existing deep learning research using ECG data from multiple perspectives and highlights existing challenges and problems to identify potential future research directions.","['Shenda Hong', 'Yuxi Zhou', 'Junyuan Shang', 'Cao Xiao', 'Jimeng Sun']","Hong, Zhou, Shang, Xiao, Sun",https://export.arxiv.org/abs/2001.01550,https://doi.org/10.1016/j.compbiomed.2020.103801,2020-11-02
9856.0,,pubmed,Deep learning with sentence embeddings pre-trained on biomedical corpora   improves the performance of finding similar sentences in electronic medical   records,Deep learning with sentence embeddings pre-trained on biomedical corpora improves the performance of finding similar sentences in electronic medical records,"Capturing sentence semantics plays a vital role in a range of text mining applications. Despite continuous efforts on the development of related datasets and models in the general domain, both datasets and models are limited in biomedical and clinical domains. The BioCreative/OHNLP organizers have made the first attempt to annotate 1,068 sentence pairs from clinical notes and have called for a community effort to tackle the Semantic Textual Similarity (BioCreative/OHNLP STS) challenge. We developed models using traditional machine learning and deep learning approaches. For the post challenge, we focus on two models: the Random Forest and the Encoder Network. We applied sentence embeddings pre-trained on PubMed abstracts and MIMIC-III clinical notes and updated the Random Forest and the Encoder Network accordingly. The official results demonstrated our best submission was the ensemble of eight models. It achieved a Person correlation coefficient of 0.8328, the highest performance among 13 submissions from 4 teams. For the post challenge, the performance of both Random Forest and the Encoder Network was improved; in particular, the correlation of the Encoder Network was improved by ~13%. During the challenge task, no end-to-end deep learning models had better performance than machine learning models that take manually-crafted features. In contrast, with the sentence embeddings pre-trained on biomedical corpora, the Encoder Network now achieves a correlation of ~0.84, which is higher than the original best model. The ensembled model taking the improved versions of the Random Forest and Encoder Network as inputs further increased performance to 0.8528. Deep learning models with sentence embeddings pre-trained on biomedical corpora achieve the highest performance on the test set.","Capturing sentence semantics plays a vital role in a range of text mining applications. Despite continuous efforts on the development of related datasets and models in the general domain, both datasets and models are limited in biomedical and clinical domains. The BioCreative/OHNLP2018 organizers have made the first attempt to annotate 1068 sentence pairs from clinical notes and have called for a community effort to tackle the Semantic Textual Similarity (BioCreative/OHNLP STS) challenge. We developed models using traditional machine learning and deep learning approaches. For the post challenge, we focused on two models: the Random Forest and the Encoder Network. We applied sentence embeddings pre-trained on PubMed abstracts and MIMIC-III clinical notes and updated the Random Forest and the Encoder Network accordingly. The official results demonstrated our best submission was the ensemble of eight models. It achieved a Person correlation coefficient of 0.8328 - the highest performance among 13 submissions from 4 teams. For the post challenge, the performance of both Random Forest and the Encoder Network was improved; in particular, the correlation of the Encoder Network was improved by ~â€‰13%. During the challenge task, no end-to-end deep learning models had better performance than machine learning models that take manually-crafted features. In contrast, with the sentence embeddings pre-trained on biomedical corpora, the Encoder Network now achieves a correlation of ~â€‰0.84, which is higher than the original best model. The ensembled model taking the improved versions of the Random Forest and Encoder Network as inputs further increased performance to 0.8528. Deep learning models with sentence embeddings pre-trained on biomedical corpora achieve the highest performance on the test set. Through error analysis, we find that end-to-end deep learning models and traditional machine learning models with manually-crafted features complement each other by finding different types of sentences. We suggest a combination of these models can better find similar sentences in practice.","['Qingyu Chen', 'Jingcheng Du', 'Sun Kim', 'W. John Wilbur', 'Zhiyong Lu']","Chen, Du, Kim, Wilbur, Lu",https://export.arxiv.org/abs/1909.03044,https://doi.org/10.1186/s12911-020-1044-0,2020-11-02
9695.0,,pubmed,Advancing PICO Element Detection in Biomedical Text via Deep Neural   Networks,Advancing PICO element detection in biomedical text via deep neural networks,"In evidence-based medicine (EBM), defining a clinical question in terms of the specific patient problem aids the physicians to efficiently identify appropriate resources and search for the best available evidence for medical treatment. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components typically reported in clinical trials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). In this work, we propose a novel deep learning model for recognizing PICO elements in biomedical abstracts. Based on the previous state-of-the-art bidirectional long-short term memory (biLSTM) plus conditional random field (CRF) architecture, we add another layer of biLSTM upon the sentence representation vectors so that the contextual information from surrounding sentences can be gathered to help infer the interpretation of the current one. In addition, we propose two methods to further generalize and improve the model: adversarial training and unsupervised pre-training over large corpora. We tested our proposed approach over two benchmark datasets. One is the PubMed-PICO dataset, where our best results outperform the previous best by 5.5%, 7.9%, and 5.8% for P, I, and O elements in terms of F1 score, respectively. And for the other dataset named NICTA-PIBOSO, the improvements for P/I/O elements are 2.4%, 13.6%, and 1.0% in F1 score, respectively. Overall, our proposed deep learning model can obtain unprecedented PICO element detection accuracy while avoiding the need for any manual feature selection.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Advancing PICO Element Detection in Medical Text via Deep Neural Networks.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs http://arxiv.org/abs/1810.12780; Years 2018; Authors Di Jin; Peter Szolovits; Deduplication_Notes ; X Advancing PICO Element Detection in Medical Text via Deep Neural Networks.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]","In evidence-based medicine, defining a clinical question in terms of the specific patient problem aids the physicians to efficiently identify appropriate resources and search for the best available evidence for medical treatment. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components typically reported in clinical trials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). In this work, we propose a novel deep learning model for recognizing PICO elements in biomedical abstracts. Based on the previous state-of-the-art bidirectional long-short-term memory (bi-LSTM) plus conditional random field architecture, we add another layer of bi-LSTM upon the sentence representation vectors so that the contextual information from surrounding sentences can be gathered to help infer the interpretation of the current one. In addition, we propose two methods to further generalize and improve the model: adversarial training and unsupervised pre-training over large corpora. We tested our proposed approach over two benchmark datasets. One is the PubMed-PICO dataset, where our best results outperform the previous best by 5.5%, 7.9% and 5.8% for P, I and O elements in terms of F1 score, respectively. And for the other dataset named NICTA-PIBOSO, the improvements for P/I/O elements are 3.9%, 15.6% and 1.3% in F1 score, respectively. Overall, our proposed deep learning model can obtain unprecedented PICO element detection accuracy while avoiding the need for any manual feature selection. Code is available at https://github.com/jind11/Deep-PICO-Detection.","['Di Jin', 'Peter Szolovits']","Jin, Szolovits",https://export.arxiv.org/abs/1810.12780,https://doi.org/10.1093/bioinformatics/btaa256,2020-11-02
10304.0,,pubmed,DES-ROD: Exploring Literature to Develop New Links between RNA Oxidation and Human Diseases,DES-ROD: Exploring Literature to Develop New Links between RNA Oxidation and Human Diseases,"Normal cellular physiology and biochemical processes require undamaged RNA molecules. However, RNAs are frequently subjected to oxidative damage. Overproduction of reactive oxygen species (ROS) leads to RNA oxidation and disturbs redox (oxidation-reduction reaction) homeostasis. When oxidation damage affects RNA carrying protein-coding information, this may result in the synthesis of aberrant proteins as well as a lower efficiency of translation. Both of these, as well as imbalanced redox homeostasis, may lead to numerous human diseases. The number of studies on the effects of RNA oxidative damage in mammals is increasing by year due to the understanding that this oxidation fundamentally leads to numerous human diseases. To enable researchers in this field to explore information relevant to RNA oxidation and effects on human diseases, we developed DES-ROD, an online knowledgebase that contains processed information from 298,603 relevant documents that consist of PubMed abstracts and PubMed Central full-text articles. The system utilizes concepts/terms from 38 curated thematic dictionaries mapped to the analyzed documents. Researchers can explore enriched concepts, as well as enriched pairs of putatively associated concepts. In this way, one can explore mutual relationships between any combinations of two concepts from used dictionaries. Dictionaries cover a wide range of biomedical topics, such as human genes and proteins, pathways, Gene Ontology categories, mutations, noncoding RNAs, enzymes, toxins, metabolites, and diseases. This makes insights into different facets of the effects of RNA oxidation and the control of this process possible. The usefulness of the DES-ROD system is demonstrated by case studies on some known information, as well as potentially novel information involving RNA oxidation and diseases. DES-ROD is the first knowledgebase based on text and data mining that focused on the exploration of RNA oxidation and human diseases.","Normal cellular physiology and biochemical processes require undamaged RNA molecules. However, RNAs are frequently subjected to oxidative damage. Overproduction of reactive oxygen species (ROS) leads to RNA oxidation and disturbs redox (oxidation-reduction reaction) homeostasis. When oxidation damage affects RNA carrying protein-coding information, this may result in the synthesis of aberrant proteins as well as a lower efficiency of translation. Both of these, as well as imbalanced redox homeostasis, may lead to numerous human diseases. The number of studies on the effects of RNA oxidative damage in mammals is increasing by year due to the understanding that this oxidation fundamentally leads to numerous human diseases. To enable researchers in this field to explore information relevant to RNA oxidation and effects on human diseases, we developed DES-ROD, an online knowledgebase that contains processed information from 298,603 relevant documents that consist of PubMed abstracts and PubMed Central full-text articles. The system utilizes concepts/terms from 38 curated thematic dictionaries mapped to the analyzed documents. Researchers can explore enriched concepts, as well as enriched pairs of putatively associated concepts. In this way, one can explore mutual relationships between any combinations of two concepts from used dictionaries. Dictionaries cover a wide range of biomedical topics, such as human genes and proteins, pathways, Gene Ontology categories, mutations, noncoding RNAs, enzymes, toxins, metabolites, and diseases. This makes insights into different facets of the effects of RNA oxidation and the control of this process possible. The usefulness of the DES-ROD system is demonstrated by case studies on some known information, as well as potentially novel information involving RNA oxidation and diseases. DES-ROD is the first knowledgebase based on text and data mining that focused on the exploration of RNA oxidation and human diseases.","Essack, M. Salhi, A. Van Neste, C. Raies, A. B. Tifratene, F. Uludag, M. Hungler, A. Zaric, B. Zafirovic, S. Gojobori, T. Isenovic, E. Bajic, V. P.","Essack, Salhi, Van Neste, Raies, Tifratene, Uludag, Hungler, Zaric, Zafirovic, Gojobori, Isenovic, Bajic",<Go to ISI>://WOS:000524523900001,https://doi.org/10.1155/2020/5904315,2020-11-02
1947.0,,pubmed,Prediction Models for Childhood Asthma: A Systematic Review,Prediction models for childhood asthma: A systematic review,"BACKGROUND: The inability to objectively diagnose childhood asthma before age five often results in both under- and over-treatment of asthma in preschool children. Prediction tools for estimating a child's risk of developing asthma by school-age could assist physicians in early asthma care for preschool children. This review aimed to systematically identify and critically appraise studies which either developed novel or updated existing prediction models for predicting school-age asthma. METHODS: Three databases (Medline, Embase and Web of Science Core Collection) were searched up to July 2019 to identify studies utilising information from children <=5 years of age to predict asthma in school-age children (6-13 years). Validation studies were evaluated as a secondary objective. RESULTS: Twenty-four studies describing the development of 26 predictive models published between 2000 and 2019 were identified. Models were either regression-based (n=21) or utilised machine learning approaches (n=5). Nine studies conducted validation of six regression-based models. Fifteen (out of 21) models required additional clinical tests. Overall model performance, assessed by Area Under the Receiver-Operator-Curve (AUC), ranged between 0.66-0.87. Models demonstrated moderate ability to either rule in or rule out asthma development, but not both. Where external validation was performed, models demonstrated modest generalisability (AUC range: 0.62-0.83). CONCLUSION: Existing prediction models demonstrated moderate predictive performance, often with modest generalisability when independently validated. Limitations of traditional methods have shown to impair predictive accuracy and resolution. Exploration of novel methods such as machine learning approaches may address these limitations for future school-age asthma prediction.","The inability to objectively diagnose childhood asthma before age five often results in both under-treatment and over-treatment of asthma in preschool children. Prediction tools for estimating a child's risk of developing asthma by school-age could assist physicians in early asthma care for preschool children. This review aimed to systematically identify and critically appraise studies which either developed novel or updated existing prediction models for predicting school-age asthma. Three databases (MEDLINE, Embase and Web of Science Core Collection) were searched up to July 2019 to identify studies utilizing information from children â‰¤5Â years of age to predict asthma in school-age children (6-13Â years). Validation studies were evaluated as a secondary objective. Twenty-four studies describing the development of 26 predictive models published between 2000 and 2019 were identified. Models were either regression-based (nÂ =Â 21) or utilized machine learning approaches (nÂ =Â 5). Nine studies conducted validations of six regression-based models. Fifteen (out of 21) models required additional clinical tests. Overall model performance, assessed by area under the receiver operating curve (AUC), ranged between 0.66 and 0.87. Models demonstrated moderate ability to either rule in or rule out asthma development, but not both. Where external validation was performed, models demonstrated modest generalizability (AUC range: 0.62-0.83). Existing prediction models demonstrated moderate predictive performance, often with modest generalizability when independently validated. Limitations of traditional methods have shown to impair predictive accuracy and resolution. Exploration of novel methods such as machine learning approaches may address these limitations for future school-age asthma prediction.","Kothalawala, D. M.
 and Kadalayil, L.
 and Weiss, V. B. N.
 and Kyyaly, M. A.
 and Arshad, H. S.
 and Holloway, J. W.
 and Rezwan, F. I.","Kothalawala, Kadalayil, Weiss, Kyyaly, Arshad, Holloway, Rezwan",https://dx.doi.org/10.1111/pai.13247,https://doi.org/10.1111/pai.13247,2020-11-02
3647.0,,pubmed,Effects of gender-affirming hormone therapy on insulin resistance and body composition in transgender individuals: A systematic review,Effects of gender-affirming hormone therapy on insulin resistance and body composition in transgender individuals: A systematic review,"BACKGROUND: Transgender individuals receiving masculinising or feminising gender-affirming hormone therapy with testosterone or estradiol respectively, are at increased risk of adverse cardiovascular outcomes, including myocardial infarction and stroke. This may be related to the effects of testosterone or estradiol therapy on body composition, fat distribution, and insulin resistance but the effect of gender-affirming hormone therapy on these cardiovascular risk factors has not been extensively examined. AIM: To evaluate the impact of gender-affirming hormone therapy on body composition and insulin resistance in transgender individuals, to guide clinicians in minimising cardiovascular risk. METHODS: We performed a review of the literature based on PRISMA guidelines. MEDLINE, Embase and PsycINFO databases were searched for studies examining body composition, insulin resistance or body fat distribution in transgender individuals aged over 18 years on established gender-affirming hormone therapy. Studies were selected for full-text analysis if they investigated transgender individuals on any type of gender-affirming hormone therapy and reported effects on lean mass, fat mass or insulin resistance. RESULTS: The search strategy identified 221 studies. After exclusion of studies that did not meet inclusion criteria, 26 were included (2 cross-sectional, 21 prospective-uncontrolled and 3 prospective-controlled). Evidence in transgender men suggests that testosterone therapy increases lean mass, decreases fat mass and has no impact on insulin resistance. Evidence in transgender women suggests that feminising hormone therapy (estradiol, with or without anti-androgen agents) decreases lean mass, increases fat mass, and may worsen insulin resistance. Changes to body composition were consistent across almost all studies: Transgender men on testosterone gained lean mass and lost fat mass, and transgender women on oestrogen experienced the reverse. No study directly contradicted these trends, though several small studies of short duration reported no changes. Results for insulin resistance are less consistent and uncertain. There is a paucity of prospective controlled research, and existing prospective evidence is limited by small sample sizes, short follow up periods, and young cohorts of participants. CONCLUSION: Further research is required to further characterise the impact of gender-affirming hormone therapy on body composition and insulin resistance in the medium-long term. Until further evidence is available, clinicians should aim to minimise risk by monitoring cardiovascular risk markers regularly in their patients and encouraging healthy lifestyle modifications.","Transgender individuals receiving masculinising or feminising gender-affirming hormone therapy with testosterone or estradiol respectively, are at increased risk of adverse cardiovascular outcomes, including myocardial infarction and stroke. This may be related to the effects of testosterone or estradiol therapy on body composition, fat distribution, and insulin resistance but the effect of gender-affirming hormone therapy on these cardiovascular risk factors has not been extensively examined. To evaluate the impact of gender-affirming hormone therapy on body composition and insulin resistance in transgender individuals, to guide clinicians in minimising cardiovascular risk. We performed a review of the literature based on PRISMA guidelines. MEDLINE, Embase and PsycINFO databases were searched for studies examining body composition, insulin resistance or body fat distribution in transgender individuals aged over 18 years on established gender-affirming hormone therapy. Studies were selected for full-text analysis if they investigated transgender individuals on any type of gender-affirming hormone therapy and reported effects on lean mass, fat mass or insulin resistance. The search strategy identified 221 studies. After exclusion of studies that did not meet inclusion criteria, 26 were included (2 cross-sectional, 21 prospective-uncontrolled and 3 prospective-controlled). Evidence in transgender men suggests that testosterone therapy increases lean mass, decreases fat mass and has no impact on insulin resistance. Evidence in transgender women suggests that feminising hormone therapy (estradiol, with or without anti-androgen agents) decreases lean mass, increases fat mass, and may worsen insulin resistance. Changes to body composition were consistent across almost all studies: Transgender men on testosterone gained lean mass and lost fat mass, and transgender women on oestrogen experienced the reverse. No study directly contradicted these trends, though several small studies of short duration reported no changes. Results for insulin resistance are less consistent and uncertain. There is a paucity of prospective controlled research, and existing prospective evidence is limited by small sample sizes, short follow up periods, and young cohorts of participants. Further research is required to further characterise the impact of gender-affirming hormone therapy on body composition and insulin resistance in the medium-long term. Until further evidence is available, clinicians should aim to minimise risk by monitoring cardiovascular risk markers regularly in their patients and encouraging healthy lifestyle modifications.","Spanos, C.
 and Bretherton, I.
 and Zajac, J. D.
 and Cheung, A. S.","Spanos, Bretherton, Zajac, Cheung",https://dx.doi.org/10.4239/wjd.v11.i3.66,https://doi.org/10.4239/wjd.v11.i3.66,2020-11-02
1509.0,,pubmed,Latest Advances in Cardiac CT,Latest Advances in Cardiac CT,"Recent rapid technological advancements in cardiac CT have improved image quality and reduced radiation exposure to patients. Furthermore, key insights from large cohort trials have helped delineate cardiovascular disease risk as a function of overall coronary plaque burden and the morphological appearance of individual plaques. The advent of CT-derived fractional flow reserve promises to establish an anatomical and functional test within one modality. Recent data examining the short-term impact of CT-derived fractional flow reserve on downstream care and clinical outcomes have been published. In addition, machine learning is a concept that is being increasingly applied to diagnostic medicine. Over the coming decade, machine learning will begin to be integrated into cardiac CT, and will potentially make a tangible difference to how this modality evolves. The authors have performed an extensive literature review and comprehensive analysis of the recent advances in cardiac CT. They review how recent advances currently impact on clinical care and potential future directions for this imaging modality.","Recent rapid technological advancements in cardiac CT have improved image quality and reduced radiation exposure to patients. Furthermore, key insights from large cohort trials have helped delineate cardiovascular disease risk as a function of overall coronary plaque burden and the morphological appearance of individual plaques. The advent of CT-derived fractional flow reserve promises to establish an anatomical and functional test within one modality. Recent data examining the short-term impact of CT-derived fractional flow reserve on downstream care and clinical outcomes have been published. In addition, machine learning is a concept that is being increasingly applied to diagnostic medicine. Over the coming decade, machine learning will begin to be integrated into cardiac CT, and will potentially make a tangible difference to how this modality evolves. The authors have performed an extensive literature review and comprehensive analysis of the recent advances in cardiac CT. They review how recent advances currently impact on clinical care and potential future directions for this imaging modality.","Heseltine, T. D.
 and Murray, S. W.
 and Ruzsics, B.
 and Fisher, M.","Heseltine, Murray, Ruzsics, Fisher",https://dx.doi.org/10.15420/ecr.2019.14.2,https://doi.org/10.15420/ecr.2019.14.2,2020-11-02
4427.0,,pubmed,Radiographic assessment of the cup orientation after total hip arthroplasty: a literature review,Radiographic assessment of the cup orientation after total hip arthroplasty: a literature review,"Optimal acetabular cup orientation is of substantial importance to good long-term function and low complication rates after total hip arthroplasty (THA). The radiographic anteversion (RA) and inclination (RI) angles of the cup are typically studied due to the practicability, simplicity, and ease of interpretation of their measurements. A great number of methods have been developed to date, most of which have been performed on pelvic or hip anteroposterior radiographs. However, there are primarily two influencing factors for these methods: X-ray offset and pelvic rotation. In addition, there are three types of pelvic rotations about the transverse, longitudinal, and anteroposterior axes of the body. Their effects on the RA and RI angles of the cup are interactively correlated with the position and true orientation of the cup. To date, various fitted or analytical models have been established to disclose the correlations between the X-ray offset and pelvic rotation and the RA and RI angles of the cup. Most of these models do not incorporate all the potential influencing parameters. Advanced methods for performing X-ray offset and pelvic rotation corrections are mainly performed on a single pelvic AP radiograph, two synchronized radiographs, or a two-dimensional/three-dimensional (2D-3D) registration system. Some measurement systems, originally developed for evaluating implant migration or wear, could also be used for correcting the X-ray offset and pelvic rotation simultaneously, but some drawbacks still exist with these systems. Above all, the 2D-3D registration technique might be an alternative and powerful tool for accurately measuring cup orientation. In addition to the current methods used for postoperative assessment, navigation systems and augmented reality are also used for the preoperative planning and intraoperative guidance of cup placement. With the continuing development of artificial intelligence and machine learning, these techniques could be incorporated into robot-assisted orthopaedic surgery in the future.","Optimal acetabular cup orientation is of substantial importance to good long-term function and low complication rates after total hip arthroplasty (THA). The radiographic anteversion (RA) and inclination (RI) angles of the cup are typically studied due to the practicability, simplicity, and ease of interpretation of their measurements. A great number of methods have been developed to date, most of which have been performed on pelvic or hip anteroposterior radiographs. However, there are primarily two influencing factors for these methods: X-ray offset and pelvic rotation. In addition, there are three types of pelvic rotations about the transverse, longitudinal, and anteroposterior axes of the body. Their effects on the RA and RI angles of the cup are interactively correlated with the position and true orientation of the cup. To date, various fitted or analytical models have been established to disclose the correlations between the X-ray offset and pelvic rotation and the RA and RI angles of the cup. Most of these models do not incorporate all the potential influencing parameters. Advanced methods for performing X-ray offset and pelvic rotation corrections are mainly performed on a single pelvic AP radiograph, two synchronized radiographs, or a two-dimensional/three-dimensional (2D-3D) registration system. Some measurement systems, originally developed for evaluating implant migration or wear, could also be used for correcting the X-ray offset and pelvic rotation simultaneously, but some drawbacks still exist with these systems. Above all, the 2D-3D registration technique might be an alternative and powerful tool for accurately measuring cup orientation. In addition to the current methods used for postoperative assessment, navigation systems and augmented reality are also used for the preoperative planning and intraoperative guidance of cup placement. With the continuing development of artificial intelligence and machine learning, these techniques could be incorporated into robot-assisted orthopaedic surgery in the future.","Zhao, J. X.
 and Su, X. Y.
 and Zhao, Z.
 and Xiao, R. X.
 and Zhang, L. C.
 and Tang, P. F.","Zhao, Su, Zhao, Xiao, Zhang, Tang",https://dx.doi.org/10.21037/atm.2019.12.150,https://doi.org/10.21037/atm.2019.12.150,2020-11-02
2151.0,,pubmed,Efficacy of Chinese Herbal Formula Sini Zuojin Decoction in Treating Gastroesophageal Reflux Disease: Clinical Evidence and Potential Mechanisms,Efficacy of Chinese Herbal Formula Sini Zuojin Decoction in Treating Gastroesophageal Reflux Disease: Clinical Evidence and Potential Mechanisms,"Background: Based on 122 cases reported in China, data mining indicated that Sini Powder (SNP) and the Zuojin Pill (ZJP) are both widely used as the basic recipe for treating Gastroesophageal Reflux Disease (GERD). Objectives: To evaluate the intervention effects of Sini Zuojin Decoction (SNZJD) in patients with GERD. Methods: A comprehensive collection of randomized controlled trials (RCTs) using SNZJD in patients with GERD that were published in domestic and foreign journals was made by computer retrieval. RevMan 5.3 software was used for meta-analysis and bias risk assessment, Stata 14.0 software was used for sensitivity analysis, GRADE profiler 3.6 was used to evaluate the level of evidence, and trial sequential analysis (TSA), employed to control for random errors, was performed to assess the main outcomes. Network pharmacology analysis was applied to preliminarily study the mechanisms of action of SNZJD on GERD. Results: Thirteen articles were eventually included, covering a total of 966 patients. Meta-analysis indicated that: 1 the SNZJD plus traditional stomach medicines (SPTSM) group was more effective than the traditional stomach medicines (TSM) group (RR = 1.16, 95% CI [1.04, 1.29], P = 0.009); 2 the experimental group with SNZJD was significantly better than TSM controls in improving heartburn, substernal chest pain, acid regurgitation, and food regurgitation symptoms (P < 0.0001); 3 SPTSM could significantly decrease total symptom scores with substantial effectiveness (P < 0.00001). The recurrence rate and adverse effects of SNZJD treatment were significantly reduced (P < 0.05). TSA showed that the effective rate of meta-analysis might be reliable, but the recurrence and safety results were still uncertain. According to the evaluation by the GRADE method, the quality of evidence was low. Besides, SNZJD might treat GERD by acting on related targets and pathways such as inflammation, hormone regulation, and so on. Conclusions: SNZJD might be useful in the treatment of GERD, but its long-term effects and specific clinical mechanisms are unclear. Due to the poor quality of the evidence, more samples and high-quality clinical studies should be tested and verified in the future.","Based on 122 cases reported in China, data mining indicated that Sini Powder (SNP) and the Zuojin Pill (ZJP) are both widely used as the basic recipe for treating Gastroesophageal Reflux Disease (GERD). To evaluate the intervention effects of Sini Zuojin Decoction (SNZJD) in patients with GERD. A comprehensive collection of randomized controlled trials (RCTs) using SNZJD in patients with GERD that were published in domestic and foreign journals was made by computer retrieval. RevMan 5.3 software was used for meta-analysis and bias risk assessment, Stata 14.0 software was used for sensitivity analysis, GRADE profiler 3.6 was used to evaluate the level of evidence, and trial sequential analysis (TSA), employed to control for random errors, was performed to assess the main outcomes. Network pharmacology analysis was applied to preliminarily study the mechanisms of action of SNZJD on GERD. Thirteen articles were eventually included, covering a total of 966 patients. Meta-analysis indicated that: â‘ the SNZJD plus traditional stomach medicines (SPTSM) group was more effective than the traditional stomach medicines (TSM) group (RR = 1.16, 95% CI [1.04, 1.29], P = 0.009); â‘¡ the experimental group with SNZJD was significantly better than TSM controls in improving heartburn, substernal chest pain, acid regurgitation, and food regurgitation symptoms (P &lt; 0.0001); â‘¢ SPTSM could significantly decrease total symptom scores with substantial effectiveness (P &lt; 0.00001). The recurrence rate and adverse effects of SNZJD treatment were significantly reduced (P &lt; 0.05). TSA showed that the effective rate of meta-analysis might be reliable, but the recurrence and safety results were still uncertain. According to the evaluation by the GRADE method, the quality of evidence was low. Besides, SNZJD might treat GERD by acting on related targets and pathways such as inflammation, hormone regulation, and so on. SNZJD might be useful in the treatment of GERD, but its long-term effects and specific clinical mechanisms are unclear. Due to the poor quality of the evidence, more samples and high-quality clinical studies should be tested and verified in the future.","Li, S.
 and Huang, M.
 and Wu, G.
 and Huang, W.
 and Huang, Z.
 and Yang, X.
 and Ou, J.
 and Wei, Q.
 and Liu, C.
 and Yu, S.","Li, Huang, Wu, Huang, Huang, Yang, Ou, Wei, Liu, Yu",https://dx.doi.org/10.3389/fphar.2020.00076,https://doi.org/10.3389/fphar.2020.00076,2020-11-02
949.0,,pubmed,Accuracy and reliability of automatic three-dimensional cephalometric landmarking,Accuracy and reliability of automatic three-dimensional cephalometric landmarking,"The aim of this systematic review was to assess the accuracy and reliability of automatic landmarking for cephalometric analysis of three-dimensional craniofacial images. We searched for studies that reported results of automatic landmarking and/or measurements of human head computed tomography or cone beam computed tomography scans in MEDLINE, Embase and Web of Science until March 2019. Two authors independently screened articles for eligibility. Risk of bias and applicability concerns for each included study were assessed using the QUADAS-2 tool. Eleven studies with test dataset sample sizes ranging from 18 to 77 images were included. They used knowledge-, atlas- or learning-based algorithms to landmark two to 33 points of cephalometric interest. Ten studies measured mean localization errors between manually and automatically detected landmarks. Depending on the studies and the landmarks, mean errors ranged from <0.50mm to>5mm. The two best-performing algorithms used a deep learning method and reported mean errors <2mm for every landmark, approximating results of operator variability in manual landmarking. Risk of bias regarding patient selection and implementation of the reference standard were found, therefore the studies might have yielded overoptimistic results. The robustness of these algorithms needs to be more thoroughly tested in challenging clinical settings. PROSPERO registration number: CRD42019119637.","The aim of this systematic review was to assess the accuracy and reliability of automatic landmarking for cephalometric analysis of three-dimensional craniofacial images. We searched for studies that reported results of automatic landmarking and/or measurements of human head computed tomography or cone beam computed tomography scans in MEDLINE, Embase and Web of Science until March 2019. Two authors independently screened articles for eligibility. Risk of bias and applicability concerns for each included study were assessed using the QUADAS-2 tool. Eleven studies with test dataset sample sizes ranging from 18 to 77 images were included. They used knowledge-, atlas- or learning-based algorithms to landmark two to 33 points of cephalometric interest. Ten studies measured mean localization errors between manually and automatically detected landmarks. Depending on the studies and the landmarks, mean errors ranged from &lt;0.50mm to&gt;5mm. The two best-performing algorithms used a deep learning method and reported mean errors &lt;2mm for every landmark, approximating results of operator variability in manual landmarking. Risk of bias regarding patient selection and implementation of the reference standard were found, therefore the studies might have yielded overoptimistic results. The robustness of these algorithms needs to be more thoroughly tested in challenging clinical settings. PROSPERO registration number: CRD42019119637.","Dot, G.
 and Rafflenbeul, F.
 and Arbotto, M.
 and Gajny, L.
 and Rouch, P.
 and Schouman, T.","Dot, Rafflenbeul, Arbotto, Gajny, Rouch, Schouman",https://dx.doi.org/10.1016/j.ijom.2020.02.015,https://doi.org/10.1016/j.ijom.2020.02.015,2020-11-02
1078.0,,pubmed,A Guide to Literature Informed Decisions in the Design of Real Time fMRI Neurofeedback Studies: A Systematic Review,A Guide to Literature Informed Decisions in the Design of Real Time fMRI Neurofeedback Studies: A Systematic Review,"<b>Background:</b> Although biofeedback using electrophysiology has been explored extensively, the approach of using neurofeedback corresponding to hemodynamic response is a relatively young field. Real time functional magnetic resonance imaging-based neurofeedback (rt-fMRI-NF) uses sensory feedback to operantly reinforce patterns of neural response. It can be used, for example, to alter visual perception, increase brain connectivity, and reduce depression symptoms. Within recent years, interest in rt-fMRI-NF in both research and clinical contexts has expanded considerably. As such, building a consensus regarding best practices is of great value. <b>Objective:</b> This systematic review is designed to describe and evaluate the variations in methodology used in previous rt-fMRI-NF studies to provide recommendations for rt-fMRI-NF study designs that are mostly likely to elicit reproducible and consistent effects of neurofeedback.","<b>Background:</b> Although biofeedback using electrophysiology has been explored extensively, the approach of using neurofeedback corresponding to hemodynamic response is a relatively young field. Real time functional magnetic resonance imaging-based neurofeedback (rt-fMRI-NF) uses sensory feedback to operantly reinforce patterns of neural response. It can be used, for example, to alter visual perception, increase brain connectivity, and reduce depression symptoms. Within recent years, interest in rt-fMRI-NF in both research and clinical contexts has expanded considerably. As such, building a consensus regarding best practices is of great value. <b>Objective:</b> This systematic review is designed to describe and evaluate the variations in methodology used in previous rt-fMRI-NF studies to provide recommendations for rt-fMRI-NF study designs that are mostly likely to elicit reproducible and consistent effects of neurofeedback. <b>Methods:</b> We conducted a database search for fMRI neurofeedback papers published prior to September 26th, 2019. Of 558 studies identified, 146 met criteria for inclusion. The following information was collected from each study: sample size and type, task used, neurofeedback calculation, regulation procedure, feedback, whether feedback was explicitly related to changing brain activity, feedback timing, control group for active neurofeedback, how many runs and sessions of neurofeedback, if a follow-up was conducted, and the results of neurofeedback training. <b>Results:</b> rt-fMRI-NF is typically upregulation practice based on hemodynamic response from a specific region of the brain presented using a continually updating thermometer display. Most rt-fMRI-NF studies are conducted in healthy samples and half evaluate its effect on immediate changes in behavior or affect. The most popular control group method is to provide sham signal from another region; however, many studies do not compare use a comparison group. <b>Conclusions:</b> We make several suggestions for designs of future rt-fMRI-NF studies. Researchers should use feedback calculation methods that consider neural response across regions (i.e., SVM or connectivity), which should be conveyed as intermittent, auditory feedback. Participants should be given explicit instructions and should be assessed on individual differences. Future rt-fMRI-NF studies should use clinical samples; effectiveness of rt-fMRI-NF should be evaluated on clinical/behavioral outcomes at follow-up time points in comparison to both a sham and no feedback control group.","Fede, S. J.
 and Dean, S. F.
 and Manuweera, T.
 and Momenan, R.","Fede, Dean, Manuweera, Momenan",https://dx.doi.org/10.3389/fnhum.2020.00060,https://doi.org/10.3389/fnhum.2020.00060,2020-11-02
906.0,,pubmed,"Statistical significance: p value, 005 threshold, and applications to radiomics-reasons for a conservative approach","Statistical significance: p value, 005 threshold, and applications to radiomics-reasons for a conservative approach","Here, we summarise the unresolved debate about p value and its dichotomisation. We present the statement of the American Statistical Association against the misuse of statistical significance as well as the proposals to abandon the use of p value and to reduce the significance threshold from 0.05 to 0.005. We highlight reasons for a conservative approach, as clinical research needs dichotomic answers to guide decision-making, in particular in the case of diagnostic imaging and interventional radiology. With a reduced p value threshold, the cost of research could increase while spontaneous research could be reduced. Secondary evidence from systematic reviews/meta-analyses, data sharing, and cost-effective analyses are better ways to mitigate the false discovery rate and lack of reproducibility associated with the use of the 0.05 threshold. Importantly, when reporting p values, authors should always provide the actual value, not only statements of 'p < 0.05' or 'p >= 0.05', because p values give a measure of the degree of data compatibility with the null hypothesis. Notably, radiomics and big data, fuelled by the application of artificial intelligence, involve hundreds/thousands of tested features similarly to other 'omics' such as genomics, where a reduction in the significance threshold, based on well-known corrections for multiple testing, has been already adopted.","Here, we summarise the unresolved debate about p value and its dichotomisation. We present the statement of the American Statistical Association against the misuse of statistical significance as well as the proposals to abandon the use of p value and to reduce the significance threshold from 0.05 to 0.005. We highlight reasons for a conservative approach, as clinical research needs dichotomic answers to guide decision-making, in particular in the case of diagnostic imaging and interventional radiology. With a reduced p value threshold, the cost of research could increase while spontaneous research could be reduced. Secondary evidence from systematic reviews/meta-analyses, data sharing, and cost-effective analyses are better ways to mitigate the false discovery rate and lack of reproducibility associated with the use of the 0.05 threshold. Importantly, when reporting p values, authors should always provide the actual value, not only statements of &quot;p &lt; 0.05&quot; or &quot;p â‰¥ 0.05&quot;, because p values give a measure of the degree of data compatibility with the null hypothesis. Notably, radiomics and big data, fuelled by the application of artificial intelligence, involve hundreds/thousands of tested features similarly to other &quot;omics&quot; such as genomics, where a reduction in the significance threshold, based on well-known corrections for multiple testing, has been already adopted.","Di Leo, G.
 and Sardanelli, F.","Di Leo, Sardanelli",https://dx.doi.org/10.1186/s41747-020-0145-y,https://doi.org/10.1186/s41747-020-0145-y,2020-11-02
3985.0,,pubmed,Temporal information extraction from mental health records to identify duration of untreated psychosis,Temporal information extraction from mental health records to identify duration of untreated psychosis,"BACKGROUND: Duration of untreated psychosis (DUP) is an important clinical construct in the field of mental health, as longer DUP can be associated with worse intervention outcomes. DUP estimation requires knowledge about when psychosis symptoms first started (symptom onset), and when psychosis treatment was initiated. Electronic health records (EHRs) represent a useful resource for retrospective clinical studies on DUP, but the core information underlying this construct is most likely to lie in free text, meaning it is not readily available for clinical research. Natural Language Processing (NLP) is a means to addressing this problem by automatically extracting relevant information in a structured form. As a first step, it is important to identify appropriate documents, i.e., those that are likely to include the information of interest. Next, temporal information extraction methods are needed to identify time references for early psychosis symptoms. This NLP challenge requires solving three different tasks: time expression extraction, symptom extraction, and temporal 'linking'. In this study, we focus on the first step, using two relevant EHR datasets. RESULTS: We applied a rule-based NLP system for time expression extraction that we had previously adapted to a corpus of mental health EHRs from patients with a diagnosis of schizophrenia (first referrals). We extended this work by applying this NLP system to a larger set of documents and patients, to identify additional texts that would be relevant for our long-term goal, and developed a new corpus from a subset of these new texts (early intervention services). Furthermore, we added normalized value annotations ('2011-05') to the annotated time expressions ('May 2011') in both corpora. The finalized corpora were used for further NLP development and evaluation, with promising results (normalization accuracy 71-86%). To highlight the specificities of our annotation task, we also applied the final adapted NLP system to a different temporally annotated clinical corpus. CONCLUSIONS: Developing domain-specific methods is crucial to address complex NLP tasks such as symptom onset extraction and retrospective calculation of duration of a preclinical syndrome. To the best of our knowledge, this is the first clinical text resource annotated for temporal entities in the mental health domain.","Duration of untreated psychosis (DUP) is an important clinical construct in the field of mental health, as longer DUP can be associated with worse intervention outcomes. DUP estimation requires knowledge about when psychosis symptoms first started (symptom onset), and when psychosis treatment was initiated. Electronic health records (EHRs) represent a useful resource for retrospective clinical studies on DUP, but the core information underlying this construct is most likely to lie in free text, meaning it is not readily available for clinical research. Natural Language Processing (NLP) is a means to addressing this problem by automatically extracting relevant information in a structured form. As a first step, it is important to identify appropriate documents, i.e., those that are likely to include the information of interest. Next, temporal information extraction methods are needed to identify time references for early psychosis symptoms. This NLP challenge requires solving three different tasks: time expression extraction, symptom extraction, and temporal &quot;linking&quot;. In this study, we focus on the first step, using two relevant EHR datasets. We applied a rule-based NLP system for time expression extraction that we had previously adapted to a corpus of mental health EHRs from patients with a diagnosis of schizophrenia (first referrals). We extended this work by applying this NLP system to a larger set of documents and patients, to identify additional texts that would be relevant for our long-term goal, and developed a new corpus from a subset of these new texts (early intervention services). Furthermore, we added normalized value annotations (&quot;2011-05&quot;) to the annotated time expressions (&quot;May 2011&quot;) in both corpora. The finalized corpora were used for further NLP development and evaluation, with promising results (normalization accuracy 71-86%). To highlight the specificities of our annotation task, we also applied the final adapted NLP system to a different temporally annotated clinical corpus. Developing domain-specific methods is crucial to address complex NLP tasks such as symptom onset extraction and retrospective calculation of duration of a preclinical syndrome. To the best of our knowledge, this is the first clinical text resource annotated for temporal entities in the mental health domain.","Viani, N.
 and Kam, J.
 and Yin, L.
 and Bittar, A.
 and Dutta, R.
 and Patel, R.
 and Stewart, R.
 and Velupillai, S.","Viani, Kam, Yin, Bittar, Dutta, Patel, Stewart, Velupillai",not available,https://doi.org/10.1186/s13326-020-00220-2,2020-11-02
1066.0,,pubmed,Watch this space: a systematic review of the use of video-based media as a patient education tool in ophthalmology,Watch this space: a systematic review of the use of video-based media as a patient education tool in ophthalmology,"Effective clinician-patient communication is particularly important in ophthalmology where long-term adherence to treatment is often required. However, in the context of increasingly pressurised clinics, there is a tendency to resort to written information leaflets not suited to patients with visual impairment, non-English speakers or those with low levels of literacy. Video-based media could be harnessed to enhance clinician-patient communication. This systematic review aimed to assess the efficacy of using video-based media for patient education in ophthalmology. A pre-defined search strategy was used by two independent researchers to systematically review the PubMed, MEDLINE, EMBASE and PsycINFO databases. Eligible articles included peer-reviewed studies involving ophthalmology patients, who received a solely video-based educational intervention to assess for improvement in patient knowledge, behaviour and overall health-related outcomes. The search yielded 481 studies of which 31 passed initial screening. Following full-text analysis, 12 studies met the inclusion criteria, of which seven studies (58.3%) were randomised controlled trials. The majority of studies (58.3%) reported outcomes on patient comprehension with 5/7 (71%) showing statistically significant improvement after video intervention. Four studies (33.3%) reported on patient performance in a task (e.g. drop application method) or overall health-related outcome with 2/4 (50%) showing statistically significant improvement after intervention. Though more evidence is needed, the use of video-based media appears to be effective in improving patient understanding and in certain cases may ameliorate overall outcome. There is a paucity of well-designed studies and future research is required to fully examine the role of video-based media in patient education.","Effective clinician-patient communication is particularly important in ophthalmology where long-term adherence to treatment is often required. However, in the context of increasingly pressurised clinics, there is a tendency to resort to written information leaflets not suited to patients with visual impairment, non-English speakers or those with low levels of literacy. Video-based media could be harnessed to enhance clinician-patient communication. This systematic review aimed to assess the efficacy of using video-based media for patient education in ophthalmology. A pre-defined search strategy was used by two independent researchers to systematically review the PubMed, MEDLINE, EMBASE and PsycINFO databases. Eligible articles included peer-reviewed studies involving ophthalmology patients, who received a solely video-based educational intervention to assess for improvement in patient knowledge, behaviour and overall health-related outcomes. The search yielded 481 studies of which 31 passed initial screening. Following full-text analysis, 12 studies met the inclusion criteria, of which seven studies (58.3%) were randomised controlled trials. The majority of studies (58.3%) reported outcomes on patient comprehension with 5/7 (71%) showing statistically significant improvement after video intervention. Four studies (33.3%) reported on patient performance in a task (e.g. drop application method) or overall health-related outcome with 2/4 (50%) showing statistically significant improvement after intervention. Though more evidence is needed, the use of video-based media appears to be effective in improving patient understanding and in certain cases may ameliorate overall outcome. There is a paucity of well-designed studies and future research is required to fully examine the role of video-based media in patient education.","Farwana, R.
 and Sheriff, A.
 and Manzar, H.
 and Farwana, M.
 and Yusuf, A.
 and Sheriff, I.","Farwana, Sheriff, Manzar, Farwana, Yusuf, Sheriff",https://dx.doi.org/10.1038/s41433-020-0798-z,https://doi.org/10.1038/s41433-020-0798-z,2020-11-02
2230.0,,pubmed,A Hybrid PSO-SVM Model Based on Safety Risk Prediction for the Design Process in Metro Station Construction,A Hybrid PSO-SVM Model Based on Safety Risk Prediction for the Design Process in Metro Station Construction,"Incorporating safety risk into the design process is one of the most effective design sciences to enhance the safety of metro station construction. In such a case, the concept of Design for Safety (DFS) has attracted much attention. However, most of the current research overlooks the risk-prediction process in the application of DFS. Therefore, this paper proposes a hybrid risk-prediction framework to enhance the effectiveness of DFS in practice. Firstly, 12 influencing factors related to the safety risk of metro construction are identified by adopting the literature review method and code of construction safety management analysis. Then, a structured interview is used to collect safety risk cases of metro construction projects. Next, a developed support vector machine (SVM) model based on particle swarm optimization (PSO) is presented to predict the safety risk in metro construction, in which the multi-class SVM prediction model with an improved binary tree is designed. The results show that the average accuracy of the test sets is 85.26%, and the PSO-SVM model has a high predictive accuracy for non-linear relationship and small samples. The results show that the average accuracy of the test sets is 85.26%, and the PSO-SVM model has a high predictive accuracy for non-linear relationship and small samples. Finally, the proposed framework is applied to a case study of metro station construction. The prediction results show the PSO-SVM model is applicable and reasonable for safety risk prediction. This research also identifies the most important influencing factors to reduce the safety risk of metro station construction, which provides a guideline for the safety risk prediction of metro construction for design process.","Incorporating safety risk into the design process is one of the most effective design sciences to enhance the safety of metro station construction. In such a case, the concept of Design for Safety (DFS) has attracted much attention. However, most of the current research overlooks the risk-prediction process in the application of DFS. Therefore, this paper proposes a hybrid risk-prediction framework to enhance the effectiveness of DFS in practice. Firstly, 12 influencing factors related to the safety risk of metro construction are identified by adopting the literature review method and code of construction safety management analysis. Then, a structured interview is used to collect safety risk cases of metro construction projects. Next, a developed support vector machine (SVM) model based on particle swarm optimization (PSO) is presented to predict the safety risk in metro construction, in which the multi-class SVM prediction model with an improved binary tree is designed. The results show that the average accuracy of the test sets is 85.26%, and the PSO-SVM model has a high predictive accuracy for non-linear relationship and small samples. The results show that the average accuracy of the test sets is 85.26%, and the PSO-SVM model has a high predictive accuracy for non-linear relationship and small samples. Finally, the proposed framework is applied to a case study of metro station construction. The prediction results show the PSO-SVM model is applicable and reasonable for safety risk prediction. This research also identifies the most important influencing factors to reduce the safety risk of metro station construction, which provides a guideline for the safety risk prediction of metro construction for design process.","Liu, P.
 and Xie, M.
 and Bian, J.
 and Li, H.
 and Song, L.","Liu, Xie, Bian, Li, Song",https://dx.doi.org/10.3390/ijerph17051714,https://doi.org/10.3390/ijerph17051714,2020-11-02
839.0,,pubmed,Effectiveness of Conversational Agents (Virtual Assistants) in Health Care: Protocol for a Systematic Review,Effectiveness of Conversational Agents (Virtual Assistants) in Health Care: Protocol for a Systematic Review,"BACKGROUND: Conversational agents (also known as chatbots) have evolved in recent decades to become multimodal, multifunctional platforms with potential to automate a diverse range of health-related activities supporting the general public, patients, and physicians. Multiple studies have reported the development of these agents, and recent systematic reviews have described the scope of use of conversational agents in health care. However, there is scarce research on the effectiveness of these systems; thus, their viability and applicability are unclear. OBJECTIVE: The objective of this systematic review is to assess the effectiveness of conversational agents in health care and to identify limitations, adverse events, and areas for future investigation of these agents. METHODS: The Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols will be used to structure this protocol. The focus of the systematic review is guided by a population, intervention, comparator, and outcome framework. A systematic search of the PubMed (Medline), EMBASE, CINAHL, and Web of Science databases will be conducted. Two authors will independently screen the titles and abstracts of the identified references and select studies according to the eligibility criteria. Any discrepancies will then be discussed and resolved. Two reviewers will independently extract and validate data from the included studies into a standardized form and conduct quality appraisal. RESULTS: As of January 2020, we have begun a preliminary literature search and piloting of the study selection process. CONCLUSIONS: This systematic review aims to clarify the effectiveness, limitations, and future applications of conversational agents in health care. Our findings may be useful to inform the future development of conversational agents and promote the personalization of patient care. International registered report identifier (irrid): prr1-10.2196/16934.","Conversational agents (also known as chatbots) have evolved in recent decades to become multimodal, multifunctional platforms with potential to automate a diverse range of health-related activities supporting the general public, patients, and physicians. Multiple studies have reported the development of these agents, and recent systematic reviews have described the scope of use of conversational agents in health care. However, there is scarce research on the effectiveness of these systems; thus, their viability and applicability are unclear. The objective of this systematic review is to assess the effectiveness of conversational agents in health care and to identify limitations, adverse events, and areas for future investigation of these agents. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols will be used to structure this protocol. The focus of the systematic review is guided by a population, intervention, comparator, and outcome framework. A systematic search of the PubMed (Medline), EMBASE, CINAHL, and Web of Science databases will be conducted. Two authors will independently screen the titles and abstracts of the identified references and select studies according to the eligibility criteria. Any discrepancies will then be discussed and resolved. Two reviewers will independently extract and validate data from the included studies into a standardized form and conduct quality appraisal. As of January 2020, we have begun a preliminary literature search and piloting of the study selection process. This systematic review aims to clarify the effectiveness, limitations, and future applications of conversational agents in health care. Our findings may be useful to inform the future development of conversational agents and promote the personalization of patient care. PRR1-10.2196/16934.","de Cock, C.
 and Milne-Ives, M.
 and van Velthoven, M. H.
 and Alturkistani, A.
 and Lam, C.
 and Meinert, E.","de Cock, Milne-Ives, van Velthoven, Alturkistani, Lam, Meinert",not available,https://doi.org/10.2196/16934,2020-11-02
755.0,,pubmed,Future of evidence ecosystem series: 2 Current opportunities and need for better tools and methods,Future of evidence ecosystem series: 2 current opportunities and need for better tools and methods,"To become user-driven and more useful for decision-making, the current evidence synthesis ecosystem requires significant changes (Paper 1.Future of evidence ecosystem series). Reviewers have access to new sources of data (clinical trial registries, protocols, clinical study reports from regulatory agencies or pharmaceutical companies) for more information on randomized control trials. With all these new available data, the management of multiple and scattered trial reports is even more challenging. New types of data are also becoming available: individual patient data and routinely collected data. With the increasing number of diverse sources to be searched and the amount of data to be extracted, the process needs to be rethought. New approaches and tools, such as automation technologies and crowdsourcing, should help accelerate the process. The implementation of these new approaches and methods requires a substantial rethinking and redesign of the current evidence synthesis ecosystem. The concept of a 'living' evidence synthesis enterprise, with living systematic review and living network meta-analysis, has recently emerged. Such an evidence synthesis ecosystem implies conceptualizing evidence synthesis as a continuous process built around a clinical question of interest and no longer as a small team independently answering a specific clinical question at a single point in time.","To become user driven and more useful for decision-making, the current evidence synthesis ecosystem requires significant changes (Paper 1. Future of evidence ecosystem series). Reviewers have access to new sources of data (clinical trial registries, protocols, and clinical study reports from regulatory agencies or pharmaceutical companies) for more information on randomized control trials. With all these newly available data, the management of multiple and scattered trial reports is even more challenging. New types of data are also becoming available: individual patient data and routinely collected data. With the increasing number of diverse sources to be searched and the amount of data to be extracted, the process needs to be rethought. New approaches and tools, such as automation technologies and crowdsourcing, should help accelerate the process. The implementation of these new approaches and methods requires a substantial rethinking and redesign of the current evidence synthesis ecosystem. The concept of a &quot;living&quot; evidence synthesis enterprise, with living systematic review and living network meta-analysis, has recently emerged. Such an evidence synthesis ecosystem implies conceptualizing evidence synthesis as a continuous process built around a clinical question of interest and no longer as a small team independently answering a specific clinical question at a single point in time.","Crequit, P.
 and Boutron, I.
 and Meerpohl, J.
 and Williams, H.
 and Craig, J.
 and Ravaud, P.","CrÃ©quit, Boutron, Meerpohl, Williams, Craig, Ravaud",not available,https://doi.org/10.1016/j.jclinepi.2020.01.023,2020-11-02
4177.0,,pubmed,Artificial Intelligence for chemical risk assessment,Artificial Intelligence for chemical risk assessment,"As the basis for managing the risks of chemical exposure, the Chemical Risk Assessment (CRA) process can impact a substantial part of the economy, the health of hundreds of millions of people, and the condition of the environment. However, the number of properly assessed chemicals falls short of societal needs due to a lack of experts for evaluation, interference of third party interests, and the sheer volume of potentially relevant information on the chemicals from disparate sources. In order to explore ways in which computational methods may help overcome this discrepancy between the number of chemical risk assessments required on the one hand and the number and adequateness of assessments actually being conducted on the other, the European Commission's Joint Research Centre organised a workshop on Artificial Intelligence for Chemical Risk Assessment (AI4CRA). The workshop identified a number of areas where Artificial Intelligence could potentially increase the number and quality of regulatory risk management decisions based on CRA, involving process simulation, supporting evaluation, identifying problems, facilitating collaboration, finding experts, evidence gathering, systematic review, knowledge discovery, and building cognitive models. Although these are interconnected, they are organised and discussed under two main themes: scientific-technical process and social aspects and the decision making process.","As the basis for managing the risks of chemical exposure, the Chemical Risk Assessment (CRA) process can impact a substantial part of the economy, the health of hundreds of millions of people, and the condition of the environment. However, the number of properly assessed chemicals falls short of societal needs due to a lack of experts for evaluation, interference of third party interests, and the sheer volume of potentially relevant information on the chemicals from disparate sources. In order to explore ways in which computational methods may help overcome this discrepancy between the number of chemical risk assessments required on the one hand and the number and adequateness of assessments actually being conducted on the other, the European Commission's Joint Research Centre organised a workshop on Artificial Intelligence for Chemical Risk Assessment (AI4CRA). The workshop identified a number of areas where Artificial Intelligence could potentially increase the number and quality of regulatory risk management decisions based on CRA, involving process simulation, supporting evaluation, identifying problems, facilitating collaboration, finding experts, evidence gathering, systematic review, knowledge discovery, and building cognitive models. Although these are interconnected, they are organised and discussed under two main themes: scientific-technical process and social aspects and the decision making process.","Wittwehr, C.
 and Blomstedt, P.
 and Gosling, J. P.
 and Peltola, T.
 and Raffael, B.
 and Richarz, A. N.
 and Sienkiewicz, M.
 and Whaley, P.
 and Worth, A.
 and Whelan, M.","Wittwehr, Blomstedt, Gosling, Peltola, Raffael, Richarz, Sienkiewicz, Whaley, Worth, Whelan",https://dx.doi.org/10.1016/j.comtox.2019.100114,https://doi.org/10.1016/j.comtox.2019.100114,2020-11-02
3491.0,,pubmed,Ethical Issues Posed by Field Research Using Highly Portable and Cloud-Enabled Neuroimaging,Ethical Issues Posed by Field Research Using Highly Portable and Cloud-Enabled Neuroimaging,"Highly portable, cloud-enabled neuroimaging technologies will fundamentally change neuroimaging research. Instead of participants traveling to the scanner, the scanner will now come to them. Field-based brain imaging research, including populations underrepresented in neuroscience research to date, will enlarge and diversify databases and pave the way for clinical and direct-to-consumer (DTC) applications. Yet these technological developments urgently require analysis of their ethical, legal, and social implications (ELSI). No consensus ethical frameworks for mobile neuroimaging exist, and existing policies for traditional MRI research are inadequate. Based on literature review and ethics analysis of neurotechnology development efforts, Shen et al. identify seven foundational, yet unresolved, ELSI issues posed by portable neuroimaging: (1) informed consent; (2) privacy; (3) capacity to accurately communicate neuroimaging results to remote participants; (4) extensive reliance on cloud-based artificial intelligence (AI) for data analysis; (5) potential bias of interpretive algorithms in diverse populations; (6) return of research results and incidental (or secondary) findings to research participants; and (7) responding to participant requests for access to their data. The article proposes a path forward to address these urgent issues.","Highly portable, cloud-enabled neuroimaging technologies will fundamentally change neuroimaging research. Instead of participants traveling to the scanner, the scanner will now come to them. Field-based brain imaging research, including populations underrepresented in neuroscience research to date, will enlarge and diversify databases and pave the way for clinical and direct-to-consumer (DTC) applications. Yet these technological developments urgently require analysis of their ethical, legal, and social implications (ELSI). No consensus ethical frameworks for mobile neuroimaging exist, and existing policies for traditional MRI research are inadequate. Based on literature review and ethics analysis of neurotechnology development efforts, Shen etÂ al. identify seven foundational, yet unresolved, ELSI issues posed by portable neuroimaging: (1) informed consent; (2) privacy; (3) capacity to accurately communicate neuroimaging results to remote participants; (4) extensive reliance on cloud-based artificial intelligence (AI) for data analysis; (5) potential bias of interpretive algorithms in diverse populations; (6) return of research results and incidental (or secondary) findings to research participants; and (7) responding to participant requests for access to their data. The article proposes a path forward to address these urgent issues.","Shen, F. X.
 and Wolf, S. M.
 and Gonzalez, R. G.
 and Garwood, M.","Shen, Wolf, Gonzalez, Garwood",https://dx.doi.org/10.1016/j.neuron.2020.01.041,https://doi.org/10.1016/j.neuron.2020.01.041,2020-11-02
2833.0,,pubmed,A systematic review of interventions for adults with social communication impairments due to an acquired brain injury: Significant other reports,A systematic review of interventions for adults with social communication impairments due to an acquired brain injury: Significant other reports,"Purpose: To determine the most effective intervention for adults with social communication impairments due to an Acquired Brain Injury (ABI), using standardised outcome measures completed by significant others. Method: A systematic literature review was conducted. Four electronic databases relevant to the field of speech-language pathology or brain injury were searched: Medline, CINAHL, AMED and Embase. Grey literature, reference lists and citation indexes were also hand searched for additional research. Studies that met the broad inclusion and exclusion criteria were initially screened to determine articles for full text reviews by two independent reviewers. Reviewers independently extracted data from full-text reviews using a data extraction form and performed bias analysis using the Downs and Black quality checklist (Downs, S.H., & Black, N. (1998). The feasibility of creating a checklist for the assessment of the methodological quality both of randomised and non-randomised studies of health care interventions. Journal of Epidemiology & Community Health, 52, 377-384.). Studies were categorised using a five-phase model of evidence (Robey, R.R., & Schultz, M.C. (1998). A model for conducting clinical-outcome research: An adaptation of the standard protocol for use in aphasiology. Aphasiology, 12, 787-810.). Result: 681 articles were identified after duplicates were removed. 15 articles were reviewed for full-text analysis. Six studies were ultimately included in the review. Of these, three were randomised controlled trials and three others were not. Four studies delivered intervention solely to the individual with an ABI, one to the communication partner only, and one delivered intervention both to the individual and their communication partner. Intervention programmes ranged from four to 12 weeks and from 12 to 48 hours total contact time. The reviews studies were heterogeneous, which made comparisons difficult. Risk of bias was also present to varying degrees in all studies. The current level of evidence has focussed on efficacy of treatments and effectiveness of treatment is not yet established. Conclusion: The current level of evidence is not yet established to make clear clinical guidelines on which interventions are most effective, based on significant others' reports. Further research is required, incorporating more rigorous study designs and larger sample sizes to enable accurate conclusions to be drawn.","<i>Purpose</i>: To determine the most effective intervention for adults with social communication impairments due to an Acquired Brain Injury (ABI), using standardised outcome measures completed by significant others.<i>Method</i>: A systematic literature review was conducted. Four electronic databases relevant to the field of speech-language pathology or brain injury were searched: Medline, CINAHL, AMED and Embase. Grey literature, reference lists and citation indexes were also hand searched for additional research. Studies that met the broad inclusion and exclusion criteria were initially screened to determine articles for full text reviews by two independent reviewers. Reviewers independently extracted data from full-text reviews using a data extraction form and performed bias analysis using the Downs and Black quality checklist (Downs, S.H., &amp; Black, N. (1998). The feasibility of creating a checklist for the assessment of the methodological quality both of randomised and non-randomised studies of health care interventions. Journal of Epidemiology &amp; Community Health, 52, 377-384.). Studies were categorised using a five-phase model of evidence (Robey, R.R., &amp; Schultz, M.C. (1998). A model for conducting clinical-outcome research: An adaptation of the standard protocol for use in aphasiology. Aphasiology, 12, 787-810.).<i>Result</i>: 681 articles were identified after duplicates were removed. 15 articles were reviewed for full-text analysis. Six studies were ultimately included in the review. Of these, three were randomised controlled trials and three others were not. Four studies delivered intervention solely to the individual with an ABI, one to the communication partner only, and one delivered intervention both to the individual and their communication partner. Intervention programmes ranged from four to 12 weeks and from 12 to 48â€‰hours total contact time. The reviews studies were heterogeneous, which made comparisons difficult. Risk of bias was also present to varying degrees in all studies. The current level of evidence has focussed on efficacy of treatments and effectiveness of treatment is not yet established.<i>Conclusion</i>: The current level of evidence is not yet established to make clear clinical guidelines on which interventions are most effective, based on significant others' reports. Further research is required, incorporating more rigorous study designs and larger sample sizes to enable accurate conclusions to be drawn.","Paice, L.
 and Aleligay, A.
 and Checklin, M.","Paice, Aleligay, Checklin",not available,https://doi.org/10.1080/17549507.2019.1701082,2020-11-02
4380.0,,pubmed,Systematic Review of Privacy-Preserving Distributed Machine Learning From Federated Databases in Health Care,Systematic Review of Privacy-Preserving Distributed Machine Learning From Federated Databases in Health Care,"Big data for health care is one of the potential solutions to deal with the numerous challenges of health care, such as rising cost, aging population, precision medicine, universal health coverage, and the increase of noncommunicable diseases. However, data centralization for big data raises privacy and regulatory concerns.Covered topics include (1) an introduction to privacy of patient data and distributed learning as a potential solution to preserving these data, a description of the legal context for patient data research, and a definition of machine/deep learning concepts; (2) a presentation of the adopted review protocol; (3) a presentation of the search results; and (4) a discussion of the findings, limitations of the review, and future perspectives.Distributed learning from federated databases makes data centralization unnecessary. Distributed algorithms iteratively analyze separate databases, essentially sharing research questions and answers between databases instead of sharing the data. In other words, one can learn from separate and isolated datasets without patient data ever leaving the individual clinical institutes.Distributed learning promises great potential to facilitate big data for medical application, in particular for international consortiums. Our purpose is to review the major implementations of distributed learning in health care.","Big data for health care is one of the potential solutions to deal with the numerous challenges of health care, such as rising cost, aging population, precision medicine, universal health coverage, and the increase of noncommunicable diseases. However, data centralization for big data raises privacy and regulatory concerns.Covered topics include (1) an introduction to privacy of patient data and distributed learning as a potential solution to preserving these data, a description of the legal context for patient data research, and a definition of machine/deep learning concepts; (2) a presentation of the adopted review protocol; (3) a presentation of the search results; and (4) a discussion of the findings, limitations of the review, and future perspectives.Distributed learning from federated databases makes data centralization unnecessary. Distributed algorithms iteratively analyze separate databases, essentially sharing research questions and answers between databases instead of sharing the data. In other words, one can learn from separate and isolated datasets without patient data ever leaving the individual clinical institutes.Distributed learning promises great potential to facilitate big data for medical application, in particular for international consortiums. Our purpose is to review the major implementations of distributed learning in health care.","Zerka, F.
 and Barakat, S.
 and Walsh, S.
 and Bogowicz, M.
 and Leijenaar, R. T. H.
 and Jochems, A.
 and Miraglio, B.
 and Townend, D.
 and Lambin, P.","Zerka, Barakat, Walsh, Bogowicz, Leijenaar, Jochems, Miraglio, Townend, Lambin",https://dx.doi.org/10.1200/CCI.19.00047,https://doi.org/10.1200/CCI.19.00047,2020-11-02
4190.0,,pubmed,Artificial intelligence and convolution neural networks assessing mammographic images: a narrative literature review,Artificial intelligence and convolution neural networks assessing mammographic images: a narrative literature review,"Studies have shown that the use of artificial intelligence can reduce errors in medical image assessment. The diagnosis of breast cancer is an essential task; however, diagnosis can include 'detection' and 'interpretation' errors. Studies to reduce these errors have shown the feasibility of using convolution neural networks (CNNs). This narrative review presents recent studies in diagnosing mammographic malignancy investigating the accuracy and reliability of these CNNs. Databases including ScienceDirect, PubMed, MEDLINE, British Medical Journal and Medscape were searched using the terms 'convolutional neural network or artificial intelligence', 'breast neoplasms [MeSH] or breast cancer or breast carcinoma' and 'mammography [MeSH Terms]'. Articles collected were screened under the inclusion and exclusion criteria, accounting for the publication date and exclusive use of mammography images, and included only literature in English. After extracting data, results were compared and discussed. This review included 33 studies and identified four recurring categories of studies: the differentiation of benign and malignant masses, the localisation of masses, cancer-containing and cancer-free breast tissue differentiation and breast classification based on breast density. CNN's application in detecting malignancy in mammography appears promising but requires further standardised investigations before potentially becoming an integral part of the diagnostic routine in mammography.","Studies have shown that the use of artificial intelligence can reduce errors in medical image assessment. The diagnosis of breast cancer is an essential task; however, diagnosis can include 'detection' and 'interpretation' errors. Studies to reduce these errors have shown the feasibility of using convolution neural networks (CNNs). This narrative review presents recent studies in diagnosing mammographic malignancy investigating the accuracy and reliability of these CNNs. Databases including ScienceDirect, PubMed, MEDLINE, British Medical Journal and Medscape were searched using the terms 'convolutional neural network or artificial intelligence', 'breast neoplasms [MeSH] or breast cancer or breast carcinoma' and 'mammography [MeSH Terms]'. Articles collected were screened under the inclusion and exclusion criteria, accounting for the publication date and exclusive use of mammography images, and included only literature in English. After extracting data, results were compared and discussed. This review included 33 studies and identified four recurring categories of studies: the differentiation of benign and malignant masses, the localisation of masses, cancer-containing and cancer-free breast tissue differentiation and breast classification based on breast density. CNN's application in detecting malignancy in mammography appears promising but requires further standardised investigations before potentially becoming an integral part of the diagnostic routine in mammography.","Wong, D. J.
 and Gandomkar, Z.
 and Wu, W. J.
 and Zhang, G.
 and Gao, W.
 and He, X.
 and Wang, Y.
 and Reed, W.","Wong, Gandomkar, Wu, Zhang, Gao, He, Wang, Reed",https://dx.doi.org/10.1002/jmrs.385,https://doi.org/10.1002/jmrs.385,2020-11-02
4185.0,,pubmed,The Economic Impact of Artificial Intelligence in Health Care: Systematic Review,The Economic Impact of Artificial Intelligence in Health Care: Systematic Review,"BACKGROUND: Positive economic impact is a key decision factor in making the case for or against investing in an artificial intelligence (AI) solution in the health care industry. It is most relevant for the care provider and insurer as well as for the pharmaceutical and medical technology sectors. Although the broad economic impact of digital health solutions in general has been assessed many times in literature and the benefit for patients and society has also been analyzed, the specific economic impact of AI in health care has been addressed only sporadically. OBJECTIVE: This study aimed to systematically review and summarize the cost-effectiveness studies dedicated to AI in health care and to assess whether they meet the established quality criteria. METHODS: In a first step, the quality criteria for economic impact studies were defined based on the established and adapted criteria schemes for cost impact assessments. In a second step, a systematic literature review based on qualitative and quantitative inclusion and exclusion criteria was conducted to identify relevant publications for an in-depth analysis of the economic impact assessment. In a final step, the quality of the identified economic impact studies was evaluated based on the defined quality criteria for cost-effectiveness studies. RESULTS: Very few publications have thoroughly addressed the economic impact assessment, and the economic assessment quality of the reviewed publications on AI shows severe methodological deficits. Only 6 out of 66 publications could be included in the second step of the analysis based on the inclusion criteria. Out of these 6 studies, none comprised a methodologically complete cost impact analysis. There are two areas for improvement in future studies. First, the initial investment and operational costs for the AI infrastructure and service need to be included. Second, alternatives to achieve similar impact must be evaluated to provide a comprehensive comparison. CONCLUSIONS: This systematic literature analysis proved that the existing impact assessments show methodological deficits and that upcoming evaluations require more comprehensive economic analyses to enable economic decisions for or against implementing AI technology in health care.","Positive economic impact is a key decision factor in making the case for or against investing in an artificial intelligence (AI) solution in the health care industry. It is most relevant for the care provider and insurer as well as for the pharmaceutical and medical technology sectors. Although the broad economic impact of digital health solutions in general has been assessed many times in literature and the benefit for patients and society has also been analyzed, the specific economic impact of AI in health care has been addressed only sporadically. This study aimed to systematically review and summarize the cost-effectiveness studies dedicated to AI in health care and to assess whether they meet the established quality criteria. In a first step, the quality criteria for economic impact studies were defined based on the established and adapted criteria schemes for cost impact assessments. In a second step, a systematic literature review based on qualitative and quantitative inclusion and exclusion criteria was conducted to identify relevant publications for an in-depth analysis of the economic impact assessment. In a final step, the quality of the identified economic impact studies was evaluated based on the defined quality criteria for cost-effectiveness studies. Very few publications have thoroughly addressed the economic impact assessment, and the economic assessment quality of the reviewed publications on AI shows severe methodological deficits. Only 6 out of 66 publications could be included in the second step of the analysis based on the inclusion criteria. Out of these 6 studies, none comprised a methodologically complete cost impact analysis. There are two areas for improvement in future studies. First, the initial investment and operational costs for the AI infrastructure and service need to be included. Second, alternatives to achieve similar impact must be evaluated to provide a comprehensive comparison. This systematic literature analysis proved that the existing impact assessments show methodological deficits and that upcoming evaluations require more comprehensive economic analyses to enable economic decisions for or against implementing AI technology in health care.","Wolff, J.
 and Pauling, J.
 and Keck, A.
 and Baumbach, J.","Wolff, Pauling, Keck, Baumbach",https://dx.doi.org/10.2196/16866,https://doi.org/10.2196/16866,2020-11-02
2267.0,,pubmed,Automated assessment of psychiatric disorders using speech: A systematic review,Automated assessment of psychiatric disorders using speech: A systematic review,"Objective: There are many barriers to accessing mental health assessments including cost and stigma. Even when individuals receive professional care, assessments are intermittent and may be limited partly due to the episodic nature of psychiatric symptoms. Therefore, machine-learning technology using speech samples obtained in the clinic or remotely could one day be a biomarker to improve diagnosis and treatment. To date, reviews have only focused on using acoustic features from speech to detect depression and schizophrenia. Here, we present the first systematic review of studies using speech for automated assessments across a broader range of psychiatric disorders. Methods: We followed the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines. We included studies from the last 10 years using speech to identify the presence or severity of disorders within the Diagnostic and Statistical Manual of Mental Disorders (DSM-5). For each study, we describe sample size, clinical evaluation method, speech-eliciting tasks, machine learning methodology, performance, and other relevant findings. Results: 1395 studies were screened of which 127 studies met the inclusion criteria. The majority of studies were on depression, schizophrenia, and bipolar disorder, and the remaining on post-traumatic stress disorder, anxiety disorders, and eating disorders. 63% of studies built machine learning predictive models, and the remaining 37% performed null-hypothesis testing only. We provide an online database with our search results and synthesize how acoustic features appear in each disorder. Conclusion: Speech processing technology could aid mental health assessments, but there are many obstacles to overcome, especially the need for comprehensive transdiagnostic and longitudinal studies. Given the diverse types of data sets, feature extraction, computational methodologies, and evaluation criteria, we provide guidelines for both acquiring data and building machine learning models with a focus on testing hypotheses, open science, reproducibility, and generalizability. Level of Evidence: 3a.","There are many barriers to accessing mental health assessments including cost and stigma. Even when individuals receive professional care, assessments are intermittent and may be limited partly due to the episodic nature of psychiatric symptoms. Therefore, machine-learning technology using speech samples obtained in the clinic or remotely could one day be a biomarker to improve diagnosis and treatment. To date, reviews have only focused on using acoustic features from speech to detect depression and schizophrenia. Here, we present the first systematic review of studies using speech for automated assessments across a broader range of psychiatric disorders. We followed the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines. We included studies from the last 10â€‰years using speech to identify the presence or severity of disorders within the Diagnostic and Statistical Manual of Mental Disorders (DSM-5). For each study, we describe sample size, clinical evaluation method, speech-eliciting tasks, machine learning methodology, performance, and other relevant findings. 1395 studies were screened of which 127 studies met the inclusion criteria. The majority of studies were on depression, schizophrenia, and bipolar disorder, and the remaining on post-traumatic stress disorder, anxiety disorders, and eating disorders. 63% of studies built machine learning predictive models, and the remaining 37% performed null-hypothesis testing only. We provide an online database with our search results and synthesize how acoustic features appear in each disorder. Speech processing technology could aid mental health assessments, but there are many obstacles to overcome, especially the need for comprehensive transdiagnostic and longitudinal studies. Given the diverse types of dataâ€‰sets, feature extraction, computational methodologies, and evaluation criteria, we provide guidelines for both acquiring data and building machine learning models with a focus on testing hypotheses, open science, reproducibility, and generalizability. 3a.","Low, D. M.
 and Bentley, K. H.
 and Ghosh, S. S.","Low, Bentley, Ghosh",https://dx.doi.org/10.1002/lio2.354,https://doi.org/10.1002/lio2.354,2020-11-02
947.0,,pubmed,Automated recognition of functional compound-protein relationships in literature,Automated recognition of functional compound-protein relationships in literature,"MOTIVATION: Much effort has been invested in the identification of protein-protein interactions using text mining and machine learning methods. The extraction of functional relationships between chemical compounds and proteins from literature has received much less attention, and no ready-to-use open-source software is so far available for this task. METHOD: We created a new benchmark dataset of 2,613 sentences from abstracts containing annotations of proteins, small molecules, and their relationships. Two kernel methods were applied to classify these relationships as functional or non-functional, named shallow linguistic and all-paths graph kernel. Furthermore, the benefit of interaction verbs in sentences was evaluated. RESULTS: The cross-validation of the all-paths graph kernel (AUC value: 84.6%, F1 score: 79.0%) shows slightly better results than the shallow linguistic kernel (AUC value: 82.5%, F1 score: 77.2%) on our benchmark dataset. Both models achieve state-of-the-art performance in the research area of relation extraction. Furthermore, the combination of shallow linguistic and all-paths graph kernel could further increase the overall performance slightly. We used each of the two kernels to identify functional relationships in all PubMed abstracts (29 million) and provide the results, including recorded processing time. AVAILABILITY: The software for the tested kernels, the benchmark, the processed 29 million PubMed abstracts, all evaluation scripts, as well as the scripts for processing the complete PubMed database are freely available at https://github.com/KerstenDoering/CPI-Pipeline.","Much effort has been invested in the identification of protein-protein interactions using text mining and machine learning methods. The extraction of functional relationships between chemical compounds and proteins from literature has received much less attention, and no ready-to-use open-source software is so far available for this task. We created a new benchmark dataset of 2,613 sentences from abstracts containing annotations of proteins, small molecules, and their relationships. Two kernel methods were applied to classify these relationships as functional or non-functional, named shallow linguistic and all-paths graph kernel. Furthermore, the benefit of interaction verbs in sentences was evaluated. The cross-validation of the all-paths graph kernel (AUC value: 84.6%, F1 score: 79.0%) shows slightly better results than the shallow linguistic kernel (AUC value: 82.5%, F1 score: 77.2%) on our benchmark dataset. Both models achieve state-of-the-art performance in the research area of relation extraction. Furthermore, the combination of shallow linguistic and all-paths graph kernel could further increase the overall performance slightly. We used each of the two kernels to identify functional relationships in all PubMed abstracts (29 million) and provide the results, including recorded processing time. The software for the tested kernels, the benchmark, the processed 29 million PubMed abstracts, all evaluation scripts, as well as the scripts for processing the complete PubMed database are freely available at https://github.com/KerstenDoering/CPI-Pipeline.","Doring, K.
 and Qaseem, A.
 and Becer, M.
 and Li, J.
 and Mishra, P.
 and Gao, M.
 and Kirchner, P.
 and Sauter, F.
 and Telukunta, K. K.
 and Moumbock, A. F. A.
 and Thomas, P.
 and Gunther, S.","DÃ¶ring, Qaseem, Becer, Li, Mishra, Gao, Kirchner, Sauter, Telukunta, Moumbock, Thomas, GÃ¼nther",https://dx.doi.org/10.1371/journal.pone.0220925,https://doi.org/10.1371/journal.pone.0220925,2020-11-02
873.0,,pubmed,Identifying Drugs Inducing Prematurity by Mining Claims Data with High-Dimensional Confounder Score Strategies,Identifying Drugs Inducing Prematurity by Mining Claims Data with High-Dimensional Confounder Score Strategies,"BACKGROUND: Pregnant women are largely exposed to medications. However, knowledge is lacking about their effects on pregnancy and the fetus. OBJECTIVE: This study sought to evaluate the potential of high-dimensional propensity scores and high-dimensional disease risk scores for automated signal detection in pregnant women from medico-administrative databases in the context of drug-induced prematurity. METHODS: We used healthcare claims and hospitalization discharges of a 1/97th representative sample of the French population. We tested the association between prematurity and drug exposure during the trimester before delivery, for all drugs prescribed to at least five pregnancies. We compared different strategies (1) for building the two scores, including two machine-learning methods and (2) to account for these scores in the final logistic regression models: adjustment, weighting, and matching. We also proposed a new signal detection criterion derived from these scores: the p value relative decrease. Evaluation was performed by assessing the relevance of the signals using a literature review and clinical expertise. RESULTS: Screening 400 drugs from a cohort of 57,407 pregnancies, we observed that choosing between the two machine-learning methods had little impact on the generated signals. Score adjustment performed better than weighting and matching. Using the p value relative decrease efficiently filtered out spurious signals while maintaining a number of relevant signals similar to score adjustment. Most of the relevant signals belonged to the psychotropic class with benzodiazepines, antidepressants, and antipsychotics. CONCLUSIONS: Mining complex healthcare databases with statistical methods from the high-dimensional inference field may improve signal detection in pregnant women.","Pregnant women are largely exposed to medications. However, knowledge is lacking about their effects on pregnancy and the fetus. This study sought to evaluate the potential of high-dimensional propensity scores and high-dimensional disease risk scores for automated signal detection in pregnant women from medico-administrative databases in the context of drug-induced prematurity. We used healthcare claims and hospitalization discharges of a 1/97th representative sample of the French population. We tested the association between prematurity and drug exposure during the trimester before delivery, for all drugs prescribed to at least five pregnancies. We compared different strategies (1) for building the two scores, including two machine-learning methods and (2) to account for these scores in the final logistic regression models: adjustment, weighting, and matching. We also proposed a new signal detection criterion derived from these scores: the p value relative decrease. Evaluation was performed by assessing the relevance of the signals using a literature review and clinical expertise. Screening 400 drugs from a cohort of 57,407 pregnancies, we observed that choosing between the two machine-learning methods had little impact on the generated signals. Score adjustment performed better than weighting and matching. Using the p value relative decrease efficiently filtered out spurious signals while maintaining a number of relevant signals similar to score adjustment. Most of the relevant signals belonged to the psychotropic class with benzodiazepines, antidepressants, and antipsychotics. Mining complex healthcare databases with statistical methods from the high-dimensional inference field may improve signal detection in pregnant women.","Demailly, R.
 and Escolano, S.
 and Haramburu, F.
 and Tubert-Bitter, P.
 and Ahmed, I.","Demailly, Escolano, Haramburu, Tubert-Bitter, Ahmed",https://dx.doi.org/10.1007/s40264-020-00916-5,https://doi.org/10.1007/s40264-020-00916-5,2020-11-02
3841.0,,pubmed,Global Mapping of Interventions to Improve Quality of Life of People with Diabetes in 1990-2018,Global Mapping of Interventions to Improve Quality of Life of People with Diabetes in 1990-2018,"Improving the quality of life (QOL) of people living with diabetes is the ultimate goal of diabetes care. This study provides a quantitative overview of global research on interventions aiming to improve QOL among people with diabetes. A total of 700 English peer-reviewed papers published during 1990-2018 were collected and extracted from the Web of Science databases. Latent Dirichlet Allocation (LDA) analysis was undertaken to categorize papers by topic or theme. Results showed an increase in interventions to improve the QOL of patients with diabetes across the time period, with major contributions from high-income countries. Community- and family-based interventions, including those focused on lifestyle and utilizing digital technologies, were common approaches. Interventions that addressed comorbidities in people with diabetes also increased. Our findings emphasize the necessity of translating the evidence from clinical interventions to community interventions. In addition, they underline the importance of developing collaborative research between developed and developing countries.","Improving the quality of life (QOL) of people living with diabetes is the ultimate goal of diabetes care. This study provides a quantitative overview of global research on interventions aiming to improve QOL among people with diabetes. A total of 700 English peer-reviewed papers published during 1990-2018 were collected and extracted from the Web of Science databases. Latent Dirichlet Allocation (LDA) analysis was undertaken to categorize papers by topic or theme. Results showed an increase in interventions to improve the QOL of patients with diabetes across the time period, with major contributions from high-income countries. Community- and family-based interventions, including those focused on lifestyle and utilizing digital technologies, were common approaches. Interventions that addressed comorbidities in people with diabetes also increased. Our findings emphasize the necessity of translating the evidence from clinical interventions to community interventions. In addition, they underline the importance of developing collaborative research between developed and developing countries.","Tran, B. X.
 and Nguyen, L. H.
 and Pham, N. M.
 and Vu, H. T. T.
 and Nguyen, H. T.
 and Phan, D. H.
 and Ha, G. H.
 and Pham, H. Q.
 and Nguyen, T. P.
 and Latkin, C. A.
 and Ho, C. S. H.
 and Ho, R. C. M.","Tran, Nguyen, Pham, Vu, Nguyen, Phan, Ha, Pham, Nguyen, Latkin, Ho, Ho",https://dx.doi.org/10.3390/ijerph17051597,https://doi.org/10.3390/ijerph17051597,2020-11-02
2295.0,,pubmed,Accuracy of Artificial Intelligence on Histology Prediction and Detection of Colorectal Polyps: A Systematic Review and Meta-Analysis,Accuracy of artificial intelligence on histology prediction andÂ detection of colorectal polyps: a systematic review andÂ meta-analysis,"BACKGROUND AND AIMS: We perform a meta-analysis of all published studies to determine the diagnostic accuracy of AI on histology prediction and detection of colorectal polyps. METHOD: We searched Embase, PubMed, Medline, Web of Science and Cochrane library databases to identify studies using AI for colorectal polyp histology prediction and detection. The quality of the included studies was measured by the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) tool. We used a bivariate meta-analysis following a random effects model to summarize the data and plotted hierarchical summary receiver-operating characteristic (HSROC) curves. The area under the HSROC curve (AUC) served as an indicator of the diagnostic accuracy and during head-to-head comparison. RESULT: A total of 7,680 images of colorectal polyps from 18 studies were included in the analysis of histology prediction. The accuracy of the AI (AUC) was 0.96 (95% CI, 0.95-0.98), with corresponding pooled sensitivity of 92.3% (95% CI, 88.8%-94.9%) and specificity of 89.8% (95% CI, 85.3%-93.0%). The AUC of AI using narrow-band imaging (NBI) was significantly higher than non-NBI (0.98 vs 0.84, p<0.01). The performance of AI was superior to nonexpert endoscopists (0.97 vs 0.90, p<0.01). For characterization of diminutive polyps using deep learning model with non-magnifying NBI, the pooled negative predictive value was 95.1% (95% CI, 87.7%-98.1%). For polyp detection, the pooled AUC was 0.90 (95% CI, 0.67-1.00) with sensitivity of 95.0% (95% CI, 91.0%-97.0%) and specificity of 88.0% (95% CI, 58.0%-99.0%). CONCLUSION: AI was accurate in histology prediction and detection of colorectal polyps, including diminutive polyps. The performance of AI was better under NBI and was superior to non-expert endoscopists. Despite the difference in AI models and study designs, the AI performances are rather consistent, which could serve a reference for future AI studies.","We performed a meta-analysis of all published studies to determine the diagnostic accuracy of artificial intelligence (AI) on histology prediction and detection of colorectal polyps. We searched Embase, PubMed, Medline, Web of Science, and Cochrane library databases to identify studies using AI for colorectal polyp histology prediction and detection. The quality of included studies was measured by the Quality Assessment of Diagnostic Accuracy Studies tool. We used a bivariate meta-analysis following a random-effects model to summarize the data and plotted hierarchical summary receiver operating characteristic curves. The area under the hierarchical summary receiver operating characteristic curve (AUC) served as an indicator of the diagnostic accuracy and during head-to-head comparisons. A total of 7680 images of colorectal polyps from 18 studies were included in the analysis of histology prediction. The accuracy of the AI (AUC) was .96 (95% confidence interval [CI], .95-.98), with a corresponding pooled sensitivity of 92.3% (95% CI, 88.8%-94.9%) and specificity of 89.8% (95% CI, 85.3%-93.0%). The AUC of AI using narrow-band imaging (NBI) was significantly higher than the AUC using non-NBI (.98 vs .84, PÂ &lt; .01). The performance of AI was superior to nonexpert endoscopists (.97 vs .90, PÂ &lt; .01). For characterization of diminutive polyps using a deep learning model with nonmagnifying NBI, the pooled negative predictive value was 95.1% (95% CI, 87.7%-98.1%). For polyp detection, the pooled AUC was .90 (95% CI, .67-1.00) with a sensitivity of 95.0% (95% CI, 91.0%-97.0%) and a specificity of 88.0% (95% CI, 58.0%-99.0%). AI was accurate in histology prediction and detection of colorectal polyps, including diminutive polyps. The performance of AI was better under NBI and was superior to nonexpert endoscopists. Despite the difference in AI models and study designs, AI performances are rather consistent, which could serve as a reference for future AI studies.","Lui, T. K.
 and Guo, C. G.
 and Leung, W. K.","Lui, Guo, Leung",https://dx.doi.org/10.1016/j.gie.2020.02.033,https://doi.org/10.1016/j.gie.2020.02.033,2020-11-02
4070.0,,pubmed,Acupoint catgut embedding for insomnia: Protocol for a systematic review and data mining,Acupoint catgut embedding for insomnia: Protocol for a systematic review and data mining,"BACKGROUND: Insomnia is a common sleep disorder characterized by chronically disturbed sleep or loss of sleep, and even cognitive dysfunction. Acupoint catgut embedding is widely used to treat sleep disorders. However, there is no systematic review and data mining of the effectiveness and potential acupoints prescription of acupoint catgut embedding for insomnia. METHODS: Randomized controlled trials (RCTs) from the Web of Science, PubMed, Cochrane Library, Springer, Wanfang database, China National Knowledge Infrastructure, VIP Chinese Science and Technology Journals Database, and 2 clinical trial registration center will be included. The search time will be established from each database to December 30, 2019. The outcome measures will be Pittsburgh sleep quality index (PSQI), clinical effective rate, International Unified Sleep Efficiency Value (IUSEV) and adverse events. Data from RCTs that meets the inclusion criteria will be analyzed through RevMan V.5.3 software. Risk of bias and publication bias will be analyzed to identify the quality of the included studies. Besides, Traditional Chinese Medicine inheritance support system (TCMISS) will be used to analyze the potential acupoints prescriptions. RESULTS: This study will clarify PSQI, IUSEV, clinical effective rate, adverse events, and potential acupoint prescriptions of acupoint catgut embedding for patients with insomnia. CONCLUSION: Our study will provide evidence of acupoint catgut embedding for insomnia, which may be beneficial to practitioners in the field of non-pharmacological interventions.PROSPERO registration number: CRD42019144636.","Insomnia is a common sleep disorder characterized by chronically disturbed sleep or loss of sleep, and even cognitive dysfunction. Acupoint catgut embedding is widely used to treat sleep disorders. However, there is no systematic review and data mining of the effectiveness and potential acupoints prescription of acupoint catgut embedding for insomnia. Randomized controlled trials (RCTs) from the Web of Science, PubMed, Cochrane Library, Springer, Wanfang database, China National Knowledge Infrastructure, VIP Chinese Science and Technology Journals Database, and 2 clinical trial registration center will be included. The search time will be established from each database to December 30, 2019. The outcome measures will be Pittsburgh sleep quality index (PSQI), clinical effective rate, International Unified Sleep Efficiency Value (IUSEV) and adverse events. Data from RCTs that meets the inclusion criteria will be analyzed through RevMan V.5.3 software. Risk of bias and publication bias will be analyzed to identify the quality of the included studies. Besides, Traditional Chinese Medicine inheritance support system (TCMISS) will be used to analyze the potential acupoints prescriptions. This study will clarify PSQI, IUSEV, clinical effective rate, adverse events, and potential acupoint prescriptions of acupoint catgut embedding for patients with insomnia. Our study will provide evidence of acupoint catgut embedding for insomnia, which may be beneficial to practitioners in the field of non-pharmacological interventions.PROSPERO registration number: CRD42019144636.","Wang, X.
 and Huang, Y.
 and Li, M.
 and Lin, H.
 and Lin, C.
 and Yang, W.
 and Ye, X.","Wang, Huang, Li, Lin, Lin, Yang, Ye",https://dx.doi.org/10.1097/MD.0000000000019333,https://doi.org/10.1097/MD.0000000000019333,2020-11-02
4182.0,,pubmed,Effects of medical and non-medical cannabis use in older adults: protocol for a scoping review,Effects of medical and non-medical cannabis use in older adults: protocol for a scoping review,"INTRODUCTION: With its legalisation and regulation in Canada in 2018, the proportion of Canadians reporting cannabis use in 2019 increased substantially over the previous year, with half of new users being aged 45+ years. While use in older adults has been low historically, as those born in the 1950s and 1960s continue to age, this demographic will progressively have more liberal attitudes, prior cannabis exposure and higher use rates. However, older adults experience slower metabolism, increased likelihood of polypharmacy, cognitive decline and chronic physical/mental health problems. There is a need to enhance knowledge of the effects of cannabis use in older adults. The following question will be addressed using a scoping review approach: what evidence exists regarding beneficial and harmful effects of medical and non-medical cannabis use in adults >50 years of age? Given that beneficial and harmful effects of cannabis may be mediated by patient-level (eg, age, sex and race) and cannabis-related factors (eg, natural vs synthetic, consumption method), subgroup effects related to these and additional factors will be explored. METHODS AND ANALYSIS: Methods for scoping reviews outlined by Arksey & O'Malley and the Joanna Briggs Institute will be used. A librarian designed a systematic search of the literature from database inception to June 2019. Using the OVID platform, Ovid MEDLINE will be searched, including Epub Ahead of Print and In-Process and Other Non-Indexed Citations, Embase Classic+Embase, and PsycINFO for reviews, randomised trials, non-randomised trials and observational studies of cannabis use. The Cochrane Library on Wiley will also be searched. Eligibility criteria will be older adult participants, currently using cannabis (medical or non-medical), with studies required to report a cannabis-related health outcome to be eligible. Two reviewers will screen citations and full texts, with support from artificial intelligence. Two reviewers will chart data. Tables/graphics will be used to map evidence and identify evidence gaps. ETHICS AND DISSEMINATION: This research will enhance awareness of existing evidence addressing the health effects of medical and non-medical cannabis use in older adults. Findings will be disseminated through a peer-reviewed publication, conference presentations and a stakeholder meeting. Trial registration number: doi 10.17605/osf.io/5jtaq.","With its legalisation and regulation in Canada in 2018, the proportion of Canadians reporting cannabis use in 2019 increased substantially over the previous year, with half of new users being aged 45+ years. While use in older adults has been low historically, as those born in the 1950s and 1960s continue to age, this demographic will progressively have more liberal attitudes, prior cannabis exposure and higher use rates. However, older adults experience slower metabolism, increased likelihood of polypharmacy, cognitive decline and chronic physical/mental health problems. There is a need to enhance knowledge of the effects of cannabis use in older adults. The following question will be addressed using a scoping review approach: what evidence exists regarding beneficial and harmful effects of medical and non-medical cannabis use in adults &gt;50 years of age? Given that beneficial and harmful effects of cannabis may be mediated by patient-level (eg, age, sex and race) and cannabis-related factors (eg, natural vs synthetic, consumption method), subgroup effects related to these and additional factors will be explored. Methods for scoping reviews outlined by Arksey &amp; O'Malley and the Joanna Briggs Institute will be used. A librarian designed a systematic search of the literature from database inception to June 2019. Using the OVID platform, Ovid MEDLINE will be searched, including Epub Ahead of Print and In-Process and Other Non-Indexed Citations, Embase Classic+Embase, and PsycINFO for reviews, randomised trials, non-randomised trials and observational studies of cannabis use. The Cochrane Library on Wiley will also be searched. Eligibility criteria will be older adult participants, currently using cannabis (medical or non-medical), with studies required to report a cannabis-related health outcome to be eligible. Two reviewers will screen citations and full texts, with support from artificial intelligence. Two reviewers will chart data. Tables/graphics will be used to map evidence and identify evidence gaps. This research will enhance awareness of existing evidence addressing the health effects of medical and non-medical cannabis use in older adults. Findings will be disseminated through a peer-reviewed publication, conference presentations and a stakeholder meeting. DOI 10.17605/OSF.IO/5JTAQ.","Wolfe, D.
 and Corace, K.
 and Rice, D.
 and Smith, A.
 and Kanji, S.
 and Conn, D.
 and Willows, M.
 and Garber, G. E.
 and Puxty, J.
 and Moghadam, E.
 and Skidmore, B.
 and Garritty, C.
 and Thavorn, K.
 and Moher, D.
 and Hutton, B.","Wolfe, Corace, Rice, Smith, Kanji, Conn, Willows, Garber, Puxty, Moghadam, Skidmore, Garritty, Thavorn, Moher, Hutton",https://dx.doi.org/10.1136/bmjopen-2019-034301,https://doi.org/10.1136/bmjopen-2019-034301,2020-11-02
1200.0,,pubmed,Electronic health records for the diagnosis of rare diseases,Electronic health records for the diagnosis of rare diseases,"With the emergence of electronic health records, the reuse of clinical data offers new perspectives for the diagnosis and management of patients with rare diseases. However, there are many obstacles to the repurposing of clinical data. The development of decision support systems depends on the ability to recruit patients, extract and integrate the patients' data, mine and stratify these data, and integrate the decision support algorithm into patient care. This last step requires an adaptability of the electronic health records to integrate learning health system tools. In this literature review, we examine the research that provides solutions to unlock these barriers and accelerate translational research: structured electronic health records and free-text search engines to find patients, data warehouses and natural language processing to extract phenotypes, machine learning algorithms to classify patients, and similarity metrics to diagnose patients. Medical informatics is experiencing an impellent request to develop decision support systems, and this requires ethical considerations for clinicians and patients to ensure appropriate use of health data.","With the emergence of electronic health records, the reuse of clinical data offers new perspectives for the diagnosis and management of patients with rare diseases. However, there are many obstacles to the repurposing of clinical data. The development of decision support systems depends on the ability to recruit patients, extract and integrate the patients' data, mine and stratify these data, and integrate the decision support algorithm into patient care. This last step requires an adaptability of the electronic health records to integrate learning health system tools. In this literature review, we examine the research that provides solutions to unlock these barriers and accelerate translational research: structured electronic health records and free-text search engines to find patients, data warehouses and natural language processing to extract phenotypes, machine learning algorithms to classify patients, and similarity metrics to diagnose patients. Medical informatics is experiencing an impellent request to develop decision support systems, and this requires ethical considerations for clinicians and patients to ensure appropriate use of health data.","Garcelon, N.
 and Burgun, A.
 and Salomon, R.
 and Neuraz, A.","Garcelon, Burgun, Salomon, Neuraz",https://dx.doi.org/10.1016/j.kint.2019.11.037,https://doi.org/10.1016/j.kint.2019.11.037,2020-11-02
3346.0,,pubmed,Systematic review of descriptions of novel bacterial species: evaluation of the twenty-first century taxonomy through text mining,Systematic review of descriptions of novel bacterial species: evaluation of the twenty-first century taxonomy through text mining,"Although described bacterial species increased in the twenty-first century, they correspond to a tiny fraction of the actual number of species living on our planet. The volume of textual data of these descriptions constitutes valuable information for revealing trends that in turn could support strategies for improvement of bacterial taxonomy. In this study, a text mining approach was used to generate bibliometric data to verify the state-of-art of bacterial taxonomy. Around 9700 abstracts of bacterial classification containing the expression 'sp. nov.' and published between 2001 and 2018 were downloaded from PubMed and analysed. Most articles were from PR China and the Republic of Korea, and published in the International Journal of Systematic and Evolutionary Microbiology. From about 10 800 species names detected, 93.33 % were considered valid according to the rules of the Bacterial Code, and they corresponded to 82.98 % of the total number of species validated between 2001 and 2018. Streptomyces, Bacillus and Paenibacillus each had more than 200 species described in the period. However, almost 40 % of all species were from the phylum Proteobacteria. Most bacteria were Gram-stain-negative, bacilli and isolated from soil. Thirteen species and one genus homonyms were found. With respect to methodologies of bacterial characterization, the use of terms related to 16S rRNA and polar lipids increased along these years, and terms related to genome metrics only began to appear from 2009 onward, although at a relatively lower frequency. Bacterial taxonomy is known as a conservative discipline, but it gradually changed in terms of players and practices. With the advent of the mandatory use of genomic analyses for species description, we are probably witnessing a turning point in the evolution of bacterial taxonomy.","Although described bacterial species increased in the twenty-first century, they correspond to a tiny fraction of the actual number of species living on our planet. The volume of textual data of these descriptions constitutes valuable information for revealing trends that in turn could support strategies for improvement of bacterial taxonomy. In this study, a text mining approach was used to generate bibliometric data to verify the state-of-art of bacterial taxonomy. Around 9700 abstracts of bacterial classification containing the expression 'sp. nov.' and published between 2001 and 2018 were downloaded from PubMed and analysed. Most articles were from PR China and the Republic of Korea, and published in the <i>International Journal of Systematic and Evolutionary Microbiology</i>. From about 10â€Š800 species names detected, 93.33â€Š% were considered valid according to the rules of the Bacterial Code, and they corresponded to 82.98â€Š% of the total number of species validated between 2001 and 2018. <i>Streptomyces</i>, <i>Bacillus</i> and <i>Paenibacillus</i> each had more than 200 species described in the period. However, almost 40â€Š% of all species were from the phylum <i>Proteobacteria</i>. Most bacteria were Gram-stain-negative, bacilli and isolated from soil. Thirteen species and one genus homonyms were found. With respect to methodologies of bacterial characterization, the use of terms related to 16S rRNA and polar lipids increased along these years, and terms related to genome metrics only began to appear from 2009 onward, although at a relatively lower frequency. Bacterial taxonomy is known as a conservative discipline, but it gradually changed in terms of players and practices. With the advent of the mandatory use of genomic analyses for species description, we are probably witnessing a turning point in the evolution of bacterial taxonomy.","Sant'Anna, F. H.
 and Reiter, K. C.
 and Fatima Almeida, P.
 and Pereira Passaglia, L. M.","Sant'Anna, Reiter, FÃ¡tima Almeida, Pereira Passaglia",https://dx.doi.org/10.1099/ijsem.0.004070,https://doi.org/10.1099/ijsem.0.004070,2020-11-02
3097.0,,pubmed,Artificial intelligence in abdominal aortic aneurysm,Artificial intelligence in abdominal aortic aneurysm,"OBJECTIVE: Abdominal aortic aneurysm (AAA) is a life-threatening disease, and the only curative treatment relies on open or endovascular repair. The decision to treat relies on the evaluation of the risk of AAA growth and rupture, which can be difficult to assess in practice. Artificial intelligence (AI) has revealed new insights into the management of cardiovascular diseases, but its application in AAA has so far been poorly described. The aim of this review was to summarize the current knowledge on the potential applications of AI in patients with AAA. METHODS: A comprehensive literature review was performed. The MEDLINE database was searched according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. The search strategy used a combination of keywords and included studies using AI in patients with AAA published between May 2019 and January 2000. Two authors independently screened titles and abstracts and performed data extraction. The search of published literature identified 34 studies with distinct methodologies, aims, and study designs. RESULTS: AI was used in patients with AAA to improve image segmentation and for quantitative analysis and characterization of AAA morphology, geometry, and fluid dynamics. AI allowed computation of large data sets to identify patterns that may be predictive of AAA growth and rupture. Several predictive and prognostic programs were also developed to assess patients' postoperative outcomes, including mortality and complications after endovascular aneurysm repair. CONCLUSIONS: AI represents a useful tool in the interpretation and analysis of AAA imaging by enabling automatic quantitative measurements and morphologic characterization. It could be used to help surgeons in preoperative planning. AI-driven data management may lead to the development of computational programs for the prediction of AAA evolution and risk of rupture as well as postoperative outcomes. AI could also be used to better evaluate the indications and types of surgical treatment and to plan the postoperative follow-up. AI represents an attractive tool for decision-making and may facilitate development of personalized therapeutic approaches for patients with AAA.","Abdominal aortic aneurysm (AAA) is a life-threatening disease, and the only curative treatment relies on open or endovascular repair. The decision to treat relies on the evaluation of the risk of AAA growth and rupture, which can be difficult to assess in practice. Artificial intelligence (AI) has revealed new insights into the management of cardiovascular diseases, but its application in AAA has so far been poorly described. The aim of this review was to summarize the current knowledge on the potential applications of AI in patients with AAA. A comprehensive literature review was performed. The MEDLINE database was searched according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. The search strategy used a combination of keywords and included studies using AI in patients with AAA published between May 2019 and January 2000. Two authors independently screened titles and abstracts and performed data extraction. The search of published literature identified 34 studies with distinct methodologies, aims, and study designs. AI was used in patients with AAA to improve image segmentation and for quantitative analysis and characterization of AAA morphology, geometry, and fluid dynamics. AI allowed computation of large data sets to identify patterns that may be predictive of AAA growth and rupture. Several predictive and prognostic programs were also developed to assess patients' postoperative outcomes, including mortality and complications after endovascular aneurysm repair. AI represents a useful tool in the interpretation and analysis of AAA imaging by enabling automatic quantitative measurements and morphologic characterization. It could be used to help surgeons in preoperative planning. AI-driven data management may lead to the development of computational programs for the prediction of AAA evolution and risk of rupture as well as postoperative outcomes. AI could also be used to better evaluate the indications and types of surgical treatment and to plan the postoperative follow-up. AI represents an attractive tool for decision-making and may facilitate development of personalized therapeutic approaches for patients with AAA.","Raffort, J.
 and Adam, C.
 and Carrier, M.
 and Ballaith, A.
 and Coscas, R.
 and Jean-Baptiste, E.
 and Hassen-Khodja, R.
 and Chakfe, N.
 and Lareyre, F.","Raffort, Adam, Carrier, Ballaith, Coscas, Jean-Baptiste, Hassen-Khodja, ChakfÃ©, Lareyre",https://dx.doi.org/10.1016/j.jvs.2019.12.026,https://doi.org/10.1016/j.jvs.2019.12.026,2020-11-02
2054.0,,pubmed,Mobile Sensing in Substance Use Research: A Scoping Review,Mobile Sensing in Substance Use Research: A Scoping Review,"<b>Background:</b> Addictive disorders and substance use are significant health challenges worldwide, and relapse is a core component of addictive disorders. The dynamics surrounding relapse and especially the immediate period before it occurs is only partly understood, much due to difficulties collecting reliable and sufficient data from this narrow period. Mobile sensing has been an important way to improve data quality and enhance predictive capabilities for symptom worsening within physical and mental health care, but is less developed within substance use research. <b>Methodology: </b> This scoping review aimed to reviewing the currently available research on mobile sensing of substance use and relapse in substance use disorders. The search was conducted in January 2019 using PubMed and Web of Science. <b>Results:</b> Six articles were identified, all concerning subjects using alcohol. In the studies a range of mobile sensors and derived aggregated features were employed. Data collected through mobile sensing were predominantly used to make dichotomous inference on ongoing substance use or not and in some cases on the quantity of substance intake. Only one of the identified studies predicted later substance use. A range of statistical machine learning techniques was employed. <b>Conclusions:</b> The research on mobile sensing in this field remains scarce. The issues requiring further attention include more research on clinical populations in naturalistic settings, use of a priori knowledge in statistical modeling, focus on prediction of substance use rather than purely identification, and finally research on other substances than alcohol.","<b> <i>Background:</i> </b><i>Addictive disorders and substance use are significant health challenges worldwide, and relapse is a core component of addictive disorders. The dynamics surrounding relapse and especially the immediate period before it occurs is only partly understood, much due to difficulties collecting reliable and sufficient data from this narrow period. Mobile sensing has been an important way to improve data quality and enhance predictive capabilities for symptom worsening within physical and mental health care, but is less developed within substance use research.</i><b> <i>Methodology:</i> </b><i>This scoping review aimed to reviewing the currently available research on mobile sensing of substance use and relapse in substance use disorders. The search was conducted in January 2019 using PubMed and Web of Science.</i><b> <i>Results:</i> </b><i>Six articles were identified, all concerning subjects using alcohol. In the studies a range of mobile sensors and derived aggregated features were employed. Data collected through mobile sensing were predominantly used to make dichotomous inference on ongoing substance use or not and in some cases on the quantity of substance intake. Only one of the identified studies predicted later substance use. A range of statistical machine learning techniques was employed.</i><b> <i>Conclusions:</i> </b><i>The research on mobile sensing in this field remains scarce. The issues requiring further attention include more research on clinical populations in naturalistic settings, use of</i> a priori <i>knowledge in statistical modeling, focus on prediction of substance use rather than purely identification, and finally research on other substances than alcohol.</i>","Lauvsnes, A. D. F.
 and Langaas, M.
 and Toussaint, P.
 and Grawe, R. W.","Lauvsnes, Langaas, Toussaint, GrÃ¥we",https://dx.doi.org/10.1089/tmj.2019.0241,https://doi.org/10.1089/tmj.2019.0241,2020-11-02
3363.0,,pubmed,Re-examining physician-scientist training through the prism of the discovery-invention cycle,Re-examining physician-scientist training through the prism of the discovery-invention cycle,"The training of physician-scientists lies at the heart of future medical research. In this commentary, we apply Narayanamurti and Odumosu's framework of the 'discovery-invention cycle' to analyze the structure and outcomes of the integrated MD/PhD program. We argue that the linear model of 'bench-to-bedside' research, which is also reflected in the present training of MD/PhDs, merits continual re-evaluation to capitalize on the richness of opportunities arising in clinical medicine. In addition to measuring objective career outcomes, as existing research has done, we suggest that detailed characterization of researchers' efforts using both qualitative and quantitative techniques is necessary to understand if dual-degree training is being utilized. As an example, we propose that the application of machine learning and data science to corpora of biomedical literature and anonymized clinical data might allow us to see if there are objective 'signatures' of research uniquely enabled by MD/PhD training. We close by proposing several hypotheses for shaping physician-scientist training, the relative merits of which could be assessed using the techniques proposed above. Our overarching message is the importance of deeply understanding individual career trajectories as well as characterizing organizational details and cultural nuances to drive new policy which shapes the future of the physician-scientist workforce.","The training of physician-scientists lies at the heart of future medical research. In this commentary, we apply Narayanamurti and Odumosu's framework of the &quot;discovery-invention cycle&quot; to analyze the structure and outcomes of the integrated MD/PhD program. We argue that the linear model of &quot;bench-to-bedside&quot; research, which is also reflected in the present training of MD/PhDs, merits continual re-evaluation to capitalize on the richness of opportunities arising in clinical medicine. In addition to measuring objective career outcomes, as existing research has done, we suggest that detailed characterization of researchers' efforts using both qualitative and quantitative techniques is necessary to understand if dual-degree training is being utilized. As an example, we propose that the application of machine learning and data science to corpora of biomedical literature and anonymized clinical data might allow us to see if there are objective &quot;signatures&quot; of research uniquely enabled by MD/PhD training. We close by proposing several hypotheses for shaping physician-scientist training, the relative merits of which could be assessed using the techniques proposed above. Our overarching message is the importance of deeply understanding individual career trajectories as well as characterizing organizational details and cultural nuances to drive new policy which shapes the future of the physician-scientist workforce.","Sarma, G. P.
 and Levey, A.
 and Faundez, V.","Sarma, Levey, Faundez",https://dx.doi.org/10.12688/f1000research.21448.1,https://doi.org/10.12688/f1000research.21448.1,2020-11-02
1633.0,,pubmed,Deep learning algorithms for detection of diabetic retinopathy in retinal fundus photographs: A systematic review and meta-analysis,Deep learning algorithms for detection of diabetic retinopathy in retinal fundus photographs: A systematic review and meta-analysis,"BACKGROUND: Diabetic retinopathy (DR) is one of the leading causes of blindness globally. Earlier detection and timely treatment of DR are desirable to reduce the incidence and progression of vision loss. Currently, deep learning (DL) approaches have offered better performance in detecting DR from retinal fundus images. We, therefore, performed a systematic review with a meta-analysis of relevant studies to quantify the performance of DL algorithms for detecting DR. METHODS: A systematic literature search on EMBASE, PubMed, Google Scholar, Scopus was performed between January 1, 2000, and March 31, 2019. The search strategy was based on the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) reporting guidelines, and DL-based study design was mandatory for articles inclusion. Two independent authors screened abstracts and titles against inclusion and exclusion criteria. Data were extracted by two authors independently using a standard form and the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) tool was used for the risk of bias and applicability assessment. RESULTS: Twenty-three studies were included in the systematic review; 20 studies met inclusion criteria for the meta-analysis. The pooled area under the receiving operating curve (AUROC) of DR was 0.97 (95%CI: 0.95-0.98), sensitivity was 0.83 (95%CI: 0.83-0.83), and specificity was 0.92 (95%CI: 0.92-0.92). The positive- and negative-likelihood ratio were 14.11 (95%CI: 9.91-20.07), and 0.10 (95%CI: 0.07-0.16), respectively. Moreover, the diagnostic odds ratio for DL models was 136.83 (95%CI: 79.03-236.93). All the studies provided a DR-grading scale, a human grader (e.g. trained caregivers, ophthalmologists) as a reference standard. CONCLUSION: The findings of our study showed that DL algorithms had high sensitivity and specificity for detecting referable DR from retinal fundus photographs. Applying a DL-based automated tool of assessing DR from color fundus images could provide an alternative solution to reduce misdiagnosis and improve workflow. A DL-based automated tool offers substantial benefits to reduce screening costs, accessibility to healthcare and ameliorate earlier treatments.","Diabetic retinopathy (DR) is one of the leading causes of blindness globally. Earlier detection and timely treatment of DR are desirable to reduce the incidence and progression of vision loss. Currently, deep learning (DL) approaches have offered better performance in detecting DR from retinal fundus images. We, therefore, performed a systematic review with a meta-analysis of relevant studies to quantify the performance of DL algorithms for detecting DR. A systematic literature search on EMBASE, PubMed, Google Scholar, Scopus was performed between January 1, 2000, and March 31, 2019. The search strategy was based on the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) reporting guidelines, and DL-based study design was mandatory for articles inclusion. Two independent authors screened abstracts and titles against inclusion and exclusion criteria. Data were extracted by two authors independently using a standard form and the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) tool was used for the risk of bias and applicability assessment. Twenty-three studies were included in the systematic review; 20 studies met inclusion criteria for the meta-analysis. The pooled area under the receiving operating curve (AUROC) of DR was 0.97 (95%CI: 0.95-0.98), sensitivity was 0.83 (95%CI: 0.83-0.83), and specificity was 0.92 (95%CI: 0.92-0.92). The positive- and negative-likelihood ratio were 14.11 (95%CI: 9.91-20.07), and 0.10 (95%CI: 0.07-0.16), respectively. Moreover, the diagnostic odds ratio for DL models was 136.83 (95%CI: 79.03-236.93). All the studies provided a DR-grading scale, a human grader (e.g. trained caregivers, ophthalmologists) as a reference standard. The findings of our study showed that DL algorithms had high sensitivity and specificity for detecting referable DR from retinal fundus photographs. Applying a DL-based automated tool of assessing DR from color fundus images could provide an alternative solution to reduce misdiagnosis and improve workflow. A DL-based automated tool offers substantial benefits to reduce screening costs, accessibility to healthcare and ameliorate earlier treatments.","Islam, M. M.
 and Yang, H. C.
 and Poly, T. N.
 and Jian, W. S.
 and Jack Li, Y. C.","Islam, Yang, Poly, Jian, Jack Li",not available,https://doi.org/10.1016/j.cmpb.2020.105320,2020-11-02
349.0,,pubmed,A combination of 3-D discrete wavelet transform and 3-D local binary pattern for classification of mild cognitive impairment,A combination of 3-D discrete wavelet transform and 3-D local binary pattern for classification of mild cognitive impairment,"BACKGROUND: The detection of Alzheimer's Disease (AD) in its formative stages, especially in Mild Cognitive Impairments (MCI), has the potential of helping the clinicians in understanding the condition. The literature review shows that the classification of MCI-converts and MCI-non-converts has not been explored profusely and the maximum classification accuracy reported is rather low. Thus, this paper proposes a Machine Learning approach for classifying patients of MCI into two groups one who converted to AD and the others who are not diagnosed with any signs of AD. The proposed algorithm is also used to distinguish MCI patients from controls (CN). This work uses the Structural Magnetic Resonance Imaging data. METHODS: This work proposes a 3-D variant of Local Binary Pattern (LBP), called LBP-20 for extracting features. The method has been compared with 3D-Discrete Wavelet Transform (3D-DWT). Subsequently, a combination of 3D-DWT and LBP-20 has been used for extracting features. The relevant features are selected using the Fisher Discriminant Ratio (FDR) and finally the classification has been carried out using the Support Vector Machine. RESULTS: The combination of 3D-DWT with LBP-20 results in a maximum accuracy of 88.77. Similarly, the proposed combination of methods is also applied to distinguish MCI from CN. The proposed method results in the classification accuracy of 90.31 in this data. CONCLUSION: The proposed combination is able to extract relevant distribution of microstructures from each component, obtained with the use of DWT and thereby improving the classification accuracy. Moreover, the number of features used for classification is significantly less as compared to those obtained by 3D-DWT. The performance of the proposed method is measured in terms of accuracy, specificity and sensitivity and is found superior in comparison to the existing methods. Thus, the proposed method may contribute to effective diagnosis of MCI and may prove advantageous in clinical settings.","The detection of Alzheimer's Disease (AD) in its formative stages, especially in Mild Cognitive Impairments (MCI), has the potential of helping the clinicians in understanding the condition. The literature review shows that the classification of MCI-converts and MCI-non-converts has not been explored profusely and the maximum classification accuracy reported is rather low. Thus, this paper proposes a Machine Learning approach for classifying patients of MCI into two groups one who converted to AD and the others who are not diagnosed with any signs of AD. The proposed algorithm is also used to distinguish MCI patients from controls (CN). This work uses the Structural Magnetic Resonance Imaging data. This work proposes a 3-D variant of Local Binary Pattern (LBP), called LBP-20 for extracting features. The method has been compared with 3D-Discrete Wavelet Transform (3D-DWT). Subsequently, a combination of 3D-DWT and LBP-20 has been used for extracting features. The relevant features are selected using the Fisher Discriminant Ratio (FDR) and finally the classification has been carried out using the Support Vector Machine. The combination of 3D-DWT with LBP-20 results in a maximum accuracy of 88.77. Similarly, the proposed combination of methods is also applied to distinguish MCI from CN. The proposed method results in the classification accuracy of 90.31 in this data. The proposed combination is able to extract relevant distribution of microstructures from each component, obtained with the use of DWT and thereby improving the classification accuracy. Moreover, the number of features used for classification is significantly less as compared to those obtained by 3D-DWT. The performance of the proposed method is measured in terms of accuracy, specificity and sensitivity and is found superior in comparison to the existing methods. Thus, the proposed method may contribute to effective diagnosis of MCI and may prove advantageous in clinical settings.","Bhasin, H.
 and Agrawal, R. K.
 and For Alzheimer's Disease Neuroimaging, Initiative","Bhasin, Agrawal",https://dx.doi.org/10.1186/s12911-020-1055-x,https://doi.org/10.1186/s12911-020-1055-x,2020-11-02
4428.0,,pubmed,Mechanism-driven Read-Across of Chemical Hepatotoxicants Based on Chemical Structures and Biological Data,Mechanism-Driven Read-Across of Chemical Hepatotoxicants Based on Chemical Structures and Biological Data,"Hepatotoxicity is a leading cause of attrition in the drug development process. Traditional preclinical and clinical studies to evaluate hepatotoxicity liabilities are expensive and time-consuming. With the advent of critical advancements in High Throughput Screening (HTS), there has been a rapid accumulation of in vitro toxicity data available to inform the risk assessment of new pharmaceuticals and chemicals. To this end, we curated and merged all available in vivo hepatotoxicity data obtained from the literature and public resources, which yielded a comprehensive database of 4,089 compounds that includes hepatotoxicity classifications. After dividing the original database of chemicals into modeling and test sets, PubChem assay data were automatically extracted using an in-house data mining tool and clustered based on relationships between structural fragments and cellular responses in in vitro assays. The resultant PubChem assay clusters were further investigated. During cross-validation procedure, the biological data obtained from several assay clusters exhibited high predictivity of hepatotoxicity and these assays were selected to evaluate the test set compounds. The read-across results indicated that if a new compound contained identified specific chemical fragments (i.e. Molecular Initiate Event) and showed active responses in the relevant selected PubChem assays, there was potential for the chemical to be hepatotoxic in vivo. Furthermore, several mechanisms that might contribute to toxicity were derived from the modeling results including alterations in nuclear receptor signaling and inhibition of DNA repair. This modeling strategy can be further applied to the investigation of other complex chemical toxicity phenomena (e.g. developmental and reproductive toxicities) as well as drug efficacy.","Hepatotoxicity is a leading cause of attrition in the drug development process. Traditional preclinical and clinical studies to evaluate hepatotoxicity liabilities are expensive and time consuming. With the advent of critical advancements in high-throughput screening, there has been a rapid accumulation of in vitro toxicity data available to inform the risk assessment of new pharmaceuticals and chemicals. To this end, we curated and merged all available in vivo hepatotoxicity data obtained from the literature and public resources, which yielded a comprehensive database of 4089 compounds that includes hepatotoxicity classifications. After dividing the original database of chemicals into modeling and test sets, PubChem assay data were automatically extracted using an in-house data mining tool and clustered based on relationships between structural fragments and cellular responses in in vitro assays. The resultant PubChem assay clusters were further investigated. During the cross-validation procedure, the biological data obtained from several assay clusters exhibited high predictivity of hepatotoxicity and these assays were selected to evaluate the test set compounds. The read-across results indicated that if a new compound contained specific identified chemical fragments (ie, Molecular Initiating Event) and showed active responses in the relevant selected PubChem assays, there was potential for the chemical to be hepatotoxic in vivo. Furthermore, several mechanisms that might contribute to toxicity were derived from the modeling results including alterations in nuclear receptor signaling and inhibition of DNA repair. This modeling strategy can be further applied to the investigation of other complex chemical toxicity phenomena (eg, developmental and reproductive toxicities) as well as drug efficacy.","Zhao, L.
 and Russo, D. P.
 and Wang, W.
 and Aleksunes, L. M.
 and Zhu, H.","Zhao, Russo, Wang, Aleksunes, Zhu",https://dx.doi.org/10.1093/toxsci/kfaa005,https://doi.org/10.1093/toxsci/kfaa005,2020-11-02
179.0,,pubmed,Comparing machine and human reviewers to evaluate the risk of bias in randomized controlled trials,Comparing machine and human reviewers to evaluate the risk of bias in randomized controlled trials,"BACKGROUND: Evidence from new health technologies is growing, along with demands for evidence to inform policy decisions, creating challenges in completing health technology assessments (HTAs)/systematic reviews (SRs) in a timely manner. Software can decrease the time and burden by automating the process, but evidence validating such software is limited. We tested the accuracy of RobotReviewer, a semi-autonomous risk of bias (RoB) assessment tool, and its agreement with human reviewers. METHODS: Two reviewers independently conducted RoB assessments on a sample of randomized controlled trials (RCTs), and their consensus ratings were compared with those generated by RobotReviewer. Agreement with the human reviewers was assessed using percent agreement and weighted kappa (kappa). The accuracy of RobotReviewer was also assessed by calculating the sensitivity, specificity, and area under the curve in comparison to the consensus agreement of the human reviewers. RESULTS: The study included 372 RCTs. Inter-rater reliability ranged from kappa = -0.06 (no agreement) for blinding of participants and personnel to kappa = 0.62 (good agreement) for random sequence generation (excluding overall RoB). RobotReviewer was found to use a high percentage of 'irrelevant supporting quotations' to complement RoB assessments for blinding of participants and personnel (72.6%), blinding of outcome assessment (70.4%), and allocation concealment (54.3%). CONCLUSION: RobotReviewer can help with risk of bias assessment of RCTs but cannot replace human evaluations. Thus, reviewers should check and validate RoB assessments from RobotReviewer by consulting the original article when not relevant supporting quotations are provided by RobotReviewer. This consultation is in line with the recommendation provided by the developers.","Evidence from new health technologies is growing, along with demands for evidence to inform policy decisions, creating challenges in completing health technology assessments (HTAs)/systematic reviews (SRs) in a timely manner. Software can decrease the time and burden by automating the process, but evidence validating such software is limited. We tested the accuracy of RobotReviewer, a semi-autonomous risk of bias (RoB) assessment tool, and its agreement with human reviewers. Two reviewers independently conducted RoB assessments on a sample of randomized controlled trials (RCTs), and their consensus ratings were compared with those generated by RobotReviewer. Agreement with the human reviewers was assessed using percent agreement and weighted kappa (Îº). The accuracy of RobotReviewer was also assessed by calculating the sensitivity, specificity, and area under the curve in comparison to the consensus agreement of the human reviewers. The study included 372 RCTs. Inter-rater reliability ranged from Îº = -0.06 (no agreement) for blinding of participants and personnel to Îº = 0.62 (good agreement) for random sequence generation (excluding overall RoB). RobotReviewer was found to use a high percentage of &quot;irrelevant supporting quotations&quot; to complement RoB assessments for blinding of participants and personnel (72.6%), blinding of outcome assessment (70.4%), and allocation concealment (54.3%). RobotReviewer can help with risk of bias assessment of RCTs but cannot replace human evaluations. Thus, reviewers should check and validate RoB assessments from RobotReviewer by consulting the original article when not relevant supporting quotations are provided by RobotReviewer. This consultation is in line with the recommendation provided by the developers.","Armijo-Olivo, S.
 and Craig, R.
 and Campbell, S.","Armijo-Olivo, Craig, Campbell",not available,https://doi.org/10.1002/jrsm.1398,2020-11-02
3890.0,,pubmed,Radiomics of computed tomography and magnetic resonance imaging in renal cell carcinoma-a systematic review and meta-analysis,Radiomics of computed tomography and magnetic resonance imaging in renal cell carcinoma-a systematic review and meta-analysis,"OBJECTIVES: (1) To assess the methodological quality of radiomics studies investigating histological subtypes, therapy response, and survival in patients with renal cell carcinoma (RCC) and (2) to determine the risk of bias in these radiomics studies. METHODS: In this systematic review, literature published since 2000 on radiomics in RCC was included and assessed for methodological quality using the Radiomics Quality Score. The risk of bias was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool and a meta-analysis of radiomics studies focusing on differentiating between angiomyolipoma without visible fat and RCC was performed. RESULTS: Fifty-seven studies investigating the use of radiomics in renal cancer were identified, including 4590 patients in total. The average Radiomics Quality Score was 3.41 (9.4% of total) with good inter-rater agreement (ICC 0.96, 95% CI 0.93-0.98). Three studies validated results with an independent dataset, one used a publically available validation dataset. None of the studies shared the code, images, or regions of interest. The meta-analysis showed moderate heterogeneity among the included studies and an odds ratio of 6.24 (95% CI 4.27-9.12; p < 0.001) for the differentiation of angiomyolipoma without visible fat from RCC. CONCLUSIONS: Radiomics algorithms show promise for answering clinical questions where subjective interpretation is challenging or not established. However, the generalizability of findings to prospective cohorts needs to be demonstrated in future trials for progression towards clinical translation. Improved sharing of methods including code and images could facilitate independent validation of radiomics signatures. KEY POINTS: * Studies achieved an average Radiomics Quality Score of 10.8%. Common reasons for low Radiomics Quality Scores were unvalidated results, retrospective study design, absence of open science, and insufficient control for multiple comparisons. * A previous training phase allowed reaching almost perfect inter-rater agreement in the application of the Radiomics Quality Score. * Meta-analysis of radiomics studies distinguishing angiomyolipoma without visible fat from renal cell carcinoma show moderate diagnostic odds ratios of 6.24 and moderate methodological diversity.","(1) To assess the methodological quality of radiomics studies investigating histological subtypes, therapy response, and survival in patients with renal cell carcinoma (RCC) and (2) to determine the risk of bias in these radiomics studies. In this systematic review, literature published since 2000 on radiomics in RCC was included and assessed for methodological quality using the Radiomics Quality Score. The risk of bias was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool and a meta-analysis of radiomics studies focusing on differentiating between angiomyolipoma without visible fat and RCC was performed. Fifty-seven studies investigating the use of radiomics in renal cancer were identified, including 4590 patients in total. The average Radiomics Quality Score was 3.41 (9.4% of total) with good inter-rater agreement (ICC 0.96, 95% CI 0.93-0.98). Three studies validated results with an independent dataset, one used a publically available validation dataset. None of the studies shared the code, images, or regions of interest. The meta-analysis showed moderate heterogeneity among the included studies and an odds ratio of 6.24 (95% CI 4.27-9.12; pâ€‰&lt;â€‰0.001) for the differentiation of angiomyolipoma without visible fat from RCC. Radiomics algorithms show promise for answering clinical questions where subjective interpretation is challenging or not established. However, the generalizability of findings to prospective cohorts needs to be demonstrated in future trials for progression towards clinical translation. Improved sharing of methods including code and images could facilitate independent validation of radiomics signatures. â€¢ Studies achieved an average Radiomics Quality Score of 10.8%. Common reasons for low Radiomics Quality Scores were unvalidated results, retrospective study design, absence of open science, and insufficient control for multiple comparisons. â€¢ A previous training phase allowed reaching almost perfect inter-rater agreement in the application of the Radiomics Quality Score. â€¢ Meta-analysis of radiomics studies distinguishing angiomyolipoma without visible fat from renal cell carcinoma show moderate diagnostic odds ratios of 6.24 and moderate methodological diversity.","Ursprung, S.
 and Beer, L.
 and Bruining, A.
 and Woitek, R.
 and Stewart, G. D.
 and Gallagher, F. A.
 and Sala, E.","Ursprung, Beer, Bruining, Woitek, Stewart, Gallagher, Sala",https://dx.doi.org/10.1007/s00330-020-06666-3,https://doi.org/10.1007/s00330-020-06666-3,2020-11-02
36.0,,pubmed,Telemedicine in the driver's seat: new role for primary care access in Brazil and Canada: The Besrour Papers: a series on the state of family medicine in Canada and Brazil,Telemedicine in the driver's seat: new role for primary care access in Brazil and Canada: The Besrour Papers: a series on the state of family medicine in Canada and Brazil,"OBJECTIVE: To contrast how Brazil's and Canada's different jurisdictional and judicial realities have led to different types of telemedicine and how further scale and improvement can be achieved. COMPOSITION OF THE COMMITTEE: A subgroup of the Besrour Centre of the College of Family Physicians of Canada and Canadian telemedicine experts developed connections with colleagues in Porto Alegre, Brazil, and collaborated to undertake a between-country comparison of their respective telemedicine programs. METHODS: Following a literature review, the authors collectively reflected on their experiences in an attempt to explore the past and current state of telemedicine in Canada and Brazil. REPORT: Both Brazil and Canada share expansive geographies, creating substantial barriers to health for rural patients. Telemedicine is an important part of a universal health system. Both countries have achieved telemedicine programs that have scaled up across large regions and are showing important effects on health care costs and outcomes. However, each system is unique in design and implementation and faces unique challenges for further scale and improvement. Addressing regional differences, the normalization of telemedicine, and potential alignment of telemedicine and artificial intelligence technologies for health care are seen as promising approaches to scaling up and improving telemedicine in both countries.","To contrast how Brazil's and Canada's different jurisdictional and judicial realities have led to different types of telemedicine and how further scale and improvement can be achieved. A subgroup of the Besrour Centre of the College of Family Physicians of Canada and Canadian telemedicine experts developed connections with colleagues in Porto Alegre, Brazil, and collaborated to undertake a between-country comparison of their respective telemedicine programs. Following a literature review, the authors collectively reflected on their experiences in an attempt to explore the past and current state of telemedicine in Canada and Brazil. Both Brazil and Canada share expansive geographies, creating substantial barriers to health for rural patients. Telemedicine is an important part of a universal health system. Both countries have achieved telemedicine programs that have scaled up across large regions and are showing important effects on health care costs and outcomes. However, each system is unique in design and implementation and faces unique challenges for further scale and improvement. Addressing regional differences, the normalization of telemedicine, and potential alignment of telemedicine and artificial intelligence technologies for health care are seen as promising approaches to scaling up and improving telemedicine in both countries.","Agarwal, P.
 and Kithulegoda, N.
 and Umpierre, R.
 and Pawlovich, J.
 and Pfeil, J. N.
 and D'Avila, O. P.
 and Goncalves, M.
 and Harzheim, E.
 and Ponka, D.","Agarwal, Kithulegoda, Umpierre, Pawlovich, Pfeil, D'Avila, Goncalves, Harzheim, Ponka",not available,https://www.google.com/search?q=Telemedicine+in+the+driver's+seat:+new+role+for+primary+care+access+in+Brazil+and+Canada:+The+Besrour+Papers:+a+series+on+the+state+of+family+medicine+in+Canada+and+Brazil.,2020-11-02
3459.0,,pubmed,Systematic review of using medical informatics in lung transplantation studies,Systematic review of using medical informatics in lung transplantation studies,"BACKGROUND: Lung transplantation is one of the advanced treatment options performed even in patients suffering from end-stage lung disease. Due to the positive results of medical informatics in other fields of medicine, lung transplant researchers have also conducted remarkable studies to improve transplant outcomes. The main objective of this article was to review the current studies of health information technology used in lung transplantation. METHODS: A systematic search was performed in four scientific databases (Web of Science, Scopus, Science Direct, and PubMed) from January 2000 to December 2018. The criteria for inclusion were included in any study describing the use of health information technology or medical informatics in terms of lung transplantation, English papers, and original researchers. The retrieved articles were accordingly screened based on the inclusion and exclusion criteria to select relevant studies. The survey and synthesis of included articles were conducted based on predefined classification. RESULTS: Out of 263 articles, 27 studies met our inclusion criteria. All included studies involved the application of health information technology in lung transplantation. The types of health information technology methods applied in reviewed articles included mhealth (11.1 %), DSS (7.4 %), decision aid tools (7.4 %), telemedicine (22.2 %), AI methods (11.1 %), data mining (37 %), and patient education (3.7 %). The majority of studies (88.9 %) showed the positive impact of health information technology to enhance lung transplantation outcomes. Finally, the main approaches in different phases of lung transplantation processes were interpreted and summarized in the visual model. CONCLUSION: This systematic review provides new insights regarding the application of medical informatics in the lung transplantation domain. The missing areas of medical informatics in the lung transplantation domain were recognized through this study.","Lung transplantation is one of the advanced treatment options performed even in patients suffering from end-stage lung disease. Due to the positive results of medical informatics in other fields of medicine, lung transplant researchers have also conducted remarkable studies to improve transplant outcomes. The main objective of this article was to review the current studies of health information technology used in lung transplantation. A systematic search was performed in four scientific databases (Web of Science, Scopus, Science Direct, and PubMed) from January 2000 to December 2018. The criteria for inclusion were included in any study describing the use of health information technology or medical informatics in terms of lung transplantation, English papers, and original researchers. The retrieved articles were accordingly screened based on the inclusion and exclusion criteria to select relevant studies. The survey and synthesis of included articles were conducted based on predefined classification. Out of 263 articles, 27 studies met our inclusion criteria. All included studies involved the application of health information technology in lung transplantation. The types of health information technology methods applied in reviewed articles included mhealth (11.1 %), DSS (7.4 %), decision aid tools (7.4 %), telemedicine (22.2 %), AI methods (11.1 %), data mining (37 %), and patient education (3.7 %). The majority of studies (88.9 %) showed the positive impact of health information technology to enhance lung transplantation outcomes. Finally, the main approaches in different phases of lung transplantation processes were interpreted and summarized in the visual model. This systematic review provides new insights regarding the application of medical informatics in the lung transplantation domain. The missing areas of medical informatics in the lung transplantation domain were recognized through this study.","Shahmoradi, L.
 and Abtahi, H.
 and Amini, S.
 and Gholamzadeh, M.","Shahmoradi, Abtahi, Amini, Gholamzadeh",https://dx.doi.org/10.1016/j.ijmedinf.2020.104096,https://doi.org/10.1016/j.ijmedinf.2020.104096,2020-11-02
1681.0,,pubmed,Association Between Periodontitis and Nosocomial Pneumonia: A Systematic Review and Meta-analysis of Observational Studies,Association Between Periodontitis and Nosocomial Pneumonia: A Systematic Review and Meta-analysis of Observational Studies,"PURPOSE: To assess the relationship between periodontitis and nosocomial pneumonia in intensive care unit (ICU) patients. MATERIALS AND METHODS: The present study was conducted in accordance with the guidelines of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement and registered (CRD42018105124) with PROSPERO (International prospective register for systematic reviews, University of York, York, UK). A search was conducted in five databases without restrictions regarding language or date of publication. From 560 studies selected, 10 underwent full-text analysis. Five studies were eligible (five case-control studies), and all were entered in the meta-analysis. Meta-analysis was performed with tests for sensitivity and statistical heterogeneity. Summary effect measures were calculated by odds ratio (OR) and 95% confidence interval (CI). RESULTS: There was a significant association between periodontitis and nosocomial pneumonia in the meta-analysis (OR 2.55, 95% CI 1.68 to 3.86). In this meta-analysis, I2 = 0%. CONCLUSIONS: The evidence demonstrates a positive association between periodontitis and nosocomial pneumonia. Individuals with periodontitis admitted to the ICU were more likely to present nosocomial pneumonia than individuals without periodontitis.","To assess the relationship between periodontitis and nosocomial pneumonia in intensive care unit (ICU) patients. The present study was conducted in accordance with the guidelines of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement and registered (CRD42018105124) with PROSPERO (International prospective register for systematic reviews, University of York, York, UK). A search was conducted in five databases without restrictions regarding language or date of publication. From 560 studies selected, 10 underwent full-text analysis. Five studies were eligible (five case-control studies), and all were entered in the meta-analysis. Meta-analysis was performed with tests for sensitivity and statistical heterogeneity. Summary effect measures were calculated by odds ratio (OR) and 95% confidence interval (CI). There was a significant association between periodontitis and nosocomial pneumonia in the meta-analysis (OR 2.55, 95% CI 1.68 to 3.86). In this meta-analysis, I2 = 0%. The evidence demonstrates a positive association between periodontitis and nosocomial pneumonia. Individuals with periodontitis admitted to the ICU were more likely to present nosocomial pneumonia than individuals without periodontitis.","Jeronimo, L. S.
 and Abreu, L. G.
 and Cunha, F. A.
 and Esteves Lima, R. P.","JerÃ´nimo, Abreu, Cunha, Esteves Lima",not available,https://doi.org/10.3290/j.ohpd.a44114,2020-11-02
3984.0,,pubmed,Effectiveness of Active Learning that Combines Physical Activity and Math in Schoolchildren: A Systematic Review,Effectiveness of Active Learning that Combines Physical Activity and Math in Schoolchildren: A Systematic Review,"BACKGROUND: Despite increased interest in combining learning and physical activity (PA), the academic and PA benefits of active learning are uncertain. METHODS: A systematic search of 5 databases for studies combining learning math with PA in primary/elementary schools was conducted. Academic benefit was evaluated by pre-post intervention math scores compared to a control group. Effect sizes (ES) were extracted/calculated when possible. Due to study heterogeneity, meta-analysis was not conducted. RESULTS: Six randomized controlled trials and 5 quasi-experimental studies evaluating 4082 participants (53% girls; mean age 7.5-11.1 years) were eligible. Math scores were significantly better in the intervention group in 6 of 11 studies on at least 1 test (ES: 0.42-4.7; p <= .03). Other math tests either were not all statistically significant (2 studies) or the benefit varied across grades (1 study). No studies reported a decline in math scores. Of studies measuring PA with accelerometers, 4 of 5 reported significantly greater PA in the intervention group during the intervention (p < .05) or across the school day (p < .01). CONCLUSIONS: Undertaking PA while learning was largely equivocal for math scores but showed promising results for increasing daily PA, without detrimental effects on math performance. The need for more rigorous studies with comprehensive assessment of academic performance and PA is highlighted.","Despite increased interest in combining learning and physical activity (PA), the academic and PA benefits of active learning are uncertain. A systematic search of 5 databases for studies combining learning math with PA in primary/elementary schools was conducted. Academic benefit was evaluated by pre-post intervention math scores compared to a control group. Effect sizes (ES) were extracted/calculated when possible. Due to study heterogeneity, meta-analysis was not conducted. Six randomized controlled trials and 5 quasi-experimental studies evaluating 4082 participants (53% girls; mean age 7.5-11.1â€‰years) were eligible. Math scores were significantly better in the intervention group in 6 of 11 studies on at least 1 test (ES: 0.42-4.7; pâ€‰â‰¤â€‰.03). Other math tests either were not all statistically significant (2 studies) or the benefit varied across grades (1 study). No studies reported a decline in math scores. Of studies measuring PA with accelerometers, 4 of 5 reported significantly greater PA in the intervention group during the intervention (pâ€‰&lt;â€‰.05) or across the school day (pâ€‰&lt;â€‰.01). Undertaking PA while learning was largely equivocal for math scores but showed promising results for increasing daily PA, without detrimental effects on math performance. The need for more rigorous studies with comprehensive assessment of academic performance and PA is highlighted.","Vetter, M.
 and Orr, R.
 and O'Dwyer, N.
 and O'Connor, H.","Vetter, Orr, O'Dwyer, O'Connor",https://dx.doi.org/10.1111/josh.12878,https://doi.org/10.1111/josh.12878,2020-11-02
10013.0,,arxiv,A Fast Fully Octave Convolutional Neural Network for Document Image   Segmentation,A Fast Fully Octave Convolutional Neural Network for Document Image   Segmentation,"The Know Your Customer (KYC) and Anti Money Laundering (AML) are worldwide practices to online customer identification based on personal identification documents, similarity and liveness checking, and proof of address. To answer the basic regulation question: are you whom you say you are? The customer needs to upload valid identification documents (ID). This task imposes some computational challenges since these documents are diverse, may present different and complex backgrounds, some occlusion, partial rotation, poor quality, or damage. Advanced text and document segmentation algorithms were used to process the ID images. In this context, we investigated a method based on U-Net to detect the document edges and text regions in ID images. Besides the promising results on image segmentation, the U-Net based approach is computationally expensive for a real application, since the image segmentation is a customer device task. We propose a model optimization based on Octave Convolutions to qualify the method to situations where storage, processing, and time resources are limited, such as in mobile and robotic applications. We conducted the evaluation experiments in two new datasets CDPhotoDataset and DTDDataset, which are composed of real ID images of Brazilian documents. Our results showed that the proposed models are efficient to document segmentation tasks and portable.","The Know Your Customer (KYC) and Anti Money Laundering (AML) are worldwide practices to online customer identification based on personal identification documents, similarity and liveness checking, and proof of address. To answer the basic regulation question: are you whom you say you are? The customer needs to upload valid identification documents (ID). This task imposes some computational challenges since these documents are diverse, may present different and complex backgrounds, some occlusion, partial rotation, poor quality, or damage. Advanced text and document segmentation algorithms were used to process the ID images. In this context, we investigated a method based on U-Net to detect the document edges and text regions in ID images. Besides the promising results on image segmentation, the U-Net based approach is computationally expensive for a real application, since the image segmentation is a customer device task. We propose a model optimization based on Octave Convolutions to qualify the method to situations where storage, processing, and time resources are limited, such as in mobile and robotic applications. We conducted the evaluation experiments in two new datasets CDPhotoDataset and DTDDataset, which are composed of real ID images of Brazilian documents. Our results showed that the proposed models are efficient to document segmentation tasks and portable.","['Ricardo Batista das Neves Junior', 'Luiz Felipe Verçosa', 'David Macêdo', 'Byron Leite Dantas Bezerra', 'Cleber Zanchettin']","['Ricardo Batista das Neves Junior', 'Luiz Felipe Verçosa', 'David Macêdo', 'Byron Leite Dantas Bezerra', 'Cleber Zanchettin']",https://export.arxiv.org/abs/2004.01317,https://export.arxiv.org/abs/2004.01317,2020-11-06
10014.0,,arxiv,Modeling Rare Interactions in Time Series Data Through Qualitative   Change: Application to Outcome Prediction in Intensive Care Units,Modeling Rare Interactions in Time Series Data Through Qualitative   Change: Application to Outcome Prediction in Intensive Care Units,"Many areas of research are characterised by the deluge of large-scale highly-dimensional time-series data. However, using the data available for prediction and decision making is hampered by the current lag in our ability to uncover and quantify true interactions that explain the outcomes.We are interested in areas such as intensive care medicine, which are characterised by i) continuous monitoring of multivariate variables and non-uniform sampling of data streams, ii) the outcomes are generally governed by interactions between a small set of rare events, iii) these interactions are not necessarily definable by specific values (or value ranges) of a given group of variables, but rather, by the deviations of these values from the normal state recorded over time, iv) the need to explain the predictions made by the model. Here, while numerous data mining models have been formulated for outcome prediction, they are unable to explain their predictions.   We present a model for uncovering interactions with the highest likelihood of generating the outcomes seen from highly-dimensional time series data. Interactions among variables are represented by a relational graph structure, which relies on qualitative abstractions to overcome non-uniform sampling and to capture the semantics of the interactions corresponding to the changes and deviations from normality of variables of interest over time. Using the assumption that similar templates of small interactions are responsible for the outcomes (as prevalent in the medical domains), we reformulate the discovery task to retrieve the most-likely templates from the data.","Many areas of research are characterised by the deluge of large-scale highly-dimensional time-series data. However, using the data available for prediction and decision making is hampered by the current lag in our ability to uncover and quantify true interactions that explain the outcomes.We are interested in areas such as intensive care medicine, which are characterised by i) continuous monitoring of multivariate variables and non-uniform sampling of data streams, ii) the outcomes are generally governed by interactions between a small set of rare events, iii) these interactions are not necessarily definable by specific values (or value ranges) of a given group of variables, but rather, by the deviations of these values from the normal state recorded over time, iv) the need to explain the predictions made by the model. Here, while numerous data mining models have been formulated for outcome prediction, they are unable to explain their predictions.   We present a model for uncovering interactions with the highest likelihood of generating the outcomes seen from highly-dimensional time series data. Interactions among variables are represented by a relational graph structure, which relies on qualitative abstractions to overcome non-uniform sampling and to capture the semantics of the interactions corresponding to the changes and deviations from normality of variables of interest over time. Using the assumption that similar templates of small interactions are responsible for the outcomes (as prevalent in the medical domains), we reformulate the discovery task to retrieve the most-likely templates from the data.","['Zina Ibrahim', 'Honghan Wu', 'Richard Dobson']","['Zina Ibrahim', 'Honghan Wu', 'Richard Dobson']",https://export.arxiv.org/abs/2004.01431,https://export.arxiv.org/abs/2004.01431,2020-11-06
10016.0,,arxiv,Google Landmarks Dataset v2 -- A Large-Scale Benchmark for   Instance-Level Recognition and Retrieval,Google Landmarks Dataset v2 -- A Large-Scale Benchmark for   Instance-Level Recognition and Retrieval,"While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance -- while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks. GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by real world applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world's largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at https://github.com/cvdfoundation/google-landmark.","While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance -- while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks. GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by real world applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world's largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at https://github.com/cvdfoundation/google-landmark.","['Tobias Weyand', 'Andre Araujo', 'Bingyi Cao', 'Jack Sim']","['Tobias Weyand', 'Andre Araujo', 'Bingyi Cao', 'Jack Sim']",https://export.arxiv.org/abs/2004.01804,https://export.arxiv.org/abs/2004.01804,2020-11-06
10017.0,,arxiv,Mining Shape of Expertise: A Novel Approach Based on Convolutional   Neural Network,Mining Shape of Expertise: A Novel Approach Based on Convolutional   Neural Network,"Expert finding addresses the task of retrieving and ranking talented people on the subject of user query. It is a practical issue in the Community Question Answering networks. Recruiters looking for knowledgeable people for their job positions are the most important clients of expert finding systems. In addition to employee expertise, the cost of hiring new staff is another significant concern for organizations. An efficient solution to cope with this concern is to hire T-shaped experts that are cost-effective. In this study, we have proposed a new deep model for T-shaped experts finding based on Convolutional Neural Networks. The proposed model tries to match queries and users by extracting local and position-invariant features from their corresponding documents. In other words, it detects users' shape of expertise by learning patterns from documents of users and queries simultaneously. The proposed model contains two parallel CNN's that extract latent vectors of users and queries based on their corresponding documents and join them together in the last layer to match queries with users. Experiments on a large subset of Stack Overflow documents indicate the effectiveness of the proposed method against baselines in terms of NDCG, MRR, and ERR evaluation metrics.","Expert finding addresses the task of retrieving and ranking talented people on the subject of user query. It is a practical issue in the Community Question Answering networks. Recruiters looking for knowledgeable people for their job positions are the most important clients of expert finding systems. In addition to employee expertise, the cost of hiring new staff is another significant concern for organizations. An efficient solution to cope with this concern is to hire T-shaped experts that are cost-effective. In this study, we have proposed a new deep model for T-shaped experts finding based on Convolutional Neural Networks. The proposed model tries to match queries and users by extracting local and position-invariant features from their corresponding documents. In other words, it detects users' shape of expertise by learning patterns from documents of users and queries simultaneously. The proposed model contains two parallel CNN's that extract latent vectors of users and queries based on their corresponding documents and join them together in the last layer to match queries with users. Experiments on a large subset of Stack Overflow documents indicate the effectiveness of the proposed method against baselines in terms of NDCG, MRR, and ERR evaluation metrics.","['Mahdi Dehghan', 'Hossein A. Rahmani', 'Ahmad Ali Abin', 'Viet-Vu Vu']","['Mahdi Dehghan', 'Hossein A. Rahmani', 'Ahmad Ali Abin', 'Viet-Vu Vu']",https://export.arxiv.org/abs/2004.02184,https://export.arxiv.org/abs/2004.02184,2020-11-06
10018.0,,arxiv,Natural language processing for word sense disambiguation and   information extraction,Natural language processing for word sense disambiguation and   information extraction,"This research work deals with Natural Language Processing (NLP) and extraction of essential information in an explicit form. The most common among the information management strategies is Document Retrieval (DR) and Information Filtering. DR systems may work as combine harvesters, which bring back useful material from the vast fields of raw material. With large amount of potentially useful information in hand, an Information Extraction (IE) system can then transform the raw material by refining and reducing it to a germ of original text. A Document Retrieval system collects the relevant documents carrying the required information, from the repository of texts. An IE system then transforms them into information that is more readily digested and analyzed. It isolates relevant text fragments, extracts relevant information from the fragments, and then arranges together the targeted information in a coherent framework. The thesis presents a new approach for Word Sense Disambiguation using thesaurus. The illustrative examples supports the effectiveness of this approach for speedy and effective disambiguation. A Document Retrieval method, based on Fuzzy Logic has been described and its application is illustrated. A question-answering system describes the operation of information extraction from the retrieved text documents. The process of information extraction for answering a query is considerably simplified by using a Structured Description Language (SDL) which is based on cardinals of queries in the form of who, what, when, where and why. The thesis concludes with the presentation of a novel strategy based on Dempster-Shafer theory of evidential reasoning, for document retrieval and information extraction. This strategy permits relaxation of many limitations, which are inherent in Bayesian probabilistic approach.","This research work deals with Natural Language Processing (NLP) and extraction of essential information in an explicit form. The most common among the information management strategies is Document Retrieval (DR) and Information Filtering. DR systems may work as combine harvesters, which bring back useful material from the vast fields of raw material. With large amount of potentially useful information in hand, an Information Extraction (IE) system can then transform the raw material by refining and reducing it to a germ of original text. A Document Retrieval system collects the relevant documents carrying the required information, from the repository of texts. An IE system then transforms them into information that is more readily digested and analyzed. It isolates relevant text fragments, extracts relevant information from the fragments, and then arranges together the targeted information in a coherent framework. The thesis presents a new approach for Word Sense Disambiguation using thesaurus. The illustrative examples supports the effectiveness of this approach for speedy and effective disambiguation. A Document Retrieval method, based on Fuzzy Logic has been described and its application is illustrated. A question-answering system describes the operation of information extraction from the retrieved text documents. The process of information extraction for answering a query is considerably simplified by using a Structured Description Language (SDL) which is based on cardinals of queries in the form of who, what, when, where and why. The thesis concludes with the presentation of a novel strategy based on Dempster-Shafer theory of evidential reasoning, for document retrieval and information extraction. This strategy permits relaxation of many limitations, which are inherent in Bayesian probabilistic approach.",['K. R. Chowdhary'],['K. R. Chowdhary'],https://export.arxiv.org/abs/2004.02256,https://export.arxiv.org/abs/2004.02256,2020-11-06
10021.0,,arxiv,Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle,Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle,"The computation and memory needed for Convolutional Neural Network (CNN) inference can be reduced by pruning weights from the trained network. Pruning is guided by a pruning saliency, which heuristically approximates the change in the loss function associated with the removal of specific weights. Many pruning signals have been proposed, but the performance of each heuristic depends on the particular trained network. This leaves the data scientist with a difficult choice. When using any one saliency metric for the entire pruning process, we run the risk of the metric assumptions being invalidated, leading to poor decisions being made by the metric. Ideally we could combine the best aspects of different saliency metrics. However, despite an extensive literature review, we are unable to find any prior work on composing different saliency metrics. The chief difficulty lies in combining the numerical output of different saliency metrics, which are not directly comparable.   We propose a method to compose several primitive pruning saliencies, to exploit the cases where each saliency measure does well. Our experiments show that the composition of saliencies avoids many poor pruning choices identified by individual saliencies. In most cases our method finds better selections than even the best individual pruning saliency.","The computation and memory needed for Convolutional Neural Network (CNN) inference can be reduced by pruning weights from the trained network. Pruning is guided by a pruning saliency, which heuristically approximates the change in the loss function associated with the removal of specific weights. Many pruning signals have been proposed, but the performance of each heuristic depends on the particular trained network. This leaves the data scientist with a difficult choice. When using any one saliency metric for the entire pruning process, we run the risk of the metric assumptions being invalidated, leading to poor decisions being made by the metric. Ideally we could combine the best aspects of different saliency metrics. However, despite an extensive literature review, we are unable to find any prior work on composing different saliency metrics. The chief difficulty lies in combining the numerical output of different saliency metrics, which are not directly comparable.   We propose a method to compose several primitive pruning saliencies, to exploit the cases where each saliency measure does well. Our experiments show that the composition of saliencies avoids many poor pruning choices identified by individual saliencies. In most cases our method finds better selections than even the best individual pruning saliency.","['Kaveena Persand', 'Andrew Anderson', 'David Gregg']","['Kaveena Persand', 'Andrew Anderson', 'David Gregg']",https://export.arxiv.org/abs/2004.03376,https://export.arxiv.org/abs/2004.03376,2020-11-06
10022.0,,arxiv,Deep Learning Based Text Classification: A Comprehensive Review,Deep Learning Based Text Classification: A Comprehensive Review,"Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this work, we provide a detailed review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Deep Learning Based Text Classification: A Comprehensive Review.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs https://arxiv.org/abs/2004.03705; Years 2020; Authors Shervin Minaee; Nal Kalchbrenner; Erik Cambria; Narjes Nikzad; Meysam Chenaghlu; Jianfeng Gao; Deduplication_Notes ; X Deep Learning Based Text Classification: A Comprehensive Review.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]","Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this work, we provide a detailed review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.","['Shervin Minaee', 'Nal Kalchbrenner', 'Erik Cambria', 'Narjes Nikzad', 'Meysam Chenaghlu', 'Jianfeng Gao']","['Shervin Minaee', 'Nal Kalchbrenner', 'Erik Cambria', 'Narjes Nikzad', 'Meysam Chenaghlu', 'Jianfeng Gao']",https://export.arxiv.org/abs/2004.03705,https://export.arxiv.org/abs/2004.03705,2020-11-06
10023.0,,arxiv,A Review of Vibration-Based Damage Detection in Civil Structures: From   Traditional Methods to Machine Learning and Deep Learning Applications,A Review of Vibration-Based Damage Detection in Civil Structures: From   Traditional Methods to Machine Learning and Deep Learning Applications,"Monitoring structural damage is extremely important for sustaining and preserving the service life of civil structures. While successful monitoring provides resolute and staunch information on the health, serviceability, integrity and safety of structures; maintaining continuous performance of a structure depends highly on monitoring the occurrence, formation and propagation of damage. Damage may accumulate on structures due to different environmental and human-induced factors. Numerous monitoring and detection approaches have been developed to provide practical means for early warning against structural damage or any type of anomaly. Considerable effort has been put into vibration-based methods, which utilize the vibration response of the monitored structure to assess its condition and identify structural damage. Meanwhile, with emerging computing power and sensing technology in the last decade, Machine Learning (ML) and especially Deep Learning (DL) algorithms have become more feasible and extensively used in vibration-based structural damage detection with elegant performance and often with rigorous accuracy. While there have been multiple review studies published on vibration-based structural damage detection, there has not been a study where the transition from traditional methods to ML and DL methods are described and discussed. This paper aims to fulfill this gap by presenting the highlights of the traditional methods and provide a comprehensive review of the most recent applications of ML and DL algorithms utilized for vibration-based structural damage detection in civil structures.","Monitoring structural damage is extremely important for sustaining and preserving the service life of civil structures. While successful monitoring provides resolute and staunch information on the health, serviceability, integrity and safety of structures; maintaining continuous performance of a structure depends highly on monitoring the occurrence, formation and propagation of damage. Damage may accumulate on structures due to different environmental and human-induced factors. Numerous monitoring and detection approaches have been developed to provide practical means for early warning against structural damage or any type of anomaly. Considerable effort has been put into vibration-based methods, which utilize the vibration response of the monitored structure to assess its condition and identify structural damage. Meanwhile, with emerging computing power and sensing technology in the last decade, Machine Learning (ML) and especially Deep Learning (DL) algorithms have become more feasible and extensively used in vibration-based structural damage detection with elegant performance and often with rigorous accuracy. While there have been multiple review studies published on vibration-based structural damage detection, there has not been a study where the transition from traditional methods to ML and DL methods are described and discussed. This paper aims to fulfill this gap by presenting the highlights of the traditional methods and provide a comprehensive review of the most recent applications of ML and DL algorithms utilized for vibration-based structural damage detection in civil structures.","['Onur Avci', 'Osama Abdeljaber', 'Serkan Kiranyaz', 'Mohammed Hussein', 'Moncef Gabbouj', 'Daniel J. Inman']","['Onur Avci', 'Osama Abdeljaber', 'Serkan Kiranyaz', 'Mohammed Hussein', 'Moncef Gabbouj', 'Daniel J. Inman']",https://export.arxiv.org/abs/2004.04373,https://export.arxiv.org/abs/2004.04373,2020-11-06
10024.0,,arxiv,An Overview of Federated Deep Learning Privacy Attacks and Defensive   Strategies,An Overview of Federated Deep Learning Privacy Attacks and Defensive   Strategies,"With the increased attention and legislation for data-privacy, collaborative machine learning (ML) algorithms are being developed to ensure the protection of private data used for processing. Federated learning (FL) is the most popular of these methods, which provides privacy preservation by facilitating collaborative training of a shared model without the need to exchange any private data with a centralized server. Rather, an abstraction of the data in the form of a machine learning model update is sent. Recent studies showed that such model updates may still very well leak private information and thus more structured risk assessment is needed. In this paper, we analyze existing vulnerabilities of FL and subsequently perform a literature review of the possible attack methods targetingFL privacy protection capabilities. These attack methods are then categorized by a basic taxonomy. Additionally, we provide a literature study of the most recent defensive strategies and algorithms for FL aimed to overcome these attacks. These defensive strategies are categorized by their respective underlying defence principle. The paper concludes that the application of a single defensive strategy is not enough to provide adequate protection to all available attack methods.","With the increased attention and legislation for data-privacy, collaborative machine learning (ML) algorithms are being developed to ensure the protection of private data used for processing. Federated learning (FL) is the most popular of these methods, which provides privacy preservation by facilitating collaborative training of a shared model without the need to exchange any private data with a centralized server. Rather, an abstraction of the data in the form of a machine learning model update is sent. Recent studies showed that such model updates may still very well leak private information and thus more structured risk assessment is needed. In this paper, we analyze existing vulnerabilities of FL and subsequently perform a literature review of the possible attack methods targetingFL privacy protection capabilities. These attack methods are then categorized by a basic taxonomy. Additionally, we provide a literature study of the most recent defensive strategies and algorithms for FL aimed to overcome these attacks. These defensive strategies are categorized by their respective underlying defence principle. The paper concludes that the application of a single defensive strategy is not enough to provide adequate protection to all available attack methods.","['David Enthoven', 'Zaid Al-Ars']","['David Enthoven', 'Zaid Al-Ars']",https://export.arxiv.org/abs/2004.04676,https://export.arxiv.org/abs/2004.04676,2020-11-06
10025.0,,arxiv,Machine Learning Based Solutions for Security of Internet of Things   (IoT): A Survey,Machine Learning Based Solutions for Security of Internet of Things   (IoT): A Survey,"Over the last decade, IoT platforms have been developed into a global giant that grabs every aspect of our daily lives by advancing human life with its unaccountable smart services. Because of easy accessibility and fast-growing demand for smart devices and network, IoT is now facing more security challenges than ever before. There are existing security measures that can be applied to protect IoT. However, traditional techniques are not as efficient with the advancement booms as well as different attack types and their severeness. Thus, a strong-dynamically enhanced and up to date security system is required for next-generation IoT system. A huge technological advancement has been noticed in Machine Learning (ML) which has opened many possible research windows to address ongoing and future challenges in IoT. In order to detect attacks and identify abnormal behaviors of smart devices and networks, ML is being utilized as a powerful technology to fulfill this purpose. In this survey paper, the architecture of IoT is discussed, following a comprehensive literature review on ML approaches the importance of security of IoT in terms of different types of possible attacks. Moreover, ML-based potential solutions for IoT security has been presented and future challenges are discussed.","Over the last decade, IoT platforms have been developed into a global giant that grabs every aspect of our daily lives by advancing human life with its unaccountable smart services. Because of easy accessibility and fast-growing demand for smart devices and network, IoT is now facing more security challenges than ever before. There are existing security measures that can be applied to protect IoT. However, traditional techniques are not as efficient with the advancement booms as well as different attack types and their severeness. Thus, a strong-dynamically enhanced and up to date security system is required for next-generation IoT system. A huge technological advancement has been noticed in Machine Learning (ML) which has opened many possible research windows to address ongoing and future challenges in IoT. In order to detect attacks and identify abnormal behaviors of smart devices and networks, ML is being utilized as a powerful technology to fulfill this purpose. In this survey paper, the architecture of IoT is discussed, following a comprehensive literature review on ML approaches the importance of security of IoT in terms of different types of possible attacks. Moreover, ML-based potential solutions for IoT security has been presented and future challenges are discussed.","['Syeda Manjia Tahsien', 'Hadis Karimipour', 'Petros Spachos']","['Syeda Manjia Tahsien', 'Hadis Karimipour', 'Petros Spachos']",https://export.arxiv.org/abs/2004.05289,https://export.arxiv.org/abs/2004.05289,2020-11-06
10027.0,,arxiv,Cascade Neural Ensemble for Identifying Scientifically Sound Articles,Cascade Neural Ensemble for Identifying Scientifically Sound Articles,"Background: A significant barrier to conducting systematic reviews and meta-analysis is efficiently finding scientifically sound relevant articles. Typically, less than 1% of articles match this requirement which leads to a highly imbalanced task. Although feature-engineered and early neural networks models were studied for this task, there is an opportunity to improve the results.   Methods: We framed the problem of filtering articles as a classification task, and trained and tested several ensemble architectures of SciBERT, a variant of BERT pre-trained on scientific articles, on a manually annotated dataset of about 50K articles from MEDLINE. Since scientifically sound articles are identified through a multi-step process we proposed a novel cascade ensemble analogous to the selection process. We compared the performance of the cascade ensemble with a single integrated model and other types of ensembles as well as with results from previous studies.   Results: The cascade ensemble architecture achieved 0.7505 F measure, an impressive 49.1% error rate reduction, compared to a CNN model that was previously proposed and evaluated on a selected subset of the 50K articles. On the full dataset, the cascade ensemble achieved 0.7639 F measure, resulting in an error rate reduction of 19.7% compared to the best performance reported in a previous study that used the full dataset.   Conclusion: Pre-trained contextual encoder neural networks (e.g. SciBERT) perform better than the models studied previously and manually created search filters in filtering for scientifically sound relevant articles. The superior performance achieved by the cascade ensemble is a significant result that generalizes beyond this task and the dataset, and is analogous to query optimization in IR and databases.","Background: A significant barrier to conducting systematic reviews and meta-analysis is efficiently finding scientifically sound relevant articles. Typically, less than 1% of articles match this requirement which leads to a highly imbalanced task. Although feature-engineered and early neural networks models were studied for this task, there is an opportunity to improve the results.   Methods: We framed the problem of filtering articles as a classification task, and trained and tested several ensemble architectures of SciBERT, a variant of BERT pre-trained on scientific articles, on a manually annotated dataset of about 50K articles from MEDLINE. Since scientifically sound articles are identified through a multi-step process we proposed a novel cascade ensemble analogous to the selection process. We compared the performance of the cascade ensemble with a single integrated model and other types of ensembles as well as with results from previous studies.   Results: The cascade ensemble architecture achieved 0.7505 F measure, an impressive 49.1% error rate reduction, compared to a CNN model that was previously proposed and evaluated on a selected subset of the 50K articles. On the full dataset, the cascade ensemble achieved 0.7639 F measure, resulting in an error rate reduction of 19.7% compared to the best performance reported in a previous study that used the full dataset.   Conclusion: Pre-trained contextual encoder neural networks (e.g. SciBERT) perform better than the models studied previously and manually created search filters in filtering for scientifically sound relevant articles. The superior performance achieved by the cascade ensemble is a significant result that generalizes beyond this task and the dataset, and is analogous to query optimization in IR and databases.","['Ashwin Karthik Ambalavanan', 'Murthy Devarakonda']","['Ashwin Karthik Ambalavanan', 'Murthy Devarakonda']",https://export.arxiv.org/abs/2004.06222,https://export.arxiv.org/abs/2004.06222,2020-11-06
10028.0,,arxiv,Boosting algorithms in energy research: A systematic review,Boosting algorithms in energy research: A systematic review,"Machine learning algorithms have been extensively exploited in (renewable) energy research, due to their flexibility, automation and ability to handle big data. Among the most prominent machine learning algorithms are the boosting ones, which are known to be 'garnering wisdom from a council of fools', thereby transforming weak learners to strong learners. Boosting algorithms are characterized by both high flexibility and high interpretability. The latter property is the result of recent developments by the statistical community. In this work, we provide understanding on the properties of boosting algorithms to facilitate a better exploitation of their strengths in energy research. In this respect, (a) we summarize recent advances on boosting algorithms, (b) we review relevant applications in energy research with those focusing on renewable energy (in particular those focusing on wind energy and solar energy) consisting a significant portion of the total ones, and (c) we describe how boosting algorithms are implemented and how their use is related to their properties. We show that boosting has been underexploited so far, while great advances in the energy field (in which renewable sources play a key role) are possible both in terms of explanation and interpretation, and in terms of predictive performance.","Machine learning algorithms have been extensively exploited in (renewable) energy research, due to their flexibility, automation and ability to handle big data. Among the most prominent machine learning algorithms are the boosting ones, which are known to be ""garnering wisdom from a council of fools"", thereby transforming weak learners to strong learners. Boosting algorithms are characterized by both high flexibility and high interpretability. The latter property is the result of recent developments by the statistical community. In this work, we provide understanding on the properties of boosting algorithms to facilitate a better exploitation of their strengths in energy research. In this respect, (a) we summarize recent advances on boosting algorithms, (b) we review relevant applications in energy research with those focusing on renewable energy (in particular those focusing on wind energy and solar energy) consisting a significant portion of the total ones, and (c) we describe how boosting algorithms are implemented and how their use is related to their properties. We show that boosting has been underexploited so far, while great advances in the energy field (in which renewable sources play a key role) are possible both in terms of explanation and interpretation, and in terms of predictive performance.","['Hristos Tyralis', 'Georgia Papacharalampous']","['Hristos Tyralis', 'Georgia Papacharalampous']",https://export.arxiv.org/abs/2004.07049,https://export.arxiv.org/abs/2004.07049,2020-11-06
10030.0,,arxiv,Transformer Reasoning Network for Image-Text Matching and Retrieval,Transformer Reasoning Network for Image-Text Matching and Retrieval,"Image-text matching is an interesting and fascinating task in modern AI research. Despite the evolution of deep-learning-based image and text processing systems, multi-modal matching remains a challenging problem. In this work, we consider the problem of accurate image-text matching for the task of multi-modal large-scale information retrieval. State-of-the-art results in image-text matching are achieved by inter-playing image and text features from the two different processing pipelines, usually using mutual attention mechanisms. However, this invalidates any chance to extract separate visual and textual features needed for later indexing steps in large-scale retrieval systems. In this regard, we introduce the Transformer Encoder Reasoning Network (TERN), an architecture built upon one of the modern relationship-aware self-attentive architectures, the Transformer Encoder (TE). This architecture is able to separately reason on the two different modalities and to enforce a final common abstract concept space by sharing the weights of the deeper transformer layers. Thanks to this design, the implemented network is able to produce compact and very rich visual and textual features available for the successive indexing step. Experiments are conducted on the MS-COCO dataset, and we evaluate the results using a discounted cumulative gain metric with relevance computed exploiting caption similarities, in order to assess possibly non-exact but relevant search results. We demonstrate that on this metric we are able to achieve state-of-the-art results in the image retrieval task. Our code is freely available at https://github.com/mesnico/TERN","Image-text matching is an interesting and fascinating task in modern AI research. Despite the evolution of deep-learning-based image and text processing systems, multi-modal matching remains a challenging problem. In this work, we consider the problem of accurate image-text matching for the task of multi-modal large-scale information retrieval. State-of-the-art results in image-text matching are achieved by inter-playing image and text features from the two different processing pipelines, usually using mutual attention mechanisms. However, this invalidates any chance to extract separate visual and textual features needed for later indexing steps in large-scale retrieval systems. In this regard, we introduce the Transformer Encoder Reasoning Network (TERN), an architecture built upon one of the modern relationship-aware self-attentive architectures, the Transformer Encoder (TE). This architecture is able to separately reason on the two different modalities and to enforce a final common abstract concept space by sharing the weights of the deeper transformer layers. Thanks to this design, the implemented network is able to produce compact and very rich visual and textual features available for the successive indexing step. Experiments are conducted on the MS-COCO dataset, and we evaluate the results using a discounted cumulative gain metric with relevance computed exploiting caption similarities, in order to assess possibly non-exact but relevant search results. We demonstrate that on this metric we are able to achieve state-of-the-art results in the image retrieval task. Our code is freely available at https://github.com/mesnico/TERN","['Nicola Messina', 'Fabrizio Falchi', 'Andrea Esuli', 'Giuseppe Amato']","['Nicola Messina', 'Fabrizio Falchi', 'Andrea Esuli', 'Giuseppe Amato']",https://export.arxiv.org/abs/2004.09144,https://export.arxiv.org/abs/2004.09144,2020-11-06
10031.0,,arxiv,Explainable Goal-Driven Agents and Robots -- A Comprehensive Review and   New Framework,Explainable Goal-Driven Agents and Robots -- A Comprehensive Review and   New Framework,"Recent applications of autonomous agents and robots, for example, self-driving cars, scenario-based trainers, exploration robots, service robots, have brought attention to crucial trust-related problems associated with the current generation of artificial intelligence (AI) systems. AI systems particularly dominated by the connectionist deep learning neural network approach lack capabilities of explaining their decisions and actions to others, despite their great successes. They are fundamentally non-intuitive black boxes, which renders their decision or actions opaque, making it difficult to trust them in safety-critical applications. The recent stance on the explainability of AI systems has witnessed several works on eXplainable Artificial Intelligence; however, most of the studies have focused on data-driven XAI systems applied in computational sciences. Studies addressing the increasingly pervasive goal-driven agents and robots are still missing. This paper reviews works on explainable goal-driven intelligent agents and robots, focusing on techniques for explaining and communicating agents perceptual functions (for example, senses, vision, etc.) and cognitive reasoning (for example, beliefs, desires, intention, plans, and goals) with humans in the loop. The review highlights key strategies that emphasize transparency and understandability, and continual learning for explainability. Finally, the paper presents requirements for explainability and suggests a roadmap for the possible realization of effective goal-driven explainable agents and robots","Recent applications of autonomous agents and robots, for example, self-driving cars, scenario-based trainers, exploration robots, service robots, have brought attention to crucial trust-related problems associated with the current generation of artificial intelligence (AI) systems. AI systems particularly dominated by the connectionist deep learning neural network approach lack capabilities of explaining their decisions and actions to others, despite their great successes. They are fundamentally non-intuitive black boxes, which renders their decision or actions opaque, making it difficult to trust them in safety-critical applications. The recent stance on the explainability of AI systems has witnessed several works on eXplainable Artificial Intelligence; however, most of the studies have focused on data-driven XAI systems applied in computational sciences. Studies addressing the increasingly pervasive goal-driven agents and robots are still missing. This paper reviews works on explainable goal-driven intelligent agents and robots, focusing on techniques for explaining and communicating agents perceptual functions (for example, senses, vision, etc.) and cognitive reasoning (for example, beliefs, desires, intention, plans, and goals) with humans in the loop. The review highlights key strategies that emphasize transparency and understandability, and continual learning for explainability. Finally, the paper presents requirements for explainability and suggests a roadmap for the possible realization of effective goal-driven explainable agents and robots","['Fatai Sado', 'Chu Kiong Loo', 'Matthias Kerzel', 'Stefan Wermter']","['Fatai Sado', 'Chu Kiong Loo', 'Matthias Kerzel', 'Stefan Wermter']",https://export.arxiv.org/abs/2004.09705,https://export.arxiv.org/abs/2004.09705,2020-11-06
16603.0,pubmed,arxiv,Building a PubMed knowledge graph,Building a PubMed knowledge graph,"PubMed<sup>Ã‚Â®</sup> is an essential resource for the medical domain, but useful concepts are either difficult to extract or are ambiguous, which has significantly hindered knowledge discovery. To address this issue, we constructed a PubMed knowledge graph (PKG) by extracting bio-entities from 29 million PubMed abstracts, disambiguating author names, integrating funding data through the National Institutes of Health (NIH) ExPORTER, collectingÃ‚Â affiliation history and educational background of authors from ORCID<sup>Ã‚Â®</sup>, and identifyingÃ‚Â fine-grained affiliation data from MapAffil. Through theÃ‚Â integration of these credible multi-source data, we could create connections among the bio-entities, authors, articles, affiliations, and funding. Data validation revealed that the BioBERT deep learning method of bio-entity extraction significantly outperformed the state-of-the-art models based on the F1 score (by 0.51%), with the author name disambiguation (AND) achieving an F1 score of 98.09%. PKG can trigger broader innovations, not only enabling us to measure scholarly impact, knowledge usage, and knowledge transfer, but also assisting us in profiling authors and organizations based on their connections with bio-entities.","PubMed is an essential resource for the medical domain, but useful concepts are either difficult to extract or are ambiguated, which has significantly hindered knowledge discovery. To address this issue, we constructed a PubMed knowledge graph (PKG) by extracting bio-entities from 29 million PubMed abstracts, disambiguating author names, integrating funding data through the National Institutes of Health (NIH) ExPORTER, collecting affiliation history and educational background of authors from ORCID, and identifying fine-grained affiliation data from MapAffil. Through the integration of the credible multi-source data, we could create connections among the bio-entities, authors, articles, affiliations, and funding. Data validation revealed that the BioBERT deep learning method of bio-entity extraction significantly outperformed the state-of-the-art models based on the F1 score (by 0.51%), with the author name disambiguation (AND) achieving a F1 score of 98.09%. PKG can trigger broader innovations, not only enabling us to measure scholarly impact, knowledge usage, and knowledge transfer, but also assisting us in profiling authors and organizations based on their connections with bio-entities. The PKG is freely available on Figshare (https://figshare.com/s/6327a55355fc2c99f3a2, simplified version that exclude PubMed raw data) and TACC website (http://er.tacc.utexas.edu/datasets/ped, full version).","Xu, Kim, Song, Jeong, Kim, Kang, Rousseau, Li, Xu, Torvik, Bu, Chen, Ebeid, Li, Ding","['Jian Xu', 'Sunkyu Kim', 'Min Song', 'Minbyul Jeong', 'Donghyeon Kim', 'Jaewoo Kang', 'Justin F. Rousseau', 'Xin Li', 'Weijia Xu', 'Vetle I. Torvik', 'Yi Bu', 'Chongyan Chen', 'Islam Akef Ebeid', 'Daifeng Li', 'Ying Ding']",https://doi.org/10.1038/s41597-020-0543-2,https://export.arxiv.org/abs/2005.04308,2020-11-06
16306.0,pubmed,arxiv,Deep Learning for LiDAR Point Clouds in Autonomous Driving: A Review,Deep Learning for LiDAR Point Clouds in Autonomous Driving: A Review,"Recently, the advancement of deep learning (DL) in discriminative feature learning from 3-D LiDAR data has led to rapid development in the field of autonomous driving. However, automated processing uneven, unstructured, noisy, and massive 3-D point clouds are a challenging and tedious task. In this article, we provide a systematic review of existing compelling DL architectures applied in LiDAR point clouds, detailing for specific tasks in autonomous driving, such as segmentation, detection, and classification. Although several published research articles focus on specific topics in computer vision for autonomous vehicles, to date, no general survey on DL applied in LiDAR point clouds for autonomous vehicles exists. Thus, the goal of this article is to narrow the gap in this topic. More than 140 key contributions in the recent five years are summarized in this survey, including the milestone 3-D deep architectures, the remarkable DL applications in 3-D semantic segmentation, object detection, and classification; specific data sets, evaluation metrics, and the state-of-the-art performance. Finally, we conclude the remaining challenges and future researches.","Recently, the advancement of deep learning in discriminative feature learning from 3D LiDAR data has led to rapid development in the field of autonomous driving. However, automated processing uneven, unstructured, noisy, and massive 3D point clouds is a challenging and tedious task. In this paper, we provide a systematic review of existing compelling deep learning architectures applied in LiDAR point clouds, detailing for specific tasks in autonomous driving such as segmentation, detection, and classification. Although several published research papers focus on specific topics in computer vision for autonomous vehicles, to date, no general survey on deep learning applied in LiDAR point clouds for autonomous vehicles exists. Thus, the goal of this paper is to narrow the gap in this topic. More than 140 key contributions in the recent five years are summarized in this survey, including the milestone 3D deep architectures, the remarkable deep learning applications in 3D semantic segmentation, object detection, and classification; specific datasets, evaluation metrics, and the state of the art performance. Finally, we conclude the remaining challenges and future researches.","Li, Ma, Zhong, Liu, Chapman, Cao, Li","['Ying Li', 'Lingfei Ma', 'Zilong Zhong', 'Fei Liu', 'Dongpu Cao', 'Jonathan Li', 'Michael A. Chapman']",https://doi.org/10.1109/TNNLS.2020.3015992,https://export.arxiv.org/abs/2005.09830,2020-11-06
9109.0,,arxiv,Random Walks: A Review of Algorithms and Applications,Random Walks: A Review of Algorithms and Applications,"A random walk is known as a random process which describes a path including a succession of random steps in the mathematical space. It has increasingly been popular in various disciplines such as mathematics and computer science. Furthermore, in quantum mechanics, quantum walks can be regarded as quantum analogues of classical random walks. Classical random walks and quantum walks can be used to calculate the proximity between nodes and extract the topology in the network. Various random walk related models can be applied in different fields, which is of great significance to downstream tasks such as link prediction, recommendation, computer vision, semi-supervised learning, and network embedding. In this article, we aim to provide a comprehensive review of classical random walks and quantum walks. We first review the knowledge of classical random walks and quantum walks, including basic concepts and some typical algorithms. We also compare the algorithms based on quantum walks and classical random walks from the perspective of time complexity. Then we introduce their applications in the field of computer science. Finally we discuss the open issues from the perspectives of efficiency, main-memory volume, and computing time of existing algorithms. This study aims to contribute to this growing area of research by exploring random walks and quantum walks together.","A random walk is known as a random process which describes a path including a succession of random steps in the mathematical space. It has increasingly been popular in various disciplines such as mathematics and computer science. Furthermore, in quantum mechanics, quantum walks can be regarded as quantum analogues of classical random walks. Classical random walks and quantum walks can be used to calculate the proximity between nodes and extract the topology in the network. Various random walk related models can be applied in different fields, which is of great significance to downstream tasks such as link prediction, recommendation, computer vision, semi-supervised learning, and network embedding. In this paper, we aim to provide a comprehensive review of classical random walks and quantum walks. We first review the knowledge of classical random walks and quantum walks, including basic concepts and some typical algorithms. We also compare the algorithms based on quantum walks and classical random walks from the perspective of time complexity. Then we introduce their applications in the field of computer science. Finally we discuss the open issues from the perspectives of efficiency, main-memory volume, and computing time of existing algorithms. This study aims to contribute to this growing area of research by exploring random walks and quantum walks together.","F. Xia
 and J. Liu
 and H. Nie
 and Y. Fu
 and L. Wan
 and X. Kong","['Feng Xia', 'Jiaying Liu', 'Hansong Nie', 'Yonghao Fu', 'Liangtian Wan', 'Xiangjie Kong']",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911513,https://export.arxiv.org/abs/2008.03639,2020-11-06
17250.0,arxiv,dblp,Plant Diseases recognition on images using Convolutional Neural   Networks: A Systematic Review,Plant diseases recognition on images using convolutional neural networks: A systematic review,"Plant diseases are considered one of the main factors influencing food production and minimize losses in production, and it is essential that crop diseases have fast detection and recognition. The recent expansion of deep learning methods has found its application in plant disease detection, offering a robust tool with highly accurate results. In this context, this work presents a systematic review of the literature that aims to identify the state of the art of the use of convolutional neural networks(CNN) in the process of identification and classification of plant diseases, delimiting trends, and indicating gaps. In this sense, we present 121 papers selected in the last ten years with different approaches to treat aspects related to disease detection, characteristics of the data set, the crops and pathogens investigated. From the results of the systematic review, it is possible to understand the innovative trends regarding the use of CNNs in the identification of plant diseases and to identify the gaps that need the attention of the research community.",,"['Andre S. Abade', 'Paulo Afonso Ferreira', 'Flavio de Barros Vidal']",Andre da Silva Abade; Paulo Afonso Ferreira; Flavio de Barros Vidal,https://export.arxiv.org/abs/2009.04365,https://doi.org/10.1016/j.compag.2021.106125,2021-08-03
4668.0,,dblp,Using Visual Text Mining to Support the Study Selection Activity in Systematic Literature Reviews,Using Visual Text Mining to Support the Study Selection Activity in Systematic Literature Reviews,"Background: A systematic literature review (SLR) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process used to select primary studies can be arduous, time consuming, and must often be conducted manually. Objective: We propose a novel approach, known as 'Systematic Literature Review based on Visual Text Mining' or simply SLR-VTM, to support the primary study selection activity using visual text mining (VTM) techniques. Method: We conducted a case study to compare the performance and effectiveness of four doctoral students in selecting primary studies manually and using the SLR-VTM approach. To enable the comparison, we also developed a VTM tool that implemented our approach. We hypothesized that students using SLR-VTM would present improved selection performance and effectiveness. Results: Our results show that incorporating VTM in the SLR study selection activity reduced the time spent in this activity and also increased the number of studies correctly included. Conclusions: Our pilot case study presents promising results suggesting that the use of VTM may indeed be beneficial during the study selection activity when performing an SLR.",,"K. R. Felizardo
 and N. Salleh
 and R. M. Martins
 and E. Mendes
 and S. G. MacDonell
 and J. C. Maldonado",Katia Romero Felizardo; Norsaremah Salleh; Rafael Messias Martins; Emilia Mendes; Stephen G. MacDonell; José Carlos Maldonado,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092556,https://arxiv.org/abs/2102.02934,2021-08-03
10022.0,,dblp,Deep Learning Based Text Classification: A Comprehensive Review,Deep Learning-based Text Classification: A Comprehensive Review,"Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this work, we provide a detailed review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Deep Learning Based Text Classification: A Comprehensive Review.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs https://arxiv.org/abs/2004.03705; Years 2020; Authors Shervin Minaee; Nal Kalchbrenner; Erik Cambria; Narjes Nikzad; Meysam Chenaghlu; Jianfeng Gao; Deduplication_Notes ; X Deep Learning Based Text Classification: A Comprehensive Review.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]",,"['Shervin Minaee', 'Nal Kalchbrenner', 'Erik Cambria', 'Narjes Nikzad', 'Meysam Chenaghlu', 'Jianfeng Gao']",Shervin Minaee; Nal Kalchbrenner; Erik Cambria; Narjes Nikzad; Meysam Chenaghlu; Jianfeng Gao,https://export.arxiv.org/abs/2004.03705,https://doi.org/10.1145/3439726,2021-08-03
17201.0,arxiv,dblp,A Systematic Literature Review on Federated Machine Learning: From A   Software Engineering Perspective,A Systematic Literature Review on Federated Machine Learning: From a Software Engineering Perspective,"Federated learning is an emerging machine learning paradigm where multiple clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning from a software engineering perspective, we performed a systematic literature review with the extracted 231 primary studies. The results show that most of the known motivations of federated learning appear to be the most studied federated learning challenges, such as communication efficiency and statistical heterogeneity. Also, there are only a few real-world applications of federated learning. Hence, more studies in this area are needed before the actual industrial-level adoption of federated learning.",,"['Sin Kit Lo', 'Qinghua Lu', 'Chen Wang', 'Hye-Young Paik', 'Liming Zhu']",Sin Kit Lo; Qinghua Lu 0001; Chen Wang 0008; Hye-Young Paik; Liming Zhu,https://export.arxiv.org/abs/2007.11354,https://doi.org/10.1145/3450288,2021-08-03
17122.0,arxiv,dblp,A scoping review of transfer learning research on medical image analysis   using ImageNet,A scoping review of transfer learning research on medical image analysis using ImageNet,"Objective: Employing transfer learning (TL) with convolutional neural networks (CNNs), well-trained on non-medical ImageNet dataset, has shown promising results for medical image analysis in recent years. We aimed to conduct a scoping review to identify these studies and summarize their characteristics in terms of the problem description, input, methodology, and outcome. Materials and Methods: To identify relevant studies, MEDLINE, IEEE, and ACM digital library were searched. Two investigators independently reviewed articles to determine eligibility and to extract data according to a study protocol defined a priori. Results: After screening of 8,421 articles, 102 met the inclusion criteria. Of 22 anatomical areas, eye (18%), breast (14%), and brain (12%) were the most commonly studied. Data augmentation was performed in 72% of fine-tuning TL studies versus 15% of the feature-extracting TL studies. Inception models were the most commonly used in breast related studies (50%), while VGGNet was the common in eye (44%), skin (50%) and tooth (57%) studies. AlexNet for brain (42%) and DenseNet for lung studies (38%) were the most frequently used models. Inception models were the most frequently used for studies that analyzed ultrasound (55%), endoscopy (57%), and skeletal system X-rays (57%). VGGNet was the most common for fundus (42%) and optical coherence tomography images (50%). AlexNet was the most frequent model for brain MRIs (36%) and breast X-Rays (50%). 35% of the studies compared their model with other well-trained CNN models and 33% of them provided visualization for interpretation. Discussion: Various methods have been used in TL approaches from non-medical to medical image analysis. The findings of the scoping review can be used in future TL studies to guide the selection of appropriate research approaches, as well as identify research gaps and opportunities for innovation.",,"['Mohammad Amin Morid', 'Alireza Borjali', 'Guilherme Del Fiol']",Mohammad Amin Morid; Alireza Borjali; Guilherme Del Fiol,https://export.arxiv.org/abs/2004.13175,https://doi.org/10.1016/j.compbiomed.2020.104115 ; https://www.wikidata.org/entity/Q102331835,2021-08-03
17179.0,arxiv,dblp,MIRA: Leveraging Multi-Intention Co-click Information in Web-scale   Document Retrieval using Deep Neural Networks,MIRA: Leveraging Multi-Intention Co-click Information in Web-scale Document Retrieval using Deep Neural Networks,"We study the problem of deep recall model in industrial web search, which is, given a user query, retrieve hundreds of most relevance documents from billions of candidates. The common framework is to train two encoding models based on neural embedding which learn the distributed representations of queries and documents separately and match them in the latent semantic space. However, all the exiting encoding models only leverage the information of the document itself, which is often not sufficient in practice when matching with query terms, especially for the hard tail queries. In this work we aim to leverage the additional information for each document from its co-click neighbour to help document retrieval. The challenges include how to effectively extract information and eliminate noise when involving co-click information in deep model while meet the demands of billion-scale data size for real time online inference.   To handle the noise in co-click relations, we firstly propose a web-scale Multi-Intention Co-click document Graph(MICG) which builds the co-click connections between documents on click intention level but not on document level. Then we present an encoding framework MIRA based on Bert and graph attention networks which leverages a two-factor attention mechanism to aggregate neighbours. To meet the online latency requirements, we only involve neighbour information in document side, which can save the time-consuming query neighbor search in real time serving. We conduct extensive offline experiments on both public dataset and private web-scale dataset from two major commercial search engines demonstrating the effectiveness and scalability of the proposed method compared with several baselines. And a further case study reveals that co-click relations mainly help improve web search quality from two aspects: key concept enhancing and query term complementary.",,"['Yusi Zhang', 'Chuanjie Liu', 'Angen Luo', 'Hui Xue', 'Xuan Shan', 'Yuxiang Luo', 'Yiqian Xia', 'Yuanchi Yan', 'Haidong Wang']",Yusi Zhang; Chuanjie Liu; Angen Luo; Hui Xue; Xuan Shan; Yuxiang Luo; Yiqian Xia; Yuanchi Yan; Haidong Wang,https://export.arxiv.org/abs/2007.01510,https://doi.org/10.1145/3442381.3449865,2021-08-03
4668.0,,arxiv,Using Visual Text Mining to Support the Study Selection Activity in Systematic Literature Reviews,Using Visual Text Mining to Support the Study Selection Activity in   Systematic Literature Reviews,"Background: A systematic literature review (SLR) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process used to select primary studies can be arduous, time consuming, and must often be conducted manually. Objective: We propose a novel approach, known as 'Systematic Literature Review based on Visual Text Mining' or simply SLR-VTM, to support the primary study selection activity using visual text mining (VTM) techniques. Method: We conducted a case study to compare the performance and effectiveness of four doctoral students in selecting primary studies manually and using the SLR-VTM approach. To enable the comparison, we also developed a VTM tool that implemented our approach. We hypothesized that students using SLR-VTM would present improved selection performance and effectiveness. Results: Our results show that incorporating VTM in the SLR study selection activity reduced the time spent in this activity and also increased the number of studies correctly included. Conclusions: Our pilot case study presents promising results suggesting that the use of VTM may indeed be beneficial during the study selection activity when performing an SLR.","Background: A systematic literature review (SLR) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process used to select primary studies can be arduous, time consuming, and must often be conducted manually. Objective: We propose a novel approach, known as 'Systematic Literature Review based on Visual Text Mining' or simply SLR-VTM, to support the primary study selection activity using visual text mining (VTM) techniques. Method: We conducted a case study to compare the performance and effectiveness of four doctoral students in selecting primary studies manually and using the SLR-VTM approach. To enable the comparison, we also developed a VTM tool that implemented our approach. We hypothesized that students using SLR-VTM would present improved selection performance and effectiveness. Results: Our results show that incorporating VTM in the SLR study selection activity reduced the time spent in this activity and also increased the number of studies correctly included. Conclusions: Our pilot case study presents promising results suggesting that the use of VTM may indeed be beneficial during the study selection activity when performing an SLR.","K. R. Felizardo
 and N. Salleh
 and R. M. Martins
 and E. Mendes
 and S. G. MacDonell
 and J. C. Maldonado","['Katia Romero Felizardo', 'Norsaremah Salleh', 'Rafael M. Martins', 'Emília Mendes', 'Stephen G. MacDonell', 'José Carlos Maldonado']",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092556,https://export.arxiv.org/abs/2102.02934,2021-08-03
2330.0,,pubmed,Validity of automated threshold audiometry: a systematic review and meta-analysis,Validity of Automated Threshold Audiometry: A Systematic Review and Meta-Analysis,"OBJECTIVES: A systematic literature review and meta-analysis on the validity (test-retest reliability and accuracy) of automated threshold audiometry compared with the gold standard of manual threshold audiometry was conducted. DESIGN: A systematic literature review was completed in peer-reviewed databases on automated compared with manual threshold audiometry. Subsequently a meta-analysis was conducted on the validity of automated audiometry. METHODS: A multifaceted approach, covering several databases and using different search strategies was used to ensure comprehensive coverage and to cross-check search findings. Databases included: MEDLINE, Scopus, and PubMed; a secondary search strategy was the review of references from identified reports. Reports including within-subject comparisons of manual and automated threshold audiometry were selected according to inclusion/exclusion criteria before data were extracted. For the meta-analysis weighted mean differences (and standard deviations) on test-retest reliability for automated compared with manual audiometry were determined to assess the validity of automated threshold audiometry. RESULTS: In total, 29 reports on automated audiometry (method of limits and the method of adjustment techniques) met the inclusion criteria and were included in this review. Most reports included data on adult populations using air conduction testing with limited data on children, bone conduction testing and the effects of hearing status on automated audiometry. Meta-analysis test-retest reliability for automated audiometry was within typical test-retest variability for manual audiometry. Accuracy results on the meta-analysis indicated overall average differences between manual and automated air conduction audiometry (0.4 dB, 6.1 SD) to be comparable with test-retest differences for manual (1.3 dB, 6.1 SD) and automated (0.3 dB, 6.9 SD) audiometry. No significant differences (p > 0.01; summarized data analysis of variance) were seen in any of the comparisons between test-retest reliability of manual and automated audiometry compared with differences between manual and automated audiometry. CONCLUSION: Automated audiometry provides an accurate measure of hearing threshold, but validation data are still limited for (1) automated bone conduction audiometry; (2) automated audiometry in children and difficult-to-test populations and; (3) different types and degrees of hearing loss.","A systematic literature review and meta-analysis on the validity (test-retest reliability and accuracy) of automated threshold audiometry compared with the gold standard of manual threshold audiometry was conducted. A systematic literature review was completed in peer-reviewed databases on automated compared with manual threshold audiometry. Subsequently a meta-analysis was conducted on the validity of automated audiometry. A multifaceted approach, covering several databases and using different search strategies was used to ensure comprehensive coverage and to cross-check search findings. Databases included: MEDLINE, SCOPUS, and PubMed with a secondary search strategy reviewing references from identified reports. Reports including within-subject comparisons of manual and automated threshold audiometry were selected according to inclusion/exclusion criteria before data were extracted. For the meta-analysis weighted mean differences (and standard deviations) on test-retest reliability for automated compared with manual audiometry were determined to assess the validity of automated threshold audiometry. In total, 29 reports on automated audiometry (method of limits and the method of adjustment techniques) met the inclusion criteria and were included in this review. Most reports included data on adult populations using air conduction testing with limited data on children, bone conduction testing, and the effects of hearing status on automated audiometry. Meta-analysis test-retest reliability for automated audiometry was within typical test-retest variability for manual audiometry. Accuracy results on the meta-analysis indicated overall average differences between manual and automated air conduction audiometry (0.4 dB; 6.1 SD) to be comparable with test-retest differences for manual (1.3 dB; 6.1 SD) and automated (0.3 dB; 6.9 SD) audiometry. Nosignificant differences (p &gt; 0.01; summarized data analysis of variance) were seen in any of the comparisons between test-retest reliability of manual and automated audiometry compared with differences between manual and automated audiometry. Automated audiometry provides an accurate measure of hearing threshold, but validation data are still limited for (a) automated bone conduction audiometry; (b) automated audiometry in children and difficult-to-test populations; and (c) different types and degrees of hearing loss.","Mahomed, F.
 and Swanepoel de, W.
 and Eikelboom, R. H.
 and Soer, M.","Mahomed, Swanepoel, Eikelboom, Soer",https://dx.doi.org/10.1097/01.aud.0000436255.53747.a4,https://doi.org/10.1097/AUD.0b013e3182944bdf,2021-08-03
17283.0,arxiv,pubmed,Deep Representation Learning of Patient Data from Electronic Health   Records (EHR): A Systematic Review,Deep representation learning of patient data from Electronic Health Records (EHR): A systematic review,"Patient representation learning refers to learning a dense mathematical representation of a patient that encodes meaningful information from Electronic Health Records (EHRs). This is generally performed using advanced deep learning methods. This study presents a systematic review of this field and provides both qualitative and quantitative analyses from a methodological perspective. We identified studies developing patient representations from EHRs with deep learning methods from MEDLINE, EMBASE, Scopus, the Association for Computing Machinery (ACM) Digital Library, and Institute of Electrical and Electronics Engineers (IEEE) Xplore Digital Library. After screening 362 articles, 48 papers were included for a comprehensive data collection. We noticed a typical workflow starting with feeding raw data, applying deep learning models, and ending with clinical outcome predictions as evaluations of the learned representations. Specifically, learning representations from structured EHR data was dominant (36 out of 48 studies). Recurrent Neural Networks were widely applied as the deep learning architecture (LSTM: 13 studies, GRU: 11 studies). Disease prediction was the most common application and evaluation (30 studies). Benchmark datasets were mostly unavailable (28 studies) due to privacy concerns of EHR data, and code availability was assured in 20 studies. We show the importance and feasibility of learning comprehensive representations of patient EHR data through a systematic review. Advances in patient representation learning techniques will be essential for powering patient-level EHR analyses. Future work will still be devoted to leveraging the richness and potential of available EHR data. Knowledge distillation and advanced learning techniques will be exploited to assist the capability of learning patient representation further.","Patient representation learning refers to learning a dense mathematical representation of a patient that encodes meaningful information from Electronic Health Records (EHRs). This is generally performed using advanced deep learning methods. This study presents a systematic review of this field and provides both qualitative and quantitative analyses from a methodological perspective. We identified studies developing patient representations from EHRs with deep learning methods from MEDLINE, EMBASE, Scopus, the Association for Computing Machinery (ACM) Digital Library, and the Institute of Electrical and Electronics Engineers (IEEE) Xplore Digital Library. After screening 363 articles, 49 papers were included for a comprehensive data collection. Publications developing patient representations almost doubled each year from 2015 until 2019. We noticed a typical workflow starting with feeding raw data, applying deep learning models, and ending with clinical outcome predictions as evaluations of the learned representations. Specifically, learning representations from structured EHR data was dominant (37 out of 49 studies). Recurrent Neural Networks were widely applied as the deep learning architecture (Long short-term memory: 13 studies, Gated recurrent unit: 11 studies). Learning was mainly performed in a supervised manner (30 studies) optimized with cross-entropy loss. Disease prediction was the most common application and evaluation (31 studies). Benchmark datasets were mostly unavailable (28 studies) due to privacy concerns of EHR data, and code availability was assured in 20 studies. The existing predictive models mainly focus on the prediction of single diseases, rather than considering the complex mechanisms of patients from a holistic review. We show the importance and feasibility of learning comprehensive representations of patient EHR data through a systematic review. Advances in patient representation learning techniques will be essential for powering patient-level EHR analyses. Future work will still be devoted to leveraging the richness and potential of available EHR data. Reproducibility and transparency of reported results will hopefully improve. Knowledge distillation and advanced learning techniques will be exploited to assist the capability of learning patient representation further.","['Yuqi Si', 'Jingcheng Du', 'Zhao Li', 'Xiaoqian Jiang', 'Timothy Miller', 'Fei Wang', 'W. Jim Zheng', 'Kirk Roberts']","Si, Du, Li, Jiang, Miller, Wang, Jim Zheng, Roberts",https://export.arxiv.org/abs/2010.02809,https://doi.org/10.1016/j.jbi.2020.103671,2021-08-03
17203.0,arxiv,pubmed,A Comprehensive Review of Deep Learning Applications in Hydrology and   Water Resources,A comprehensive review of deep learning applications in hydrology and water resources,"The global volume of digital data is expected to reach 175 zettabytes by 2025. The volume, variety, and velocity of water-related data are increasing due to large-scale sensor networks and increased attention to topics such as disaster response, water resources management, and climate change. Combined with the growing availability of computational resources and popularity of deep learning, these data are transformed into actionable and practical knowledge, revolutionizing the water industry. In this article, a systematic review of literature is conducted to identify existing research which incorporates deep learning methods in the water sector, with regard to monitoring, management, governance and communication of water resources. The study provides a comprehensive review of state-of-the-art deep learning approaches used in the water industry for generation, prediction, enhancement, and classification tasks, and serves as a guide for how to utilize available deep learning methods for future water resources challenges. Key issues and challenges in the application of these techniques in the water domain are discussed, including the ethics of these technologies for decision-making in water resources management and governance. Finally, we provide recommendations and future directions for the application of deep learning models in hydrology and water resources.","The global volume of digital data is expected to reach 175 zettabytes by 2025. The volume, variety and velocity of water-related data are increasing due to large-scale sensor networks and increased attention to topics such as disaster response, water resources management, and climate change. Combined with the growing availability of computational resources and popularity of deep learning, these data are transformed into actionable and practical knowledge, revolutionizing the water industry. In this article, a systematic review of literature is conducted to identify existing research that incorporates deep learning methods in the water sector, with regard to monitoring, management, governance and communication of water resources. The study provides a comprehensive review of state-of-the-art deep learning approaches used in the water industry for generation, prediction, enhancement, and classification tasks, and serves as a guide for how to utilize available deep learning methods for future water resources challenges. Key issues and challenges in the application of these techniques in the water domain are discussed, including the ethics of these technologies for decision-making in water resources management and governance. Finally, we provide recommendations and future directions for the application of deep learning models in hydrology and water resources.","['Muhammed Sit', 'Bekir Z. Demiray', 'Zhongrun Xiang', 'Gregory J. Ewing', 'Yusuf Sermet', 'Ibrahim Demir']","Sit, Demiray, Xiang, Ewing, Sermet, Demir",https://export.arxiv.org/abs/2007.12269,https://doi.org/10.2166/wst.2020.369,2021-08-03
16211.0,pubmed,pubmed,Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension,Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension,"The SPIRIT 2013 statement aims to improve the completeness of clinical trial protocol reporting by providing evidence-based recommendations for the minimum set of items to be addressed. This guidance has been instrumental in promoting transparent evaluation of new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate their impact on health outcomes. The SPIRIT-AI (Standard Protocol Items: Recommendations for Interventional Trials-Artificial Intelligence) extension is a new reporting guideline for clinical trial protocols evaluating interventions with an AI component. It was developed in parallel with its companion statement for trial reports: CONSORT-AI (Consolidated Standards of Reporting Trials-Artificial Intelligence). Both guidelines were developed through a staged consensus process involving literature review and expert consultation to generate 26 candidate items, which were consulted upon by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed upon in a consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The SPIRIT-AI extension includes 15 new items that were considered sufficiently important for clinical trial protocols of AI interventions. These new items should be routinely reported in addition to the core SPIRIT 2013 items. SPIRIT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention will be integrated, considerations for the handling of input and output data, the human-AI interaction and analysis of error cases. SPIRIT-AI will help promote transparency and completeness for clinical trial protocols for AI interventions. Its use will assist editors and peer reviewers, as well as the general readership, to understand, interpret and critically appraise the design and risk of bias for a planned clinical trial.","The SPIRIT 2013 statement aims to improve the completeness of clinical trial protocol reporting by providing evidence-based recommendations for the minimum set of items to be addressed. This guidance has been instrumental in promoting transparent evaluation of new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate their impact on health outcomes. The SPIRIT-AI (Standard Protocol Items: Recommendations for Interventional Trials-Artificial Intelligence) extension is a new reporting guideline for clinical trial protocols evaluating interventions with an AI component. It was developed in parallel with its companion statement for trial reports: CONSORT-AI (Consolidated Standards of Reporting Trials-Artificial Intelligence). Both guidelines were developed through a staged consensus process involving literature review and expert consultation to generate 26 candidate items, which were consulted upon by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed upon in a consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The SPIRIT-AI extension includes 15 new items that were considered sufficiently important for clinical trial protocols of AI interventions. These new items should be routinely reported in addition to the core SPIRIT 2013 items. SPIRIT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention will be integrated, considerations for the handling of input and output data, the human-AI interaction and analysis of error cases. SPIRIT-AI will help promote transparency and completeness for clinical trial protocols for AI interventions. Its use will assist editors and peer reviewers, as well as the general readership, to understand, interpret, and critically appraise the design and risk of bias for a planned clinical trial.","Cruz Rivera, Liu, Chan, Denniston, Calvert, Darzi, Holmes, Yau, Moher, Ashrafian, Deeks, Ferrante di Ruffano, Faes, Keane, Vollmer, Lee, Jonas, Esteva, Beam, Panico, Lee, Haug, Kelly, Yau, Mulrow, Espinoza, Fletcher, Moher, Paltoo, Manna, Price, Collins, Harvey, Matcham, Monteiro, ElZarrad, Ferrante di Ruffano, Oakden-Rayner, McCradden, Keane, Savage, Golub, Sarkar, Rowley","Cruz Rivera, Liu, Chan, Denniston, Calvert, Ashrafian, Beam, Collins, Darzi, Deeks, ElZarrad, Espinoza, Esteva, Faes, Ferrante di Ruffano, Fletcher, Golub, Harvey, Haug, Holmes, Jonas, Keane, Kelly, Lee, Lee, Manna, Matcham, McCradden, Moher, Monteiro, Mulrow, Oakden-Rayner, Paltoo, Panico, Price, Rowley, Savage, Sarkar, Vollmer, Yau",https://doi.org/10.1038/s41591-020-1037-7,https://doi.org/10.1016/S2589-7500(20)30219-3,2021-08-03
16212.0,pubmed,pubmed,Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension,Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension,"The CONSORT 2010 statement provides minimum guidelines for reporting randomized trials. Its widespread use has been instrumental in ensuring transparency in the evaluation of new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate impact on health outcomes. The CONSORT-AI (Consolidated Standards of Reporting Trials-Artificial Intelligence) extension is a new reporting guideline for clinical trials evaluating interventions with an AI component. It was developed in parallel with its companion statement for clinical trial protocols: SPIRIT-AI (Standard Protocol Items: Recommendations for Interventional Trials-Artificial Intelligence). Both guidelines were developed through a staged consensus process involving literature review and expert consultation to generate 29 candidate items, which were assessed by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed upon in a two-day consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The CONSORT-AI extension includes 14 new items that were considered sufficiently important for AI interventions that they should be routinely reported in addition to the core CONSORT 2010 items. CONSORT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention is integrated, the handling of inputs and outputs of the AI intervention, the human-AI interaction and provision of an analysis of error cases. CONSORT-AI will help promote transparency and completeness in reporting clinical trials for AI interventions. It will assist editors and peer reviewers, as well as the general readership, to understand, interpret and critically appraise the quality of clinical trial design and risk of bias in the reported outcomes.","The CONSORT 2010 statement provides minimum guidelines for reporting randomised trials. Its widespread use has been instrumental in ensuring transparency in the evaluation of new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate impact on health outcomes. The CONSORT-AI (Consolidated Standards of Reporting Trials-Artificial Intelligence) extension is a new reporting guideline for clinical trials evaluating interventions with an AI component. It was developed in parallel with its companion statement for clinical trial protocols: SPIRIT-AI (Standard Protocol Items: Recommendations for Interventional Trials-Artificial Intelligence). Both guidelines were developed through a staged consensus process involving literature review and expert consultation to generate 29 candidate items, which were assessed by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed upon in a two-day consensus meeting (31 stakeholders), and refined through a checklist pilot (34 participants). The CONSORT-AI extension includes 14 new items that were considered sufficiently important for AI interventions that they should be routinely reported in addition to the core CONSORT 2010 items. CONSORT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention is integrated, the handling of inputs and outputs of the AI intervention, the human-AI interaction and provision of an analysis of error cases. CONSORT-AI will help promote transparency and completeness in reporting clinical trials for AI interventions. It will assist editors and peer reviewers, as well as the general readership, to understand, interpret, and critically appraise the quality of clinical trial design and risk of bias in the reported outcomes.","Liu, Cruz Rivera, Moher, Calvert, Denniston","Liu, Cruz Rivera, Moher, Calvert, Denniston, Ashrafian, Beam, Chan, Collins, Deeks, ElZarrad, Espinoza, Esteva, Faes, Ferrante di Ruffano, Fletcher, Golub, Harvey, Haug, Holmes, Jonas, Keane, Kelly, Lee, Lee, Manna, Matcham, McCradden, Monteiro, Mulrow, Oakden-Rayner, Paltoo, Panico, Price, Rowley, Savage, Sarkar, Vollmer, Yau",https://doi.org/10.1038/s41591-020-1034-x,https://doi.org/10.1016/S2589-7500(20)30218-1,2021-08-03
17122.0,arxiv,pubmed,A scoping review of transfer learning research on medical image analysis   using ImageNet,A scoping review of transfer learning research on medical image analysis using ImageNet,"Objective: Employing transfer learning (TL) with convolutional neural networks (CNNs), well-trained on non-medical ImageNet dataset, has shown promising results for medical image analysis in recent years. We aimed to conduct a scoping review to identify these studies and summarize their characteristics in terms of the problem description, input, methodology, and outcome. Materials and Methods: To identify relevant studies, MEDLINE, IEEE, and ACM digital library were searched. Two investigators independently reviewed articles to determine eligibility and to extract data according to a study protocol defined a priori. Results: After screening of 8,421 articles, 102 met the inclusion criteria. Of 22 anatomical areas, eye (18%), breast (14%), and brain (12%) were the most commonly studied. Data augmentation was performed in 72% of fine-tuning TL studies versus 15% of the feature-extracting TL studies. Inception models were the most commonly used in breast related studies (50%), while VGGNet was the common in eye (44%), skin (50%) and tooth (57%) studies. AlexNet for brain (42%) and DenseNet for lung studies (38%) were the most frequently used models. Inception models were the most frequently used for studies that analyzed ultrasound (55%), endoscopy (57%), and skeletal system X-rays (57%). VGGNet was the most common for fundus (42%) and optical coherence tomography images (50%). AlexNet was the most frequent model for brain MRIs (36%) and breast X-Rays (50%). 35% of the studies compared their model with other well-trained CNN models and 33% of them provided visualization for interpretation. Discussion: Various methods have been used in TL approaches from non-medical to medical image analysis. The findings of the scoping review can be used in future TL studies to guide the selection of appropriate research approaches, as well as identify research gaps and opportunities for innovation.","Employing transfer learning (TL) with convolutional neural networks (CNNs), well-trained on non-medical ImageNet dataset, has shown promising results for medical image analysis in recent years. We aimed to conduct a scoping review to identify these studies and summarize their characteristics in terms of the problem description, input, methodology, and outcome. To identify relevant studies, MEDLINE, IEEE, and ACM digital library were searched for studies published between June 1st<sup>,</sup> 2012 and January 2nd, 2020. Two investigators independently reviewed articles to determine eligibility and to extract data according to a study protocol defined a priori. After screening of 8421 articles, 102 met the inclusion criteria. Of 22 anatomical areas, eye (18%), breast (14%), and brain (12%) were the most commonly studied. Data augmentation was performed in 72% of fine-tuning TL studies versus 15% of the feature-extracting TL studies. Inception models were the most commonly used in breast related studies (50%), while VGGNet was the common in eye (44%), skin (50%) and tooth (57%) studies. AlexNet for brain (42%) and DenseNet for lung studies (38%) were the most frequently used models. Inception models were the most frequently used for studies that analyzed ultrasound (55%), endoscopy (57%), and skeletal system X-rays (57%). VGGNet was the most common for fundus (42%) and optical coherence tomography images (50%). AlexNet was the most frequent model for brain MRIs (36%) and breast X-Rays (50%). 35% of the studies compared their model with other well-trained CNN models and 33% of them provided visualization for interpretation. This study identified the most prevalent tracks of implementation in the literature for data preparation, methodology selection and output evaluation for various medical image analysis tasks. Also, we identified several critical research gaps existing in the TL studies on medical image analysis. The findings of this scoping review can be used in future TL studies to guide the selection of appropriate research approaches, as well as identify research gaps and opportunities for innovation.","['Mohammad Amin Morid', 'Alireza Borjali', 'Guilherme Del Fiol']","Morid, Borjali, Del Fiol",https://export.arxiv.org/abs/2004.13175,https://doi.org/10.1016/j.compbiomed.2020.104115,2021-08-03
17293.0,arxiv,pubmed,"Artificial Intelligence, speech and language processing approaches to   monitoring Alzheimer's Disease: a systematic review","Artificial Intelligence, Speech, and Language Processing Approaches to Monitoring Alzheimer's Disease: A Systematic Review","Language is a valuable source of clinical information in Alzheimer's Disease, as it declines concurrently with neurodegeneration. Consequently, speech and language data have been extensively studied in connection with its diagnosis. This paper summarises current findings on the use of artificial intelligence, speech and language processing to predict cognitive decline in the context of Alzheimer's Disease, detailing current research procedures, highlighting their limitations and suggesting strategies to address them. We conducted a systematic review of original research between 2000 and 2019, registered in PROSPERO (reference CRD42018116606). An interdisciplinary search covered six databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine (PubMed and Embase) and Web of Science. Bibliographies of relevant papers were screened until December 2019. From 3,654 search results 51 articles were selected against the eligibility criteria. Four tables summarise their findings: study details (aim, population, interventions, comparisons, methods and outcomes), data details (size, type, modalities, annotation, balance, availability and language of study), methodology (pre-processing, feature generation, machine learning, evaluation and results) and clinical applicability (research implications, clinical potential, risk of bias and strengths/limitations). While promising results are reported across nearly all 51 studies, very few have been implemented in clinical research or practice. We concluded that the main limitations of the field are poor standardisation, limited comparability of results, and a degree of disconnect between study aims and clinical applications. Attempts to close these gaps should support translation of future research into clinical practice.","Language is a valuable source of clinical information in Alzheimer's disease, as it declines concurrently with neurodegeneration. Consequently, speech and language data have been extensively studied in connection with its diagnosis. Firstly, to summarize the existing findings on the use of artificial intelligence, speech, and language processing to predict cognitive decline in the context of Alzheimer's disease. Secondly, to detail current research procedures, highlight their limitations, and suggest strategies to address them. Systematic review of original research between 2000 and 2019, registered in PROSPERO (reference CRD42018116606). An interdisciplinary search covered six databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine (PubMed and Embase), and Web of Science. Bibliographies of relevant papers were screened until December 2019. From 3,654 search results, 51 articles were selected against the eligibility criteria. Four tables summarize their findings: study details (aim, population, interventions, comparisons, methods, and outcomes), data details (size, type, modalities, annotation, balance, availability, and language of study), methodology (pre-processing, feature generation, machine learning, evaluation, and results), and clinical applicability (research implications, clinical potential, risk of bias, and strengths/limitations). Promising results are reported across nearly all 51 studies, but very few have been implemented in clinical research or practice. The main limitations of the field are poor standardization, limited comparability of results, and a degree of disconnect between study aims and clinical applications. Active attempts to close these gaps will support translation of future research into clinical practice.","['Sofia de la Fuente Garcia', 'Craig Ritchie', 'Saturnino Luz']","de la Fuente Garcia, Ritchie, Luz",https://export.arxiv.org/abs/2010.06047,https://doi.org/10.3233/JAD-200888,2021-08-03
15943.0,pubmed,pubmed,Development of a Model for Predicting Early Discontinuation of Adjuvant Chemotherapy in Stage III Colon Cancer,Development of a Model for Predicting Early Discontinuation of Adjuvant Chemotherapy in Stage III Colon Cancer,"To develop a tool that can be used to predict early discontinuation of adjuvant chemotherapy among patients with stage III colon cancer. Through record linkage of Alberta administrative and tumor registry databases, we identified a cohort of individuals age Ã¢â€°Â¥ 18 years who were diagnosed with stage III colon cancer and who received adjuvant chemotherapy in Alberta between 2004 and 2015. Early discontinuation was defined as receipt of &lt; 5 months of a planned 6-month course of chemotherapy. By a systematic review of the literature and a survey of medical oncologists, the following candidate variables were identified: age (years), number of comorbidities (0, 1, Ã¢â€°Â¥ 2), cancer stage (IIIC <i>v</i> IIIA-B), type of chemotherapy (fluorouracil, leucovorin, and oxaliplatin; capecitabine and oxaliplatin; or monotherapy), time from surgery to chemotherapy initiation (weeks), type of treatment facility (academic or community), and distance from home to treatment center (kilometers). Models developed using penalized logistic regression and the random forest algorithm were compared. Model performance was assessed using the C-statistic, Brier score, and a calibration plot. Internal validation was performed using the bootstrap method. From an initial 3,115 patients identified, 1,378 were deemed eligible for inclusion. Of these patients, 474 patients (34.4%) failed to complete at least 5 months of chemotherapy. Although well calibrated, the penalized logistic regression model had poor discrimination (optimism-adjusted C-statistic, 0.63; 95% CI, 0.60 to 0.67). In contrast, the random forest model achieved adequate discrimination (optimism-adjusted C-statistic, 0.80; 95% CI, 0.79 to 0.82). Although the degree of calibration of the random forest was acceptable, it was slightly worse than that of the penalized logistic regression model. Internal validation of our random forest model suggests that it may have clinical utility. Additional research regarding its external validation and clinical impact is needed.","To develop a tool that can be used to predict early discontinuation of adjuvant chemotherapy among patients with stage III colon cancer. Through record linkage of Alberta administrative and tumor registry databases, we identified a cohort of individuals age â‰¥ 18 years who were diagnosed with stage III colon cancer and who received adjuvant chemotherapy in Alberta between 2004 and 2015. Early discontinuation was defined as receipt of &lt; 5 months of a planned 6-month course of chemotherapy. By a systematic review of the literature and a survey of medical oncologists, the following candidate variables were identified: age (years), number of comorbidities (0, 1, â‰¥ 2), cancer stage (IIIC <i>v</i> IIIA-B), type of chemotherapy (fluorouracil, leucovorin, and oxaliplatin; capecitabine and oxaliplatin; or monotherapy), time from surgery to chemotherapy initiation (weeks), type of treatment facility (academic or community), and distance from home to treatment center (kilometers). Models developed using penalized logistic regression and the random forest algorithm were compared. Model performance was assessed using the C-statistic, Brier score, and a calibration plot. Internal validation was performed using the bootstrap method. From an initial 3,115 patients identified, 1,378 were deemed eligible for inclusion. Of these patients, 474 patients (34.4%) failed to complete at least 5 months of chemotherapy. Although well calibrated, the penalized logistic regression model had poor discrimination (optimism-adjusted C-statistic, 0.63; 95% CI, 0.60 to 0.67). In contrast, the random forest model achieved adequate discrimination (optimism-adjusted C-statistic, 0.80; 95% CI, 0.79 to 0.82). Although the degree of calibration of the random forest was acceptable, it was slightly worse than that of the penalized logistic regression model. Internal validation of our random forest model suggests that it may have clinical utility. Additional research regarding its external validation and clinical impact is needed.","Boyne, Brenner, Sajobi, Hilsden, Yusuf, Xu, Friedenreich, Cheung","Boyne, Brenner, Sajobi, Hilsden, Yusuf, Xu, Friedenreich, Cheung",https://doi.org/10.1200/CCI.20.00065,https://doi.org/10.1200/CCI.20.00065,2021-08-03
15946.0,pubmed,pubmed,A Systematic Review of Machine Learning Techniques in Hematopoietic Stem Cell Transplantation (HSCT),A Systematic Review of Machine Learning Techniques in Hematopoietic Stem Cell Transplantation (HSCT),"Machine learning techniques are widely used nowadays in the healthcare domain for the diagnosis, prognosis, and treatment of diseases. These techniques have applications in the field of hematopoietic cell transplantation (HCT), which is a potentially curative therapy for hematological malignancies. Herein, a systematic review of the application of machine learning (ML) techniques in the HCT setting was conducted. We examined the type of data streams included, specific ML techniques used, and type of clinical outcomes measured. A systematic review of English articles using PubMed, Scopus, Web of Science, and IEEE Xplore databases was performed. Search terms included &quot;hematopoietic cell transplantation (HCT),&quot; &quot;autologous HCT,&quot; &quot;allogeneic HCT,&quot; &quot;machine learning,&quot; and &quot;artificial intelligence.&quot; Only full-text studies reported between January 2015 and July 2020 were included. Data were extracted by two authors using predefined data fields. Following PRISMA guidelines, a total of 242 studies were identified, of which 27 studies met the inclusion criteria. These studies were sub-categorized into three broad topics and the type of ML techniques used included ensemble learning (63%), regression (44%), Bayesian learning (30%), and support vector machine (30%). The majority of studies examined models to predict HCT outcomes (e.g., survival, relapse, graft-versus-host disease). Clinical and genetic data were the most commonly used predictors in the modeling process. Overall, this review provided a systematic review of ML techniques applied in the context of HCT. The evidence is not sufficiently robust to determine the optimal ML technique to use in the HCT setting and/or what minimal data variables are required.","Machine learning techniques are widely used nowadays in the healthcare domain for the diagnosis, prognosis, and treatment of diseases. These techniques have applications in the field of hematopoietic cell transplantation (HCT), which is a potentially curative therapy for hematological malignancies. Herein, a systematic review of the application of machine learning (ML) techniques in the HCT setting was conducted. We examined the type of data streams included, specific ML techniques used, and type of clinical outcomes measured. A systematic review of English articles using PubMed, Scopus, Web of Science, and IEEE Xplore databases was performed. Search terms included ""hematopoietic cell transplantation (HCT),"" ""autologous HCT,"" ""allogeneic HCT,"" ""machine learning,"" and ""artificial intelligence."" Only full-text studies reported between January 2015 and July 2020 were included. Data were extracted by two authors using predefined data fields. Following PRISMA guidelines, a total of 242 studies were identified, of which 27 studies met the inclusion criteria. These studies were sub-categorized into three broad topics and the type of ML techniques used included ensemble learning (63%), regression (44%), Bayesian learning (30%), and support vector machine (30%). The majority of studies examined models to predict HCT outcomes (e.g., survival, relapse, graft-versus-host disease). Clinical and genetic data were the most commonly used predictors in the modeling process. Overall, this review provided a systematic review of ML techniques applied in the context of HCT. The evidence is not sufficiently robust to determine the optimal ML technique to use in the HCT setting and/or what minimal data variables are required.","Gupta, Braun, Chowdhury, Tewari, Choi","Gupta, Braun, Chowdhury, Tewari, Choi",https://doi.org/10.3390/s20216100,https://doi.org/10.3390/s20216100,2021-08-03
15953.0,pubmed,pubmed,Physical activity interventions for people with congenital heart disease,Physical activity interventions for people with congenital heart disease,"Congenital heart disease (ConHD) affectsÃ‚Â approximately 1% of all live births. People with ConHD are living longer due to improved medical intervention and are at risk of developing non-communicable diseases. Cardiorespiratory fitness (CRF) is reduced in people with ConHD, who deteriorateÃ‚Â faster compared to healthy people. CRF is known to be prognostic of future mortality and morbidity: itÃ‚Â isÃ‚Â thereforeÃ‚Â important to assess the evidence base on physical activity interventions in this population to inform decision making. To assess the effectiveness and safety of all types of physical activity interventions versusÃ‚Â standard careÃ‚Â in individuals with congenital heart disease. We undertook aÃ‚Â systematicÃ‚Â search onÃ‚Â 23Ã‚Â September 2019 of the following databases: CENTRAL,Ã‚Â MEDLINE, Embase,Ã‚Â CINAHL, AMED, BIOSIS Citation Index,Ã‚Â Web of Science Core Collection, LILACS andÃ‚Â DARE. We also searched ClinicalTrials.gov and weÃ‚Â reviewed the reference lists of relevant systematic reviews. We includedÃ‚Â randomised controlled trialsÃ‚Â (RCT) that compared any type of physical activity intervention against a 'no physical activity' (usual care) control. We included all individualsÃ‚Â with a diagnosis ofÃ‚Â congenital heart disease, regardless of age or previous medical interventions.Ã‚ DATA COLLECTION AND ANALYSIS: Two review authors (CAW and CW)Ã‚Â independently screened all the identified references for inclusion. We retrieved and read all full papers; and we contacted study authors if we needed any further information. The same two independent reviewers whoÃ‚Â extracted the data thenÃ‚Â processed the included papers, assessed their risk of bias using RoB 2Ã‚Â and assessed the certainty of the evidence using the GRADE approach.Ã‚Â The primary outcomes were: maximal cardiorespiratory fitness (CRF) assessed byÃ‚Â peak oxygen consumption; health-related quality of life (HRQoL) determined by a validated questionnaire; and device-worn 'objective' measures of physical activity. We included 15Ã‚Â RCTsÃ‚Â with 924Ã‚Â participantsÃ‚Â in the review. The median intervention length/follow-up length was 12 weeks (12 to 26 interquartile range (IQR)).Ã‚Â There were five RCTs of children and adolescents (n = 500) and 10 adult RCTs (n = 424). We identified three types of intervention: physical activity promotion; exercise training; and inspiratory muscle training. We assessed the risk of bias of results for CRF as either being of some concern (n = 12) or atÃ‚Â a high risk of bias (nÃ‚Â =Ã‚Â 2), due to aÃ‚Â failure to blindÃ‚Â intervention staff. One study did not report this outcome. Using the GRADE method, we assessed the certainty of evidence as moderate to very low across measured outcomes. When we pooled all types of interventions (physical activity promotion, exercise training and inspiratory muscle training), compared to a 'noÃ‚Â exercise' controlÃ‚Â CRF may slightly increase, with aÃ‚Â mean difference (MD) of 1.89 mL/kg<sup>-1</sup>/min<sup>-1</sup> (95% CI -0.22 toÃ‚Â 3.99; n = 732; moderate-certainty evidence).Ã‚Â The evidence is very uncertain about the effect of physical activity and exercise interventions on HRQoL. There was a standardised mean difference (SMD) of 0.76Ã‚Â (95% CI -0.13 to 1.65;Ã‚Â n = 163; very low certainty evidence) in HRQoL. However, we could pool only threeÃ‚Â studies in a meta-analysis, due to different ways of reporting. Only one study out of eight showed a positive effect on HRQoL. There may be a small improvement in mean daily physical activity (PA) (SMD 0.38,Ã‚Â 95% CI -0.15 toÃ‚Â 0.92; n = 328; low-certainty evidence), which equates toÃ‚Â approximately an additional 10 minutes of physical activity daily (95% CI -2.50 toÃ‚Â 22.20). Physical activity and exercise interventions likely result in an increase in submaximal cardiorespiratory fitness (MD 2.05,Ã‚Â 95% CI 0.05 to 4.05; n = 179; moderate-certainty evidence). Physical activity and exercise interventions likely increase muscular strength (MD 17.13, 95% CI 3.45 to 30.81; n = 18; moderate-certainty evidence).Ã‚Â Eleven studies (n = 501) reported on the outcome of adverse events (73% of total studies). Of the 11 studies, sixÃ‚Â studies reported zeroÃ‚Â adverse events. FiveÃ‚Â studies reported a total of 11 adverse events; 36% of adverse events were cardiac relatedÃ‚Â (nÃ‚Â = 4); there were, however, no serious adverse eventsÃ‚Â related to the interventions or reportedÃ‚Â fatalities (moderate-certainty evidence).Ã‚Â No studies reported hospital admissions. This review summarises the latest evidence on CRF, HRQoL and PA. Although there were only small improvements in CRF and PA, and small to noÃ‚Â improvements in HRQoL, there were no reported seriousÃ‚Â adverse events related to the interventions. Although these data are promising, there is currentlyÃ‚Â insufficient evidence to definitively determine the impact of physical activityÃ‚Â interventions in ConHD. Further high-quality randomised controlled trials are therefore needed, utilisingÃ‚Â a longer duration of follow-up.","Congenital heart disease (ConHD) affectsÂ approximately 1% of all live births. People with ConHD are living longer due to improved medical intervention and are at risk of developing non-communicable diseases. Cardiorespiratory fitness (CRF) is reduced in people with ConHD, who deteriorateÂ faster compared to healthy people. CRF is known to be prognostic of future mortality and morbidity: itÂ isÂ thereforeÂ important to assess the evidence base on physical activity interventions in this population to inform decision making. To assess the effectiveness and safety of all types of physical activity interventions versusÂ standard careÂ in individuals with congenital heart disease. We undertook aÂ systematicÂ search onÂ 23Â September 2019 of the following databases: CENTRAL,Â MEDLINE, Embase,Â CINAHL, AMED, BIOSIS Citation Index,Â Web of Science Core Collection, LILACS andÂ DARE. We also searched ClinicalTrials.gov and weÂ reviewed the reference lists of relevant systematic reviews. We includedÂ randomised controlled trialsÂ (RCT) that compared any type of physical activity intervention against a 'no physical activity' (usual care) control. We included all individualsÂ with a diagnosis ofÂ congenital heart disease, regardless of age or previous medical interventions.Â DATA COLLECTION AND ANALYSIS: Two review authors (CAW and CW)Â independently screened all the identified references for inclusion. We retrieved and read all full papers; and we contacted study authors if we needed any further information. The same two independent reviewers whoÂ extracted the data thenÂ processed the included papers, assessed their risk of bias using RoB 2Â and assessed the certainty of the evidence using the GRADE approach.Â The primary outcomes were: maximal cardiorespiratory fitness (CRF) assessed byÂ peak oxygen consumption; health-related quality of life (HRQoL) determined by a validated questionnaire; and device-worn 'objective' measures of physical activity. We included 15Â RCTsÂ with 924Â participantsÂ in the review. The median intervention length/follow-up length was 12 weeks (12 to 26 interquartile range (IQR)).Â There were five RCTs of children and adolescents (n = 500) and 10 adult RCTs (n = 424). We identified three types of intervention: physical activity promotion; exercise training; and inspiratory muscle training. We assessed the risk of bias of results for CRF as either being of some concern (n = 12) or atÂ a high risk of bias (nÂ =Â 2), due to aÂ failure to blindÂ intervention staff. One study did not report this outcome. Using the GRADE method, we assessed the certainty of evidence as moderate to very low across measured outcomes. When we pooled all types of interventions (physical activity promotion, exercise training and inspiratory muscle training), compared to a 'noÂ exercise' controlÂ CRF may slightly increase, with aÂ mean difference (MD) of 1.89 mL/kg<sup>-1</sup>/min<sup>-1</sup> (95% CI -0.22 toÂ 3.99; n = 732; moderate-certainty evidence).Â The evidence is very uncertain about the effect of physical activity and exercise interventions on HRQoL. There was a standardised mean difference (SMD) of 0.76Â (95% CI -0.13 to 1.65;Â n = 163; very low certainty evidence) in HRQoL. However, we could pool only threeÂ studies in a meta-analysis, due to different ways of reporting. Only one study out of eight showed a positive effect on HRQoL. There may be a small improvement in mean daily physical activity (PA) (SMD 0.38,Â 95% CI -0.15 toÂ 0.92; n = 328; low-certainty evidence), which equates toÂ approximately an additional 10 minutes of physical activity daily (95% CI -2.50 toÂ 22.20). Physical activity and exercise interventions likely result in an increase in submaximal cardiorespiratory fitness (MD 2.05,Â 95% CI 0.05 to 4.05; n = 179; moderate-certainty evidence). Physical activity and exercise interventions likely increase muscular strength (MD 17.13, 95% CI 3.45 to 30.81; n = 18; moderate-certainty evidence).Â Eleven studies (n = 501) reported on the outcome of adverse events (73% of total studies). Of the 11 studies, sixÂ studies reported zeroÂ adverse events. FiveÂ studies reported a total of 11 adverse events; 36% of adverse events were cardiac relatedÂ (nÂ = 4); there were, however, no serious adverse eventsÂ related to the interventions or reportedÂ fatalities (moderate-certainty evidence).Â No studies reported hospital admissions. This review summarises the latest evidence on CRF, HRQoL and PA. Although there were only small improvements in CRF and PA, and small to noÂ improvements in HRQoL, there were no reported seriousÂ adverse events related to the interventions. Although these data are promising, there is currentlyÂ insufficient evidence to definitively determine the impact of physical activityÂ interventions in ConHD. Further high-quality randomised controlled trials are therefore needed, utilisingÂ a longer duration of follow-up.","Williams, Wadey, Pieles, Stuart, Taylor, Long","Williams, Wadey, Pieles, Stuart, Taylor, Long",https://doi.org/10.1002/14651858.CD013400.pub2,https://doi.org/10.1002/14651858.CD013400.pub2,2021-08-03
15957.0,pubmed,pubmed,Predicting population health with machine learning: a scoping review,Predicting population health with machine learning: a scoping review,"To determine how machine learning has been applied to prediction applications in population health contexts. Specifically, to describe which outcomes have been studied, the data sources most widely used and whether reporting of machine learning predictive models aligns with established reporting guidelines. A scoping review. MEDLINE, EMBASE, CINAHL, ProQuest, Scopus, Web of Science, Cochrane Library, INSPEC and ACM Digital Library were searched on 18 July 2018. We included English articles published between 1980 and 2018 that used machine learning to predict population-health-related outcomes. We excluded studies that only used logistic regression or were restricted to a clinical context. We summarised findings extracted from published reports, which included general study characteristics, aspects of model development, reporting of results and model discussion items. Of 22 618 articles found by our search, 231 were included in the review. The USA (n=71, 30.74%) and China (n=40, 17.32%) produced the most studies. Cardiovascular disease (n=22, 9.52%) was the most studied outcome. The median number of observations was 5414 (IQR=16Ã¢â‚¬â€°543.5) and the median number of features was 17 (IQR=31). Health records (n=126, 54.5%) and investigator-generated data (n=86, 37.2%) were the most common data sources. Many studies did not incorporate recommended guidelines on machine learning and predictive modelling. Predictive discrimination was commonly assessed using area under the receiver operator curve (n=98, 42.42%) and calibration was rarely assessed (n=22, 9.52%). Machine learning applications in population health have concentrated on regions and diseases well represented in traditional data sources, infrequently using big data. Important aspects of model development were under-reported. Greater use of big data and reporting guidelines for predictive modelling could improve machine learning applications in population health. Registered on the Open Science Framework on 17 July 2018 (available at https://osf.io/rnqe6/).","To determine how machine learning has been applied to prediction applications in population health contexts. Specifically, to describe which outcomes have been studied, the data sources most widely used and whether reporting of machine learning predictive models aligns with established reporting guidelines. A scoping review. MEDLINE, EMBASE, CINAHL, ProQuest, Scopus, Web of Science, Cochrane Library, INSPEC and ACM Digital Library were searched on 18 July 2018. We included English articles published between 1980 and 2018 that used machine learning to predict population-health-related outcomes. We excluded studies that only used logistic regression or were restricted to a clinical context. We summarised findings extracted from published reports, which included general study characteristics, aspects of model development, reporting of results and model discussion items. Of 22 618 articles found by our search, 231 were included in the review. The USA (n=71, 30.74%) and China (n=40, 17.32%) produced the most studies. Cardiovascular disease (n=22, 9.52%) was the most studied outcome. The median number of observations was 5414 (IQR=16â€‰543.5) and the median number of features was 17 (IQR=31). Health records (n=126, 54.5%) and investigator-generated data (n=86, 37.2%) were the most common data sources. Many studies did not incorporate recommended guidelines on machine learning and predictive modelling. Predictive discrimination was commonly assessed using area under the receiver operator curve (n=98, 42.42%) and calibration was rarely assessed (n=22, 9.52%). Machine learning applications in population health have concentrated on regions and diseases well represented in traditional data sources, infrequently using big data. Important aspects of model development were under-reported. Greater use of big data and reporting guidelines for predictive modelling could improve machine learning applications in population health. Registered on the Open Science Framework on 17 July 2018 (available at https://osf.io/rnqe6/).","Morgenstern, Buajitti, O'Neill, Piggott, Goel, Fridman, Kornas, Rosella","Morgenstern, Buajitti, O'Neill, Piggott, Goel, Fridman, Kornas, Rosella",https://doi.org/10.1136/bmjopen-2020-037860,https://doi.org/10.1136/bmjopen-2020-037860,2021-08-03
15959.0,pubmed,pubmed,Literature mining for context-specific molecular relations using multimodal representations (COMMODAR),Literature mining for context-specific molecular relations using multimodal representations (COMMODAR),"Biological contextual information helps understand various phenomena occurring in the biological systems consisting of complex molecular relations. The construction of context-specific relational resources vastly relies on laborious manual extraction from unstructured literature. In this paper, we propose COMMODAR, a machine learning-based literature mining framework for context-specific molecular relations using multimodal representations. The main idea of COMMODAR is the feature augmentation by the cooperation of multimodal representations for relation extraction. We leveraged biomedical domain knowledge as well as canonical linguistic information for more comprehensive representations of textual sources. The models based on multiple modalities outperformed those solely based on the linguistic modality. We applied COMMODAR to the 14 million PubMed abstracts and extracted 9214 context-specific molecular relations. All corpora, extracted data, evaluation results, and the implementation code are downloadable at https://github.com/jae-hyun-lee/commodar . CCS CONCEPTS: Ã¢â‚¬Â¢ Computing methodologies~Information extraction Ã¢â‚¬Â¢ Computing methodologies~Neural networks Ã¢â‚¬Â¢ Applied computing~Biological networks.","Biological contextual information helps understand various phenomena occurring in the biological systems consisting of complex molecular relations. The construction of context-specific relational resources vastly relies on laborious manual extraction from unstructured literature. In this paper, we propose COMMODAR, a machine learning-based literature mining framework for context-specific molecular relations using multimodal representations. The main idea of COMMODAR is the feature augmentation by the cooperation of multimodal representations for relation extraction. We leveraged biomedical domain knowledge as well as canonical linguistic information for more comprehensive representations of textual sources. The models based on multiple modalities outperformed those solely based on the linguistic modality. We applied COMMODAR to the 14 million PubMed abstracts and extracted 9214 context-specific molecular relations. All corpora, extracted data, evaluation results, and the implementation code are downloadable at https://github.com/jae-hyun-lee/commodar . CCS CONCEPTS: â€¢ Computing methodologies~Information extraction â€¢ Computing methodologies~Neural networks â€¢ Applied computing~Biological networks.","Lee, Lee, Lee","Lee, Lee, Lee",https://doi.org/10.1186/s12859-020-3396-y,https://doi.org/10.1186/s12859-020-3396-y,2021-08-03
15961.0,pubmed,pubmed,Room for interpersonal relationships in online educational spaces - a philosophical discussion,Room for interpersonal relationships in online educational spaces - a philosophical discussion,"<b>Purpose:</b> To explore interpersonal relationships within online educational spaces and to connect the discussion to health and well-being among students and teachers. <b>Method:</b> We apply different perspectives to analyse the complexity of interpersonal relationships in online educational spaces, based on the philosophies of Nel Nodding, Maurice Merleau-Ponty, and Alfred Schutz. We use aÃ‚Â qualitative methodological combination-philosophical explorations, literature review, and text analysis-to offer significant insights that will substantially inform contemporary theories in research addressing interpersonal relationships in online education. <b>Results:</b> We illuminate and theorize about interpersonal relationships in terms of being cared for, confirmed by, and connected to others as dimensions of significance for health and well-being in educational spaces in the form of flexible online courses at the university level. <b>Conclusion:</b> We argue that all education should strive to provide caring relationships and educate for both self-understanding and group understanding, and contribute to school as a place where happiness and joy for genuine learning and knowledge are promoted. To achieve this, it is of significance to also be connected to and confirmed by others, teachers as well as students, and also to exist in a (class)room that provides room for interpersonal relationships: in real life or online.","<b>Purpose:</b> To explore interpersonal relationships within online educational spaces and to connect the discussion to health and well-being among students and teachers. <b>Method:</b> We apply different perspectives to analyse the complexity of interpersonal relationships in online educational spaces, based on the philosophies of Nel Nodding, Maurice Merleau-Ponty, and Alfred Schutz. We use aÂ qualitative methodological combination-philosophical explorations, literature review, and text analysis-to offer significant insights that will substantially inform contemporary theories in research addressing interpersonal relationships in online education. <b>Results:</b> We illuminate and theorize about interpersonal relationships in terms of being cared for, confirmed by, and connected to others as dimensions of significance for health and well-being in educational spaces in the form of flexible online courses at the university level. <b>Conclusion:</b> We argue that all education should strive to provide caring relationships and educate for both self-understanding and group understanding, and contribute to school as a place where happiness and joy for genuine learning and knowledge are promoted. To achieve this, it is of significance to also be connected to and confirmed by others, teachers as well as students, and also to exist in a (class)room that provides room for interpersonal relationships: in real life or online.","Kostenius, Alerby","Kostenius, Alerby",https://doi.org/10.1080/17482631.2019.1689603,https://doi.org/10.1080/17482631.2019.1689603,2021-08-03
15962.0,pubmed,pubmed,A Comparison of the Development of Medical Informatics in China and That in Western Countries from 2008 to 2018: A Bibliometric Analysis of Official Journal Publications,A Comparison of the Development of Medical Informatics in China and That in Western Countries from 2008 to 2018: A Bibliometric Analysis of Official Journal Publications,"We focused on medical informatics journal publications rather than on conference proceedings by comparing and analyzing the data from journals and conferences from a broader perspective. The aim is to summarize the unique contributions of China to medical digitization and foster more multilevel international cooperation. In February 2019, publications from 2008 to 2018 in three major English-language medical informatics journals were retrieved through Scopus, including the journals, namely, International Journal of Medical Informatics (IJMI, international community), JAMIA (United States), and Methods of Information in Medicine (MIM, Europe). Three major Chinese-language journals, namely, China Digital Medicine (CDM), Chinese Journal of Health Informatics and Management (CJHIM), and Chinese Journal of Medical Library and Information Science (CJMLIS), were searched within the major three Chinese literature databases. The datasets were preprocessed using the NLP package on Python, and a smart local moving algorithm was used as a clustering method for identifying the aforementioned journals. Between 2008 and 2018, the total number of published papers and H-index of the three English-language journals was 1371 and 67 (IJMI), 1752 and 86 (JAMIA), and 637 and 35 (MIM), respectively. In the same period, the total number of published papers and H-index in the three Chinese-language journals was 6668 and 23 (CDM), 1668 and 22 (CJHIM), and 2557 and 25 (CJMLIS), respectively. IJMI, JAMIA, and MIM received submissions from 82, 59, and 62 countries/regions, respectively. By contrast, the three Chinese journals only received submissions from seven foreign countries. The proportions of authors from institutional affiliations were similar between the three English-language journals (IJMI, JAMIA, and MIM) and CJMLIS because the majority of the authors were from universities (81%, 74%, 73%, and 65.2%), followed by medical institutions (12%, 10%, 9%, and 23.4%) or research institutes (2%, 4%, 10%, and 4.3%). Furthermore, the proportions of the authors from enterprises were low (2%, 6%, 4%, and 0.3%) for all journals. However, the authors in CDM and CJHIM were mainly from medical institutions (50% and 40%), followed by universities (33% and 32%) and research institutes (3% and 4%). In addition, the proportions of enterprises were only 3% and 2%, respectively. Among the top five authors in three English-language journals (ranked in terms of the number of published papers), 100% had doctoral or master's degrees, compared with only 60% in the Chinese journals. Additionally, 28204 different keywords were extracted from the aforementioned papers, covering 275 specific high-frequency key terms. Based on these key terms, four clusters were found in the English literature-&quot;Health and Clinical Information Systems,&quot; &quot;Internet and Telemedicine,&quot; &quot;Medical Data Statistical Analysis,&quot; and &quot;EHRs and Information Management&quot;-and three clusters were found in the Chinese literature: &quot;Hospital Information Systems and EMR,&quot; &quot;Library Science and Bibliometrics Analysis,&quot; and &quot;Medical Reform Policy and Health Digitization.&quot; Only two clusters are similar, and Chinese-language journals focus more on health information in technology and industrial applications than in medical informatics basic research. This study provides important insights into the development of medical informatics (MI) in China and Western countries showing that the medical informatics journals of China, the United States, and Europe have distinct characteristics. Specifically, first, compared with the Western journals, the number of papers published in the journals of professional associations in the field of MI in China is large and the application value is high, but the academic influence and academic value are relatively low; second, most of the authors of the Chinese papers are from hospitals, and most of the counterparts in the Western countries are from universities. The proportion of master's or doctoral degrees in the former is also lower than that of the latter; furthermore, regarding paper themes, on the one hand, China MI has no theoretical and basic research on medical data statistics and consumer health based on the Internet and telemedicine; on the other hand, after nearly 10 years of hospital digital development, China has fully used the latecomer and application advantages in hospitals and, through extensive international cooperation, has made significant advancements in and contributions to the development of medical information.","We focused on medical informatics journal publications rather than on conference proceedings by comparing and analyzing the data from journals and conferences from a broader perspective. The aim is to summarize the unique contributions of China to medical digitization and foster more multilevel international cooperation. In February 2019, publications from 2008 to 2018 in three major English-language medical informatics journals were retrieved through Scopus, including the journals, namely, International Journal of Medical Informatics (IJMI, international community), JAMIA (United States), and Methods of Information in Medicine (MIM, Europe). Three major Chinese-language journals, namely, China Digital Medicine (CDM), Chinese Journal of Health Informatics and Management (CJHIM), and Chinese Journal of Medical Library and Information Science (CJMLIS), were searched within the major three Chinese literature databases. The datasets were preprocessed using the NLP package on Python, and a smart local moving algorithm was used as a clustering method for identifying the aforementioned journals. Between 2008 and 2018, the total number of published papers and H-index of the three English-language journals was 1371 and 67 (IJMI), 1752 and 86 (JAMIA), and 637 and 35 (MIM), respectively. In the same period, the total number of published papers and H-index in the three Chinese-language journals was 6668 and 23 (CDM), 1668 and 22 (CJHIM), and 2557 and 25 (CJMLIS), respectively. IJMI, JAMIA, and MIM received submissions from 82, 59, and 62 countries/regions, respectively. By contrast, the three Chinese journals only received submissions from seven foreign countries. The proportions of authors from institutional affiliations were similar between the three English-language journals (IJMI, JAMIA, and MIM) and CJMLIS because the majority of the authors were from universities (81%, 74%, 73%, and 65.2%), followed by medical institutions (12%, 10%, 9%, and 23.4%) or research institutes (2%, 4%, 10%, and 4.3%). Furthermore, the proportions of the authors from enterprises were low (2%, 6%, 4%, and 0.3%) for all journals. However, the authors in CDM and CJHIM were mainly from medical institutions (50% and 40%), followed by universities (33% and 32%) and research institutes (3% and 4%). In addition, the proportions of enterprises were only 3% and 2%, respectively. Among the top five authors in three English-language journals (ranked in terms of the number of published papers), 100% had doctoral or master's degrees, compared with only 60% in the Chinese journals. Additionally, 28204 different keywords were extracted from the aforementioned papers, covering 275 specific high-frequency key terms. Based on these key terms, four clusters were found in the English literature-""Health and Clinical Information Systems,"" ""Internet and Telemedicine,"" ""Medical Data Statistical Analysis,"" and ""EHRs and Information Management""-and three clusters were found in the Chinese literature: ""Hospital Information Systems and EMR,"" ""Library Science and Bibliometrics Analysis,"" and ""Medical Reform Policy and Health Digitization."" Only two clusters are similar, and Chinese-language journals focus more on health information in technology and industrial applications than in medical informatics basic research. This study provides important insights into the development of medical informatics (MI) in China and Western countries showing that the medical informatics journals of China, the United States, and Europe have distinct characteristics. Specifically, first, compared with the Western journals, the number of papers published in the journals of professional associations in the field of MI in China is large and the application value is high, but the academic influence and academic value are relatively low; second, most of the authors of the Chinese papers are from hospitals, and most of the counterparts in the Western countries are from universities. The proportion of master's or doctoral degrees in the former is also lower than that of the latter; furthermore, regarding paper themes, on the one hand, China MI has no theoretical and basic research on medical data statistics and consumer health based on the Internet and telemedicine; on the other hand, after nearly 10 years of hospital digital development, China has fully used the latecomer and application advantages in hospitals and, through extensive international cooperation, has made significant advancements in and contributions to the development of medical information.","Liang, Zhang, Fan, Shen, Chen, Xu, Ge, Xin, Lei","Liang, Zhang, Fan, Shen, Chen, Xu, Ge, Xin, Lei",https://doi.org/10.1155/2020/8822311,https://doi.org/10.1155/2020/8822311,2021-08-03
15964.0,pubmed,pubmed,Psychosocial and pharmacologic interventions for methamphetamine addiction: protocol for a scoping review of the literature,Psychosocial and pharmacologic interventions for methamphetamine addiction: protocol for a scoping review of the literature,"Methamphetamine use and harms are rising rapidly. Management of patients with methamphetamine use disorder (MUD) and problematic methamphetamine use (PMU) is challenging, with no clearly established best approach; both psychosocial and pharmacologic interventions have been described. Furthermore, given the diversity of individuals that use methamphetamines, there is a need to assess evidence for treatments for subgroups including youths; gay, bisexual, and other men who have sex with men; individuals with mental health comorbidities; and individuals in correction services. Establishing awareness of the messages regarding treatment from recent clinical practice guidelines (CPG) in the field is also of value. The first study objective will be to establish a greater understanding of the methods, populations, and findings of controlled studies for psychosocial and pharmacologic treatments for MUD and PMU. Investigation of this information can help establish the potential for advanced syntheses of the evidence (such as network meta-analysis) to compare therapies for this condition and to identify gaps related to key populations where more primary research is needed. Summarizing the recommendations regarding treatment of MUD/PMU from recent CPGs and systematic reviews will be an important secondary objective. A scoping review will be performed. Using the OVID platform, MEDLINE, Embase, PsycINFO, and relevant Cochrane databases from EBM Reviews will be searched (from databases' inception onwards). Eligibility criteria will include individuals described as having MUD or PMU, with designs of interest including randomized trials, non-randomized trials, and controlled cohort studies with three or more months of follow-up; systematic reviews and CPGs will also be sought. Two reviewers (with support from automation tools) will independently screen all citations, full-text articles, and chart data. Different approaches to handling and summarizing the data will be implemented for each type of study design. Tables and graphics will be used to map evidence sources and identify evidence gaps. This research will enhance awareness of evidence addressing the effects of psychosocial and pharmacologic interventions for MUD/PMU overall and in sub-populations, both in terms of recent CPGs/reviews and primary studies; inspection of the latter will also help establish the feasibility of future syntheses to compare treatments, such as network meta-analysis. SYSTEMATIC REVIEWÃ‚Â PROTOCOL REGISTRATION: Open Science Framework ( https://osf.io/9wy8p ).","Methamphetamine use and harms are rising rapidly. Management of patients with methamphetamine use disorder (MUD) and problematic methamphetamine use (PMU) is challenging, with no clearly established best approach; both psychosocial and pharmacologic interventions have been described. Furthermore, given the diversity of individuals that use methamphetamines, there is a need to assess evidence for treatments for subgroups including youths; gay, bisexual, and other men who have sex with men; individuals with mental health comorbidities; and individuals in correction services. Establishing awareness of the messages regarding treatment from recent clinical practice guidelines (CPG) in the field is also of value. The first study objective will be to establish a greater understanding of the methods, populations, and findings of controlled studies for psychosocial and pharmacologic treatments for MUD and PMU. Investigation of this information can help establish the potential for advanced syntheses of the evidence (such as network meta-analysis) to compare therapies for this condition and to identify gaps related to key populations where more primary research is needed. Summarizing the recommendations regarding treatment of MUD/PMU from recent CPGs and systematic reviews will be an important secondary objective. A scoping review will be performed. Using the OVID platform, MEDLINE, Embase, PsycINFO, and relevant Cochrane databases from EBM Reviews will be searched (from databases' inception onwards). Eligibility criteria will include individuals described as having MUD or PMU, with designs of interest including randomized trials, non-randomized trials, and controlled cohort studies with three or more months of follow-up; systematic reviews and CPGs will also be sought. Two reviewers (with support from automation tools) will independently screen all citations, full-text articles, and chart data. Different approaches to handling and summarizing the data will be implemented for each type of study design. Tables and graphics will be used to map evidence sources and identify evidence gaps. This research will enhance awareness of evidence addressing the effects of psychosocial and pharmacologic interventions for MUD/PMU overall and in sub-populations, both in terms of recent CPGs/reviews and primary studies; inspection of the latter will also help establish the feasibility of future syntheses to compare treatments, such as network meta-analysis. SYSTEMATIC REVIEWÂ PROTOCOL REGISTRATION: Open Science Framework ( https://osf.io/9wy8p ).","Hamel, Corace, Hersi, Rice, Willows, Macpherson, Sproule, Flores-Aranda, Garber, Esmaeilisaraji, Skidmore, Porath, Ortiz Nunez, Hutton","Hamel, Corace, Hersi, Rice, Willows, Macpherson, Sproule, Flores-Aranda, Garber, Esmaeilisaraji, Skidmore, Porath, Ortiz Nunez, Hutton",https://doi.org/10.1186/s13643-020-01499-z,https://doi.org/10.1186/s13643-020-01499-z,2021-08-03
15967.0,pubmed,pubmed,A systematic review and meta-analysis of the prognostic value of radiomics based models in non-small cell lung cancer treated with curative radiotherapy,A systematic review and meta-analysis of the prognostic value of radiomics based models in non-small cell lung cancer treated with curative radiotherapy,"Radiomics allows extraction of quantifiable features from imaging. This study performs a systematic review and meta-analysis of the performance of radiomics based prognostic models in non-small cell lung cancer (NSCLC). A literature review was performed following PRISMA guidelines. Medline, EMBASE and Cochrane databases were searched for articles investigating radiomics features predictive of overall survival (OS) in NSCLC treated with curative intent radiotherapy. A random-effects meta-analysis of Harrell's Concordance Index (C-index) was performed on the performance of radiomics models. Of the 2746 articles retrieved, 40 studies of 55 datasets and 6223 patients were eligible for inclusion in the systematic review. There was significant heterogeneity in the methodology for feature selection and model development. Twelve datasets reported the C-index of radiomics based models in predicting OS and were included in the meta-analysis. The C-index random effects estimate was 0.57 (95% CI 0.53 - 0.62). There was significant heterogeneity (I<sup>2</sup> = 70.3%). Based on this review, radiomics based models for lung cancer have to date demonstrated modest prognostic capabilities. Future research should consider using standardised radiomics features, robust feature selection and model development, and deep learning techniques, absolving the need for pre-defined features, to improve imaging-based models.","Radiomics allows extraction of quantifiable features from imaging. This study performs a systematic review and meta-analysis of the performance of radiomics based prognostic models in non-small cell lung cancer (NSCLC). A literature review was performed following PRISMA guidelines. Medline, EMBASE and Cochrane databases were searched for articles investigating radiomics features predictive of overall survival (OS) in NSCLC treated with curative intent radiotherapy. A random-effects meta-analysis of Harrell's Concordance Index (C-index) was performed on the performance of radiomics models. Of the 2746 articles retrieved, 40 studies of 55 datasets and 6223 patients were eligible for inclusion in the systematic review. There was significant heterogeneity in the methodology for feature selection and model development. Twelve datasets reported the C-index of radiomics based models in predicting OS and were included in the meta-analysis. The C-index random effects estimate was 0.57 (95% CI 0.53-0.62). There was significant heterogeneity (I<sup>2</sup>Â =Â 70.3%). Based on this review, radiomics based models for lung cancer have to date demonstrated modest prognostic capabilities. Future research should consider using standardised radiomics features, robust feature selection and model development, and deep learning techniques, absolving the need for pre-defined features, to improve imaging-based models.","Kothari, Korte, Lehrer, Zaorsky, Lazarakis, Kron, Hardcastle, Siva","Kothari, Korte, Lehrer, Zaorsky, Lazarakis, Kron, Hardcastle, Siva",https://doi.org/10.1016/j.radonc.2020.10.023,https://doi.org/10.1016/j.radonc.2020.10.023,2021-08-03
15968.0,pubmed,pubmed,Clinical Context-Aware Biomedical Text Summarization Using Deep Neural Network: Model Development and Validation,Clinical Context-Aware Biomedical Text Summarization Using Deep Neural Network: Model Development and Validation,"Automatic text summarization (ATS) enables users to retrieve meaningful evidence from big data of biomedical repositories to make complex clinical decisions. Deep neural and recurrent networks outperform traditional machine-learning techniques in areas of natural language processing and computer vision; however, they are yet to be explored in the ATS domain, particularly for medical text summarization. Traditional approaches in ATS for biomedical text suffer from fundamental issues such as an inability to capture clinical context, quality of evidence, and purpose-driven selection of passages for the summary. We aimed to circumvent these limitations through achieving precise, succinct, and coherent information extraction from credible published biomedical resources, and to construct a simplified summary containing the most informative content that can offer a review particular to clinical needs. In our proposed approach, we introduce a novel framework, termed Biomed-Summarizer, that provides quality-aware Patient/Problem, Intervention, Comparison, and Outcome (PICO)-based intelligent and context-enabled summarization of biomedical text. Biomed-Summarizer integrates the prognosis quality recognition model with a clinical context-aware model to locate text sequences in the body of a biomedical article for use in the final summary. First, we developed a deep neural network binary classifier for quality recognition to acquire scientifically sound studies and filter out others. Second, we developed a bidirectional long-short term memory recurrent neural network as a clinical context-aware classifier, which was trained on semantically enriched features generated using a word-embedding tokenizer for identification of meaningful sentences representing PICO text sequences. Third, we calculated the similarity between query and PICO text sequences using Jaccard similarity with semantic enrichments, where the semantic enrichments are obtained using medical ontologies. Last, we generated a representative summary from the high-scoring PICO sequences aggregated by study type, publication credibility, and freshness score. Evaluation of the prognosis quality recognition model using a large dataset of biomedical literature related to intracranial aneurysm showed an accuracy of 95.41% (2562/2686) in terms of recognizing quality articles. The clinical context-aware multiclass classifier outperformed the traditional machine-learning algorithms, including support vector machine, gradient boosted tree, linear regression, K-nearest neighbor, and naÃƒÂ¯ve Bayes, by achieving 93% (16127/17341) accuracy for classifying five categories: aim, population, intervention, results, and outcome. The semantic similarity algorithm achieved a significant Pearson correlation coefficient of 0.61 (0-1 scale) on a well-known BIOSSES dataset (with 100 pair sentences) after semantic enrichment, representing an improvement of 8.9% over baseline Jaccard similarity. Finally, we found a highly positive correlation among the evaluations performed by three domain experts concerning different metrics, suggesting that the automated summarization is satisfactory. By employing the proposed method Biomed-Summarizer, high accuracy in ATS was achieved, enabling seamless curation of research evidence from the biomedical literature to use for clinical decision-making.","Automatic text summarization (ATS) enables users to retrieve meaningful evidence from big data of biomedical repositories to make complex clinical decisions. Deep neural and recurrent networks outperform traditional machine-learning techniques in areas of natural language processing and computer vision; however, they are yet to be explored in the ATS domain, particularly for medical text summarization. Traditional approaches in ATS for biomedical text suffer from fundamental issues such as an inability to capture clinical context, quality of evidence, and purpose-driven selection of passages for the summary. We aimed to circumvent these limitations through achieving precise, succinct, and coherent information extraction from credible published biomedical resources, and to construct a simplified summary containing the most informative content that can offer a review particular to clinical needs. In our proposed approach, we introduce a novel framework, termed Biomed-Summarizer, that provides quality-aware Patient/Problem, Intervention, Comparison, and Outcome (PICO)-based intelligent and context-enabled summarization of biomedical text. Biomed-Summarizer integrates the prognosis quality recognition model with a clinical context-aware model to locate text sequences in the body of a biomedical article for use in the final summary. First, we developed a deep neural network binary classifier for quality recognition to acquire scientifically sound studies and filter out others. Second, we developed a bidirectional long-short term memory recurrent neural network as a clinical context-aware classifier, which was trained on semantically enriched features generated using a word-embedding tokenizer for identification of meaningful sentences representing PICO text sequences. Third, we calculated the similarity between query and PICO text sequences using Jaccard similarity with semantic enrichments, where the semantic enrichments are obtained using medical ontologies. Last, we generated a representative summary from the high-scoring PICO sequences aggregated by study type, publication credibility, and freshness score. Evaluation of the prognosis quality recognition model using a large dataset of biomedical literature related to intracranial aneurysm showed an accuracy of 95.41% (2562/2686) in terms of recognizing quality articles. The clinical context-aware multiclass classifier outperformed the traditional machine-learning algorithms, including support vector machine, gradient boosted tree, linear regression, K-nearest neighbor, and naÃ¯ve Bayes, by achieving 93% (16127/17341) accuracy for classifying five categories: aim, population, intervention, results, and outcome. The semantic similarity algorithm achieved a significant Pearson correlation coefficient of 0.61 (0-1 scale) on a well-known BIOSSES dataset (with 100 pair sentences) after semantic enrichment, representing an improvement of 8.9% over baseline Jaccard similarity. Finally, we found a highly positive correlation among the evaluations performed by three domain experts concerning different metrics, suggesting that the automated summarization is satisfactory. By employing the proposed method Biomed-Summarizer, high accuracy in ATS was achieved, enabling seamless curation of research evidence from the biomedical literature to use for clinical decision-making.","Afzal, Alam, Malik, Malik","Afzal, Alam, Malik, Malik",https://doi.org/10.2196/19810,https://doi.org/10.2196/19810,2021-08-03
15969.0,pubmed,pubmed,Implementation of Convolutional Neural Network Approach for COVID-19 Disease Detection,Implementation of convolutional neural network approach for COVID-19 disease detection,"In this paper two novel, powerful and robust Convolutional Neural Network (CNN) architectures are designed and proposed for two different classification tasks using publicly available datasets. The first architecture is able to decide whether a given chest X-ray image of a patient contains COVID-19 or not with 98.92% average accuracy. The second CNN architecture is able to divide a given chest X-ray image of a patient into three classes (COVID-19 vs. Normal vs. Pneumonia) with 98.27% average accuracy. The hyper-parameters of the both CNN models are automatically determined using Grid Search. Experimental results on large clinical datasets show the effectiveness of the proposed architectures and demonstrate that the proposed algorithms can overcome disadvantages mentioned above. Moreover, the proposed CNN models are fully-automatic in terms of not requiring the extraction of diseased tissue; which is a great improvement of available automatic methods in the literature. To the best of author's knowledge, this study is the first study to detect COVID-19 disease from given chest X-ray images, using CNN whose hyper parameters are automatically determined by the Grid Search. Another important contribution of this study is that it is the first CNN based COVID-19 chest X-ray image classification study which uses the largest possible clinical dataset. A total of 1524 COVID-19, 1527 pneumonia and 1524 normal X-ray images are collected. It is aimed to collect the largest number of COVID-19 X-ray images that exist in the literature until the writing of this research paper.","In this paper, two novel, powerful, and robust convolutional neural network (CNN) architectures are designed and proposed for two different classification tasks using publicly available data sets. The first architecture is able to decide whether a given chest X-ray image of a patient contains COVID-19 or not with 98.92% average accuracy. The second CNN architecture is able to divide a given chest X-ray image of a patient into three classes (COVID-19 versus normal versus pneumonia) with 98.27% average accuracy. The hyperparameters of both CNN models are automatically determined using Grid Search. Experimental results on large clinical data sets show the effectiveness of the proposed architectures and demonstrate that the proposed algorithms can overcome the disadvantages mentioned above. Moreover, the proposed CNN models are fully automatic in terms of not requiring the extraction of diseased tissue, which is a great improvement of available automatic methods in the literature. To the best of the author's knowledge, this study is the first study to detect COVID-19 disease from given chest X-ray images, using CNN, whose hyperparameters are automatically determined by the Grid Search. Another important contribution of this study is that it is the first CNN-based COVID-19 chest X-ray image classification study that uses the largest possible clinical data set. A total of 1,524 COVID-19, 1,527 pneumonia, and 1524 normal X-ray images are collected. It is aimed to collect the largest number of COVID-19 X-ray images that exist in the literature until the writing of this research paper.",Irmak,Irmak,https://doi.org/10.1152/physiolgenomics.00084.2020,https://doi.org/10.1152/physiolgenomics.00084.2020,2021-08-03
15972.0,pubmed,pubmed,Recommendations for hemodynamic monitoring for critically ill children-expert consensus statement issued by the cardiovascular dynamics section of the European Society of Paediatric and Neonatal Intensive Care (ESPNIC),Recommendations for hemodynamic monitoring for critically ill children-expert consensus statement issued by the cardiovascular dynamics section of the European Society of Paediatric and Neonatal Intensive Care (ESPNIC),"Cardiovascular instability is common in critically ill children. There is a scarcity of published high-quality studies to develop meaningful evidence-based hemodynamic monitoring guidelines and hence, with the exception of management of shock, currently there are no published guidelines for hemodynamic monitoring in children. The European Society of Paediatric and Neonatal Intensive Care (ESPNIC) Cardiovascular Dynamics section aimed to provide expert consensus recommendations on hemodynamic monitoring in critically ill children. Creation of a panel of experts in cardiovascular hemodynamic assessment and hemodynamic monitoring and review of relevant literature-a literature search was performed, and recommendations were developed through discussions managed following a Quaker-based consensus technique and evaluating appropriateness using a modified blind RAND/UCLA voting method. The AGREE statement was followed to prepare this document. Of 100 suggested recommendations across 12 subgroups concerning hemodynamic monitoring in critically ill children, 72 reached &quot;strong agreement,&quot; 20 &quot;weak agreement,&quot; and 2 had &quot;no agreement.&quot; Six statements were considered as redundant after rephrasing of statements following the first round of voting. The agreed 72 recommendations were then coalesced into 36 detailing four key areas of hemodynamic monitoring in the main manuscript. Due to a lack of published evidence to develop evidence-based guidelines, most of the recommendations are based upon expert consensus. These expert consensus-based recommendations may be used to guide clinical practice for hemodynamic monitoring in critically ill children, and they may serve as a basis for highlighting gaps in the knowledge base to guide further research in hemodynamic monitoring.","Cardiovascular instability is common in critically ill children. There is a scarcity of published high-quality studies to develop meaningful evidence-based hemodynamic monitoring guidelines and hence, with the exception of management of shock, currently there are no published guidelines for hemodynamic monitoring in children. The European Society of Paediatric and Neonatal Intensive Care (ESPNIC) Cardiovascular Dynamics section aimed to provide expert consensus recommendations on hemodynamic monitoring in critically ill children. Creation of a panel of experts in cardiovascular hemodynamic assessment and hemodynamic monitoring and review of relevant literature-a literature search was performed, and recommendations were developed through discussions managed following a Quaker-based consensus technique and evaluating appropriateness using a modified blind RAND/UCLA voting method. The AGREE statement was followed to prepare this document. Of 100 suggested recommendations across 12 subgroups concerning hemodynamic monitoring in critically ill children, 72 reached ""strong agreement,"" 20 ""weak agreement,"" and 2 had ""no agreement."" Six statements were considered as redundant after rephrasing of statements following the first round of voting. The agreed 72 recommendations were then coalesced into 36 detailing four key areas of hemodynamic monitoring in the main manuscript. Due to a lack of published evidence to develop evidence-based guidelines, most of the recommendations are based upon expert consensus. These expert consensus-based recommendations may be used to guide clinical practice for hemodynamic monitoring in critically ill children, and they may serve as a basis for highlighting gaps in the knowledge base to guide further research in hemodynamic monitoring.","Singh, Villaescusa, da Cruz, Tibby, Bottari, Saxena, GuillÃƒÂ©n, Herce, Di Nardo, Cecchetti, Brierley, de Boode, Lemson","Singh, Villaescusa, da Cruz, Tibby, Bottari, Saxena, GuillÃ©n, Herce, Di Nardo, Cecchetti, Brierley, de Boode, Lemson",https://doi.org/10.1186/s13054-020-03326-2,https://doi.org/10.1186/s13054-020-03326-2,2021-08-03
15973.0,pubmed,pubmed,Automatic classification of scanned electronic health record documents,Automatic classification of scanned electronic health record documents,"Electronic Health Records (EHRs) contain scanned documents from a variety of sources such as identification cards, radiology reports, clinical correspondence, and many other document types. We describe the distribution of scanned documents at one health institution and describe the design and evaluation of a system to categorize documents into clinically relevant and non-clinically relevant categories as well as further sub-classifications. Our objective is to demonstrate that text classification systems can accurately classify scanned documents. We extracted text using Optical Character Recognition (OCR). We then created and evaluated multiple text classification machine learning models, including both &quot;bag of words&quot; and deep learning approaches. We evaluated the system on three different levels of classification using both the entire document as input, as well as the individual pages of the document. Finally, we compared the effects of different text processing methods. A deep learning model using ClinicalBERT performed best. This model distinguished between clinically-relevant documents and not clinically-relevant documents with an accuracy of 0.973; between intermediate sub-classifications with an accuracy of 0.949; and between individual classes with an accuracy of 0.913. Within the EHR, some document categories such as &quot;external medical records&quot; may contain hundreds of scanned pages without clear document boundaries. Without further sub-classification, clinicians must view every page or risk missing clinically-relevant information. Machine learning can automatically classify these scanned documents to reduce clinician burden. Using machine learning applied to OCR-extracted text has the potential to accurately identify clinically-relevant scanned content within EHRs.","Electronic Health Records (EHRs) contain scanned documents from a variety of sources such as identification cards, radiology reports, clinical correspondence, and many other document types. We describe the distribution of scanned documents at one health institution and describe the design and evaluation of a system to categorize documents into clinically relevant and non-clinically relevant categories as well as further sub-classifications. Our objective is to demonstrate that text classification systems can accurately classify scanned documents. We extracted text using Optical Character Recognition (OCR). We then created and evaluated multiple text classification machine learning models, including both ""bag of words"" and deep learning approaches. We evaluated the system on three different levels of classification using both the entire document as input, as well as the individual pages of the document. Finally, we compared the effects of different text processing methods. A deep learning model using ClinicalBERT performed best. This model distinguished between clinically-relevant documents and not clinically-relevant documents with an accuracy of 0.973; between intermediate sub-classifications with an accuracy of 0.949; and between individual classes with an accuracy of 0.913. Within the EHR, some document categories such as ""external medical records"" may contain hundreds of scanned pages without clear document boundaries. Without further sub-classification, clinicians must view every page or risk missing clinically-relevant information. Machine learning can automatically classify these scanned documents to reduce clinician burden. Using machine learning applied to OCR-extracted text has the potential to accurately identify clinically-relevant scanned content within EHRs.","Goodrum, Roberts, Bernstam","Goodrum, Roberts, Bernstam",https://doi.org/10.1016/j.ijmedinf.2020.104302,https://doi.org/10.1016/j.ijmedinf.2020.104302,2021-08-03
15975.0,pubmed,pubmed,Clinical Characteristics and Prognostic Factors for Intensive Care Unit Admission of Patients With COVID-19: Retrospective Study Using Machine Learning and Natural Language Processing,Clinical Characteristics and Prognostic Factors for Intensive Care Unit Admission of Patients With COVID-19: Retrospective Study Using Machine Learning and Natural Language Processing,"Many factors involved in the onset and clinical course of the ongoing COVID-19 pandemic are still unknown. Although big data analytics and artificial intelligence are widely used in the realms of health and medicine, researchers are only beginning to use these tools to explore the clinical characteristics and predictive factors of patients with COVID-19. Our primary objectives are to describe the clinical characteristics and determine the factors that predict intensive care unit (ICU) admission of patients with COVID-19. Determining these factors using a well-defined population can increase our understanding of the real-world epidemiology of the disease. We used a combination of classic epidemiological methods, natural language processing (NLP), and machine learning (for predictive modeling) to analyze the electronic health records (EHRs) of patients with COVID-19. We explored the unstructured free text in the EHRs within the Servicio de Salud de Castilla-La Mancha (SESCAM) Health Care Network (Castilla-La Mancha, Spain) from the entire population with available EHRs (1,364,924 patients) from January 1 to March 29, 2020. We extracted related clinical information regarding diagnosis, progression, and outcome for all COVID-19 cases. A total of 10,504 patients with a clinical or polymerase chain reaction-confirmed diagnosis of COVID-19 were identified; 5519 (52.5%) were male, with a mean age of 58.2 years (SD 19.7). Upon admission, the most common symptoms were cough, fever, and dyspnea; however, all three symptoms occurred in fewer than half of the cases. Overall, 6.1% (83/1353) of hospitalized patients required ICU admission. Using a machine-learning, data-driven algorithm, we identified that a combination of age, fever, and tachypnea was the most parsimonious predictor of ICU admission; patients younger than 56 years, without tachypnea, and temperature &lt;39 degrees Celsius (or &gt;39 Ã‚ÂºC without respiratory crackles) were not admitted to the ICU. In contrast, patients with COVID-19 aged 40 to 79 years were likely to be admitted to the ICU if they had tachypnea and delayed their visit to the emergency department after being seen in primary care. Our results show that a combination of easily obtainable clinical variables (age, fever, and tachypnea with or without respiratory crackles) predicts whether patients with COVID-19 will require ICU admission.","Many factors involved in the onset and clinical course of the ongoing COVID-19 pandemic are still unknown. Although big data analytics and artificial intelligence are widely used in the realms of health and medicine, researchers are only beginning to use these tools to explore the clinical characteristics and predictive factors of patients with COVID-19. Our primary objectives are to describe the clinical characteristics and determine the factors that predict intensive care unit (ICU) admission of patients with COVID-19. Determining these factors using a well-defined population can increase our understanding of the real-world epidemiology of the disease. We used a combination of classic epidemiological methods, natural language processing (NLP), and machine learning (for predictive modeling) to analyze the electronic health records (EHRs) of patients with COVID-19. We explored the unstructured free text in the EHRs within the Servicio de Salud de Castilla-La Mancha (SESCAM) Health Care Network (Castilla-La Mancha, Spain) from the entire population with available EHRs (1,364,924 patients) from January 1 to March 29, 2020. We extracted related clinical information regarding diagnosis, progression, and outcome for all COVID-19 cases. A total of 10,504 patients with a clinical or polymerase chain reaction-confirmed diagnosis of COVID-19 were identified; 5519 (52.5%) were male, with a mean age of 58.2 years (SD 19.7). Upon admission, the most common symptoms were cough, fever, and dyspnea; however, all three symptoms occurred in fewer than half of the cases. Overall, 6.1% (83/1353) of hospitalized patients required ICU admission. Using a machine-learning, data-driven algorithm, we identified that a combination of age, fever, and tachypnea was the most parsimonious predictor of ICU admission; patients younger than 56 years, without tachypnea, and temperature &lt;39 degrees Celsius (or &gt;39 ÂºC without respiratory crackles) were not admitted to the ICU. In contrast, patients with COVID-19 aged 40 to 79 years were likely to be admitted to the ICU if they had tachypnea and delayed their visit to the emergency department after being seen in primary care. Our results show that a combination of easily obtainable clinical variables (age, fever, and tachypnea with or without respiratory crackles) predicts whether patients with COVID-19 will require ICU admission.","Izquierdo, Ancochea, Soriano","Izquierdo, Ancochea, Soriano, Medrano, Tello, Porras, Serrano, Lumbreras, Universidad Pontificia Comillas, Rio-Bermudez, Marchesseau, Salcedo, MartÃ­nez, MatÃ©, Collazo, Barea, Villamayor, Urda, Pinta, Zubizarreta, GonzÃ¡lez, Menke",https://doi.org/10.2196/21801,https://doi.org/10.2196/21801,2021-08-03
15980.0,pubmed,pubmed,Old Drugs with New Tricks; Paradigm in Drug Development Pipeline for Alzheimer's Disease,Old Drugs with New Tricks: Paradigm in Drug Development Pipeline for Alzheimer's Disease,"The most common reason behind dementia is Alzheimer's disease (AD) and it is predicted to be the third lifethreatening disease apart from stroke and cancer for the geriatric population. Till now only four drugs are available in the market for symptomatic relief. The complex nature of disease pathophysiology and lack of concrete evidences of molecular targets are the major hurdles for developing new drug to treat AD. The the rate of attrition of many advanced drugs at clinical stages, makes the de novo discovery process very expensive. Alternatively, Drug Repurposing (DR) is an attractive tool to develop drugs for AD in a less tedious and economic way. Therefore, continuous efforts are being made to develop a new drug for AD by repursing old drugs through screening and data mining. For example, the survey in the drug pipeline for Phase III clinical trials (till February 2019) which has 27 candidates, and around half of the number are drugs which have already been approved for other indications. Although in the past the drug repurposing process for AD has been reviewed in the context of disease areas, molecular targets, there is no systematic review of repurposed drugs for AD from the recent drug development pipeline (2019-2020). In this manuscript, we are reviewing the clinical candidates for AD with emphasis on their development history including molecular targets and the relevance of the target for AD.","The most common reason behind dementia is Alzheimer's disease (AD) and it is predicted to be the third life-threatening disease apart from stroke and cancer for the geriatric population. Till now, only four drugs are available on the market for symptomatic relief. The complex nature of disease pathophysiology and lack of concrete evidence of molecular targets are the major hurdles for developing a new drug to treat AD. The rate of attrition of many advanced drugs at clinical stages makes the de novo discovery process very expensive. Alternatively, Drug Repurposing (DR) is an attractive tool to develop drugs for AD in a less tedious and economic way. Therefore, continuous efforts are being made to develop a new drug for AD by repurposing old drugs through screening and data mining. For example, the survey in the drug pipeline for Phase III clinical trials (till February 2019) consists of 27 candidates, and around half of the number are drugs which have already been approved for other indications. Although in the past, the drug repurposing process for AD has been reviewed in the context of disease areas, molecular targets, there is no systematic review of repurposed drugs for AD from the recent drug development pipeline (2019-2020). In this manuscript, we have reviewed the clinical candidates for AD with emphasis on their development history, including molecular targets and the relevance of the target for AD.","Dalvi, Dewangan, Das, Rani, Shinde, Vhora, Jain, Sahu","Dalvi, Dewangan, Das, Rani, Shinde, Vhora, Jain, Sahu",https://doi.org/10.2174/1871524920666201021164805,https://doi.org/10.2174/1871524920666201021164805,2021-08-03
15982.0,pubmed,pubmed,Use of zebrafish larvae lateral line to study protection against cisplatin-induced ototoxicity: A scoping review,Use of zebrafish larvae lateral line to study protection against cisplatin-induced ototoxicity: A scoping review,"The present review aimed to consolidate and analyze the recent information about the use of zebrafish in studies concerning cisplatin-induced ototoxicity and otoprotection. The PubMed, Web of Science, and Scopus databanks were searched using the following MESH terms: zebrafish, cisplatin, ototoxicity. The identified publications were screened according to inclusion and exclusion criteria and the 26 qualifying manuscripts were included in the full-text analysis. The experimental protocols, including cisplatin concentrations, the exposure duration and the outcome measurements used in zebrafish larvae studies, were evaluated and the reported knowledge was summarized. Twenty-six substances protecting from cisplatin-induced toxicity were identified with the use of zebrafish larvae. These substances include quinine, salvianolic acid B, berbamine 6, benzamil, quercetin, dexmedetomidine, dexamethsanone, quinoxaline, edaravone, apocynin, dimethyl sulfoxide, KR-22335, SRT1720, ORC-13661, 3-MA, D-methionine, mdivi-1, FUT-175, rapamycin, Z-LLF-CHO, ATX, NAC, CYM-5478, CHCP1, CHCP2 and leupeptin. The otoprotective effects of compounds were attributed to their anti-ROS, anti-apoptotic and cisplatin uptake-blocking properties. The broadest range of protection was achieved when the experimental flow used preconditioning with an otoprotective compound and later a co-incubation with cisplatin. Protection against a high concentration of cisplatin was observed only in protocols using short exposure times (4 and 6Ã¢â‚¬â€°h). The data extracted from the selected papers confirm that despite the differences between the human and the zebra fish hearing thresholds (as affected by cisplatin), the sensory cells of zebrafish and larval zebrafish are a valuable tool which could be used: (i) for the discovery of novel otoprotective substances and compounds; (ii) to screen their side effects and (iii) to extend the knowledge on the mechanisms of cisplatin-induced inner ear damage. For future studies, the development of a consensus experimental protocol is highly recommended.","The present review aimed to consolidate and analyze the recent information about the use of zebrafish in studies concerning cisplatin-induced ototoxicity and otoprotection. The PubMed, Web of Science, and Scopus databanks were searched using the following MESH terms: zebrafish, cisplatin, ototoxicity. The identified publications were screened according to inclusion and exclusion criteria and the 26 qualifying manuscripts were included in the full-text analysis. The experimental protocols, including cisplatin concentrations, the exposure duration and the outcome measurements used in zebrafish larvae studies, were evaluated and the reported knowledge was summarized. Twenty-six substances protecting from cisplatin-induced toxicity were identified with the use of zebrafish larvae. These substances include quinine, salvianolic acid B, berbamine 6, benzamil, quercetin, dexmedetomidine, dexamethsanone, quinoxaline, edaravone, apocynin, dimethyl sulfoxide, KR-22335, SRT1720, ORC-13661, 3-MA, D-methionine, mdivi-1, FUT-175, rapamycin, Z-LLF-CHO, ATX, NAC, CYM-5478, CHCP1, CHCP2 and leupeptin. The otoprotective effects of compounds were attributed to their anti-ROS, anti-apoptotic and cisplatin uptake-blocking properties. The broadest range of protection was achieved when the experimental flow used preconditioning with an otoprotective compound and later a co-incubation with cisplatin. Protection against a high concentration of cisplatin was observed only in protocols using short exposure times (4 and 6â€‰h). The data extracted from the selected papers confirm that despite the differences between the human and the zebra fish hearing thresholds (as affected by cisplatin), the sensory cells of zebrafish and larval zebrafish are a valuable tool which could be used: (i) for the discovery of novel otoprotective substances and compounds; (ii) to screen their side effects and (iii) to extend the knowledge on the mechanisms of cisplatin-induced inner ear damage. For future studies, the development of a consensus experimental protocol is highly recommended.","Domarecka, Skarzynska, Szczepek, Hatzopoulos","Domarecka, Skarzynska, Szczepek, Hatzopoulos",https://doi.org/10.1177/2058738420959554,https://doi.org/10.1177/2058738420959554,2021-08-03
15986.0,pubmed,pubmed,Significant Increase of Sexual Dysfunction in Patients With Renal Failure Receiving Renal Replacement Therapy: A Systematic Review and Meta-Analysis,Significant Increase of Sexual Dysfunction in Patients With Renal Failure Receiving Renal Replacement Therapy: A Systematic Review and Meta-Analysis,"It has been shown that sexual dysfunction (SD) is highly prevalent among patients with chronic renal failure (CRF), and starting renal replacement therapy may even increase it. However, SD is an infrequently reported problem in these treated patients. To investigate the prevalence of SD among patients with CRF undergoing renal replacement therapy, by a meta-analysis method. PubMed, Embase, and the Cochrane Library were systematically searched for all studies assessing sexual function in patients with CRF receiving renal replacement therapy from January 2000 to April 2020. Relative risk (RR) with 95% CIs was used for analysis to assess the risk of SD in patients with CRF receiving renal replacement therapy. The cross-sectional study quality methodology checklist was used for the cross-sectional study. The methodologic quality of the case-control and cohort studies was assessed with the Newcastle-Ottawa Scale. Data were pooled for the random-effect model. Sensitivity analyses were conducted to assess potential bias. The Begg and Egger tests were used for publication bias analysis. The prevalence of SD among patients with CRF receiving renal replacement therapy was summarized using pooled RR and 95% CI. This meta-analysis included 3,725 participants from 10 studies. Of these, 737 were patients with CRF receiving renal replacement therapy. The mean age of participants ranged from 32.75 to 56.1Ã‚Â years. Based on the random-effect model, synthesis of results demonstrated that the prevalence of SD was significantly increased among patients with CRF receiving renal replacement therapy in women (RRÃ‚Â =Ã‚Â 2.07, 95% CI: 1.47-2.91, PÃ‚Â =Ã‚Â .000; heterogeneity: I<sup>2</sup>Ã‚Â =Ã‚Â 78.7%, PÃ‚Â =Ã‚Â .000) and in men (RRÃ‚Â =Ã‚Â 2.95, 95% CI: 2.16-4.02, PÃ‚Â =Ã‚Â .000; heterogeneity: I<sup>2</sup>Ã‚Â =Ã‚Â 86.1%, PÃ‚Â =Ã‚Â .000). Estimates of the total effects were generally consistent in the sensitivity analysis. No evidence of publication bias was observed. Patients with CRF receiving renal replacement therapy had a significantly increased risk of SD, which suggests that clinicians should evaluate sexual function, when managing patients with CRF receiving renal replacement therapy. This is the first study to explore the prevalence of SD among patients with CRF undergoing renal replacement therapy based on all available epidemiologic studies. However, all included studies were an observational design, which may downgrade this evidence. The prevalence of SD is significantly increased among patients with CRF receiving renal replacement therapy. More research studies are warranted to clarify the relationship. Luo L, Xiao C, Xiang Q, etÃ‚Â al. Significant Increase of Sexual Dysfunction in Patients With Renal Failure Receiving Renal Replacement Therapy: A Systematic Review and Meta-Analysis. J Sex Med 2020;XX:XXX-XXX.","It has been shown that sexual dysfunction (SD) is highly prevalent among patients with chronic renal failure (CRF), and starting renal replacement therapy may even increase it. However, SD is an infrequently reported problem in these treated patients. To investigate the prevalence of SD among patients with CRF undergoing renal replacement therapy, by a meta-analysis method. PubMed, Embase, and the Cochrane Library were systematically searched for all studies assessing sexual function in patients with CRF receiving renal replacement therapy from January 2000 to April 2020. Relative risk (RR) with 95% CIs was used for analysis to assess the risk of SD in patients with CRF receiving renal replacement therapy. The cross-sectional study quality methodology checklist was used for the cross-sectional study. The methodologic quality of the case-control and cohort studies was assessed with the Newcastle-Ottawa Scale. Data were pooled for the random-effect model. Sensitivity analyses were conducted to assess potential bias. The Begg and Egger tests were used for publication bias analysis. The prevalence of SD among patients with CRF receiving renal replacement therapy was summarized using pooled RR and 95% CI. This meta-analysis included 3,725 participants from 10 studies. Of these, 737 were patients with CRF receiving renal replacement therapy. The mean age of participants ranged from 32.75 to 56.1Â years. Based on the random-effect model, synthesis of results demonstrated that the prevalence of SD was significantly increased among patients with CRF receiving renal replacement therapy in women (RRÂ =Â 2.07, 95% CI: 1.47-2.91, PÂ =Â .000; heterogeneity: I<sup>2</sup>Â =Â 78.7%, PÂ =Â .000) and in men (RRÂ =Â 2.95, 95% CI: 2.16-4.02, PÂ =Â .000; heterogeneity: I<sup>2</sup>Â =Â 86.1%, PÂ =Â .000). Estimates of the total effects were generally consistent in the sensitivity analysis. No evidence of publication bias was observed. Patients with CRF receiving renal replacement therapy had a significantly increased risk of SD, which suggests that clinicians should evaluate sexual function, when managing patients with CRF receiving renal replacement therapy. This is the first study to explore the prevalence of SD among patients with CRF undergoing renal replacement therapy based on all available epidemiologic studies. However, all included studies were an observational design, which may downgrade this evidence. The prevalence of SD is significantly increased among patients with CRF receiving renal replacement therapy. More research studies are warranted to clarify the relationship. Luo L, Xiao C, Xiang Q, etÂ al. Significant Increase of Sexual Dysfunction in Patients With Renal Failure Receiving Renal Replacement Therapy: A Systematic Review and Meta-Analysis. J Sex Med 2020;17:2382-2393.","Luo, Xiao, Xiang, Zhu, Liu, Wang, Deng, Zhao","Luo, Xiao, Xiang, Zhu, Liu, Wang, Deng, Zhao",https://doi.org/10.1016/j.jsxm.2020.08.019,https://doi.org/10.1016/j.jsxm.2020.08.019,2021-08-03
15992.0,pubmed,pubmed,Role of machine learning in gait analysis: a review,Role of machine learning in gait analysis: a review,"Human biomechanics and gait form an integral part of life. The gait analysis involves a large number of interdependent parameters that were difficult to interpret due to a vast amount of data and their inter-relations. To simplify evaluation, the integration of machine learning (ML) with biomechanics is a promising solution. The purpose of this review is to familiarise the readers with key directions of implementation of ML techniques for gait analysis and gait rehabilitation. An extensive literature survey was based on research articles from nine databases published from 1980 to 2019. With over 943 studies identified, finally, 43 studies met the inclusion criteria. The outcome reported illustrates that supervised ML techniques showed accuracies above 90% in the identified gait analysis domain. The statistical results revealed support vector machine (SVM) as the best classifier (mean-score = 0.87Ã¢â‚¬â€°Ã‚Â±Ã¢â‚¬â€°0.07) with remarkable generalisation capability even on small to medium datasets. It has also been analysed that the control strategies for gait rehabilitation are benefitted from reinforcement learning and (deep) neural-networks due to their ability to capture participants' variability. This review paper shows the success of ML techniques in detecting disorders, predicting rehabilitation length, and control of rehabilitation devices which make them suitable for clinical diagnosis.","Human biomechanics and gait form an integral part of life. The gait analysis involves a large number of interdependent parameters that were difficult to interpret due to a vast amount of data and their inter-relations. To simplify evaluation, the integration of machine learning (ML) with biomechanics is a promising solution. The purpose of this review is to familiarise the readers with key directions of implementation of ML techniques for gait analysis and gait rehabilitation. An extensive literature survey was based on research articles from nine databases published from 1980 to 2019. With over 943 studies identified, finally, 43 studies met the inclusion criteria. The outcome reported illustrates that supervised ML techniques showed accuracies above 90% in the identified gait analysis domain. The statistical results revealed support vector machine (SVM) as the best classifier (mean-score = 0.87â€‰Â±â€‰0.07) with remarkable generalisation capability even on small to medium datasets. It has also been analysed that the control strategies for gait rehabilitation are benefitted from reinforcement learning and (deep) neural-networks due to their ability to capture participants' variability. This review paper shows the success of ML techniques in detecting disorders, predicting rehabilitation length, and control of rehabilitation devices which make them suitable for clinical diagnosis.","Khera, Kumar","Khera, Kumar",https://doi.org/10.1080/03091902.2020.1822940,https://doi.org/10.1080/03091902.2020.1822940,2021-08-03
15994.0,pubmed,pubmed,Sustained acoustic medicine as a non-surgical and non-opioid knee osteoarthritis treatment option: a health economic cost-effectiveness analysis for symptom management,Sustained acoustic medicine as a non-surgical and non-opioid knee osteoarthritis treatment option: a health economic cost-effectiveness analysis for symptom management,"Patients diagnosed with osteoarthritis (OA) and presenting with symptoms are seeking conservative treatment options to reduce pain, improve function, and avoid surgery. Sustained acoustic medicine (SAM), a multi-hour treatment has demonstrated improved clinical outcomes for patients with knee OA. The purpose of this analysis was to compare the costs and effectiveness of multi-hour SAM treatment versus the standard of care (SOC) over a 6-month timeframe for OA symptom management. A decision tree analysis was used to compare the costs and effectiveness of SAM treatment versus SOC in patients with OA. Probabilities of success for OA treatment and effectiveness were derived from the literature using systematic reviews and meta-analyses. Costs were derived from Medicare payment rates and manufacturer prices. Functional effectiveness was measured as the effect size of a therapy and treatment pathways compared to a SOC treatment pathway. A sensitivity analysis was performed to determine which cost variables had the greatest effect on deciding which option was the least costly. An incremental cost-effectiveness plot comparing SAM treatment vs. SOC was also generated using 1000 iterations of the model. Lastly, the incremental cost-effectiveness ratio (ICER) was calculated as the (cost of SAM minus cost of SOC) divided by (functional effectiveness of SAM minus functional effectiveness of SOC). Base case demonstrated that over 6 months, the cost and functional effectiveness of SAM was $8641 and 0.52 versus SOC at: $6281 and 0.39, respectively. Sensitivity analysis demonstrated that in order for SAM to be the less expensive option, the cost per 15-min session of PT would need to be greater than $88, or SAM would need to be priced at less than or equal to $2276. Incremental cost-effectiveness demonstrated that most of the time (84%) SAM treatment resulted in improved functional effectiveness but at a higher cost than SOC. In patients with osteoarthritis, SAM treatment demonstrated improved pain and functional gains compared to SOC but at an increased cost. Based on the SAM treatment ICER score being Ã¢â€°Â¤ $50,000, it appears that SAM is a cost-effective treatment for knee OA.","Patients diagnosed with osteoarthritis (OA) and presenting with symptoms are seeking conservative treatment options to reduce pain, improve function, and avoid surgery. Sustained acoustic medicine (SAM), a multi-hour treatment has demonstrated improved clinical outcomes for patients with knee OA. The purpose of this analysis was to compare the costs and effectiveness of multi-hour SAM treatment versus the standard of care (SOC) over a 6-month timeframe for OA symptom management. A decision tree analysis was used to compare the costs and effectiveness of SAM treatment versus SOC in patients with OA. Probabilities of success for OA treatment and effectiveness were derived from the literature using systematic reviews and meta-analyses. Costs were derived from Medicare payment rates and manufacturer prices. Functional effectiveness was measured as the effect size of a therapy and treatment pathways compared to a SOC treatment pathway. A sensitivity analysis was performed to determine which cost variables had the greatest effect on deciding which option was the least costly. An incremental cost-effectiveness plot comparing SAM treatment vs. SOC was also generated using 1000 iterations of the model. Lastly, the incremental cost-effectiveness ratio (ICER) was calculated as the (cost of SAM minus cost of SOC) divided by (functional effectiveness of SAM minus functional effectiveness of SOC). Base case demonstrated that over 6 months, the cost and functional effectiveness of SAM was $8641 and 0.52 versus SOC at: $6281 and 0.39, respectively. Sensitivity analysis demonstrated that in order for SAM to be the less expensive option, the cost per 15-min session of PT would need to be greater than $88, or SAM would need to be priced at less than or equal to $2276. Incremental cost-effectiveness demonstrated that most of the time (84%) SAM treatment resulted in improved functional effectiveness but at a higher cost than SOC. In patients with osteoarthritis, SAM treatment demonstrated improved pain and functional gains compared to SOC but at an increased cost. Based on the SAM treatment ICER score being â‰¤ $50,000, it appears that SAM is a cost-effective treatment for knee OA.","Best, Petterson, Plancher","Best, Petterson, Plancher",https://doi.org/10.1186/s13018-020-01987-x,https://doi.org/10.1186/s13018-020-01987-x,2021-08-03
15996.0,pubmed,pubmed,Brain Metastasis Detection using Machine Learning: A Systematic Review and Meta-analysis,Brain metastasis detection using machine learning: a systematic review and meta-analysis,"Accurate detection of brain metastasis (BM) is important for cancer patients. We aimed to systematically review the performance and quality of machine-learning-based BM detection on MRI in the relevant literature. A systematic literature search was performed for relevant studies reported before April 27, 2020. We assessed the quality of the studies using modified tailored questionnaires of the Quality Assessment of Diagnostic Accuracy Studies-2 (QUADAS-2) criteria and the Checklist for Artificial Intelligence in Medical Imaging. Pooled detectability was calculated using an inverse-variance weighting model. A total of 12 studies were included, which showed a clear transition from classical machine learning (cML) to deep learning (DL) after 2018. The studies on DL used a larger sample size than those on cML. The cML and DL groups also differed in the composition of the data set, and technical details such as data augmentation. The pooled proportion of detectability of BM were 88.7% (95% CI, 84-93%) and 90.1% (95% CI, 84-95%) in the cML and DL groups, respectively. The false-positive rate per person was lower in the DL group than the cML group (10 vs. 135, P &lt; 0.001). In the patient selection domain of QUADAS-2, three studies (25%) were designated as high risk due to non-consecutive enrollment and arbitrary exclusion of nodules. A comparable detectability of BM with a low false-positive rate per person was found in the DL group compared to the cML group. Improvements are required in terms of quality and study design.","Accurate detection of brain metastasis (BM) is important for cancer patients. We aimed to systematically review the performance and quality of machine-learning-based BM detection on MRI in the relevant literature. A systematic literature search was performed for relevant studies reported before April 27, 2020. We assessed the quality of the studies using modified tailored questionnaires of the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) criteria and the Checklist for Artificial Intelligence in Medical Imaging (CLAIM). Pooled detectability was calculated using an inverse-variance weighting model. A total of 12 studies were included, which showed a clear transition from classical machine learning (cML) to deep learning (DL) after 2018. The studies on DL used a larger sample size than those on cML. The cML and DL groups also differed in the composition of the dataset, and technical details such as data augmentation. The pooled proportions of detectability of BM were 88.7% (95% CI, 84-93%) and 90.1% (95% CI, 84-95%) in the cML and DL groups, respectively. The false-positive rate per person was lower in the DL group than the cML group (10 vs 135, P &lt; 0.001). In the patient selection domain of QUADAS-2, three studies (25%) were designated as high risk due to non-consecutive enrollment and arbitrary exclusion of nodules. A comparable detectability of BM with a low false-positive rate per person was found in the DL group compared with the cML group. Improvements are required in terms of quality and study design.","Cho, Sunwoo, Baik, Bae, Choi, Kim","Cho, Sunwoo, Baik, Bae, Choi, Kim",https://doi.org/10.1093/neuonc/noaa232,https://doi.org/10.1093/neuonc/noaa232,2021-08-03
15997.0,pubmed,pubmed,Natural Language Processing in Surgery: A Systematic Review and Meta-Analysis,Natural Language Processing in Surgery: A Systematic Review and Meta-analysis,"The aim of this study was to systematically assess the application and potential benefits of natural language processing (NLP) in surgical outcomes research. Widespread implementation of electronic health records (EHRs) has generated a massive patient data source. Traditional methods of data capture, such as billing codes and/or manual review of free-text narratives in EHRs, are highly labor-intensive, costly, subjective, and potentially prone to bias. A literature search of PubMed, MEDLINE, Web of Science, and Embase identified all articles published starting in 2000 that used NLP models to assess perioperative surgical outcomes. Evaluation metrics of NLP systems were assessed by means of pooled analysis and meta-analysis. Qualitative synthesis was carried out to assess the results and risk of bias on outcomes. The present study included 29 articles, with over half (n = 15) published after 2018. The most common outcome identified using NLP was postoperative complications (n = 14). Compared to traditional non-NLP models, NLP models identified postoperative complications with higher sensitivity [0.92 (0.87-0.95) vs 0.58 (0.33-0.79), P &lt; 0.001]. The specificities were comparable at 0.99 (0.96-1.00) and 0.98 (0.95-0.99), respectively. Using summary of likelihood ratio matrices, traditional non-NLP models have clinical utility for confirming documentation of outcomes/diagnoses, whereas NLP models may be reliably utilized for both confirming and ruling out documentation of outcomes/diagnoses. NLP usage to extract a range of surgical outcomes, particularly postoperative complications, is accelerating across disciplines and areas of clinical outcomes research. NLP and traditional non-NLP approaches demonstrate similar performance measures, but NLP is superior in ruling out documentation of surgical outcomes.","The aim of this study was to systematically assess the application and potential benefits of natural language processing (NLP) in surgical outcomes research. Widespread implementation of electronic health records (EHRs) has generated a massive patient data source. Traditional methods of data capture, such as billing codes and/or manual review of free-text narratives in EHRs, are highly labor-intensive, costly, subjective, and potentially prone to bias. A literature search of PubMed, MEDLINE, Web of Science, and Embase identified all articles published starting in 2000 that used NLP models to assess perioperative surgical outcomes. Evaluation metrics of NLP systems were assessed by means of pooled analysis and meta-analysis. Qualitative synthesis was carried out to assess the results and risk of bias on outcomes. The present study included 29 articles, with over half (n = 15) published after 2018. The most common outcome identified using NLP was postoperative complications (n = 14). Compared to traditional non-NLP models, NLP models identified postoperative complications with higher sensitivity [0.92 (0.87-0.95) vs 0.58 (0.33-0.79), P &lt; 0.001]. The specificities were comparable at 0.99 (0.96-1.00) and 0.98 (0.95-0.99), respectively. Using summary of likelihood ratio matrices, traditional non-NLP models have clinical utility for confirming documentation of outcomes/diagnoses, whereas NLP models may be reliably utilized for both confirming and ruling out documentation of outcomes/diagnoses. NLP usage to extract a range of surgical outcomes, particularly postoperative complications, is accelerating across disciplines and areas of clinical outcomes research. NLP and traditional non-NLP approaches demonstrate similar performance measures, but NLP is superior in ruling out documentation of surgical outcomes.","Mellia, Basta, Toyoda, Othman, Elfanagely, Morris, Torre-Healy, Ungar, Fischer","Mellia, Basta, Toyoda, Othman, Elfanagely, Morris, Torre-Healy, Ungar, Fischer",https://doi.org/10.1097/SLA.0000000000004419,https://doi.org/10.1097/SLA.0000000000004419,2021-08-03
15999.0,pubmed,pubmed,Reversal learning in young and middle-age neurotypicals: Individual difference reaction time considerations,Reversal learning in young and middle-age neurotypicals: Individual difference reaction time considerations,"Reversal learning is frequently used to assess components of executive function that contribute to understanding age-related cognitive differences. Reaction time (RT) is less characterized in the reversal learning literature, perhaps due to the daunting task of analyzing the entire RT distribution, but has been deemed a generally sensitive measure of cognitive aging. The current study extends our prior work to further characterize distributional properties of the reversal RT distribution and to distinguish groups of individuals with fractionated profiles of performance, which may be of clinical importance within the context of cognitive aging. Participant sample included young (<i>n</i> =Ã‚Â 43) and community-dwelling, healthy, middle-aged (<i>n</i> =Ã‚Â 139) adults. To explore individual differences, recursive partitioning analysis achieved a high classification rate by specifying decision tree rules that split participants into young and middle-aged groups. Mu (ÃŽÂ¼, efficient RT) was the most successful parameter in distinguishing age groups while sigma ( <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mi>ÃÆ’</mml:mi> <mml:mo>)</mml:mo> </mml:math> and tau ( <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mi>Ãâ€ž</mml:mi> </mml:math> , ex-Gaussian indices of intra-individual variability) revealed more subtle individual differences. Accuracy measures did not contribute to separating the groups, suggesting that fractionated components of RT, as opposed to accuracy, can distinguish differences between young and middle-aged participants.","Reversal learning is frequently used to assess components of executive function that contribute to understanding age-related cognitive differences. Reaction time (RT) is less characterized in the reversal learning literature, perhaps due to the daunting task of analyzing the entire RT distribution, but has been deemed a generally sensitive measure of cognitive aging. The current study extends our prior work to further characterize distributional properties of the reversal RT distribution and to distinguish groups of individuals with fractionated profiles of performance, which may be of clinical importance within the context of cognitive aging. Participant sample included young (<i>n</i> =Â 43) and community-dwelling, healthy, middle-aged (<i>n</i> =Â 139) adults. To explore individual differences, recursive partitioning analysis achieved a high classification rate by specifying decision tree rules that split participants into young and middle-aged groups. Mu (Î¼, efficient RT) was the most successful parameter in distinguishing age groups while sigma ( <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi>Ïƒ</mml:mi> <mml:mo>)</mml:mo></mml:math> and tau ( <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mi>Ï„</mml:mi></mml:math> , ex-Gaussian indices of intra-individual variability) revealed more subtle individual differences. Accuracy measures did not contribute to separating the groups, suggesting that fractionated components of RT, as opposed to accuracy, can distinguish differences between young and middle-aged participants.","Osmon, Leclaire, Driscoll, Zolliecoffer","Osmon, Leclaire, Driscoll, Zolliecoffer",https://doi.org/10.1080/13803395.2020.1825635,https://doi.org/10.1080/13803395.2020.1825635,2021-08-03
16002.0,pubmed,pubmed,Identification of Patients with Nontraumatic Intracranial Hemorrhage Using Administrative Claims Data,Identification of Patients with Nontraumatic Intracranial Hemorrhage Using Administrative Claims Data,"Nontraumatic intracranial hemorrhage (ICH) is a neurological emergency of research interest; however, unlike ischemic stroke, has not been well studied in large datasets due to the lack of an established administrative claims-based definition. We aimed to evaluate both explicit diagnosis codes and machine learning methods to create a claims-based definition for this clinical phenotype. We examined all patients admitted to our tertiary medical center with a primary or secondary International Classification of Disease version 9 (ICD-9) or 10 (ICD-10) code for ICH in claims from any portion of the hospitalization in 2014-2015. As a gold standard, we defined the nontraumatic ICH phenotype based on manual chart review. We tested explicit definitions based on ICD-9 and ICD-10 that had been previously published in the literature as well as four machine learning classifiers including support vector machine (SVM), logistic regression with LASSO, random forest and xgboost. We report five standard measures of model performance for each approach. A total of 1830 patients with 2145 unique ICD-10 codes were included in the initial dataset, of which 437 (24%) were true positive based on manual review. The explicit ICD-10 definition performed best (SensitivityÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.89 (95% CI 0.85-0.92), SpecificityÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.83 (0.81-0.85), F-scoreÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.73 (0.69-0.77)) and improves on an explicit ICD-9 definition (SensitivityÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.87 (0.83-0.90), SpecificityÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.77 (0.74-0.79), F-scoreÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.67 (0.63-0.71). Among machine learning classifiers, SVM performed best (SensitivityÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.78 (0.75-0.82), SpecificityÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.84 (0.81-0.87), AUCÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.89 (0.87-0.92), F-scoreÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.66 (0.62-0.69)). An explicit ICD-10 definition can be used to accurately identify patients with a nontraumatic ICH phenotype with substantially better performance than ICD-9. An explicit ICD-10 based definition is easier to implement and quantitatively not appreciably improved with the additional application of machine learning classifiers. Future research utilizing large datasets should utilize this definition to address important research gaps.","Nontraumatic intracranial hemorrhage (ICH) is a neurological emergency of research interest; however, unlike ischemic stroke, has not been well studied in large datasets due to the lack of an established administrative claims-based definition. We aimed to evaluate both explicit diagnosis codes and machine learning methods to create a claims-based definition for this clinical phenotype. We examined all patients admitted to our tertiary medical center with a primary or secondary International Classification of Disease version 9 (ICD-9) or 10 (ICD-10) code for ICH in claims from any portion of the hospitalization in 2014-2015. As a gold standard, we defined the nontraumatic ICH phenotype based on manual chart review. We tested explicit definitions based on ICD-9 and ICD-10 that had been previously published in the literature as well as four machine learning classifiers including support vector machine (SVM), logistic regression with LASSO, random forest and xgboost. We report five standard measures of model performance for each approach. A total of 1830 patients with 2145 unique ICD-10 codes were included in the initial dataset, of which 437 (24%) were true positive based on manual review. The explicit ICD-10 definition performed best (Sensitivityâ€¯=â€¯0.89 (95% CI 0.85-0.92), Specificityâ€¯=â€¯0.83 (0.81-0.85), F-scoreâ€¯=â€¯0.73 (0.69-0.77)) and improves on an explicit ICD-9 definition (Sensitivityâ€¯=â€¯0.87 (0.83-0.90), Specificityâ€¯=â€¯0.77 (0.74-0.79), F-scoreâ€¯=â€¯0.67 (0.63-0.71). Among machine learning classifiers, SVM performed best (Sensitivityâ€¯=â€¯0.78 (0.75-0.82), Specificityâ€¯=â€¯0.84 (0.81-0.87), AUCâ€¯=â€¯0.89 (0.87-0.92), F-scoreâ€¯=â€¯0.66 (0.62-0.69)). An explicit ICD-10 definition can be used to accurately identify patients with a nontraumatic ICH phenotype with substantially better performance than ICD-9. An explicit ICD-10 based definition is easier to implement and quantitatively not appreciably improved with the additional application of machine learning classifiers. Future research utilizing large datasets should utilize this definition to address important research gaps.","Sangal, Fodeh, Taylor, Rothenberg, Finn, Sheth, Matouk, Ulrich, Parwani, Sather, Venkatesh","Sangal, Fodeh, Taylor, Rothenberg, Finn, Sheth, Matouk, Ulrich, Parwani, Sather, Venkatesh",https://doi.org/10.1016/j.jstrokecerebrovasdis.2020.105306,https://doi.org/10.1016/j.jstrokecerebrovasdis.2020.105306,2021-08-03
16004.0,pubmed,pubmed,"TILES-2018, a longitudinal physiologic and behavioral data set of hospital workers","TILES-2018, a longitudinal physiologic and behavioral data set of hospital workers","We present a novel longitudinal multimodal corpus of physiological and behavioral data collected from direct clinical providers in a hospital workplace. We designed the study to investigate the use of off-the-shelf wearable and environmental sensors to understand individual-specific constructs such as job performance, interpersonal interaction, and well-being of hospital workers over time in their natural day-to-day job settings. We collected behavioral and physiological data from nÃ¢â‚¬â€°=Ã¢â‚¬â€°212 participants through Internet-of-Things Bluetooth data hubs, wearable sensors (including a wristband, a biometrics-tracking garment, a smartphone, and an audio-feature recorder), together with a battery of surveys to assess personality traits, behavioral states, job performance, and well-being over time. Besides the default use of the data set, we envision several novel research opportunities and potential applications, including multi-modal and multi-task behavioral modeling, authentication through biometrics, and privacy-aware and privacy-preserving machine learning.","We present a novel longitudinal multimodal corpus of physiological and behavioral data collected from direct clinical providers in a hospital workplace. We designed the study to investigate the use of off-the-shelf wearable and environmental sensors to understand individual-specific constructs such as job performance, interpersonal interaction, and well-being of hospital workers over time in their natural day-to-day job settings. We collected behavioral and physiological data from nâ€‰=â€‰212 participants through Internet-of-Things Bluetooth data hubs, wearable sensors (including a wristband, a biometrics-tracking garment, a smartphone, and an audio-feature recorder), together with a battery of surveys to assess personality traits, behavioral states, job performance, and well-being over time. Besides the default use of the data set, we envision several novel research opportunities and potential applications, including multi-modal and multi-task behavioral modeling, authentication through biometrics, and privacy-aware and privacy-preserving machine learning.","Mundnich, Booth, L'Hommedieu, Feng, Girault, L'Hommedieu, Wildman, Skaaden, Nadarajan, Villatte, Falk, Lerman, Ferrara, Narayanan","Mundnich, Booth, L'Hommedieu, Feng, Girault, L'Hommedieu, Wildman, Skaaden, Nadarajan, Villatte, Falk, Lerman, Ferrara, Narayanan",https://doi.org/10.1038/s41597-020-00655-3,https://doi.org/10.1038/s41597-020-00655-3,2021-08-03
16005.0,pubmed,pubmed,"Top consumer uses of bestselling, single-ingredient vitamin and mineral supplements","Top consumer uses of bestselling, single-ingredient vitamin and mineral supplements","Vitamin and mineral supplements are widely used for self-care of a variety of medical conditions, but little is known about the specific conditions for which they are used. This study mined consumer product reviews to determine specific ways vitamin and mineral supplements are being used therapeutically. A cross-sectional analysis of user reviews for top-selling, single-ingredient vitamin and mineral products from a popular online retailer was performed to identify the most frequently appearing words associated with medical conditions. Results of individual analyses were compared to achieve consensus on the top, relevant keywords for each supplement. The full text of the reviews was searched for these keywords to distinguish whether they referred to therapeutic uses or adverse effects. A total of 14 vitamin and 11 mineral supplements were analyzed. The number of user reviews for the analyzed products varied from 41 for manganese to over 5000 for biotin and vitamin D (medianÃ¢â‚¬â€°=Ã¢â‚¬â€°547 reviews per product). Cohen's kappa test for investigator-selected keywords related to medical conditions was generally greater than 0.6, indicating good interrater reliability. From these lists, the top consumer self-care uses were identified for 24 supplements. Commonly reported adverse effects were also noted for several products. This study used data mining to identify the top ways consumers use an array of bestselling, single-ingredient vitamin and mineral supplements. These results can provide healthcare and nutrition professionals with information to anticipate the supplement-related education needs of patients and provide researchers with priority areas for clinical studies.","Vitamin and mineral supplements are widely used for self-care of a variety of medical conditions, but little is known about the specific conditions for which they are used. This study mined consumer product reviews to determine specific ways vitamin and mineral supplements are being used therapeutically. A cross-sectional analysis of user reviews for top-selling, single-ingredient vitamin and mineral products from a popular online retailer was performed to identify the most frequently appearing words associated with medical conditions. Results of individual analyses were compared to achieve consensus on the top, relevant keywords for each supplement. The full text of the reviews was searched for these keywords to distinguish whether they referred to therapeutic uses or adverse effects. A total of 14 vitamin and 11 mineral supplements were analyzed. The number of user reviews for the analyzed products varied from 41 for manganese to over 5000 for biotin and vitamin D (medianâ€‰=â€‰547 reviews per product). Cohen's kappa test for investigator-selected keywords related to medical conditions was generally greater than 0.6, indicating good interrater reliability. From these lists, the top consumer self-care uses were identified for 24 supplements. Commonly reported adverse effects were also noted for several products. This study used data mining to identify the top ways consumers use an array of bestselling, single-ingredient vitamin and mineral supplements. These results can provide healthcare and nutrition professionals with information to anticipate the supplement-related education needs of patients and provide researchers with priority areas for clinical studies.","Johanson, Stirnaman, Rose","Johanson, Stirnaman, Rose",https://doi.org/10.1016/j.ctim.2020.102540,https://doi.org/10.1016/j.ctim.2020.102540,2021-08-03
16006.0,pubmed,pubmed,Artificial Intelligence for the Management of Pancreatic Diseases,Artificial intelligence for the management of pancreatic diseases,"Novel artificial intelligence techniques are emerging in all fields of healthcare, including Gastroenterology. The aim of this review is to give an overview of artificial intelligence applications in the management of pancreatic diseases. We performed a systematic literature search in PubMed and Medline up to May 2020 to identify relevant articles. Our results showed that the development of machine-learning based applications is rapidly evolving in the management of pancreatic diseases, guiding precision medicine in clinical, endoscopic and radiologic settings. Before implementation into clinical practice, further research should focus on the external validation of novel techniques, clarifying the accuracy and robustness of these models.","Novel artificial intelligence techniques are emerging in all fields of healthcare, including gastroenterology. The aim of this review is to give an overview of artificial intelligence applications in the management of pancreatic diseases. We performed a systematic literature search in PubMed and Medline up to May 2020 to identify relevant articles. Our results showed that the development of machine-learning based applications is rapidly evolving in the management of pancreatic diseases, guiding precision medicine in clinical, endoscopic and radiologic settings. Before implementation into clinical practice, further research should focus on the external validation of novel techniques, clarifying the accuracy and robustness of these models.","Gorris, Hoogenboom, Wallace, van Hooft","Gorris, Hoogenboom, Wallace, van Hooft",https://doi.org/10.1111/den.13875,https://doi.org/10.1111/den.13875,2021-08-03
16009.0,pubmed,pubmed,Forty years of emergency medicine research: Uncovering research themes and trends through topic modeling,Forty years of emergency medicine research: Uncovering research themes and trends through topic modeling,"Topic identification can facilitate knowledge curation, discover thematic relationships, trends, and predict future direction. We aimed to determine through an unsupervised, machine learning approach to topic modeling the most common research themes in emergency medicine over the last 40Ã‚Â years and summarize their trends and characteristics. We retrieved the complete reference entries including article abstracts from Ovid for all original research articles from 1980 to 2019 within emergency medicine for six widely-cited journals. Abstracts were processed through a natural language pipeline and analyzed by a latent Dirichlet allocation topic modeling algorithm for unsupervised topic discovery. Topics were further examined through trend analysis, word associations, co-occurrence metrics, and two-dimensional embeddings. We retrieved 47,158 articles during the defined time period that were filtered to 20,528 articles for further analysis. Forty topics covering methodologic and clinical areas were discovered. These topics separated into distinct clusters when embedded in two-dimensional space and exhibited consistent patterns of interaction. We observed the greatest increase in popularity in research themes involving risk factors (0.4% to 5.2%), health utilization (1.2% to 5.0%), and ultrasound (0.7% to 3.3%), and a relative decline in research involving basic science (8.9% to 1.1%), cardiac arrest (6.5% to 2.2%), and vitals (6.3% to 1.3%) over the past 40Ã‚Â years. Our data show only very modest growth in mental health and substance abuse research (1.0% to 1.6%), despite ongoing crises. Topic modeling via unsupervised machine learning applied to emergency medicine abstracts discovered coherent topics, trends, and patterns of interaction.","Topic identification can facilitate knowledge curation, discover thematic relationships, trends, and predict future direction. We aimed to determine through an unsupervised, machine learning approach to topic modeling the most common research themes in emergency medicine over the last 40Â years and summarize their trends and characteristics. We retrieved the complete reference entries including article abstracts from Ovid for all original research articles from 1980 to 2019 within emergency medicine for six widely-cited journals. Abstracts were processed through a natural language pipeline and analyzed by a latent Dirichlet allocation topic modeling algorithm for unsupervised topic discovery. Topics were further examined through trend analysis, word associations, co-occurrence metrics, and two-dimensional embeddings. We retrieved 47,158 articles during the defined time period that were filtered to 20,528 articles for further analysis. Forty topics covering methodologic and clinical areas were discovered. These topics separated into distinct clusters when embedded in two-dimensional space and exhibited consistent patterns of interaction. We observed the greatest increase in popularity in research themes involving risk factors (0.4% to 5.2%), health utilization (1.2% to 5.0%), and ultrasound (0.7% to 3.3%), and a relative decline in research involving basic science (8.9% to 1.1%), cardiac arrest (6.5% to 2.2%), and vitals (6.3% to 1.3%) over the past 40Â years. Our data show only very modest growth in mental health and substance abuse research (1.0% to 1.6%), despite ongoing crises. Topic modeling via unsupervised machine learning applied to emergency medicine abstracts discovered coherent topics, trends, and patterns of interaction.","Porturas, Taylor","Porturas, Taylor",https://doi.org/10.1016/j.ajem.2020.08.036,https://doi.org/10.1016/j.ajem.2020.08.036,2021-08-03
16010.0,pubmed,pubmed,An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening - impact on reviewer-relevant outcomes,An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening - impact on reviewer-relevant outcomes,"Systematic reviews often require substantial resources, partially due to the large number of records identified during searching. Although artificial intelligence may not be ready to fully replace human reviewers, it may accelerate and reduce the screening burden. Using DistillerSR (May 2020 release), we evaluated the performance of the prioritization simulation tool to determine the reduction in screening burden and time savings. Using a true recall @ 95%, response sets from 10 completed systematic reviews were used to evaluate: (i) the reduction of screening burden; (ii) the accuracy of the prioritization algorithm; and (iii) the hours saved when a modified screening approach was implemented. To account for variation in the simulations, and to introduce randomness (through shuffling the references), 10 simulations were run for each review. Means, standard deviations, medians and interquartile ranges (IQR) are presented. Among the 10 systematic reviews, using true recall @ 95% there was a median reduction in screening burden of 47.1% (IQR: 37.5 to 58.0%). A median of 41.2% (IQR: 33.4 to 46.9%) of the excluded records needed to be screened to achieve true recall @ 95%. The median title/abstract screening hours saved using a modified screening approach at a true recall @ 95% was 29.8Ã¢â‚¬â€°h (IQR: 28.1 to 74.7Ã¢â‚¬â€°h). This was increased to a median of 36Ã¢â‚¬â€°h (IQR: 32.2 to 79.7Ã¢â‚¬â€°h) when considering the time saved not retrieving and screening full texts of the remaining 5% of records not yet identified as included at title/abstract. Among the 100 simulations (10 simulations per review), none of these 5% of records were a final included study in the systematic review. The reduction in screening burden to achieve true recall @ 95% compared to @ 100% resulted in a reduced screening burden median of 40.6% (IQR: 38.3 to 54.2%). The prioritization tool in DistillerSR can reduce screening burden. A modified or stop screening approach once a true recall @ 95% is achieved appears to be a valid method for rapid reviews, and perhaps systematic reviews. This needs to be further evaluated in prospective reviews using the estimated recall.","Systematic reviews often require substantial resources, partially due to the large number of records identified during searching. Although artificial intelligence may not be ready to fully replace human reviewers, it may accelerate and reduce the screening burden. Using DistillerSR (May 2020 release), we evaluated the performance of the prioritization simulation tool to determine the reduction in screening burden and time savings. Using a true recall @ 95%, response sets from 10 completed systematic reviews were used to evaluate: (i) the reduction of screening burden; (ii) the accuracy of the prioritization algorithm; and (iii) the hours saved when a modified screening approach was implemented. To account for variation in the simulations, and to introduce randomness (through shuffling the references), 10 simulations were run for each review. Means, standard deviations, medians and interquartile ranges (IQR) are presented. Among the 10 systematic reviews, using true recall @ 95% there was a median reduction in screening burden of 47.1% (IQR: 37.5 to 58.0%). A median of 41.2% (IQR: 33.4 to 46.9%) of the excluded records needed to be screened to achieve true recall @ 95%. The median title/abstract screening hours saved using a modified screening approach at a true recall @ 95% was 29.8â€‰h (IQR: 28.1 to 74.7â€‰h). This was increased to a median of 36â€‰h (IQR: 32.2 to 79.7â€‰h) when considering the time saved not retrieving and screening full texts of the remaining 5% of records not yet identified as included at title/abstract. Among the 100 simulations (10 simulations per review), none of these 5% of records were a final included study in the systematic review. The reduction in screening burden to achieve true recall @ 95% compared to @ 100% resulted in a reduced screening burden median of 40.6% (IQR: 38.3 to 54.2%). The prioritization tool in DistillerSR can reduce screening burden. A modified or stop screening approach once a true recall @ 95% is achieved appears to be a valid method for rapid reviews, and perhaps systematic reviews. This needs to be further evaluated in prospective reviews using the estimated recall.","Hamel, Kelly, Thavorn, Rice, Wells, Hutton","Hamel, Kelly, Thavorn, Rice, Wells, Hutton",https://doi.org/10.1186/s12874-020-01129-1,https://doi.org/10.1186/s12874-020-01129-1,2021-08-03
16011.0,pubmed,pubmed,Extracting medication information from unstructured public health data: a demonstration on data from population-based and tertiary-based samples,Extracting medication information from unstructured public health data: a demonstration on data from population-based and tertiary-based samples,"Unstructured data from clinical epidemiological studies can be valuable and easy to obtain. However, it requires further extraction and processing for data analysis. Doing this manually is labor-intensive, slow and subject to error. In this study, we propose an automation framework for extracting and processing unstructured data. The proposed automation framework consisted of two natural language processing (NLP) based tools for unstructured text data for medications and reasons for medication use. We first checked spelling using a spell-check program trained on publicly available knowledge sources and then applied NLP techniques. We mapped medication names into generic names using vocabulary from publicly available knowledge sources. We used WHO's Anatomical Therapeutic Chemical (ATC) classification system to map generic medication names to medication classes. We processed the reasons for medication with the Lancaster stemmer method and then grouped and mapped to disease classes based on organ systems. Finally, we demonstrated this automation framework on two data sources for Mylagic Encephalomyelitis/ Chronic Fatigue Syndrome (ME/CFS): tertiary-based (nÃ¢â‚¬â€°=Ã¢â‚¬â€°378) and population-based (nÃ¢â‚¬â€°=Ã¢â‚¬â€°664) samples. A total of 8681 raw medication records were used for this demonstration. The 1266 distinct medication names (omitting supplements) were condensed to 89 ATC classification system categories. The 1432 distinct raw reasons for medication use were condensed to 65 categories via NLP. Compared to completion of the entire process manually, our automation process reduced the number of the terms requiring manual labor for mapping by 84.4% for medications and 59.4% for reasons for medication use. Additionally, this process improved the precision of the mapped results. Our automation framework demonstrates the usefulness of NLP strategies even when there is no established mapping database. For a less established database (e.g., reasons for medication use), the method is easily modifiable as new knowledge sources for mapping are introduced. The capability to condense large features into interpretable ones will be valuable for subsequent analytical studies involving techniques such as machine learning and data mining.","Unstructured data from clinical epidemiological studies can be valuable and easy to obtain. However, it requires further extraction and processing for data analysis. Doing this manually is labor-intensive, slow and subject to error. In this study, we propose an automation framework for extracting and processing unstructured data. The proposed automation framework consisted of two natural language processing (NLP) based tools for unstructured text data for medications and reasons for medication use. We first checked spelling using a spell-check program trained on publicly available knowledge sources and then applied NLP techniques. We mapped medication names into generic names using vocabulary from publicly available knowledge sources. We used WHO's Anatomical Therapeutic Chemical (ATC) classification system to map generic medication names to medication classes. We processed the reasons for medication with the Lancaster stemmer method and then grouped and mapped to disease classes based on organ systems. Finally, we demonstrated this automation framework on two data sources for Mylagic Encephalomyelitis/ Chronic Fatigue Syndrome (ME/CFS): tertiary-based (nâ€‰=â€‰378) and population-based (nâ€‰=â€‰664) samples. A total of 8681 raw medication records were used for this demonstration. The 1266 distinct medication names (omitting supplements) were condensed to 89 ATC classification system categories. The 1432 distinct raw reasons for medication use were condensed to 65 categories via NLP. Compared to completion of the entire process manually, our automation process reduced the number of the terms requiring manual labor for mapping by 84.4% for medications and 59.4% for reasons for medication use. Additionally, this process improved the precision of the mapped results. Our automation framework demonstrates the usefulness of NLP strategies even when there is no established mapping database. For a less established database (e.g., reasons for medication use), the method is easily modifiable as new knowledge sources for mapping are introduced. The capability to condense large features into interpretable ones will be valuable for subsequent analytical studies involving techniques such as machine learning and data mining.","Chen, Ho, Lin","Chen, Ho, Lin",https://doi.org/10.1186/s12874-020-01131-7,https://doi.org/10.1186/s12874-020-01131-7,2021-08-03
16014.0,pubmed,pubmed,Using the contextual language model BERT for multi-criteria classification of scientific articles,Using the contextual language model BERT for multi-criteria classification of scientific articles,"Finding specific scientific articles in a large collection is an important natural language processing challenge in the biomedical domain. Systematic reviews and interactive article search are the type of downstream applications that benefit from addressing this problem. The task often involves screening articles for a combination of selection criteria. While machine learning was previously used for this purpose, it is not known if different criteria should be modeled together or separately in an ensemble model. The performance impact of the modern contextual language models on the task is also not known. We framed the problem as text classification and conducted experiments to compare ensemble architectures, where the selection criteria were mapped to the components of the ensemble. We proposed a novel cascade ensemble analogous to the step-wise screening process employed in developing the gold standard. We compared performance of the ensembles with a single integrated model, which we refer to as the individual task learner (ITL). We used SciBERT, a variant of BERT pre-trained on scientific articles, and conducted experiments using a manually annotated dataset of ~49Ã‚Â K MEDLINE abstracts, known as Clinical Hedges. The cascade ensemble had significantly higher precision (0.663 vs. 0.388 vs. 0.478 vs. 0.320) and F measure (0.753 vs. 0.553 vs. 0.628 vs. 0.477) than ITL and ensembles using Boolean logic and a feed-forward network. However, ITL had significantly higher recall than the other classifiers (0.965 vs. 0.872 vs. 0.917 vs. 0.944). In fixed high recall studies, ITL achieved 0.509 precision @ 0.970 recall and 0.381 precision @ 0.985 recall on a subset that was studied earlier, and 0.295 precision @ 0.985 recall on the full dataset, all of which were improvements over the previous studies. Pre-trained neural contextual language models (e.g. SciBERT) performed well for screening scientific articles. Performance at high fixed recall makes the single integrated model (ITL) more suitable among the architectures considered here, for systematic reviews. However, high F measure of the cascade ensemble makes it a better approach for interactive search applications. The effectiveness of the cascade ensemble architecture suggests broader applicability beyond this task and the dataset, and the approach is analogous to query optimization in Information Retrieval and query optimization in databases.","Finding specific scientific articles in a large collection is an important natural language processing challenge in the biomedical domain. Systematic reviews and interactive article search are the type of downstream applications that benefit from addressing this problem. The task often involves screening articles for a combination of selection criteria. While machine learning was previously used for this purpose, it is not known if different criteria should be modeled together or separately in an ensemble model. The performance impact of the modern contextual language models on the task is also not known. We framed the problem as text classification and conducted experiments to compare ensemble architectures, where the selection criteria were mapped to the components of the ensemble. We proposed a novel cascade ensemble analogous to the step-wise screening process employed in developing the gold standard. We compared performance of the ensembles with a single integrated model, which we refer to as the individual task learner (ITL). We used SciBERT, a variant of BERT pre-trained on scientific articles, and conducted experiments using a manually annotated dataset of ~49Â K MEDLINE abstracts, known as Clinical Hedges. The cascade ensemble had significantly higher precision (0.663 vs. 0.388 vs. 0.478 vs. 0.320) and F measure (0.753 vs. 0.553 vs. 0.628 vs. 0.477) than ITL and ensembles using Boolean logic and a feed-forward network. However, ITL had significantly higher recall than the other classifiers (0.965 vs. 0.872 vs. 0.917 vs. 0.944). In fixed high recall studies, ITL achieved 0.509 precision @ 0.970 recall and 0.381 precision @ 0.985 recall on a subset that was studied earlier, and 0.295 precision @ 0.985 recall on the full dataset, all of which were improvements over the previous studies. Pre-trained neural contextual language models (e.g. SciBERT) performed well for screening scientific articles. Performance at high fixed recall makes the single integrated model (ITL) more suitable among the architectures considered here, for systematic reviews. However, high F measure of the cascade ensemble makes it a better approach for interactive search applications. The effectiveness of the cascade ensemble architecture suggests broader applicability beyond this task and the dataset, and the approach is analogous to query optimization in Information Retrieval and query optimization in databases.","Ambalavanan, Devarakonda","Ambalavanan, Devarakonda",https://doi.org/10.1016/j.jbi.2020.103578,https://doi.org/10.1016/j.jbi.2020.103578,2021-08-03
16016.0,pubmed,pubmed,Designing an openEHR-Based Pipeline for Extracting and Standardizing Unstructured Clinical Data Using Natural Language Processing,Designing an openEHR-Based Pipeline for Extracting and Standardizing Unstructured Clinical Data Using Natural Language Processing,"Ã¢â‚¬Æ’Merging disparate and heterogeneous datasets from clinical routine in a standardized and semantically enriched format to enable a multiple use of data also means incorporating unstructured data such as medical free texts. Although the extraction of structured data from texts, known as natural language processing (NLP), has been researched at least for the English language extensively, it is not enough to get a structured output in any format. NLP techniques need to be used together with clinical information standards such as openEHR to be able to reuse and exchange still unstructured data sensibly. Ã¢â‚¬Æ’The aim of the study is to automatically extract crucial information from medical free texts and to transform this unstructured clinical data into a standardized and structured representation by designing and implementing an exemplary pipeline for the processing of pediatric medical histories. Ã¢â‚¬Æ’We constructed a pipeline that allows reusing medical free texts such as pediatric medical histories in a structured and standardized way by (1) selecting and modeling appropriate openEHR archetypes as standard clinical information models, (2) defining a German dictionary with crucial text markers serving as expert knowledge base for a NLP pipeline, and (3) creating mapping rules between the NLP output and the archetypes. The approach was evaluated in a first pilot study by using 50 manually annotated medical histories from the pediatric intensive care unit of the Hannover Medical School. Ã¢â‚¬Æ’We successfully reused 24 existing international archetypes to represent the most crucial elements of unstructured pediatric medical histories in a standardized form. The self-developed NLP pipeline was constructed by defining 3.055 text marker entries, 132 text events, 66 regular expressions, and a text corpus consisting of 776 entries for automatic correction of spelling mistakes. A total of 123 mapping rules were implemented to transform the extracted snippets to an openEHR-based representation to be able to store them together with other structured data in an existing openEHR-based data repository. In the first evaluation, the NLP pipeline yielded 97% precision and 94% recall. Ã¢â‚¬Æ’The use of NLP and openEHR archetypes was demonstrated as a viable approach for extracting and representing important information from pediatric medical histories in a structured and semantically enriched format. We designed a promising approach with potential to be generalized, and implemented a prototype that is extensible and reusable for other use cases concerning German medical free texts. In a long term, this will harness unstructured clinical data for further research purposes such as the design of clinical decision support systems. Together with structured data already integrated in openEHR-based representations, we aim at developing an interoperable openEHR-based application that is capable of automatically assessing a patient's risk status based on the patient's medical history at time of admission.","Merging disparate and heterogeneous datasets from clinical routine in a standardized and semantically enriched format to enable a multiple use of data also means incorporating unstructured data such as medical free texts. Although the extraction of structured data from texts, known as natural language processing (NLP), has been researched at least for the English language extensively, it is not enough to get a structured output in any format. NLP techniques need to be used together with clinical information standards such as openEHR to be able to reuse and exchange still unstructured data sensibly. The aim of the study is to automatically extract crucial information from medical free texts and to transform this unstructured clinical data into a standardized and structured representation by designing and implementing an exemplary pipeline for the processing of pediatric medical histories. We constructed a pipeline that allows reusing medical free texts such as pediatric medical histories in a structured and standardized way by (1) selecting and modeling appropriate openEHR archetypes as standard clinical information models, (2) defining a German dictionary with crucial text markers serving as expert knowledge base for a NLP pipeline, and (3) creating mapping rules between the NLP output and the archetypes. The approach was evaluated in a first pilot study by using 50 manually annotated medical histories from the pediatric intensive care unit of the Hannover Medical School. We successfully reused 24 existing international archetypes to represent the most crucial elements of unstructured pediatric medical histories in a standardized form. The self-developed NLP pipeline was constructed by defining 3.055 text marker entries, 132 text events, 66 regular expressions, and a text corpus consisting of 776 entries for automatic correction of spelling mistakes. A total of 123 mapping rules were implemented to transform the extracted snippets to an openEHR-based representation to be able to store them together with other structured data in an existing openEHR-based data repository. In the first evaluation, the NLP pipeline yielded 97% precision and 94% recall. The use of NLP and openEHR archetypes was demonstrated as a viable approach for extracting and representing important information from pediatric medical histories in a structured and semantically enriched format. We designed a promising approach with potential to be generalized, and implemented a prototype that is extensible and reusable for other use cases concerning German medical free texts. In a long term, this will harness unstructured clinical data for further research purposes such as the design of clinical decision support systems. Together with structured data already integrated in openEHR-based representations, we aim at developing an interoperable openEHR-based application that is capable of automatically assessing a patient's risk status based on the patient's medical history at time of admission.","Wulff, Mast, Hassler, Montag, Marschollek, Jack","Wulff, Mast, Hassler, Montag, Marschollek, Jack",https://doi.org/10.1055/s-0040-1716403,https://doi.org/10.1055/s-0040-1716403,2021-08-03
16017.0,pubmed,pubmed,Artificial Intelligence and Its Role in Identifying Esophageal Neoplasia,Artificial Intelligence and Its Role in Identifying Esophageal Neoplasia,"Randomized trials have demonstrated that ablation of dysplastic Barrett's esophagus can reduce the risk of progression to cancer. Endoscopic resection for early stage esophageal adenocarcinoma and squamous cell carcinoma can significantly reduce postoperative morbidity compared to esophagectomy. Unfortunately, current endoscopic surveillance technologies (e.g., high-definition white light, electronic, and dye-based chromoendoscopy) lack sensitivity at identifying subtle areas of dysplasia and cancer. Random biopsies sample only approximately 5% of the esophageal mucosa at risk, and there is poor agreement among pathologists in identifying low-grade dysplasia. Machine-based deep learning medical image and video assessment technologies have progressed significantly in recent years, enabled in large part by advances in computer processing capabilities. In deep learning, sequential layers allow models to transform input data (e.g., pixels for imaging data) into a composite representation that allows for classification and feature identification. Several publications have attempted to use this technology to help identify dysplasia and early esophageal cancer. The aims of this reviews are as follows: (a) discussing limitations in our current strategies to identify esophageal dysplasia and cancer, (b) explaining the concepts behind deep learning and convolutional neural networks using language appropriate for clinicians without an engineering background, (c) systematically reviewing the literature for studies that have used deep learning to identify esophageal neoplasia, and (d) based on the systemic review, outlining strategies on further work necessary before these technologies are ready for &quot;prime-time,&quot; i.e., use in routine clinical care.","Randomized trials have demonstrated that ablation of dysplastic Barrett's esophagus can reduce the risk of progression to cancer. Endoscopic resection for early stage esophageal adenocarcinoma and squamous cell carcinoma can significantly reduce postoperative morbidity compared to esophagectomy. Unfortunately, current endoscopic surveillance technologies (e.g., high-definition white light, electronic, and dye-based chromoendoscopy) lack sensitivity at identifying subtle areas of dysplasia and cancer. Random biopsies sample only approximately 5% of the esophageal mucosa at risk, and there is poor agreement among pathologists in identifying low-grade dysplasia. Machine-based deep learning medical image and video assessment technologies have progressed significantly in recent years, enabled in large part by advances in computer processing capabilities. In deep learning, sequential layers allow models to transform input data (e.g., pixels for imaging data) into a composite representation that allows for classification and feature identification. Several publications have attempted to use this technology to help identify dysplasia and early esophageal cancer. The aims of this reviews are as follows: (a) discussing limitations in our current strategies to identify esophageal dysplasia and cancer, (b) explaining the concepts behind deep learning and convolutional neural networks using language appropriate for clinicians without an engineering background, (c) systematically reviewing the literature for studies that have used deep learning to identify esophageal neoplasia, and (d) based on the systemic review, outlining strategies on further work necessary before these technologies are ready for ""prime-time,"" i.e., use in routine clinical care.","Syed, Doshi, Guleria, Syed, Shah","Syed, Doshi, Guleria, Syed, Shah",https://doi.org/10.1007/s10620-020-06643-2,https://doi.org/10.1007/s10620-020-06643-2,2021-08-03
16018.0,pubmed,pubmed,Effect of Integrating Machine Learning Mortality Estimates With Behavioral Nudges to Clinicians on Serious Illness Conversations Among Patients With Cancer: A Stepped-Wedge Cluster Randomized Clinical Trial,Effect of Integrating Machine Learning Mortality Estimates With Behavioral Nudges to Clinicians on Serious Illness Conversations Among Patients With Cancer: A Stepped-Wedge Cluster Randomized Clinical Trial,"Serious illness conversations (SICs) are structured conversations between clinicians and patients about prognosis, treatment goals, and end-of-life preferences. Interventions that increase the rate of SICs between oncology clinicians and patients may improve goal-concordant care and patient outcomes. To determine the effect of a clinician-directed intervention integrating machine learning mortality predictions with behavioral nudges on motivating clinician-patient SICs. This stepped-wedge cluster randomized clinical trial was conducted across 20 weeks (from June 17 to November 1, 2019) at 9 medical oncology clinics (8 subspecialty oncology and 1 general oncology clinics) within a large academic health system in Pennsylvania. Clinicians at the 2 smallest subspecialty clinics were grouped together, resulting in 8 clinic groups randomly assigned to the 4 intervention wedge periods. Included participants in the intention-to-treat analyses were 78 oncology clinicians who received SIC training and their patients (NÃ¢â‚¬â€°=Ã¢â‚¬â€°14Ã¢â‚¬Â¯607) who had an outpatient oncology encounter during the study period. (1) Weekly emails to oncology clinicians with SIC performance feedback and peer comparisons; (2) a list of up to 6 high-risk patients (Ã¢â€°Â¥10% predicted risk of 180-day mortality) scheduled for the next week, estimated using a validated machine learning algorithm; and (3) opt-out text message prompts to clinicians on the patient's appointment day to consider an SIC. Clinicians in the control group received usual care consisting of weekly emails with cumulative SIC performance. Percentage of patient encounters with an SIC in the intervention group vs the usual care (control) group. The sample consisted of 78 clinicians and 14Ã¢â‚¬Â¯607 patients. The mean (SD) age of patients was 61.9 (14.2) years, 53.7% were female, and 70.4% were White. For all encounters, SICs were conducted among 1.3% in the control group and 4.6% in the intervention group, a significant difference (adjusted difference in percentage points, 3.3; 95% CI, 2.3-4.5; PÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°.001). Among 4124 high-risk patient encounters, SICs were conducted among 3.6% in the control group and 15.2% in the intervention group, a significant difference (adjusted difference in percentage points, 11.6; 95% CI, 8.2-12.5; PÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°.001). In this stepped-wedge cluster randomized clinical trial, an intervention that delivered machine learning mortality predictions with behavioral nudges to oncology clinicians significantly increased the rate of SICs among all patients and among patients with high mortality risk who were targeted by the intervention. Behavioral nudges combined with machine learning mortality predictions can positively influence clinician behavior and may be applied more broadly to improve care near the end of life. ClinicalTrials.gov Identifier: NCT03984773.","Serious illness conversations (SICs) are structured conversations between clinicians and patients about prognosis, treatment goals, and end-of-life preferences. Interventions that increase the rate of SICs between oncology clinicians and patients may improve goal-concordant care and patient outcomes. To determine the effect of a clinician-directed intervention integrating machine learning mortality predictions with behavioral nudges on motivating clinician-patient SICs. This stepped-wedge cluster randomized clinical trial was conducted across 20 weeks (from June 17 to November 1, 2019) at 9 medical oncology clinics (8 subspecialty oncology and 1 general oncology clinics) within a large academic health system in Pennsylvania. Clinicians at the 2 smallest subspecialty clinics were grouped together, resulting in 8 clinic groups randomly assigned to the 4 intervention wedge periods. Included participants in the intention-to-treat analyses were 78 oncology clinicians who received SIC training and their patients (Nâ€‰=â€‰14â€¯607) who had an outpatient oncology encounter during the study period. (1) Weekly emails to oncology clinicians with SIC performance feedback and peer comparisons; (2) a list of up to 6 high-risk patients (â‰¥10% predicted risk of 180-day mortality) scheduled for the next week, estimated using a validated machine learning algorithm; and (3) opt-out text message prompts to clinicians on the patient's appointment day to consider an SIC. Clinicians in the control group received usual care consisting of weekly emails with cumulative SIC performance. Percentage of patient encounters with an SIC in the intervention group vs the usual care (control) group. The sample consisted of 78 clinicians and 14â€¯607 patients. The mean (SD) age of patients was 61.9 (14.2) years, 53.7% were female, and 70.4% were White. For all encounters, SICs were conducted among 1.3% in the control group and 4.6% in the intervention group, a significant difference (adjusted difference in percentage points, 3.3; 95% CI, 2.3-4.5; Pâ€‰&lt;â€‰.001). Among 4124 high-risk patient encounters, SICs were conducted among 3.6% in the control group and 15.2% in the intervention group, a significant difference (adjusted difference in percentage points, 11.6; 95% CI, 8.2-12.5; Pâ€‰&lt;â€‰.001). In this stepped-wedge cluster randomized clinical trial, an intervention that delivered machine learning mortality predictions with behavioral nudges to oncology clinicians significantly increased the rate of SICs among all patients and among patients with high mortality risk who were targeted by the intervention. Behavioral nudges combined with machine learning mortality predictions can positively influence clinician behavior and may be applied more broadly to improve care near the end of life. ClinicalTrials.gov Identifier: NCT03984773.","Manz, Parikh, Small, Evans, Chivers, Regli, Hanson, Bekelman, Rareshide, O'Connor, Schuchter, Shulman, Patel","Manz, Parikh, Small, Evans, Chivers, Regli, Hanson, Bekelman, Rareshide, O'Connor, Schuchter, Shulman, Patel",https://doi.org/10.1001/jamaoncol.2020.4759,https://doi.org/10.1001/jamaoncol.2020.4759,2021-08-03
16019.0,pubmed,pubmed,Modelling COVID 19 in the Basque Country from introduction to control measure response,Modelling COVID 19 in the Basque Country from introduction to control measure response,"In March 2020, a multidisciplinary task force (so-called Basque Modelling Task Force, BMTF) was created to assist the Basque health managers and Government during the COVID-19 responses. BMTF is a modelling team, working on different approaches, including stochastic processes, statistical methods and artificial intelligence. Here we describe the efforts and challenges to develop a flexible modeling framework able to describe the dynamics observed for the tested positive cases, including the modelling development steps. The results obtained by a new stochastic SHARUCD model framework are presented. Our models differentiate mild and asymptomatic from severe infections prone to be hospitalized and were able to predict the course of the epidemic, providing important projections on the national health system's necessities during the increased population demand on hospital admissions. Short and longer-term predictions were tested with good results adjusted to the available epidemiological data. We have shown that the partial lockdown measures were effective and enough to slow down disease transmission in the Basque Country. The growth rate [Formula: see text] was calculated from the model and from the data and the implications for the reproduction ratio r are shown. The analysis of the growth rates from the data led to improved model versions describing after the exponential phase also the new information obtained during the phase of response to the control measures. This framework is now being used to monitor disease transmission while the country lockdown was gradually lifted, with insights to specific programs for a general policy of &quot;social distancing&quot; and home quarantining.","In March 2020, a multidisciplinary task force (so-called Basque Modelling Task Force, BMTF) was created to assist the Basque health managers and Government during the COVID-19 responses. BMTF is a modelling team, working on different approaches, including stochastic processes, statistical methods and artificial intelligence. Here we describe the efforts and challenges to develop a flexible modeling framework able to describe the dynamics observed for the tested positive cases, including the modelling development steps. The results obtained by a new stochastic SHARUCD model framework are presented. Our models differentiate mild and asymptomatic from severe infections prone to be hospitalized and were able to predict the course of the epidemic, providing important projections on the national health system's necessities during the increased population demand on hospital admissions. Short and longer-term predictions were tested with good results adjusted to the available epidemiological data. We have shown that the partial lockdown measures were effective and enough to slow down disease transmission in the Basque Country. The growth rate [Formula: see text] was calculated from the model and from the data and the implications for the reproduction ratio r are shown. The analysis of the growth rates from the data led to improved model versions describing after the exponential phase also the new information obtained during the phase of response to the control measures. This framework is now being used to monitor disease transmission while the country lockdown was gradually lifted, with insights to specific programs for a general policy of ""social distancing"" and home quarantining.","Aguiar, Ortuondo, Bidaurrazaga Van-Dierdonck, Mar, Stollenwerk","Aguiar, Ortuondo, Bidaurrazaga Van-Dierdonck, Mar, Stollenwerk",https://doi.org/10.1038/s41598-020-74386-1,https://doi.org/10.1038/s41598-020-74386-1,2021-08-03
16022.0,pubmed,pubmed,Prevalence and associated factors of urinary incontinence in women living in China: a literature review,Prevalence and associated factors of urinary incontinence in women living in China: a literature review,"This review of studies on urinary incontinence (UI) was focused primarily on UI prevalence rates and associated factors across the adult lifecourse of Chinese women. UI is a urologic symptom that can have a significant impact on women's physical and mental health and quality of life. In addition, women with UI may experience socioeconomic burdens due to UI's effect on their ability to work and function in society. Although researchers from many countries have reported prevalence rates and associated factors for UI, little is known about the prevalence of UI in China's large female population. Language may act as a barrier to the inclusion of published studies in English-language journals. To overcome this barrier and to add to the global knowledge base about UI in women, the authors reviewed and discussed findings from epidemiological studies published in China and in Chinese language. The authors retrieved research studies from the Wanfang database using the following search terms: &quot;Subject: (Female)Ã¢â‚¬â€°Ãƒâ€”Ã¢â‚¬â€°Subject: (Urinary incontinence)Ã¢â‚¬â€°Ãƒâ€”Ã¢â‚¬â€°Subject: (Prevalence)Ã¢â‚¬â€°Ãƒâ€”Ã¢â‚¬â€°Date: 2013 to 2019&quot;. Searches employed the China National Knowledge Infrastructure Database, VIP Database for Chinese Technical Periodicals and China Biology Medicine Database. The authors also used PubMed to search English-language studies published in Chinese journals on UI in Chinese women. This literature review includes 48 articles published between January 2013 and December 2019. The overall UI prevalence rates reported in adult Chinese women ranged from 8.7 to 69.8%, representing 43-349 million women, respectively. For women aged 17-40Ã‚Â years, 41-59Ã‚Â years, and 60Ã‚Â years and older, prevalence rates ranged from 2.6-30.0, 8.7-47.7, to 16.9-61.6%, respectively. Significant associated factors for overall UI included age, body mass index, constipation, parity, and menopause. Despite the 17-40 age range being peak reproductive years, the literature revealed little focus on UI prevalence rates. For women aged 41-59Ã‚Â years, the main associated factors included those related to pregnancy and gynecologic diseases. For women 60Ã‚Â years and older, chronic diseases represented most of the associated factors. About 43-349 million Chinese women may experience UI. Many of the identified associated factors could be mitigated to reduce UI incidence and prevalence rates. Little is known about the prevalence rates and associated factors for UI among young (aged 17-40) Chinese women. Future research should investigate UI in young women to improve bladder health across their lifecourse.","This review of studies on urinary incontinence (UI) was focused primarily on UI prevalence rates and associated factors across the adult lifecourse of Chinese women. UI is a urologic symptom that can have a significant impact on women's physical and mental health and quality of life. In addition, women with UI may experience socioeconomic burdens due to UI's effect on their ability to work and function in society. Although researchers from many countries have reported prevalence rates and associated factors for UI, little is known about the prevalence of UI in China's large female population. Language may act as a barrier to the inclusion of published studies in English-language journals. To overcome this barrier and to add to the global knowledge base about UI in women, the authors reviewed and discussed findings from epidemiological studies published in China and in Chinese language. The authors retrieved research studies from the Wanfang database using the following search terms: ""Subject: (Female)â€‰Ã—â€‰Subject: (Urinary incontinence)â€‰Ã—â€‰Subject: (Prevalence)â€‰Ã—â€‰Date: 2013 to 2019"". Searches employed the China National Knowledge Infrastructure Database, VIP Database for Chinese Technical Periodicals and China Biology Medicine Database. The authors also used PubMed to search English-language studies published in Chinese journals on UI in Chinese women. This literature review includes 48 articles published between January 2013 and December 2019. The overall UI prevalence rates reported in adult Chinese women ranged from 8.7 to 69.8%, representing 43-349 million women, respectively. For women aged 17-40Â years, 41-59Â years, and 60Â years and older, prevalence rates ranged from 2.6-30.0, 8.7-47.7, to 16.9-61.6%, respectively. Significant associated factors for overall UI included age, body mass index, constipation, parity, and menopause. Despite the 17-40 age range being peak reproductive years, the literature revealed little focus on UI prevalence rates. For women aged 41-59Â years, the main associated factors included those related to pregnancy and gynecologic diseases. For women 60Â years and older, chronic diseases represented most of the associated factors. About 43-349 million Chinese women may experience UI. Many of the identified associated factors could be mitigated to reduce UI incidence and prevalence rates. Little is known about the prevalence rates and associated factors for UI among young (aged 17-40) Chinese women. Future research should investigate UI in young women to improve bladder health across their lifecourse.","Xue, Palmer, Zhou","Xue, Palmer, Zhou",https://doi.org/10.1186/s12894-020-00735-x,https://doi.org/10.1186/s12894-020-00735-x,2021-08-03
16025.0,pubmed,pubmed,A preliminary investigation of radiomics differences between ruptured and unruptured intracranial aneurysms,A preliminary investigation of radiomics differences between ruptured and unruptured intracranial aneurysms,"Prediction of intracranial aneurysm rupture is important in the management of unruptured aneurysms. The application of radiomics in predicting aneurysm rupture remained largely unexplored. This study aims to evaluate the radiomics differences between ruptured and unruptured aneurysms and explore its potential use in predicting aneurysm rupture. One hundred twenty-two aneurysms were included in the study (93 unruptured). Morphological and radiomics features were extracted for each case. Statistical analysis was performed to identify significant features which were incorporated into prediction models constructed with a machine learning algorithm. To investigate the usefulness of radiomics features, three models were constructed and compared. The baseline model A was constructed with morphological features, while model B was constructed with addition of radiomics shape features and model C with more radiomics features. Multivariate analysis was performed for the ten most important variables in model C to identify independent risk factors. A simplified model based on independent risk factors was constructed for clinical use. Five morphological features and 89 radiomics features were significantly associated with rupture. Model A, model B, and model C achieved the area under the receiver operating characteristic curve of 0.767, 0.807, and 0.879, respectively. Model C was significantly better than model A and model B (pÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°0.001). Multivariate analysis identified two radiomics features which were used to construct the simplified model showing an AUROC of 0.876. Radiomics signatures were different between ruptured and unruptured aneurysms. The use of radiomics features, especially texture features, may significantly improve rupture prediction performance. Ã¢â‚¬Â¢ Significant radiomics differences exist between ruptured and unruptured intracranial aneurysms. Ã¢â‚¬Â¢ Radiomics shape features can significantly improve rupture prediction performance over conventional morphology-based prediction model. The inclusion of histogram and texture radiomics features can further improve the performance. Ã¢â‚¬Â¢ A simplified model with two variables achieved a similar level of performance as the more complex ones. Our prediction model can serve as a promising tool for the risk management of intracranial aneurysms.","Prediction of intracranial aneurysm rupture is important in the management of unruptured aneurysms. The application of radiomics in predicting aneurysm rupture remained largely unexplored. This study aims to evaluate the radiomics differences between ruptured and unruptured aneurysms and explore its potential use in predicting aneurysm rupture. One hundred twenty-two aneurysms were included in the study (93 unruptured). Morphological and radiomics features were extracted for each case. Statistical analysis was performed to identify significant features which were incorporated into prediction models constructed with a machine learning algorithm. To investigate the usefulness of radiomics features, three models were constructed and compared. The baseline model A was constructed with morphological features, while model B was constructed with addition of radiomics shape features and model C with more radiomics features. Multivariate analysis was performed for the ten most important variables in model C to identify independent risk factors. A simplified model based on independent risk factors was constructed for clinical use. Five morphological features and 89 radiomics features were significantly associated with rupture. Model A, model B, and model C achieved the area under the receiver operating characteristic curve of 0.767, 0.807, and 0.879, respectively. Model C was significantly better than model A and model B (pâ€‰&lt;â€‰0.001). Multivariate analysis identified two radiomics features which were used to construct the simplified model showing an AUROC of 0.876. Radiomics signatures were different between ruptured and unruptured aneurysms. The use of radiomics features, especially texture features, may significantly improve rupture prediction performance. â€¢ Significant radiomics differences exist between ruptured and unruptured intracranial aneurysms. â€¢ Radiomics shape features can significantly improve rupture prediction performance over conventional morphology-based prediction model. The inclusion of histogram and texture radiomics features can further improve the performance. â€¢ A simplified model with two variables achieved a similar level of performance as the more complex ones. Our prediction model can serve as a promising tool for the risk management of intracranial aneurysms.","Ou, Chong, Duan, Zhang, Morgan, Qian","Ou, Chong, Duan, Zhang, Morgan, Qian",https://doi.org/10.1007/s00330-020-07325-3,https://doi.org/10.1007/s00330-020-07325-3,2021-08-03
16029.0,pubmed,pubmed,A regression framework to head-circumference delineation from US fetal images,A regression framework to head-circumference delineation from US fetal images,"Measuring head-circumference (HC) length from ultrasound (US) images is a crucial clinical task to assess fetus growth. To lower intra- and inter-operator variability in HC length measuring, several computer-assisted solutions have been proposed in the years. Recently, a large number of deep-learning approaches is addressing the problem of HC delineation through the segmentation of the whole fetal head via convolutional neural networks (CNNs). Since the task is a edge-delineation problem, we propose a different strategy based on regression CNNs. The proposed framework consists of a region-proposal CNN for head localization and centering, and a regression CNN for accurately delineate the HC. The first CNN is trained exploiting transfer learning, while we propose a training strategy for the regression CNN based on distance fields. The framework was tested on the HC18 Challenge dataset, which consists of 999 training and 335 testing images. A mean absolute difference of 1.90 (Ã‚Â Ã‚Â±Ã‚ 1.76) mm and a Dice similarity coefficient of 97.75 (Ã‚Â Ã‚Â±Ã‚ 1.32) % were achieved, overcoming approaches in the literature. The experimental results showed the effectiveness of the proposed framework, proving its potential in supporting clinicians during the clinical practice.","Measuring head-circumference (HC) length from ultrasound (US) images is a crucial clinical task to assess fetus growth. To lower intra- and inter-operator variability in HC length measuring, several computer-assisted solutions have been proposed in the years. Recently, a large number of deep-learning approaches is addressing the problem of HC delineation through the segmentation of the whole fetal head via convolutional neural networks (CNNs). Since the task is a edge-delineation problem, we propose a different strategy based on regression CNNs. The proposed framework consists of a region-proposal CNN for head localization and centering, and a regression CNN for accurately delineate the HC. The first CNN is trained exploiting transfer learning, while we propose a training strategy for the regression CNN based on distance fields. The framework was tested on the HC18 Challenge dataset, which consists of 999 training and 335 testing images. A mean absolute difference of 1.90 (Â Â±Â 1.76) mm and a Dice similarity coefficient of 97.75 (Â Â±Â 1.32) % were achieved, overcoming approaches in the literature. The experimental results showed the effectiveness of the proposed framework, proving its potential in supporting clinicians during the clinical practice.","Fiorentino, Moccia, Capparuccini, Giamberini, Frontoni","Fiorentino, Moccia, Capparuccini, Giamberini, Frontoni",https://doi.org/10.1016/j.cmpb.2020.105771,https://doi.org/10.1016/j.cmpb.2020.105771,2021-08-03
16031.0,pubmed,pubmed,"StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics","StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics","In machine learning (ML), ensemble methods-such as bagging, boosting, and stacking-are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called &quot;stacked generalization&quot;) is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work, we present a knowledge generation model, which supports ensemble learning with the use of visualization, and a visual analytics system for stacked generalization. Our system, StackGenVis, assists users in dynamically adapting performance metrics, managing data instances, selecting the most important features for a given data set, choosing a set of top-performant and diverse algorithms, and measuring the predictive performance. In consequence, our proposed tool helps users to decide between distinct models and to reduce the complexity of the resulting stack by removing overpromising and underperforming models. The applicability and effectiveness of StackGenVis are demonstrated with two use cases: a real-world healthcare data set and a collection of data related to sentiment/stance detection in texts. Finally, the tool has been evaluated through interviews with three ML experts.","In machine learning (ML), ensemble methods-such as bagging, boosting, and stacking-are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called ""stacked generalization"") is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work, we present a knowledge generation model, which supports ensemble learning with the use of visualization, and a visual analytics system for stacked generalization. Our system, StackGenVis, assists users in dynamically adapting performance metrics, managing data instances, selecting the most important features for a given data set, choosing a set of top-performant and diverse algorithms, and measuring the predictive performance. In consequence, our proposed tool helps users to decide between distinct models and to reduce the complexity of the resulting stack by removing overpromising and underperforming models. The applicability and effectiveness of StackGenVis are demonstrated with two use cases: a real-world healthcare data set and a collection of data related to sentiment/stance detection in texts. Finally, the tool has been evaluated through interviews with three ML experts.","Chatzimparmpas, Martins, Kucher, Kerren","Chatzimparmpas, Martins, Kucher, Kerren",https://doi.org/10.1109/TVCG.2020.3030352,https://doi.org/10.1109/TVCG.2020.3030352,2021-08-03
16032.0,pubmed,pubmed,Acute ischaemic stroke management: Concepts and Controversies A narrative review,Acute ischemic stroke management: concepts and controversiesA narrative review,"Amongst the 25.7 million survivors and 6.5 million deaths from stroke between 1990 and 2013, ischemic strokes accounted for approximately 70% and 50% of the cases, respectively. With patients still suffering from complications and stroke recurrence, more questions have been raised as to how we can better improve patient management. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement and Newcastle-Ottawa Scale (NOS) were adopted to ensure a comprehensive inclusion of quality literature from various sources. PubMed and Embase were searched for evidence on thrombolysis, mechanical thrombectomy, artificial intelligence (AI), antiplatelet therapy, anticoagulation and hypertension management. The directions of future research in these areas are dependent on the current level of validation. Endovascular therapy and applications of AI are relatively new compared to the other areas discussed in this review. As such, it is important for future studies to focus on validating their efficacy. As for thrombolysis, antiplatelet and anticoagulation therapy, their efficacy has been well-established and future research efforts should be directed towards adjusting its use according to patient specific factors, starting with factors with the most clinical relevance and prevalence.","Amongst the 25.7 million survivors and 6.5 million deaths from stroke between 1990 and 2013, ischemic strokes accounted for approximately 70% and 50% of the cases, respectively. With patients still suffering from complications and stroke recurrence, more questions have been raised as to how we can better improve patient management. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement and Newcastle-Ottawa Scale (NOS) were adopted to ensure a comprehensive inclusion of quality literature from various sources. PubMed and Embase were searched for evidence on thrombolysis, mechanical thrombectomy, artificial intelligence (AI), antiplatelet therapy, anticoagulation and hypertension management. The directions of future research in these areas are dependent on the current level of validation. Endovascular therapy and applications of AI are relatively new compared to the other areas discussed in this review. As such, future studies need to focus on validating their efficacy. As for thrombolysis, antiplatelet and anticoagulation therapy, their efficacy has been well-established and future research efforts should be directed toward adjusting its use according to patient-specific factors, starting with factors with the most clinical relevance and prevalence.","Li, Jesuthasan, Kui, Davies, Tse, Lip","Li, Jesuthasan, Kui, Davies, Tse, Lip",https://doi.org/10.1080/14737175.2021.1836963,https://doi.org/10.1080/14737175.2021.1836963,2021-08-03
16033.0,pubmed,pubmed,CT Quantification and Machine-learning Models for Assessment of Disease Severity and Prognosis of COVID-19 Patients,CT Quantification and Machine-learning Models for Assessment of Disease Severity and Prognosis of COVID-19 Patients,"This study was to investigate the CT quantification of COVID-19 pneumonia and its impacts on the assessment of disease severity and the prediction of clinical outcomes in the management of COVID-19 patients. Ninety-nine COVID-19 patients who were confirmed by positive nucleic acid test (NAT) of RT-PCR and hospitalized from January 19, 2020 to February 19, 2020 were collected for this retrospective study. All patients underwent arterial blood gas test, routine blood test, chest CT examination, and physical examination on admission. In addition, follow-up clinical data including the disease severity, clinical treatment, and clinical outcomes were collected for each patient. Lung volume, lesion volume, nonlesion lung volume (NLLV) (lung volume - lesion volume), and fraction of nonlesion lung volume (%NLLV) (nonlesion lung volume / lung volume) were quantified in CT images by using two U-Net models trained for segmentation of lung and COVID-19 lesions in CT images. Furthermore, we calculated 20 histogram textures for lesions volume and NLLV, respectively. To investigate the validity of CT quantification in the management of COVID-19, we built random forest (RF) models for the purpose of classification and regression to assess the disease severity (Moderate, Severe, and Critical) and to predict the need and length of ICU stay, the duration of oxygen inhalation, hospitalization, sputum NAT-positive, and patient prognosis. The performance of RF classifiers was evaluated using the area under the receiver operating characteristic curves (AUC) and that of RF regressors using the root-mean-square error. Patients were classified into three groups of disease severity: moderate (nÃ¢â‚¬Â¯=Ã¢â‚¬Â¯25), severe (nÃ¢â‚¬Â¯=Ã¢â‚¬Â¯47) and critical (nÃ¢â‚¬Â¯=Ã¢â‚¬Â¯27), according to the clinical staging. Of which, a total of 32 patients, 1 (1/25) moderate, 6 (6/47) severe, and 25 critical (25/27), respectively, were admitted to ICU. The median values of ICU stay were 0, 0, and 12 days, the duration of oxygen inhalation 10, 15, and 28 days, the hospitalization 12, 16, and 28 days, and the sputum NAT-positive 8, 9, and 13 days, in three severity groups, respectively. The clinical outcomes were complete recovery (nÃ¢â‚¬Â¯=Ã¢â‚¬Â¯3), partial recovery with residual pulmonary damage (nÃ¢â‚¬Â¯=Ã¢â‚¬Â¯80), prolonged recovery (nÃ¢â‚¬Â¯=Ã¢â‚¬Â¯15), and death (nÃ¢â‚¬Â¯=Ã¢â‚¬Â¯1). The %NLLV in three severity groups were 92.18 Ã‚Â± 9.89%, 82.94 Ã‚Â± 16.49%, and 66.19 Ã‚Â± 24.15% with p value &lt;0.05 among each two groups. The AUCs of RF classifiers using hybrid models were 0.927 and 0.929 in classification of moderate vs (severeÃ¢â‚¬Â¯+Ã¢â‚¬Â¯critical), and severe vs critical, respectively, which were significantly higher than either radiomics models or clinical models (p &lt; 0.05). The root-mean-square errors of RF regressors were 0.88 weeks for prediction of duration of hospitalization (mean: 2.60 Ã‚Â± 1.01 weeks), 0.92 weeks for duration of oxygen inhalation (mean: 2.44 Ã‚Â± 1.08 weeks), 0.90 weeks for duration of sputum NAT-positive (mean: 1.59 Ã‚Â± 0.98 weeks), and 0.69 weeks for stay of ICU (mean: 1.32 Ã‚Â± 0.67 weeks), respectively. The AUCs for prediction of ICU treatment and prognosis (partial recovery vs prolonged recovery) were 0.945 and 0.960, respectively. CT quantification and machine-learning models show great potentials for assisting decision-making in the management of COVID-19 patients by assessing disease severity and predicting clinical outcomes.","This study was to investigate the CT quantification of COVID-19 pneumonia and its impacts on the assessment of disease severity and the prediction of clinical outcomes in the management of COVID-19 patients. Ninety-nine COVID-19 patients who were confirmed by positive nucleic acid test (NAT) of RT-PCR and hospitalized from January 19, 2020 to February 19, 2020 were collected for this retrospective study. All patients underwent arterial blood gas test, routine blood test, chest CT examination, and physical examination on admission. In addition, follow-up clinical data including the disease severity, clinical treatment, and clinical outcomes were collected for each patient. Lung volume, lesion volume, nonlesion lung volume (NLLV) (lung volume - lesion volume), and fraction of nonlesion lung volume (%NLLV) (nonlesion lung volume / lung volume) were quantified in CT images by using two U-Net models trained for segmentation of lung and COVID-19 lesions in CT images. Furthermore, we calculated 20 histogram textures for lesions volume and NLLV, respectively. To investigate the validity of CT quantification in the management of COVID-19, we built random forest (RF) models for the purpose of classification and regression to assess the disease severity (Moderate, Severe, and Critical) and to predict the need and length of ICU stay, the duration of oxygen inhalation, hospitalization, sputum NAT-positive, and patient prognosis. The performance of RF classifiers was evaluated using the area under the receiver operating characteristic curves (AUC) and that of RF regressors using the root-mean-square error. Patients were classified into three groups of disease severity: moderate (nâ€¯=â€¯25), severe (nâ€¯=â€¯47) and critical (nâ€¯=â€¯27), according to the clinical staging. Of which, a total of 32 patients, 1 (1/25) moderate, 6 (6/47) severe, and 25 critical (25/27), respectively, were admitted to ICU. The median values of ICU stay were 0, 0, and 12 days, the duration of oxygen inhalation 10, 15, and 28 days, the hospitalization 12, 16, and 28 days, and the sputum NAT-positive 8, 9, and 13 days, in three severity groups, respectively. The clinical outcomes were complete recovery (nâ€¯=â€¯3), partial recovery with residual pulmonary damage (nâ€¯=â€¯80), prolonged recovery (nâ€¯=â€¯15), and death (nâ€¯=â€¯1). The %NLLV in three severity groups were 92.18 Â± 9.89%, 82.94 Â± 16.49%, and 66.19 Â± 24.15% with p value &lt;0.05 among each two groups. The AUCs of RF classifiers using hybrid models were 0.927 and 0.929 in classification of moderate vs (severeâ€¯+â€¯critical), and severe vs critical, respectively, which were significantly higher than either radiomics models or clinical models (p &lt; 0.05). The root-mean-square errors of RF regressors were 0.88 weeks for prediction of duration of hospitalization (mean: 2.60 Â± 1.01 weeks), 0.92 weeks for duration of oxygen inhalation (mean: 2.44 Â± 1.08 weeks), 0.90 weeks for duration of sputum NAT-positive (mean: 1.59 Â± 0.98 weeks), and 0.69 weeks for stay of ICU (mean: 1.32 Â± 0.67 weeks), respectively. The AUCs for prediction of ICU treatment and prognosis (partial recovery vs prolonged recovery) were 0.945 and 0.960, respectively. CT quantification and machine-learning models show great potentials for assisting decision-making in the management of COVID-19 patients by assessing disease severity and predicting clinical outcomes.","Cai, Liu, Xue, Luo, Wang, Shen, Fang, Sheng, Chen, Liang","Cai, Liu, Xue, Luo, Wang, Shen, Fang, Sheng, Chen, Liang",https://doi.org/10.1016/j.acra.2020.09.004,https://doi.org/10.1016/j.acra.2020.09.004,2021-08-03
16034.0,pubmed,pubmed,Disclosing Adverse Events in Clinical Practice: The Delicate Act of Being Open,Disclosing Adverse Events in Clinical Practice: The Delicate Act of Being Open,"Practicing a &quot;safe&quot; disclosure of adverse events remains challenging for healthcare professionals. In addition, knowledge on <i>how</i> to deliver a disclosure is still limited. This review focuses on how disclosure communication may be practiced based on the perspectives of patients and healthcare professionals. Empirical studies conducted between September 2008 and October 2019 were included from the databases PubMed, Web of Science and Psychinfo. After full text analysis and quality appraisal this scoping review included a total of 23 studies out of 2537 studies. As a first step, the needs of patients and the challenges of healthcare professionals with the practice of providing an effective disclosure were extracted from the empirical literature. Based on these findings, the review demonstrates that specific disclosure communication strategies on the level of interpersonal skills, organization, and supportive factors may facilitate healthcare professionals to provide optimal disclosure of adverse events. These may be relevant to provide patients with a tailored approach that accompanies their preferences for information and recognition. In conclusion, healthcare professionals may need training in interpersonal (verbal and nonverbal) communication skills. Furthermore, it is important to develop an open (organizational) culture that supports the communication of adverse events and disclosure as a standard practice.","Practicing a ""safe"" disclosure of adverse events remains challenging for healthcare professionals. In addition, knowledge on <i>how</i> to deliver a disclosure is still limited. This review focuses on how disclosure communication may be practiced based on the perspectives of patients and healthcare professionals. Empirical studies conducted between September 2008 and October 2019 were included from the databases PubMed, Web of Science and Psychinfo. After full text analysis and quality appraisal this scoping review included a total of 23 studies out of 2537 studies. As a first step, the needs of patients and the challenges of healthcare professionals with the practice of providing an effective disclosure were extracted from the empirical literature. Based on these findings, the review demonstrates that specific disclosure communication strategies on the level of interpersonal skills, organization, and supportive factors may facilitate healthcare professionals to provide optimal disclosure of adverse events. These may be relevant to provide patients with a tailored approach that accompanies their preferences for information and recognition. In conclusion, healthcare professionals may need training in interpersonal (verbal and nonverbal) communication skills. Furthermore, it is important to develop an open (organizational) culture that supports the communication of adverse events and disclosure as a standard practice.","Myren, de Hullu, Bastiaans, Koksma, Hermens, Zusterzeel","Myren, de Hullu, Bastiaans, Koksma, Hermens, Zusterzeel",https://doi.org/10.1080/10410236.2020.1830550,https://doi.org/10.1080/10410236.2020.1830550,2021-08-03
16035.0,pubmed,pubmed,Applications of Artificial Intelligence in Battling Against Covid-19: A Literature Review,Applications of artificial intelligence in battling against covid-19: A literature review,"Colloquially known as coronavirus, the Severe Acute Respiratory Syndrome CoronaVirus 2 (SARS-CoV-2), that causes CoronaVirus Disease 2019 (COVID-19), has become a matter of grave concern for every country around the world. The rapid growth of the pandemic has wreaked havoc and prompted the need for immediate reactions to curb the effects. To manage the problems, many research in a variety of area of science have started studying the issue. Artificial Intelligence is among the area of science that has found great applications in tackling the problem in many aspects. Here, we perform an overview on the applications of AI in a variety of fields including diagnosis of the disease via different types of tests and symptoms, monitoring patients, identifying severity of a patient, processing covid-19 related imaging tests, epidemiology, pharmaceutical studies, etc. The aim of this paper is to perform a comprehensive survey on the applications of AI in battling against the difficulties the outbreak has caused. Thus we cover every way that AI approaches have been employed and to cover all the research until the writing of this paper. We try organize the works in a way that overall picture is comprehensible. Such a picture, although full of details, is very helpful in understand where AI sits in current pandemonium. We also tried to conclude the paper with ideas on how the problems can be tackled in a better way and provide some suggestions for future works.","Colloquially known as coronavirus, the Severe Acute Respiratory Syndrome CoronaVirus 2 (SARS-CoV-2), that causes CoronaVirus Disease 2019 (COVID-19), has become a matter of grave concern for every country around the world. The rapid growth of the pandemic has wreaked havoc and prompted the need for immediate reactions to curb the effects. To manage the problems, many research in a variety of area of science have started studying the issue. Artificial Intelligence is among the area of science that has found great applications in tackling the problem in many aspects. Here, we perform an overview on the applications of AI in a variety of fields including diagnosis of the disease via different types of tests and symptoms, monitoring patients, identifying severity of a patient, processing covid-19 related imaging tests, epidemiology, pharmaceutical studies, etc. The aim of this paper is to perform a comprehensive survey on the applications of AI in battling against the difficulties the outbreak has caused. Thus we cover every way that AI approaches have been employed and to cover all the research until the writing of this paper. We try organize the works in a way that overall picture is comprehensible. Such a picture, although full of details, is very helpful in understand where AI sits in current pandemonium. We also tried to conclude the paper with ideas on how the problems can be tackled in a better way and provide some suggestions for future works.",Tayarani-N,Tayarani N,https://doi.org/10.1016/j.chaos.2020.110338,https://doi.org/10.1016/j.chaos.2020.110338,2021-08-03
16039.0,pubmed,pubmed,The future of Cochrane Neonatal,The future of Cochrane Neonatal,"Cochrane Neonatal was first established in 1993, as one of the original review groups of the Cochrane Collaboration. In fact, the origins of Cochrane Neonatal precede the establishment of the collaboration. In the 1980's, the National Perinatal Epidemiology Unit at Oxford, led by Dr. Iain Chalmers, established the &quot;Oxford Database of Perinatal Trials&quot; (ODPT), a register of virtually all randomized controlled trials in perinatal medicine to provide a resource for reviews of the safety and efficacy of interventions used in perinatal care and to foster cooperative and coordinated research efforts in the perinatal field [1]. An effort that was clearly ahead of its time, ODPT comprised four main elements: a register of published reports of trials; a register of unpublished trials; a register of ongoing and planned trials; and data derived from pooled overviews (meta-analyses) of trials. This core effort grew into the creation of the seminal books, &quot;Effective Care in Pregnancy and Childbirth&quot; as well as &quot;Effective Care of the Newborn Infant&quot; [2,3]. As these efforts in perinatal medicine grew, Iain Chalmers thought well beyond perinatal medicine into the creation of a worldwide collaboration that became Cochrane [4]. The mission of the Cochrane Collaboration is to promote evidence-informed health decision-making by producing high-quality, relevant, accessible systematic reviews and other synthesized research evidence (www.cochrane.org). Cochrane Neonatal has continued to be one of the most productive review groups, publishing between 25 tpo to 40 new or updated systematic reviews each year. The impact factor has been steadily increasing over four years and now rivals most of the elite journals in pediatric medicine. Cochrane Neonatal has been a worldwide effort. Currently, there are 404 reviews involving 1206 authors from 52 countries. What has Cochrane done for babies? Reviews from Cochrane Neonatal have informed guidelines and recommendations worldwide. From January 2018 through June 2020, 77 international guidelines cited 221 Cochrane Neonatal reviews. These recommendations have included recommendations of the use of postnatal steroids, inhaled nitric oxide, feeding guidelines for preterm infants and other core aspects of neonatal practice. In addition, Cochrane Reviews has been the impetus for important research, including the large-scale trial of prophylactic indomethacin therapy, a variety of trials of postnatal steroids, trials of emollient ointment and probiotic trials [6]. While justifiably proud of these accomplishments, one needs to examine the future contribution of Cochrane Neonatal to the neonatal community. The future of Cochrane Neonatal is inexorably linked to the future of neonatal research. Obviously, there is no synthesis of trials data if, as a community, we fail to provide the core substrate for that research. As we look at the current trials' environment, fewer randomized controlled trial related to neonates are being published in recent years. A simple search of PubMed, limiting the search to &quot;neonates&quot; and &quot;randomized controlled trials&quot; shows that in the year 2000, 321 randomized controlled trials were published. These peaked five years ago, in 2015, with close to 900 trials being published. However, in 2018, only 791 studies are identified. Does this decrease represent a meaningful change in the neonatal research environment? Quite possibly. There are shifting missions of clinical neonatology at academic medical institutions, at least in the United States, with a focus on business aspects as well as other important competing clinical activities. Quality improvement has taken over as one of the major activities at both private and academic neonatal practices. Clearly, this is a needed improvement. All units at levels need to be dedicated to improving the outcomes of the sick and fragile population we care for. However, this need not be at the expense of formal clinical trials. It is understandable that this approach would be taken. Newer interventions frequently relate to complex systems of care and not the simple single interventions. Even trials that might traditionally have been done as randomized controlled trials, such as the introduction of a new mode of ventilation, are in reality complex challenges to the ability of institutions to create systems to adapt to these new technologies. Cost of doing trials has always been a barrier. The challenging regulatory and ethical environment contributes to these problems as well [7]. Despite these barriers, how does the research agenda of the neonatal community move forward in the 21st Century? We need to reassess how we create and disseminate our research findings. Innovative trial designs will allow us to address complex issues that we may not have tackled with conventional trials. Adaptive designs may allow us to look at potentially life-saving therapies in a way that feel more efficient and more ethical [8]. Clarifying issues such as the use of inhaled nitric oxide in preterm infants would be greatly served if we even knew whether or not there are hypoxemic preterm infant who would benefit from this therapy [9]. Current trials do not suggest so, yet current practice tells us that a significant number of these babies will receive inhaled nitric oxide [10-13]. Adaptive design, such as those done with trials of extracorporeal membrane oxygenation (ECMO), would allow us to quickly assess whether, in fact, these therapies are life-saving and allow us to consider whether or not further trials are needed [14,15]. Our understanding that many interventions involve entire systems approaches does not relegate us only to doing quality improvement work. Cluster designs may allow us to test more complex interventions that have usually been under the purview of quality improvement [16-18]. Cluster trials are well suited for such investigations and can be done with the least interruption to ongoing care. Ultimately, quality improvement is the application of the best evidence available (evidence-based medicine is &quot;what to do&quot; and evidence-based practice is &quot;how to do&quot;). [19,20]. Nascent efforts, such as the statement on &quot;embedding necessary research into culture and health&quot; (the ENRICH statement) call for the conduct of large, efficient pragmatic trials to evaluate neonatal outcomes, as in part called for in the ALPHA Collaboration [21,22]. This statement envisions an international system to identify important research questions by consulting regularly with all stakeholders, including patients, public health professionals, researchers, providers, policy makers, regulators, funders of industry. The ENRICH statement envisions a pathway to enable individuals, educational institutions, hospitals and health-care facilities to confirm their status as research-friendly by integrating an understanding of trials, other research and critical thinking and to teaching learning and culture, as well as an engagement with funders, professional organizations and regulatory bodies and other stake holders to raise awareness of the value of efficient international research to reduce barriers to large international pragmatic trials and other collaborative studies. In the future, if trials are to be done on this scale or trials are prospectively designed to be analyzed together, core outcome measures must be identified and standardized. That clinical trials supply estimates of outcomes that are relevant to patients and their families is critical. In addition, current neonatal research evaluates many different outcomes using multiple measures. A given measure can have multiple widely used definitions. Bronchopulmonary dysplasia (or chronic lung disease just to add to the confusion) quickly comes to mind [23,24]. The use of multiple definitions when attempting to measure the same outcome prevents synthesis of trial results and meta-analysis and hinders efforts to refine our estimates of effects. Towards that end, Webbe and colleagues have set out to develop a core outcome set for neonatal research [25]. Key stakeholders in the neonatal community reviewed multiple outcomes reported in neonatal trials and qualitative studies. Based on consensus, key outcome measures were identified, including survival, sepsis, necrotizing enterocolitis, brain injury on imaging, retinopathy or prematurity, gross motor ability, general cognitive ability, quality of life, adverse events, visual impairment or blindness, hearing impairment or deafness, chronic lung disease/bronchopulmonary dysplasia. Trials registration has to be a continued focus of the neonatal community. Trials registration allows for systematic reviewers to understand whether or not reporting bias has occurred [26]. It also allows for transparent incorporation of these core outcome measures. Ultimately, trials registration should include public reporting of all of these core outcomes and, in the future, access to data on an individual level such that more sophisticated individual patient data meta-analysis could occur. Lastly, there is no reason to see clinical trials and quality improvement as separate or exclusive activities. In fact, in the first NICQ Collaborative, conducted by Vermont Oxford Network, participation in a trial of postnatal steroids was considered part of the quality improvement best practices as opposed to simply choosing an as-of-yet unproven approach to use of this potent drug [27]. What role will Cochrane Neonatal play as we move forward in the 21st Century? As the neonatal community moves forward with its' research agenda, Cochrane Neonatal must not only follow but also lead with innovative approaches to synthesizing research findings. Cochrane Neonatal must continue to work closely with guideline developers. The relationship between systematic review production and guideline development is clearly outlined in reports from the Institute of Medicine [28,29]. Both are essential to guideline development; the systematic review group culling the evidence for the benefits and harms of a given intervention and the guideline group addressing the contextual issues of cost, feasibility, implementation and the values and preferences of individuals and societies. Most national and international guidelines groups now routinely use systematic reviews as the evidence basis for their guidelines and recommendations. Examples of the partnership between Cochrane Neonatal and international guideline development can be seen in our support of the World Health Organization (WHO) guidelines on the use of vitamin A or the soon to be published recommendations from the International Liaison Committee on Resuscitation (ILCOR) on cord management in preterm and term infants [30]. In the future, we need to collaborate early in the guideline development process so that the reviews are fit for purpose and meet the needs of the guideline developers and the end users. Towards this end, all Cochrane Neonatal reviews now contain GRADE assessments of the key clinical findings reported in the systematic review [31]. Addition of these assessments addresses the critical issue of our confidence in the findings. We are most confident in evidence provided by randomized controlled trials but this assessment can be can be downgraded if the studies that reported on the outcome in question had a high risk of bias, indirectness, inconsistency of results, or imprecision, or where there is evidence of reporting bias. Information provided by GRADE assessments is seen as critical in the process of moving from the evidence to formal recommendations [32]. We need to explore complex reviews, such as network (NMA) or multiple treatment comparison (MCT) meta-analyses, to address issues not formally addressed in clinical trials [33]. In conditions where there are multiple effective interventions, it is rare for all possible interventions to have been tested against each other [34]. A solution could be provided by network meta-analysis, which allows for comparing all treatments with each other, even if randomized controlled trials are not available for some treatment comparisons [34]. Network meta-analysis uses both direct (head-to-head) randomized clinical trial (RCT) evidence as well as indirect evidence from RCTs to compare the relative effectiveness of all included interventions [35]. However, Mills and colleagues note that the methodological quality of MTCs may be difficult for clinicians to interpret because the number of interventions evaluated may be large and the methodological approaches may be complex [35]. Cochrane Neonatal must take a role in both the creation of such analyses and the education of the neonatal community regarding the pitfalls of such an approach. The availability of individual patient data will make more sophisticated analyses more available to the community. Although the current crop of individual patient data meta-analyses (including the reviews of elective high frequency ventilation, inhaled nitric oxide and oxygen targets) have not differed substantially from the findings of the trials level reviews (suggesting that, in fact, sick neonates are more alike that unalike), there still will be a large role for individual patient data meta-analysis, at least to end the unfound conclusions that these therapies are effective in various subgroups (be it issues of sex, disease severity, or clinical setting) [36-39]. Future trials should take a lesson from the NeOProM Collaborative [37,39]. Given the difficulty in generating significant sample size and creating funding in any single environment, trials with similar protocols should be conducted in a variety of healthcare settings with an eye towards both study level and individual patient level meta-analysis at the conclusion of those trials, allowing for broader contribution to the trials data, more rapid accrual of sample size, and more precise results. We need to educate the neonatal community regarding the use and abuse of diagnostic tests. Diagnostic tests are a critical component of healthcare but also contribute greatly to the cost of medical care worldwide. These costs include the cost of the tests themselves and the costs of misdiagnosis and treatment of individuals who will not benefit from those treatments. Clinicians may have a limited understanding of diagnostic test accuracy, the ability of a diagnostic test to distinguish between patients with and without the disease or target condition [41,42]. Efforts such as Choosing Wisely have tried to identify these deficiencies [40]. As Cochrane has increased the general literacy of both the medical and general population regarding the interpretation of the results of interventions on various diseases, so should Cochrane move forward and improve the understanding of diagnostic testing. We need to become more efficient at creating and maintaining our reviews. The time spent to produce systematic reviews is far too great. In average, it takes between 2Ã‚Â½ to 6Ã‚Â½Ã‚Â years to produce a systematic review, requiring intense time input for highly trained and expensive experts. Innovations in the ways in which we produce systematic reviews can make the review process more efficient by outsourcing some of the tasks or crowdsourcing to machine learning. We need to let the crowd and machine learning innovations help us sort the massive amounts of information needed to conduct systematic reviews. It can also allow for &quot;live&quot; updating of critical reviews where the research landscape is quickly changing [43]. Lastly, Cochrane Neonatal must focus more on users of the reviews and not necessarily authors of the reviews. Current Cochrane programming speaks of Cochrane training with an eye towards developing the skills of individuals who will conduct systematic reviews. While this is clearly needed and laudable, the fact of the matter is that most of the community will be &quot;users&quot; of the reviews. Individuals who need to understand how to use and interpret the findings of systematic reviews. These review users include clinicians, guideline developers, policy makers and families. Incorporation of GRADE guidelines has been a huge step in adding transparency to the level of uncertainty we have in our findings. From a family's perspective, we need to overcome the environment of mistrust or misunderstanding of scientific evidence and how we convey what we know, and our uncertainty about what we know, to parents and families.","Cochrane Neonatal was first established in 1993, as one of the original review groups of the Cochrane Collaboration. In fact, the origins of Cochrane Neonatal precede the establishment of the collaboration. In the 1980's, the National Perinatal Epidemiology Unit at Oxford, led by Dr. Iain Chalmers, established the ""Oxford Database of Perinatal Trials"" (ODPT), a register of virtually all randomized controlled trials in perinatal medicine to provide a resource for reviews of the safety and efficacy of interventions used in perinatal care and to foster cooperative and coordinated research efforts in the perinatal field [1]. An effort that was clearly ahead of its time, ODPT comprised four main elements: a register of published reports of trials; a register of unpublished trials; a register of ongoing and planned trials; and data derived from pooled overviews (meta-analyses) of trials. This core effort grew into the creation of the seminal books, ""Effective Care in Pregnancy and Childbirth"" as well as ""Effective Care of the Newborn Infant"" [2,3]. As these efforts in perinatal medicine grew, Iain Chalmers thought well beyond perinatal medicine into the creation of a worldwide collaboration that became Cochrane [4]. The mission of the Cochrane Collaboration is to promote evidence-informed health decision-making by producing high-quality, relevant, accessible systematic reviews and other synthesized research evidence (www.cochrane.org). Cochrane Neonatal has continued to be one of the most productive review groups, publishing between 25 tpo to 40 new or updated systematic reviews each year. The impact factor has been steadily increasing over four years and now rivals most of the elite journals in pediatric medicine. Cochrane Neonatal has been a worldwide effort. Currently, there are 404 reviews involving 1206 authors from 52 countries. What has Cochrane done for babies? Reviews from Cochrane Neonatal have informed guidelines and recommendations worldwide. From January 2018 through June 2020, 77 international guidelines cited 221 Cochrane Neonatal reviews. These recommendations have included recommendations of the use of postnatal steroids, inhaled nitric oxide, feeding guidelines for preterm infants and other core aspects of neonatal practice. In addition, Cochrane Reviews has been the impetus for important research, including the large-scale trial of prophylactic indomethacin therapy, a variety of trials of postnatal steroids, trials of emollient ointment and probiotic trials [6]. While justifiably proud of these accomplishments, one needs to examine the future contribution of Cochrane Neonatal to the neonatal community. The future of Cochrane Neonatal is inexorably linked to the future of neonatal research. Obviously, there is no synthesis of trials data if, as a community, we fail to provide the core substrate for that research. As we look at the current trials' environment, fewer randomized controlled trial related to neonates are being published in recent years. A simple search of PubMed, limiting the search to ""neonates"" and ""randomized controlled trials"" shows that in the year 2000, 321 randomized controlled trials were published. These peaked five years ago, in 2015, with close to 900 trials being published. However, in 2018, only 791 studies are identified. Does this decrease represent a meaningful change in the neonatal research environment? Quite possibly. There are shifting missions of clinical neonatology at academic medical institutions, at least in the United States, with a focus on business aspects as well as other important competing clinical activities. Quality improvement has taken over as one of the major activities at both private and academic neonatal practices. Clearly, this is a needed improvement. All units at levels need to be dedicated to improving the outcomes of the sick and fragile population we care for. However, this need not be at the expense of formal clinical trials. It is understandable that this approach would be taken. Newer interventions frequently relate to complex systems of care and not the simple single interventions. Even trials that might traditionally have been done as randomized controlled trials, such as the introduction of a new mode of ventilation, are in reality complex challenges to the ability of institutions to create systems to adapt to these new technologies. Cost of doing trials has always been a barrier. The challenging regulatory and ethical environment contributes to these problems as well [7]. Despite these barriers, how does the research agenda of the neonatal community move forward in the 21st Century? We need to reassess how we create and disseminate our research findings. Innovative trial designs will allow us to address complex issues that we may not have tackled with conventional trials. Adaptive designs may allow us to look at potentially life-saving therapies in a way that feel more efficient and more ethical [8]. Clarifying issues such as the use of inhaled nitric oxide in preterm infants would be greatly served if we even knew whether or not there are hypoxemic preterm infant who would benefit from this therapy [9]. Current trials do not suggest so, yet current practice tells us that a significant number of these babies will receive inhaled nitric oxide [10-13]. Adaptive design, such as those done with trials of extracorporeal membrane oxygenation (ECMO), would allow us to quickly assess whether, in fact, these therapies are life-saving and allow us to consider whether or not further trials are needed [14,15]. Our understanding that many interventions involve entire systems approaches does not relegate us only to doing quality improvement work. Cluster designs may allow us to test more complex interventions that have usually been under the purview of quality improvement [16-18]. Cluster trials are well suited for such investigations and can be done with the least interruption to ongoing care. Ultimately, quality improvement is the application of the best evidence available (evidence-based medicine is ""what to do"" and evidence-based practice is ""how to do""). [19,20]. Nascent efforts, such as the statement on ""embedding necessary research into culture and health"" (the ENRICH statement) call for the conduct of large, efficient pragmatic trials to evaluate neonatal outcomes, as in part called for in the ALPHA Collaboration [21,22]. This statement envisions an international system to identify important research questions by consulting regularly with all stakeholders, including patients, public health professionals, researchers, providers, policy makers, regulators, funders of industry. The ENRICH statement envisions a pathway to enable individuals, educational institutions, hospitals and health-care facilities to confirm their status as research-friendly by integrating an understanding of trials, other research and critical thinking and to teaching learning and culture, as well as an engagement with funders, professional organizations and regulatory bodies and other stake holders to raise awareness of the value of efficient international research to reduce barriers to large international pragmatic trials and other collaborative studies. In the future, if trials are to be done on this scale or trials are prospectively designed to be analyzed together, core outcome measures must be identified and standardized. That clinical trials supply estimates of outcomes that are relevant to patients and their families is critical. In addition, current neonatal research evaluates many different outcomes using multiple measures. A given measure can have multiple widely used definitions. Bronchopulmonary dysplasia (or chronic lung disease just to add to the confusion) quickly comes to mind [23,24]. The use of multiple definitions when attempting to measure the same outcome prevents synthesis of trial results and meta-analysis and hinders efforts to refine our estimates of effects. Towards that end, Webbe and colleagues have set out to develop a core outcome set for neonatal research [25]. Key stakeholders in the neonatal community reviewed multiple outcomes reported in neonatal trials and qualitative studies. Based on consensus, key outcome measures were identified, including survival, sepsis, necrotizing enterocolitis, brain injury on imaging, retinopathy or prematurity, gross motor ability, general cognitive ability, quality of life, adverse events, visual impairment or blindness, hearing impairment or deafness, chronic lung disease/bronchopulmonary dysplasia. Trials registration has to be a continued focus of the neonatal community. Trials registration allows for systematic reviewers to understand whether or not reporting bias has occurred [26]. It also allows for transparent incorporation of these core outcome measures. Ultimately, trials registration should include public reporting of all of these core outcomes and, in the future, access to data on an individual level such that more sophisticated individual patient data meta-analysis could occur. Lastly, there is no reason to see clinical trials and quality improvement as separate or exclusive activities. In fact, in the first NICQ Collaborative, conducted by Vermont Oxford Network, participation in a trial of postnatal steroids was considered part of the quality improvement best practices as opposed to simply choosing an as-of-yet unproven approach to use of this potent drug [27]. What role will Cochrane Neonatal play as we move forward in the 21st Century? As the neonatal community moves forward with its' research agenda, Cochrane Neonatal must not only follow but also lead with innovative approaches to synthesizing research findings. Cochrane Neonatal must continue to work closely with guideline developers. The relationship between systematic review production and guideline development is clearly outlined in reports from the Institute of Medicine [28,29]. Both are essential to guideline development; the systematic review group culling the evidence for the benefits and harms of a given intervention and the guideline group addressing the contextual issues of cost, feasibility, implementation and the values and preferences of individuals and societies. Most national and international guidelines groups now routinely use systematic reviews as the evidence basis for their guidelines and recommendations. Examples of the partnership between Cochrane Neonatal and international guideline development can be seen in our support of the World Health Organization (WHO) guidelines on the use of vitamin A or the soon to be published recommendations from the International Liaison Committee on Resuscitation (ILCOR) on cord management in preterm and term infants [30]. In the future, we need to collaborate early in the guideline development process so that the reviews are fit for purpose and meet the needs of the guideline developers and the end users. Towards this end, all Cochrane Neonatal reviews now contain GRADE assessments of the key clinical findings reported in the systematic review [31]. Addition of these assessments addresses the critical issue of our confidence in the findings. We are most confident in evidence provided by randomized controlled trials but this assessment can be can be downgraded if the studies that reported on the outcome in question had a high risk of bias, indirectness, inconsistency of results, or imprecision, or where there is evidence of reporting bias. Information provided by GRADE assessments is seen as critical in the process of moving from the evidence to formal recommendations [32]. We need to explore complex reviews, such as network (NMA) or multiple treatment comparison (MCT) meta-analyses, to address issues not formally addressed in clinical trials [33]. In conditions where there are multiple effective interventions, it is rare for all possible interventions to have been tested against each other [34]. A solution could be provided by network meta-analysis, which allows for comparing all treatments with each other, even if randomized controlled trials are not available for some treatment comparisons [34]. Network meta-analysis uses both direct (head-to-head) randomized clinical trial (RCT) evidence as well as indirect evidence from RCTs to compare the relative effectiveness of all included interventions [35]. However, Mills and colleagues note that the methodological quality of MTCs may be difficult for clinicians to interpret because the number of interventions evaluated may be large and the methodological approaches may be complex [35]. Cochrane Neonatal must take a role in both the creation of such analyses and the education of the neonatal community regarding the pitfalls of such an approach. The availability of individual patient data will make more sophisticated analyses more available to the community. Although the current crop of individual patient data meta-analyses (including the reviews of elective high frequency ventilation, inhaled nitric oxide and oxygen targets) have not differed substantially from the findings of the trials level reviews (suggesting that, in fact, sick neonates are more alike that unalike), there still will be a large role for individual patient data meta-analysis, at least to end the unfound conclusions that these therapies are effective in various subgroups (be it issues of sex, disease severity, or clinical setting) [36-39]. Future trials should take a lesson from the NeOProM Collaborative [37,39]. Given the difficulty in generating significant sample size and creating funding in any single environment, trials with similar protocols should be conducted in a variety of healthcare settings with an eye towards both study level and individual patient level meta-analysis at the conclusion of those trials, allowing for broader contribution to the trials data, more rapid accrual of sample size, and more precise results. We need to educate the neonatal community regarding the use and abuse of diagnostic tests. Diagnostic tests are a critical component of healthcare but also contribute greatly to the cost of medical care worldwide. These costs include the cost of the tests themselves and the costs of misdiagnosis and treatment of individuals who will not benefit from those treatments. Clinicians may have a limited understanding of diagnostic test accuracy, the ability of a diagnostic test to distinguish between patients with and without the disease or target condition [41,42]. Efforts such as Choosing Wisely have tried to identify these deficiencies [40]. As Cochrane has increased the general literacy of both the medical and general population regarding the interpretation of the results of interventions on various diseases, so should Cochrane move forward and improve the understanding of diagnostic testing. We need to become more efficient at creating and maintaining our reviews. The time spent to produce systematic reviews is far too great. In average, it takes between 2Â½ to 6Â½Â years to produce a systematic review, requiring intense time input for highly trained and expensive experts. Innovations in the ways in which we produce systematic reviews can make the review process more efficient by outsourcing some of the tasks or crowdsourcing to machine learning. We need to let the crowd and machine learning innovations help us sort the massive amounts of information needed to conduct systematic reviews. It can also allow for ""live"" updating of critical reviews where the research landscape is quickly changing [43]. Lastly, Cochrane Neonatal must focus more on users of the reviews and not necessarily authors of the reviews. Current Cochrane programming speaks of Cochrane training with an eye towards developing the skills of individuals who will conduct systematic reviews. While this is clearly needed and laudable, the fact of the matter is that most of the community will be ""users"" of the reviews. Individuals who need to understand how to use and interpret the findings of systematic reviews. These review users include clinicians, guideline developers, policy makers and families. Incorporation of GRADE guidelines has been a huge step in adding transparency to the level of uncertainty we have in our findings. From a family's perspective, we need to overcome the environment of mistrust or misunderstanding of scientific evidence and how we convey what we know, and our uncertainty about what we know, to parents and families.","Soll, Ovelman, McGuire","Soll, Ovelman, McGuire",https://doi.org/10.1016/j.earlhumdev.2020.105191,https://doi.org/10.1016/j.earlhumdev.2020.105191,2021-08-03
16041.0,pubmed,pubmed,Long-term exposure to NO<sub>2</sub> and O<sub>3</sub> and all-cause and respiratory mortality: A systematic review and meta-analysis,Long-term exposure to NO<sub>2</sub> and O<sub>3</sub> and all-cause and respiratory mortality: A systematic review and meta-analysis,"WHO has published several volumes of Global Air Quality Guidelines to provide guidance on the health risks associated with exposure to outdoor air pollution. As new scientific evidence is generated, air quality guidelines need to be periodically revised and, where necessary, updated. The aims of the study were 1) to summarise the available evidence on the effect of long-term exposure to ozone (O<sub>3</sub>) and nitrogen dioxide (NO<sub>2</sub>) on mortality; 2) and to assess concentration response functions (CRF), their shape and the minimum level of exposures measured in studies to support WHO's update of the global air quality guidelines. We conducted a systematic literature search of the Medline, Embase and Web of Science databases following a protocol proposed by WHO and applied Preferred Reporting Items for Systematic Review and Meta-Analyses (PRISMA) guidelines for reporting our results. Cohort studies in human populations (including sub-groups at risk) exposed to long-term concentrations of NO<sub>2</sub> and O<sub>3</sub>. Outcomes assessed were all-cause, respiratory, Chronic Obstructive Pulmonary Disease (COPD) and Acute Lower Respiratory Infection (ALRI) mortality. Studies included in the meta-analyses were assessed using a new Risk of Bias instrument developed by a group of experts convened by WHO. Study results are presented in forest plots and quantitative meta-analyses were conducted using random effects models. The certainty of evidence was assessed using a newly developed adaptation of GRADE. The review identified 2068 studies of which 95 were subject to full-text review with 45 meeting the inclusion criteria. An update in September 2018 identified 159 studies with 1 meeting the inclusion criteria. Of the 46 included studies, 41 reported results for NO<sub>2</sub> and 20 for O<sub>3</sub>. The majority of studies were from the USA and Europe with the remainder from Canada, China and Japan. Forty-two studies reported results for all-cause mortality and 22 for respiratory mortality. Associations for NO<sub>2</sub> and mortality were positive; random-effects summary relative risks (RR) were 1.02 (95% CI: 1.01, 1.04), 1.03 (1.00, 1.05), 1.03 (1.01, 1.04) and 1.06 (1.02, 1.10) per 10Ã‚Â ÃŽÂ¼g/m<sup>3</sup> for all-cause (24 cohorts), respiratory (15 cohorts), COPD (9 cohorts) and ALRI (5 cohorts) mortality respectively. The review identified high levels of heterogeneity for all causes of death except COPD. A small number of studies investigated the shape of the concentration-response relationship and generally found little evidence to reject the assumption of linearity across the concentration range. Studies of O<sub>3</sub> using annual metrics showed the associations with all-cause and respiratory mortality were 0.97 (0.93, 1.02) and 0.99 (0.89, 1.11) per 10Ã‚Â ÃŽÂ¼g/m<sup>3</sup> respectively. For studies using peak O<sub>3</sub> metrics, the association with all-cause mortality was 1.01 (1.00, 1.02) and for respiratory mortality 1.02 (0.99, 1.05), each per 10Ã‚Â ÃŽÂ¼g/m<sup>3</sup>. The review identified high levels of heterogeneity. Few studies investigated the shape of the concentration-response relationship. Certainty in the associations (adapted GRADE) with mortality was rated low to moderate for each exposure-outcome pair, except for NO<sub>2</sub> and COPD mortality which was rated high. The substantial heterogeneity for most outcomes in the review requires explanation. The evidence base is limited in terms of the geographical spread of the study populations and, for some outcomes, the small number of independent cohorts for meta-analysis precludes meaningful meta-regression to explore causes of heterogeneity. Relatively few studies assessed specifically the shape of the CRF or multi-pollutant models. The short-comings in the existing literature base makes determining the precise nature (magnitude and linearity) of the associations challenging. Certainty of evidence assessments were moderate or low for both NO<sub>2</sub> and O<sub>3</sub> for all causes of mortality except for NO<sub>2</sub> and COPD mortality where the certainty of the evidence was judged as high.","WHO has published several volumes of Global Air Quality Guidelines to provide guidance on the health risks associated with exposure to outdoor air pollution. As new scientific evidence is generated, air quality guidelines need to be periodically revised and, where necessary, updated. The aims of the study were 1) to summarise the available evidence on the effect of long-term exposure to ozone (O<sub>3</sub>) and nitrogen dioxide (NO<sub>2</sub>) on mortality; 2) and to assess concentration response functions (CRF), their shape and the minimum level of exposures measured in studies to support WHO's update of the global air quality guidelines. We conducted a systematic literature search of the Medline, Embase and Web of Science databases following a protocol proposed by WHO and applied Preferred Reporting Items for Systematic Review and Meta-Analyses (PRISMA) guidelines for reporting our results. Cohort studies in human populations (including sub-groups at risk) exposed to long-term concentrations of NO<sub>2</sub> and O<sub>3</sub>. Outcomes assessed were all-cause, respiratory, Chronic Obstructive Pulmonary Disease (COPD) and Acute Lower Respiratory Infection (ALRI) mortality. Studies included in the meta-analyses were assessed using a new Risk of Bias instrument developed by a group of experts convened by WHO. Study results are presented in forest plots and quantitative meta-analyses were conducted using random effects models. The certainty of evidence was assessed using a newly developed adaptation of GRADE. The review identified 2068 studies of which 95 were subject to full-text review with 45 meeting the inclusion criteria. An update in September 2018 identified 159 studies with 1 meeting the inclusion criteria. Of the 46 included studies, 41 reported results for NO<sub>2</sub> and 20 for O<sub>3</sub>. The majority of studies were from the USA and Europe with the remainder from Canada, China and Japan. Forty-two studies reported results for all-cause mortality and 22 for respiratory mortality. Associations for NO<sub>2</sub> and mortality were positive; random-effects summary relative risks (RR) were 1.02 (95% CI: 1.01, 1.04), 1.03 (1.00, 1.05), 1.03 (1.01, 1.04) and 1.06 (1.02, 1.10) per 10Â Î¼g/m<sup>3</sup> for all-cause (24 cohorts), respiratory (15 cohorts), COPD (9 cohorts) and ALRI (5 cohorts) mortality respectively. The review identified high levels of heterogeneity for all causes of death except COPD. A small number of studies investigated the shape of the concentration-response relationship and generally found little evidence to reject the assumption of linearity across the concentration range. Studies of O<sub>3</sub> using annual metrics showed the associations with all-cause and respiratory mortality were 0.97 (0.93, 1.02) and 0.99 (0.89, 1.11) per 10Â Î¼g/m<sup>3</sup> respectively. For studies using peak O<sub>3</sub> metrics, the association with all-cause mortality was 1.01 (1.00, 1.02) and for respiratory mortality 1.02 (0.99, 1.05), each per 10Â Î¼g/m<sup>3</sup>. The review identified high levels of heterogeneity. Few studies investigated the shape of the concentration-response relationship. Certainty in the associations (adapted GRADE) with mortality was rated low to moderate for each exposure-outcome pair, except for NO<sub>2</sub> and COPD mortality which was rated high. The substantial heterogeneity for most outcomes in the review requires explanation. The evidence base is limited in terms of the geographical spread of the study populations and, for some outcomes, the small number of independent cohorts for meta-analysis precludes meaningful meta-regression to explore causes of heterogeneity. Relatively few studies assessed specifically the shape of the CRF or multi-pollutant models. The short-comings in the existing literature base makes determining the precise nature (magnitude and linearity) of the associations challenging. Certainty of evidence assessments were moderate or low for both NO<sub>2</sub> and O<sub>3</sub> for all causes of mortality except for NO<sub>2</sub> and COPD mortality where the certainty of the evidence was judged as high.","Huangfu, Atkinson","Huangfu, Atkinson",https://doi.org/10.1016/j.envint.2020.105998,https://doi.org/10.1016/j.envint.2020.105998,2021-08-03
16047.0,pubmed,pubmed,Sensitivity analysis of FDG PET tumor voxel cluster radiomics and dosimetry for predicting mid-chemoradiation regional response of locally advanced lung cancer,Sensitivity analysis of FDG PET tumor voxel cluster radiomics and dosimetry for predicting mid-chemoradiation regional response of locally advanced lung cancer,"We investigated the sensitivity of regional tumor response prediction to variability in voxel clustering techniques, imaging features, and machine learning algorithms in 25 patients with locally advanced non-small cell lung cancer (LA-NSCLC) enrolled on the FLARE-RT clinical trial. Metabolic tumor volumes (MTV) from pre-chemoradiation (PETpre) and mid-chemoradiation fluorodeoxyglucose-positron emission tomography (FDG PET) images (PETmid) were subdivided into K-means or hierarchical voxel clusters by standardized uptake values (SUV) and 3D-positions. MTV cluster separability was evaluated by CH index, and morphologic changes were captured by Dice similarity and centroid Euclidean distance. PETpre conventional features included SUVmean, MTV/MTV cluster size, and mean radiation dose. PETpre radiomics consisted of 41 intensity histogram and 3D texture features (PET Oncology Radiomics Test Suite) extracted from MTV or MTV clusters. Machine learning models (multiple linear regression, support vector regression, logistic regression, support vector machines) of conventional features or radiomic features were constructed to predict PETmid response. Leave-one-out-cross-validated root-mean-squared-error (RMSE) for continuous response regression (ÃŽâ€SUVmean) and area-under-receiver-operating-characteristic-curve (AUC) for binary response classification were calculated. K-means MTV 2-clusters (MTVhi, MTVlo) achieved maximum CH index separability (Friedman p &lt; 0.001). Between PETpre and PETmid, MTV cluster pairs overlapped (Dice 0.70-0.87) and migrated 0.6-1.1 cm. PETmid ÃŽâ€SUVmean response prediction was superior in MTV and MTVlo (RMSE = 0.17-0.21) compared to MTVhi (RMSE = 0.42-0.52, Friedman p &lt; 0.001). PETmid ÃŽâ€SUVmean response class prediction performance trended higher in MTVlo (AUC = 0.83-0.88) compared to MTVhi (AUC = 0.44-0.58, Friedman p = 0.052). Models were more sensitive to MTV/MTV cluster regions (Friedman p = 0.026) than feature sets/algorithms (Wilcoxon signed-rank p = 0.36). Top-ranked radiomic features included GLZSM-LZHGE (large-zone-high-SUV), GTSDM-CP (cluster-prominence), GTSDM-CS (cluster-shade) and NGTDM-CNT (contrast). Top-ranked features were consistent between MTVhi and MTVlo cluster pairs but varied between MTVhi-MTVlo clusters, reflecting distinct regional radiomic phenotypes. Variability in tumor voxel cluster response prediction can inform robust radiomic target definition for risk-adaptive chemoradiation in patients with LA-NSCLC. FLARE-RT trial: NCT02773238.","We investigated the sensitivity of regional tumor response prediction to variability in voxel clustering techniques, imaging features, and machine learning algorithms in 25 patients with locally advanced non-small cell lung cancer (LA-NSCLC) enrolled on the FLARE-RT clinical trial. Metabolic tumor volumes (MTV) from pre-chemoradiation (PETpre) and mid-chemoradiation fluorodeoxyglucose-positron emission tomography (FDG PET) images (PETmid) were subdivided into K-means or hierarchical voxel clusters by standardized uptake values (SUV) and 3D-positions. MTV cluster separability was evaluated by CH index, and morphologic changes were captured by Dice similarity and centroid Euclidean distance. PETpre conventional features included SUVmean, MTV/MTV cluster size, and mean radiation dose. PETpre radiomics consisted of 41 intensity histogram and 3D texture features (PET Oncology Radiomics Test Suite) extracted from MTV or MTV clusters. Machine learning models (multiple linear regression, support vector regression, logistic regression, support vector machines) of conventional features or radiomic features were constructed to predict PETmid response. Leave-one-out-cross-validated root-mean-squared-error (RMSE) for continuous response regression (Î”SUVmean) and area-under-receiver-operating-characteristic-curve (AUC) for binary response classification were calculated. K-means MTV 2-clusters (MTVhi, MTVlo) achieved maximum CH index separability (Friedman p &lt; 0.001). Between PETpre and PETmid, MTV cluster pairs overlapped (Dice 0.70-0.87) and migrated 0.6-1.1 cm. PETmid Î”SUVmean response prediction was superior in MTV and MTVlo (RMSE = 0.17-0.21) compared to MTVhi (RMSE = 0.42-0.52, Friedman p &lt; 0.001). PETmid Î”SUVmean response class prediction performance trended higher in MTVlo (AUC = 0.83-0.88) compared to MTVhi (AUC = 0.44-0.58, Friedman p = 0.052). Models were more sensitive to MTV/MTV cluster regions (Friedman p = 0.026) than feature sets/algorithms (Wilcoxon signed-rank p = 0.36). Top-ranked radiomic features included GLZSM-LZHGE (large-zone-high-SUV), GTSDM-CP (cluster-prominence), GTSDM-CS (cluster-shade) and NGTDM-CNT (contrast). Top-ranked features were consistent between MTVhi and MTVlo cluster pairs but varied between MTVhi-MTVlo clusters, reflecting distinct regional radiomic phenotypes. Variability in tumor voxel cluster response prediction can inform robust radiomic target definition for risk-adaptive chemoradiation in patients with LA-NSCLC. FLARE-RT trial: NCT02773238.","Duan, Chaovalitwongse, Bai, Hippe, Wang, Thammasorn, Pierce, Liu, You, Miyaoka, Vesselle, Kinahan, Rengan, Zeng, Bowen","Duan, Chaovalitwongse, Bai, Hippe, Wang, Thammasorn, Pierce, Liu, You, Miyaoka, Vesselle, Kinahan, Rengan, Zeng, Bowen",https://doi.org/10.1088/1361-6560/abb0c7,https://doi.org/10.1088/1361-6560/abb0c7,2021-08-03
16048.0,pubmed,pubmed,Applicability of radiomics in interstitial lung disease associated with systemic sclerosis: proof of concept,Applicability of radiomics in interstitial lung disease associated with systemic sclerosis: proof of concept,"To retrospectively evaluate if texture-based radiomics features are able to detect interstitial lung disease (ILD) and to distinguish between the different disease stages in patients with systemic sclerosis (SSc) in comparison with mere visual analysis of high-resolution computed tomography (HRCT). Sixty patients (46 females, median age 56 years) with SSc who underwent HRCT of the thorax were retrospectively analyzed. Visual analysis was performed by two radiologists for the presence of ILD features. Gender, age, and pulmonary function (GAP) stage was calculated from clinical data (gender, age, pulmonary function test). Data augmentation was performed and the balanced dataset was split into a training (70%) and a testing dataset (30%). For selecting variables that allow classification of the GAP stage, single and multiple logistic regression models were fitted and compared by using the Akaike information criterion (AIC). Diagnostic accuracy was evaluated from the area under the curve (AUC) from receiver operating characteristic (ROC) analyses, and diagnostic sensitivity and specificity were calculated. Values for some radiomics features were significantly lower (p &lt; 0.05) and those of other radiomics features were significantly higher (p = 0.001) in patients with GAP2 compared with those in patients with GAP1. The combination of two specific radiomics features in a multivariable model resulted in the lowest AIC of 10.73 with an AUC of 0.96, 84% sensitivity, and 99% specificity. Visual assessment of fibrosis was inferior in predicting individual GAP stages (AUC 0.86; 83% sensitivity; 74% specificity). The correlation of radiomics with GAP stage, but not with the visually defined features of ILD-HRCT, implies that radiomics might capture features indicating severity of SSc-ILD on HRCT, which are not recognized by visual analysis. Ã¢â‚¬Â¢ Radiomics features can predict GAP stage with a sensitivity of 84% and a specificity of almost 100%. Ã¢â‚¬Â¢ Extent of fibrosis on HRCT and a combined model of different visual HRCT-ILD features perform worse in predicting GAP stage. Ã¢â‚¬Â¢ The correlation of radiomics with GAP stage, but not with the visually defined features of ILD-HRCT, implies that radiomics might capture features on HRCT, which are not recognized by visual analysis.","To retrospectively evaluate if texture-based radiomics features are able to detect interstitial lung disease (ILD) and to distinguish between the different disease stages in patients with systemic sclerosis (SSc) in comparison with mere visual analysis of high-resolution computed tomography (HRCT). Sixty patients (46 females, median age 56 years) with SSc who underwent HRCT of the thorax were retrospectively analyzed. Visual analysis was performed by two radiologists for the presence of ILD features. Gender, age, and pulmonary function (GAP) stage was calculated from clinical data (gender, age, pulmonary function test). Data augmentation was performed and the balanced dataset was split into a training (70%) and a testing dataset (30%). For selecting variables that allow classification of the GAP stage, single and multiple logistic regression models were fitted and compared by using the Akaike information criterion (AIC). Diagnostic accuracy was evaluated from the area under the curve (AUC) from receiver operating characteristic (ROC) analyses, and diagnostic sensitivity and specificity were calculated. Values for some radiomics features were significantly lower (p &lt; 0.05) and those of other radiomics features were significantly higher (p = 0.001) in patients with GAP2 compared with those in patients with GAP1. The combination of two specific radiomics features in a multivariable model resulted in the lowest AIC of 10.73 with an AUC of 0.96, 84% sensitivity, and 99% specificity. Visual assessment of fibrosis was inferior in predicting individual GAP stages (AUC 0.86; 83% sensitivity; 74% specificity). The correlation of radiomics with GAP stage, but not with the visually defined features of ILD-HRCT, implies that radiomics might capture features indicating severity of SSc-ILD on HRCT, which are not recognized by visual analysis. â€¢ Radiomics features can predict GAP stage with a sensitivity of 84% and a specificity of almost 100%. â€¢ Extent of fibrosis on HRCT and a combined model of different visual HRCT-ILD features perform worse in predicting GAP stage. â€¢ The correlation of radiomics with GAP stage, but not with the visually defined features of ILD-HRCT, implies that radiomics might capture features on HRCT, which are not recognized by visual analysis.","Martini, Baessler, Bogowicz, BlÃƒÂ¼thgen, Mannil, Tanadini-Lang, Schniering, Maurer, Frauenfelder","Martini, Baessler, Bogowicz, BlÃ¼thgen, Mannil, Tanadini-Lang, Schniering, Maurer, Frauenfelder",https://doi.org/10.1007/s00330-020-07293-8,https://doi.org/10.1007/s00330-020-07293-8,2021-08-03
16052.0,pubmed,pubmed,[Artificial intelligence in gastroenterology],[Artificial intelligence in gastroenterology],"Artificial intelligence (AI) is currently transforming all aspects of our daily life, including the practice of medicine. Artificial neural networks are a key method of AI. They can very effectively detect subtle patterns in imaging data and speech or text data. This is highly relevant for the practice of gastroenterology. Here, we summarize the state of the art in AI in gastroenterology and outline major clinical applications. Our focus is on AI-based analysis of endoscopy images, non-invasive imaging and histology images. In these applications, AI can support human pattern recognition. Beyond detection and classification of pathological findings, AI can predict clinical outcome from subtle image features. MASCHINELLES LERNEN UND KÃƒÂ¼NSTLICHE INTELLIGENZ: Ã¢â‚¬â€šKÃƒÂ¼nstliche Intelligenz (KI) verÃƒÂ¤ndert derzeit alle Bereiche unseres Lebens- und Arbeitsalltags. Die Welt der Medizin ist hiervon nicht ausgenommen. Eine Kernmethode der kÃƒÂ¼nstlichen Intelligenz Ã¢â‚¬â€œ kÃƒÂ¼nstliche neuronale Netzwerke Ã¢â‚¬â€œ ist besonders effektiv im Analysieren von Bilddaten. Dies beeinflusst auch die Gastroenterologie wesentlich. KLINISCHE ANWENDUNGEN: Ã¢â‚¬â€šInsbesondere endoskopische, radiologische und histologische Bilder kÃƒÂ¶nnen mittels kÃƒÂ¼nstlicher neuronaler Netzwerke automatisch analysiert werden. Einerseits kÃƒÂ¶nnen dadurch repetitive TÃƒÂ¤tigkeiten automatisiert werden, beispielsweise die Suche nach Polypen in der Screening-Koloskopie. Andererseits ermÃƒÂ¶glicht dies jedoch auch die Erkennung von subtilen Mustern in Bilddaten, welche als neuartige Biomarker fÃƒÂ¼r klinische VerlÃƒÂ¤ufe genutzt werden kÃƒÂ¶nnten. AUSBLICK: Ã¢â‚¬â€šDas Angebot kommerzieller KI-Systeme wird in Zukunft deutlich zunehmen. Wichtig ist die Ausbildung interdisziplinÃƒÂ¤rer Forscherinnen und Forscher, das Vorhandensein standardisierter Datenbanken und das frÃƒÂ¼hzeitige HeranfÃƒÂ¼hren der Ãƒâ€žrztinnen und Ãƒâ€žrzte an die komplexen KI-Anwendungen.","Artificial intelligence (AI) is currently transforming all aspects of our daily life, including the practice of medicine. Artificial neural networks are a key method of AI. They can very effectively detect subtle patterns in imaging data and speech or text data. This is highly relevant for the practice of gastroenterology. Here, we summarize the state of the art in AI in gastroenterology and outline major clinical applications. Our focus is on AI-based analysis of endoscopy images, non-invasive imaging and histology images. In these applications, AI can support human pattern recognition. Beyond detection and classification of pathological findings, AI can predict clinical outcome from subtle image features. MASCHINELLES LERNEN UND KÃ¼NSTLICHE INTELLIGENZ: â€‚KÃ¼nstliche Intelligenz (KI) verÃ¤ndert derzeit alle Bereiche unseres Lebens- und Arbeitsalltags. Die Welt der Medizin ist hiervon nicht ausgenommen. Eine Kernmethode der kÃ¼nstlichen Intelligenz â€“ kÃ¼nstliche neuronale Netzwerke â€“ ist besonders effektiv im Analysieren von Bilddaten. Dies beeinflusst auch die Gastroenterologie wesentlich. KLINISCHE ANWENDUNGEN: â€‚Insbesondere endoskopische, radiologische und histologische Bilder kÃ¶nnen mittels kÃ¼nstlicher neuronaler Netzwerke automatisch analysiert werden. Einerseits kÃ¶nnen dadurch repetitive TÃ¤tigkeiten automatisiert werden, beispielsweise die Suche nach Polypen in der Screening-Koloskopie. Andererseits ermÃ¶glicht dies jedoch auch die Erkennung von subtilen Mustern in Bilddaten, welche als neuartige Biomarker fÃ¼r klinische VerlÃ¤ufe genutzt werden kÃ¶nnten. AUSBLICK: â€‚Das Angebot kommerzieller KI-Systeme wird in Zukunft deutlich zunehmen. Wichtig ist die Ausbildung interdisziplinÃ¤rer Forscherinnen und Forscher, das Vorhandensein standardisierter Datenbanken und das frÃ¼hzeitige HeranfÃ¼hren der Ã„rztinnen und Ã„rzte an die komplexen KI-Anwendungen.","Kather, Krause, Luedde","Kather, Krause, Luedde",https://doi.org/10.1055/a-1013-6593,https://doi.org/10.1055/a-1013-6593,2021-08-03
16070.0,pubmed,pubmed,Cost-Effectiveness of Vitamin D Supplementation in Pregnant Woman and Young Children in Preventing Rickets: A Modeling Study,Cost-Effectiveness of Vitamin D Supplementation in Pregnant Woman and Young Children in Preventing Rickets: A Modeling Study,"<b>Background:</b> Literature on the cost of management of rickets and cost-effectiveness of vitamin D supplementation in preventing rickets is lacking. <b>Methods:</b> This study considered the cost-effectiveness of providing free vitamin D supplementation to pregnant women and children &lt;4 years of age with varying degrees of skin pigmentation to prevent rickets in children. Estimates for the prevalence of rickets were calculated using all cases of rickets diagnosed in Central Manchester, UK and census data from the region. Cost of management of rickets were calculated using National Health Service, UK tariffs. The efficacy of vitamin D supplementation was based on a similar programme implemented in Birmingham. Quality of life was assessed using utility estimates derived from a systematic literature review. In this analysis the intervention was considered cost-effective if the incremental cost-effectiveness ratio (ICER) is below the National Institute for Health and Care Excellence, UK cost-effectiveness threshold of Ã‚Â£20,000 per Quality-adjusted life year (QALY). <b>Results:</b> Fifty-seven patients (26 dark, 29 medium and 2 light skin tones) were managed for rickets and associated complications over 4-years. Rickets has an estimated annual incidence of 29Ã‚Â·75 per 100,000 children &lt;4 years of age. In the dark skin tone population vitamin D supplementation proved to be cost saving. In a medium skin tone population and light skin tone populations the ICER was Ã‚Â£19,295 per QALY and Ã‚Â£404,047 per QALY, respectively. <b>Conclusion:</b> Our study demonstrates that a vitamin D supplementation to prevent rickets is cost effective in dark and medium skin tone populations.","<b>Background:</b> Literature on the cost of management of rickets and cost-effectiveness of vitamin D supplementation in preventing rickets is lacking. <b>Methods:</b> This study considered the cost-effectiveness of providing free vitamin D supplementation to pregnant women and children &lt;4 years of age with varying degrees of skin pigmentation to prevent rickets in children. Estimates for the prevalence of rickets were calculated using all cases of rickets diagnosed in Central Manchester, UK and census data from the region. Cost of management of rickets were calculated using National Health Service, UK tariffs. The efficacy of vitamin D supplementation was based on a similar programme implemented in Birmingham. Quality of life was assessed using utility estimates derived from a systematic literature review. In this analysis the intervention was considered cost-effective if the incremental cost-effectiveness ratio (ICER) is below the National Institute for Health and Care Excellence, UK cost-effectiveness threshold of Â£20,000 per Quality-adjusted life year (QALY). <b>Results:</b> Fifty-seven patients (26 dark, 29 medium and 2 light skin tones) were managed for rickets and associated complications over 4-years. Rickets has an estimated annual incidence of 29Â·75 per 100,000 children &lt;4 years of age. In the dark skin tone population vitamin D supplementation proved to be cost saving. In a medium skin tone population and light skin tone populations the ICER was Â£19,295 per QALY and Â£404,047 per QALY, respectively. <b>Conclusion:</b> Our study demonstrates that a vitamin D supplementation to prevent rickets is cost effective in dark and medium skin tone populations.","Floreskul, Juma, Daniel, Zamir, Rawdin, Stevenson, Mughal, Padidela","Floreskul, Juma, Daniel, Zamir, Rawdin, Stevenson, Mughal, Padidela",https://doi.org/10.3389/fpubh.2020.00439,https://doi.org/10.3389/fpubh.2020.00439,2021-08-03
16073.0,pubmed,pubmed,Effectiveness of Radiofrequency Ablation in the Treatment of Painful Osseous Metastases: A Correlation Meta-Analysis with Machine Learning Cluster Identification,Effectiveness of Radiofrequency Ablation in the Treatment of Painful Osseous Metastases: A Correlation Meta-Analysis with Machine Learning Cluster Identification,"A systematic review and meta-analysis of pain response after radiofrequency (RF) ablation over time for osseous metastases was conducted in 2019. Analysis used a random-effects model with GOSH plots and meta-regression. Fourteen studies comprising 426 patients, most with recalcitrant pain, were identified. Median pain reduction after RF ablation was 67% over median follow-up of 24 weeks (R<sup>2</sup>Ã‚Â =Ã‚Â -.66, 95% confidence intervalÃ‚Â -0.76 toÃ‚Â -0.55, I<sup>2</sup>Ã‚Â = 71.24%, fail-safe NÃ‚Â = 875) with 44% pain reduction within 1 week. A low-heterogeneity subgroup was identified with median pain reduction after RF ablation of 70% over 12 weeks (R<sup>2</sup>Ã‚Â =Ã‚Â -.75, 95% confidence intervalÃ‚Â -0.80 toÃ‚Â -0.70, I<sup>2</sup>Ã‚Â = 2.66%, fail-safe NÃ‚Â = 910). Addition of cementoplasty after RF ablation did not significantly affect pain scores. Primary tumor type and tumor size did not significantly affect pain scores. A particular, positive association between pain after RF ablation and axial tumors was identified, implying possible increased palliative effects for RF ablation on axial over appendicular lesions. RF ablation is a useful palliative therapy for osseous metastases, particularly in patients with recalcitrant pain.","A systematic review and meta-analysis of pain response after radiofrequency (RF) ablation over time for osseous metastases was conducted in 2019. Analysis used a random-effects model with GOSH plots and meta-regression. Fourteen studies comprising 426 patients, most with recalcitrant pain, were identified. Median pain reduction after RF ablation was 67% over median follow-up of 24 weeks (R<sup>2</sup>Â =Â -.66, 95% confidence intervalÂ -0.76 toÂ -0.55, I<sup>2</sup>Â = 71.24%, fail-safe NÂ = 875) with 44% pain reduction within 1 week. A low-heterogeneity subgroup was identified with median pain reduction after RF ablation of 70% over 12 weeks (R<sup>2</sup>Â =Â -.75, 95% confidence intervalÂ -0.80 toÂ -0.70, I<sup>2</sup>Â = 2.66%, fail-safe NÂ = 910). Addition of cementoplasty after RF ablation did not significantly affect pain scores. Primary tumor type and tumor size did not significantly affect pain scores. A particular, positive association between pain after RF ablation and axial tumors was identified, implying possible increased palliative effects for RF ablation on axial over appendicular lesions. RF ablation is a useful palliative therapy for osseous metastases, particularly in patients with recalcitrant pain.","Mehta, Heiberger, Kazi, Brown, Weissman, Hong, Mehta, Yim","Mehta, Heiberger, Kazi, Brown, Weissman, Hong, Mehta, Yim",https://doi.org/10.1016/j.jvir.2020.08.002,https://doi.org/10.1016/j.jvir.2020.08.002,2021-08-03
16081.0,pubmed,pubmed,Cigarette Smoking and Root Filled Teeth Extraction: Systematic Review and Meta-Analysis,Cigarette Smoking and Root Filled Teeth Extraction: Systematic Review and Meta-Analysis,"The aim of this systematic review and meta-analysis was to investigate the possible association between smoking habits and the occurrence of root-filled teeth (RFT) extraction. The Population, Intervention, Comparison, and Outcome (PICO) question was in adult patients who had RFT, does the absence or presence of smoking habits affect the prevalence of extracted RFT? Systematic MEDLINE/PubMed, Wiley Online Database, Web of Science, and PRISMA protocol was used to evaluate and present the results. The Grading of Recommendations, Assessment, Development, and Evaluation (GRADE) system was used for certainty in the evidence. The risk of bias was assessed according to Cochrane Collaboration common scheme for bias and ROBINS-I tool. Cumulative meta-analysis was performed with a random effects model. PROSPERO registration code: CRD42020165279. After search strategy, 571 articles were recovered, seven were selected for full-text analysis, and two reported data on inclusion criteria, including 516 RFT, 351 in non-smokers, and 165 in smoker subjects. The meta-analysis provided an odds ratio indicating significant association between smoking and the prevalence of extracted RFT (OR = 3.43, 95% CI = 1.17-10.05, <i>p</i> = 0.02, IÃ‚Â² = 64%). The certainty of the literature assessment was low per GRADE. Both studies were considered as moderate risk of bias. Tobacco smoking should be considered a negative prognostic factor for the outcome of root canal treatment, although the quality of the evidence is low. RFT of smoking patients are three times more likely to be extracted. Continuing to smoke after endodontic treatment may increase the risk of treatment failure. However, the overall strength of evidence is low. This must be considered a limitation of the present study and the conclusion should be valued with caution.","The aim of this systematic review and meta-analysis was to investigate the possible association between smoking habits and the occurrence of root-filled teeth (RFT) extraction. The Population, Intervention, Comparison, and Outcome (PICO) question was in adult patients who had RFT, does the absence or presence of smoking habits affect the prevalence of extracted RFT? Systematic MEDLINE/PubMed, Wiley Online Database, Web of Science, and PRISMA protocol was used to evaluate and present the results. The Grading of Recommendations, Assessment, Development, and Evaluation (GRADE) system was used for certainty in the evidence. The risk of bias was assessed according to Cochrane Collaboration common scheme for bias and ROBINS-I tool. Cumulative meta-analysis was performed with a random effects model. PROSPERO registration code: CRD42020165279. After search strategy, 571 articles were recovered, seven were selected for full-text analysis, and two reported data on inclusion criteria, including 516 RFT, 351 in non-smokers, and 165 in smoker subjects. The meta-analysis provided an odds ratio indicating significant association between smoking and the prevalence of extracted RFT (OR = 3.43, 95% CI = 1.17-10.05, <i>p</i> = 0.02, IÂ² = 64%). The certainty of the literature assessment was low per GRADE. Both studies were considered as moderate risk of bias. Tobacco smoking should be considered a negative prognostic factor for the outcome of root canal treatment, although the quality of the evidence is low. RFT of smoking patients are three times more likely to be extracted. Continuing to smoke after endodontic treatment may increase the risk of treatment failure. However, the overall strength of evidence is low. This must be considered a limitation of the present study and the conclusion should be valued with caution.","Cabanillas-Balsera, Segura-Egea, JimÃƒÂ©nez-SÃƒÂ¡nchez, Areal-Quecuty, SÃƒÂ¡nchez-DomÃƒÂ­nguez, Montero-Miralles, SaÃƒÂºco-MÃƒÂ¡rquez, MartÃƒÂ­n-GonzÃƒÂ¡lez","Cabanillas-Balsera, Segura-Egea, JimÃ©nez-SÃ¡nchez, Areal-Quecuty, SÃ¡nchez-DomÃ­nguez, Montero-Miralles, SaÃºco-MÃ¡rquez, MartÃ­n-GonzÃ¡lez",https://doi.org/10.3390/jcm9103179,https://doi.org/10.3390/jcm9103179,2021-08-03
16082.0,pubmed,pubmed,Citation screening using crowdsourcing and machine learning produced accurate results: evaluation of Cochrane's modified Screen4Me service,Citation screening using crowdsourcing and machine learning produced accurate results: Evaluation of Cochrane's modified Screen4Me service,"To assess the feasibility of a modified workflow that uses machine learning and crowdsourcing to identify studies for potential inclusion in a systematic review. This was a sub-study to a larger randomised study; the main study sought to assess the performance of single screening search results versus dual screening. This sub-study assessed the performance in identifying relevant RCTs for a published Cochrane review of a modified version of Cochrane's Screen4Me workflow which uses crowdsourcing and machine learning. We included participants who had signed up for the main study but who were not eligible to be randomised to the two main arms of that study. The records were put through the modified workflow where a machine learning classifier divided the dataset into &quot;Not RCTs&quot; and &quot;Possible RCTs&quot;. The records deemed &quot;Possible RCTs&quot; were then loaded into a task created on the Cochrane Crowd platform and participants classified those records as either &quot;Potentially relevant&quot; or &quot;Not relevant&quot; to the review. Using a pre-specified agreement algorithm we calculated the performance of the crowd in correctly identifying the studies that were included in the review (sensitivity) and correctly rejecting those that were not included (specificity). The RCT machine learning classifier did not reject any of the included studies. In terms of the crowd, 112 participants were included in this sub-study. Of these, 81 completed the training module and went on to screen records in the live task. Applying the Cochrane Crowd agreement algorithm, the crowd achieved 100% sensitivity and 80.71% specificity. Using a crowd to screen search results for systematic reviews can be an accurate method as long as the agreement algorithm in place is robust. Open Science Framework: https://osf.io/3jyqt.","To assess the feasibility of a modified workflow that uses machine learning and crowdsourcing to identify studies for potential inclusion in a systematic review. This was a substudy to a larger randomized study; the main study sought to assess the performance of single screening search results versus dual screening. This substudy assessed the performance in identifying relevant randomized controlled trials (RCTs) for a published Cochrane review of a modified version of Cochrane's Screen4Me workflow which uses crowdsourcing and machine learning. We included participants who had signed up for the main study but who were not eligible to be randomized to the two main arms of that study. The records were put through the modified workflow where a machine learning classifier divided the data set into ""Not RCTs"" and ""Possible RCTs."" The records deemed ""Possible RCTs"" were then loaded into a task created on the Cochrane Crowd platform, and participants classified those records as either ""Potentially relevant"" or ""Not relevant"" to the review. Using a prespecified agreement algorithm, we calculated the performance of the crowd in correctly identifying the studies that were included in the review (sensitivity) and correctly rejecting those that were not included (specificity). The RCT machine learning classifier did not reject any of the included studies. In terms of the crowd, 112 participants were included in this substudy. Of these, 81 completed the training module and went on to screen records in the live task. Applying the Cochrane Crowd agreement algorithm, the crowd achieved 100% sensitivity and 80.71% specificity. Using a crowd to screen search results for systematic reviews can be an accurate method as long as the agreement algorithm in place is robust. Open Science Framework: https://osf.io/3jyqt.","Noel-Storr, Dooley, Affengruber, Gartlehner","Noel-Storr, Dooley, Affengruber, Gartlehner",https://doi.org/10.1016/j.jclinepi.2020.09.024,https://doi.org/10.1016/j.jclinepi.2020.09.024,2021-08-03
16084.0,pubmed,pubmed,Artificial intelligence interventions focused on opioid use disorders: A review of the gray literature,Artificial intelligence interventions focused on opioid use disorders: A review of the gray literature,"With the artificial intelligence (AI) paradigm shift comes momentum toward the development and scale-up of novel AI interventions to aid in opioid use disorder (OUD) care, in the identification of overdose risk, and in the reversal of overdose. As OUD-specific AI interventions are relatively recent, dynamic, and may not yet be captured in the peer-reviewed literature, we conducted a review of the gray literature to identify literature pertaining to OUD-specific AI interventions being developed, implemented and evaluated. Gray literature databases, customized Google searches, and targeted websites were searched from January 2013 to October 2019. Search terms include: AI, machine learning, substance use disorder (SUD), and OUD. We also requested recommendations for relevant material from experts in this area. This review yielded a total of 70 unique citations and 29 unique interventions, which can be sub-divided into five categories: smartphone applications (nÃ‚Â =Ã‚Â 12); healthcare data-related interventions (nÃ‚Â =Ã‚Â 7); biosensor-related interventions (nÃ‚Â =Ã‚Â 5); digital and virtual-related interventions (nÃ‚Â =Ã‚Â 2); and 'other', i.e., those that cannot be classified in these categories (nÃ‚Â =Ã‚Â 3). While the majority have not undergone rigorous scientific evaluation via randomized controlled trials, several AI interventions showed promise in aiding the identification of escalating opioid use patterns, informing the treatment of OUD, and detecting opioid-induced respiratory depression. This is the first gray literature synthesis to characterize the current 'landscape' of OUD-specific AI interventions. Future research should continue to assess the usability, utility, acceptability and efficacy of these interventions, in addition to the overall legal, ethical, and social implications of OUD-specific AI interventions.","With the artificial intelligence (AI) paradigm shift comes momentum toward the development and scale-up of novel AI interventions to aid in opioid use disorder (OUD) care, in the identification of overdose risk, and in the reversal of overdose. As OUD-specific AI interventions are relatively recent, dynamic, and may not yet be captured in the peer-reviewed literature, we conducted a review of the gray literature to identify literature pertaining to OUD-specific AI interventions being developed, implemented and evaluated. Gray literature databases, customized Google searches, and targeted websites were searched from January 2013 to October 2019. Search terms include: AI, machine learning, substance use disorder (SUD), and OUD. We also requested recommendations for relevant material from experts in this area. This review yielded a total of 70 unique citations and 29 unique interventions, which can be sub-divided into five categories: smartphone applications (nÂ =Â 12); healthcare data-related interventions (nÂ =Â 7); biosensor-related interventions (nÂ =Â 5); digital and virtual-related interventions (nÂ =Â 2); and 'other', i.e., those that cannot be classified in these categories (nÂ =Â 3). While the majority have not undergone rigorous scientific evaluation via randomized controlled trials, several AI interventions showed promise in aiding the identification of escalating opioid use patterns, informing the treatment of OUD, and detecting opioid-induced respiratory depression. This is the first gray literature synthesis to characterize the current 'landscape' of OUD-specific AI interventions. Future research should continue to assess the usability, utility, acceptability and efficacy of these interventions, in addition to the overall legal, ethical, and social implications of OUD-specific AI interventions.","Beaulieu, Knight, Nolan, Quick, Ti","Beaulieu, Knight, Nolan, Quick, Ti",https://doi.org/10.1080/00952990.2020.1817466,https://doi.org/10.1080/00952990.2020.1817466,2021-08-03
16088.0,pubmed,pubmed,Deep learning networks on chronic liver disease assessment with fine-tuning of shear wave elastography image sequences,Deep learning networks on chronic liver disease assessment with fine-tuning of shear wave elastography image sequences,"Chronic Liver Disease (CLD) is currently one of the major causes of death worldwide. If not treated, it may lead to cirrhosis, hepatic carcinoma, and death. Ultrasound (US) Shear Wave Elastography (SWE) is a relatively new, popular, non-invasive technique among radiologists. Although many studies have been published validating the SWE technique either in a clinical setting, or by applying machine learning on SWE elastograms, minimal work has been made on comparing the performance of popular pre-trained deep learning networks on CLD assessment. Currently available literature reports suggest technical advancements on specific deep learning structures, with specific inputs and usually on a limited CLD fibrosis stage class group, with limited comparison on competitive deep learning schemes fed with different input types. The aim of the present study is to compare some popular deep learning pre-trained networks using temporally stable and full elastograms, with or without augmentation as well as propose suitable deep learning schemes for CLD diagnosis and progress assessment. 200 liver biopsy validated patients with CLD, underwent US SWE examination. Four images from the same liver area were saved to extract elastograms and processed to exclude areas that were temporally unstable. Then, full and temporally stable masked elastograms for each patient were separately fed into GoogLeNet, AlexNet, VGG16, ResNet50 and DenseNet201 with and without augmentation. The networks were tested for differentiation of CLD stages in seven classification schemes over 30 repetitions using liver biopsy as reference. All Networks achieved maximum mean accuracies ranging from 87.2%-97.4% and AUCs ranging from 0.979-0.990 while the radiologists had AUCs ranging from 0.800-0.870. ResNet50 and DenseNet201 had better average performance than the other networks. The use of the temporal stability mask led to improved performance on about 50% of inputs and networks combinations while augmentation led to lower performance for all networks. These findings can provide potential networks with higher accuracy and better setting in the CLD diagnosis and progress assessment. A larger dataset would help identify the best network and settings for CLD assessment in clinical practice.","Chronic liver disease (CLD) is currently one of the major causes of death worldwide. If not treated, it may lead to cirrhosis, hepatic carcinoma and death. Ultrasound (US) shear wave elastography (SWE) is a relatively new, popular, non-invasive technique among radiologists. Although many studies have been published validating the SWE technique either in a clinical setting, or by applying machine learning on SWE elastograms, minimal work has been done on comparing the performance of popular pre-trained deep learning networks on CLD assessment. Currently available literature reports suggest technical advancements on specific deep learning structures, with specific inputs and usually on a limited CLD fibrosis stage class group, with limited comparison on competitive deep learning schemes fed with different input types. The aim of the present study is to compare some popular deep learning pre-trained networks using temporally stable and full elastograms, with or without augmentation as well as propose suitable deep learning schemes for CLD diagnosis and progress assessment. 200 liver biopsy validated patients with CLD, underwent US SWE examination. Four images from the same liver area were saved to extract elastograms and processed to exclude areas that were temporally unstable. Then, full and temporally stable masked elastograms for each patient were separately fed into GoogLeNet, AlexNet, VGG16, ResNet50 and DenseNet201 with and without augmentation. The networks were tested for differentiation of CLD stages in seven classification schemes over 30 repetitions using liver biopsy as the reference. All networks achieved maximum mean accuracies ranging from 87.2%-97.4% and area under the receiver operating characteristic curves (AUCs) ranging from 0.979-0.990 while the radiologists had AUCs ranging from 0.800-0.870. ResNet50 and DenseNet201 had better average performance than the other networks. The use of the temporal stability mask led to improved performance on about 50% of inputs and network combinations while augmentation led to lower performance for all networks. These findings can provide potential networks with higher accuracy and better setting in the CLD diagnosis and progress assessment. A larger data set would help identify the best network and settings for CLD assessment in clinical practice.","Kagadis, Drazinos, Gatos, Tsantis, Papadimitroulas, Spiliopoulos, Karnabatidis, Theotokas, Zoumpoulis, Hazle","Kagadis, Drazinos, Gatos, Tsantis, Papadimitroulas, Spiliopoulos, Karnabatidis, Theotokas, Zoumpoulis, Hazle",https://doi.org/10.1088/1361-6560/abae06,https://doi.org/10.1088/1361-6560/abae06,2021-08-03
16093.0,pubmed,pubmed,The Effects of Acupuncture on Cancer-Related Fatigue: Updated Systematic Review and Meta-Analysis,The Effects of Acupuncture on Cancer-Related Fatigue: Updated Systematic Review and Meta-Analysis,"Several studies have identified fatigue as one of the major symptoms experienced during and after cancer treatment. However, there are limited options to manage cancer related fatigue (CRF) with pharmacological interventions. Several acupuncture studies suggested that acupuncture has a positive impact on CRF. This review aims to assess the evidence of acupuncture for the treatment of CRF. Electronic database searches were conducted on 4 English databases (Medline, PubMed, Embase, and ScienceDirect). Search keywords were; &quot;acupuncture&quot; and &quot;cancer,&quot; or &quot;cancer related fatigue.&quot; Studies published as full text randomized controlled trials (RCTs) in English were included. Estimates of change in fatigue cores were pooled using a random effects meta-analysis where randomized comparisons were available for true acupuncture versus sham acupuncture and true acupuncture versus usual care. The quality of original papers were assessed using the Cochrane Collaboration's tool for assessing risk of bias (ROB). Nine RCTs were selected for review with a total of 809 participants and a range of 13 to 302 participants within the studies. Six RCTs reported significant improvement of CRF for the acupuncture intervention compared to the control groups. Pooled estimates suggest Brief Fatigue Inventory scores are 0.93Ã¢â‚¬â€°points lower 95% CI (-1.65, -0.20) in true acupuncture versus sham acupuncture and 2.12Ã¢â‚¬â€°points lower 95% C (-3.21, -1.04) in true acupuncture versus usual care. Six studies had low risk of bias (ROB) and 3 studies had a moderate ROB predominantly in blinding of participants, blinding of assessors and incomplete data outcomes. Among the 9 RCTs, 2 studies have reported the occurrence of minor adverse effects (spot bleeding and bruising) related to acupuncture treatment. No serious adverse reactions related to acupuncture were reported. The current literature review suggests that acupuncture has therapeutic potential in management of CRF for cancer survivors. Promotion of acupuncture in cancer care to manage CRF may improve the quality of life of cancer survivors.","Several studies have identified fatigue as one of the major symptoms experienced during and after cancer treatment. However, there are limited options to manage cancer related fatigue (CRF) with pharmacological interventions. Several acupuncture studies suggested that acupuncture has a positive impact on CRF. This review aims to assess the evidence of acupuncture for the treatment of CRF. Electronic database searches were conducted on 4 English databases (Medline, PubMed, Embase, and ScienceDirect). Search keywords were; ""acupuncture"" and ""cancer,"" or ""cancer related fatigue."" Studies published as full text randomized controlled trials (RCTs) in English were included. Estimates of change in fatigue cores were pooled using a random effects meta-analysis where randomized comparisons were available for true acupuncture versus sham acupuncture and true acupuncture versus usual care. The quality of original papers were assessed using the Cochrane Collaboration's tool for assessing risk of bias (ROB). Nine RCTs were selected for review with a total of 809 participants and a range of 13 to 302 participants within the studies. Six RCTs reported significant improvement of CRF for the acupuncture intervention compared to the control groups. Pooled estimates suggest Brief Fatigue Inventory scores are 0.93â€‰points lower 95% CI (-1.65, -0.20) in true acupuncture versus sham acupuncture and 2.12â€‰points lower 95% C (-3.21, -1.04) in true acupuncture versus usual care. Six studies had low risk of bias (ROB) and 3 studies had a moderate ROB predominantly in blinding of participants, blinding of assessors and incomplete data outcomes. Among the 9 RCTs, 2 studies have reported the occurrence of minor adverse effects (spot bleeding and bruising) related to acupuncture treatment. No serious adverse reactions related to acupuncture were reported. The current literature review suggests that acupuncture has therapeutic potential in management of CRF for cancer survivors. Promotion of acupuncture in cancer care to manage CRF may improve the quality of life of cancer survivors.","Jang, Brown, Lamoury, Morgia, Boyle, Marr, Clarke, Back, Oh","Jang, Brown, Lamoury, Morgia, Boyle, Marr, Clarke, Back, Oh",https://doi.org/10.1177/1534735420949679,https://doi.org/10.1177/1534735420949679,2021-08-03
16095.0,pubmed,pubmed,Early-rectal Cancer Treatment: A Decision-tree Making Based on Systematic Review and Meta-analysis,Early-rectal Cancer Treatment: A Decision-tree Making Based on Systematic Review and Meta-analysis,"Local excision (LE) has arisen as an alternative to total mesorectal excision for the treatment of early rectal cancer. Despite a decreased morbidity, there are still concerns about LE outcomes. This systematic-review and meta-analysis design is based on the &quot;PICO&quot; process, aiming to answer to three questions related to LE as primary treatment for early-rectal cancer, the optimal method for LE, and the potential role for completion treatment in high-risk histology tumors and outcomes of salvage surgery. The results revealed that reported overall survival (OS) and disease-specific survival (DSS) were 71%-91.7% and 80%-94% for LE, in contrast to 92.3%-94.3% and 94.4%-97% for radical surgery. Additional analysis of National Database studies revealed lower OS with LE (HR: 1.26; 95%CI, 1.09-1.45) and DSS (HR: 1.19; 95%CI, 1.01-1.41) after LE. Furthermore, patients receiving LE were significantly more prone develop local recurrence (RR: 3.44, 95%CI, 2.50-4.74). Analysis of available transanal surgical platforms was performed, finding no significant differences among them but reduced local recurrence compared to traditional transanal LE (OR:0.24;95%CI, 0.15-0.4). Finally, we found poor survival outcomes for patients undergoing salvage surgery, favoring completion treatment (chemoradiotherapy or surgery) when high-risk histology is present. In conclusion, LE could be considered adequate provided a full-thickness specimen can be achieved that the patient is informed about risk for potential requirement of completion treatment. Early-rectal cancer cases should be discussed in a multidisciplinary team, and patient's preferences must be considered in the decision-making process.","Local excision (LE) has arisen as an alternative to total mesorectal excision for the treatment of early rectal cancer. Despite a decreased morbidity, there are still concerns about LE outcomes. This systematic-review and meta-analysis design is based on the ""PICO"" process, aiming to answer to three questions related to LE as primary treatment for early-rectal cancer, the optimal method for LE, and the potential role for completion treatment in high-risk histology tumors and outcomes of salvage surgery. The results revealed that reported overall survival (OS) and disease-specific survival (DSS) were 71%-91.7% and 80%-94% for LE, in contrast to 92.3%-94.3% and 94.4%-97% for radical surgery. Additional analysis of National Database studies revealed lower OS with LE (HR: 1.26; 95%CI, 1.09-1.45) and DSS (HR: 1.19; 95%CI, 1.01-1.41) after LE. Furthermore, patients receiving LE were significantly more prone develop local recurrence (RR: 3.44, 95%CI, 2.50-4.74). Analysis of available transanal surgical platforms was performed, finding no significant differences among them but reduced local recurrence compared to traditional transanal LE (OR:0.24;95%CI, 0.15-0.4). Finally, we found poor survival outcomes for patients undergoing salvage surgery, favoring completion treatment (chemoradiotherapy or surgery) when high-risk histology is present. In conclusion, LE could be considered adequate provided a full-thickness specimen can be achieved that the patient is informed about risk for potential requirement of completion treatment. Early-rectal cancer cases should be discussed in a multidisciplinary team, and patient's preferences must be considered in the decision-making process.","Aguirre-Allende, Enriquez-Navascues, Elorza-Echaniz, Etxart-Lopetegui, Borda-Arrizabalaga, Saralegui Ansorena, Placer-Galan","Aguirre-Allende, Enriquez-Navascues, Elorza-Echaniz, Etxart-Lopetegui, Borda-Arrizabalaga, Saralegui Ansorena, Placer-Galan",https://doi.org/10.1016/j.ciresp.2020.05.035,https://doi.org/10.1016/j.ciresp.2020.05.035,2021-08-03
16097.0,pubmed,pubmed,Pharmacokinetic Drug-drug Interaction of Antibiotics Used in Sepsis Care in China,Pharmacokinetic Drug-drug Interaction of Antibiotics Used in Sepsis Care in China,"Many antibiotics have a high potential for having an interaction with drugs, as perpetrator and/or victim, in critically ill patients, and particularly in sepsis patients. The aim of this review is to summarize the pharmacokinetic drug-drug interaction (DDI) of 45 antibiotics commonly used in sepsis care in China. Literature mining was conducted to obtain human pharmacokinetics/dispositions of the antibiotics, their interactions with drug metabolizing enzymes or transporters, and their associated clinical drug interactions. Potential DDI is indicated by a DDI index &gt; 0.1 for inhibition or a treated-cell/untreated-cell ratio of enzyme activity being &gt; 2 for induction. The literature-mined information on human pharmacokinetics of the identified antibiotics and their potential drug interactions is summarized. Antibiotic-perpetrated drug interactions, involving P450 enzyme inhibition, have been reported for four lipophilic antibacterials (ciprofloxacin, erythromycin, trimethoprim, and trimethoprim-sulfamethoxazole) and three lipophilic antifungals (fluconazole, itraconazole, and voriconazole). In addition, seven hydrophilic antibacterials (ceftriaxone, cefamandole, piperacillin, penicillin G, amikacin, metronidazole, and linezolid) inhibit drug transporters in vitro. Despite no reported clinical PK drug interactions with the transporters, caution is advised in the use of these antibacterials. Eight hydrophilic antibacterials (all ÃŽÂ²-lactams; meropenem, cefotaxime, cefazolin, piperacillin, ticarcillin, penicillin G, ampicillin, and flucloxacillin), are potential victims of drug interactions due to transporter inhibition. Rifampin is reported to perpetrate drug interactions by inducing CYP3A or inhibiting OATP1B; it is also reported to be a victim of drug interactions, due to the dual inhibition of CYP3A4 and OATP1B by indinavir. In addition, three antifungals (caspofungin, itraconazole, and voriconazole) are reported to be victims of drug interactions because of P450 enzyme induction. Reports for other antibiotics acting as victims in drug interactions are scarce.","Many antibiotics have a high potential for interactions with drugs, as a perpetrator and/or victim, in critically ill patients, and particularly in sepsis patients. The aim of this review is to summarize the pharmacokinetic drug-drug interaction (DDI) of 45 antibiotics commonly used in sepsis care in China. Literature search was conducted to obtain human pharmacokinetics/ dispositions of the antibiotics, their interactions with drug-metabolizing enzymes or transporters, and their associated clinical drug interactions. Potential DDI is indicated by a DDI index â‰¥ 0.1 for inhibition or a treatedcell/ untreated-cell ratio of enzyme activity being â‰¥ 2 for induction. The literature-mined information on human pharmacokinetics of the identified antibiotics and their potential drug interactions is summarized. Antibiotic-perpetrated drug interactions, involving P450 enzyme inhibition, have been reported for four lipophilic antibacterials (ciprofloxacin, erythromycin, trimethoprim, and trimethoprim-sulfamethoxazole) and three antifungals (fluconazole, itraconazole, and voriconazole). In addition, seven hydrophilic antibacterials (ceftriaxone, cefamandole, piperacillin, penicillin G, amikacin, metronidazole, and linezolid) inhibit drug transporters in vitro. Despite no clinical PK drug interactions with the transporters, caution is advised in the use of these antibacterials. Eight hydrophilic antibiotics (all Î²-lactams; meropenem, cefotaxime, cefazolin, piperacillin, ticarcillin, penicillin G, ampicillin, and flucloxacillin), are potential victims of drug interactions due to transporter inhibition. Rifampin is reported to perpetrate drug interactions by inducing CYP3A or inhibiting OATP1B; it is also reported to be a victim of drug interactions, due to the dual inhibition of CYP3A4 and OATP1B by indinavir. In addition, three antifungals (caspofungin, itraconazole, and voriconazole) are reported to be victims of drug interactions because of P450 enzyme induction. Reports for other antibiotics acting as victims in drug interactions are scarce.","Yu, Chu, Li, He, Wang, Cheng","Yu, Chu, Li, He, Wang, Cheng",https://doi.org/10.2174/1389200221666200929115117,https://doi.org/10.2174/1389200221666200929115117,2021-08-03
16098.0,pubmed,pubmed,Claims-Based Algorithms for Identifying Patients With Pulmonary Hypertension: A Comparison of Decision Rules and Machine-Learning Approaches,Claims-Based Algorithms for Identifying Patients With Pulmonary Hypertension: A Comparison of Decision Rules and Machine-Learning Approaches,"Background Real-world healthcare data are an important resource for epidemiologic research. However, accurate identification of patient cohorts-a crucial first step underpinning the validity of research results-remains a challenge. We developed and evaluated claims-based case ascertainment algorithms for pulmonary hypertension (PH), comparing conventional decision rules with state-of-the-art machine-learning approaches. Methods and Results We analyzed an electronic health record-Medicare linked database from two large academic tertiary care hospitals (years 2007-2013). Electronic health record charts were reviewed to form a gold standard cohort of patients with (n=386) and without PH (n=164). Using health encounter data captured in Medicare claims (including patients' demographics, diagnoses, medications, and procedures), we developed and compared 2 approaches for identifying patients with PH: decision rules and machine-learning algorithms using penalized lasso regression, random forest, and gradient boosting machine. The most optimal rule-based algorithm-having Ã¢â€°Â¥3 PH-related healthcare encounters and having undergone right heart catheterization-attained an area under the receiver operating characteristic curve of 0.64 (sensitivity, 0.75; specificity, 0.48). All 3 machine-learning algorithms outperformed the most optimal rule-based algorithm (<i>P</i>&lt;0.001). A model derived from the random forest algorithm achieved an area under the receiver operating characteristic curve of 0.88 (sensitivity, 0.87; specificity, 0.70), and gradient boosting machine achieved comparable results (area under the receiver operating characteristic curve, 0.85; sensitivity, 0.87; specificity, 0.70). Penalized lasso regression achieved an area under the receiver operating characteristic curve of 0.73 (sensitivity, 0.70; specificity, 0.68). Conclusions Research-grade case identification algorithms for PH can be derived and rigorously validated using machine-learning algorithms. Simple decision rules commonly applied in published literature performed poorly; more complex rule-based algorithms may potentially address the limitation of this approach. PH research using claims data would be considerably strengthened through the use of validated algorithms for cohort ascertainment.","Background Real-world healthcare data are an important resource for epidemiologic research. However, accurate identification of patient cohorts-a crucial first step underpinning the validity of research results-remains a challenge. We developed and evaluated claims-based case ascertainment algorithms for pulmonary hypertension (PH), comparing conventional decision rules with state-of-the-art machine-learning approaches. Methods and Results We analyzed an electronic health record-Medicare linked database from two large academic tertiary care hospitals (years 2007-2013). Electronic health record charts were reviewed to form a gold standard cohort of patients with (n=386) and without PH (n=164). Using health encounter data captured in Medicare claims (including patients' demographics, diagnoses, medications, and procedures), we developed and compared 2 approaches for identifying patients with PH: decision rules and machine-learning algorithms using penalized lasso regression, random forest, and gradient boosting machine. The most optimal rule-based algorithm-having â‰¥3 PH-related healthcare encounters and having undergone right heart catheterization-attained an area under the receiver operating characteristic curve of 0.64 (sensitivity, 0.75; specificity, 0.48). All 3 machine-learning algorithms outperformed the most optimal rule-based algorithm (<i>P</i>&lt;0.001). A model derived from the random forest algorithm achieved an area under the receiver operating characteristic curve of 0.88 (sensitivity, 0.87; specificity, 0.70), and gradient boosting machine achieved comparable results (area under the receiver operating characteristic curve, 0.85; sensitivity, 0.87; specificity, 0.70). Penalized lasso regression achieved an area under the receiver operating characteristic curve of 0.73 (sensitivity, 0.70; specificity, 0.68). Conclusions Research-grade case identification algorithms for PH can be derived and rigorously validated using machine-learning algorithms. Simple decision rules commonly applied in published literature performed poorly; more complex rule-based algorithms may potentially address the limitation of this approach. PH research using claims data would be considerably strengthened through the use of validated algorithms for cohort ascertainment.","Ong, Klann, Lin, Maron, Murphy, Natter, Mandl","Ong, Klann, Lin, Maron, Murphy, Natter, Mandl",https://doi.org/10.1161/JAHA.120.016648,https://doi.org/10.1161/JAHA.120.016648,2021-08-03
16099.0,pubmed,pubmed,"The cost-effectiveness of isavuconazole compared to voriconazole, the standard of care in the treatment of patients with invasive mould diseases, prior to differential pathogen diagnosis in Spain","The cost-effectiveness of isavuconazole compared to voriconazole, the standard of care in the treatment of patients with invasive mould diseases, prior to differential pathogen diagnosis in Spain","Invasive mould diseases are associated with high morbidity, mortality and economic impact. Its treatment is often started prior to differential pathogen diagnosis. Isavuconazole is approved for treatment of invasive aspergillosis (IA) and invasive mucormycosis (IM) when amphotericin-B is not indicated. To estimate the cost-effectiveness of isavuconazole vs voriconazole for the treatment of adult patients with possible IA prior to differential pathogen diagnosis, in Spain. A decision tree analysis was performed using the Spanish Healthcare System perspective. Among all patients with possible IA, it was considered that 7.81% actually had IM. Costs for laboratory analysis, management of adverse events, hospitalisation and drugs per patient, deaths and long-term effects in life years (LYs) and quality-adjusted LYs (QALYs) were considered. Efficacy data were obtained from clinical trials and utilities from the literature. Deterministic and probabilistic sensitivity analyses (PSA) were conducted. In patients with possible IA and when compared to voricanozole, isavuconazole showed an incremental cost of 4758.53Ã¢â€šÂ¬, besides an incremental effectiveness of +0.49 LYs and +0.41 QALYs per patient. The Incremental Cost Effectiveness Ratio was 9622.52Ã¢â€šÂ¬ per LY gained and 11,734.79Ã¢â€šÂ¬ per QALY gained. The higher cost of isavuconazole was due to drug acquisition. Main parameters influencing results were mortality, treatment duration and hospitalisation days. The PSA results showed that isavuconazole has a probability of being cost-effective of 67.34%, being dominant in 24.00% of cases. Isavuconazole is a cost-effective treatment compared to voriconazole for patients with possible IA for a willingness to pay threshold of 25,000Ã¢â€šÂ¬ per additional QALY.","Invasive mould diseases are associated with high morbidity, mortality and economic impact. Its treatment is often started prior to differential pathogen diagnosis. Isavuconazole is approved for treatment of invasive aspergillosis (IA) and invasive mucormycosis (IM) when amphotericin-B is not indicated. To estimate the cost-effectiveness of isavuconazole vs voriconazole for the treatment of adult patients with possible IA prior to differential pathogen diagnosis, in Spain. A decision tree analysis was performed using the Spanish Healthcare System perspective. Among all patients with possible IA, it was considered that 7.81% actually had IM. Costs for laboratory analysis, management of adverse events, hospitalisation and drugs per patient, deaths and long-term effects in life years (LYs) and quality-adjusted LYs (QALYs) were considered. Efficacy data were obtained from clinical trials and utilities from the literature. Deterministic and probabilistic sensitivity analyses (PSA) were conducted. In patients with possible IA and when compared to voricanozole, isavuconazole showed an incremental cost of 4758.53â‚¬, besides an incremental effectiveness of +0.49 LYs and +0.41 QALYs per patient. The Incremental Cost Effectiveness Ratio was 9622.52â‚¬ per LY gained and 11,734.79â‚¬ per QALY gained. The higher cost of isavuconazole was due to drug acquisition. Main parameters influencing results were mortality, treatment duration and hospitalisation days. The PSA results showed that isavuconazole has a probability of being cost-effective of 67.34%, being dominant in 24.00% of cases. Isavuconazole is a cost-effective treatment compared to voriconazole for patients with possible IA for a willingness to pay threshold of 25,000â‚¬ per additional QALY.","Azanza, Grau, VÃƒÂ¡zquez, Rebollo, Peral, LÃƒÂ³pez-IbÃƒÂ¡ÃƒÂ±ez de Aldecoa, LÃƒÂ³pez-GÃƒÂ³mez","Azanza, Grau, VÃ¡zquez, Rebollo, Peral, LÃ³pez-IbÃ¡Ã±ez de Aldecoa, LÃ³pez-GÃ³mez",https://doi.org/10.1111/myc.13189,https://doi.org/10.1111/myc.13189,2021-08-03
16101.0,pubmed,pubmed,Occupational noise-induced hearing loss in China: a systematic review and meta-analysis,Occupational noise-induced hearing loss in China: a systematic review and meta-analysis,"Most of the Chinese occupational population are becoming at risk of noise-induced hearing loss (NIHL). However, there is a limited number of literature reviews on occupational NIHL in China. This study aimed to analyse the prevalence and characteristics of occupational NIHL in the Chinese population using data from relevant studies. Systematic review and meta-analysis. From December 2019 to February 2020, we searched the literature through databases, including Web of Science, PubMed, MEDLINE, Scopus, the China National Knowledge Internet, Chinese Sci-Tech Journal Database (weip.com), WanFang Database and China United Library Database, for studies on NIHL in China published in 1993-2019 and analysed the correlation between NIHL and occupational exposure to noise, including exposure to complex noise and coexposure to noise and chemicals. A total of 71 865 workers aged 33.5Ã‚Â±8.7 years were occupationally exposed to 98.6Ã‚Â±7.2 dB(A) (A-weighted decibels) noise for a duration of 9.9Ã‚Â±8.4 years in the transportation, mining and typical manufacturing industries. The prevalence of occupational NIHL in China was 21.3%, of which 30.2% was related to high-frequency NIHL (HFNIHL), 9.0% to speech-frequency NIHL and 5.8% to noise-induced deafness. Among manufacturing workers, complex noise contributed to greater HFNIHL than Gaussian noise (overall weighted OR (OR)=1.95). Coexposure to noise and chemicals such as organic solvents, welding fumes, carbon monoxide and hydrogen sulfide led to greater HFNIHL than noise exposure alone (overall weighted OR=2.36). Male workers were more likely to experience HFNIHL than female workers (overall weighted OR=2.26). Age, noise level and exposure duration were also risk factors for HFNIHL (overall weighted OR=1.35, 5.63 and 1.75, respectively). The high prevalence of occupational NIHL in China was related to the wide distribution of noise in different industries as well as high-level and long-term noise exposure. The prevalence was further aggravated by exposure to complex noise or coexposure to noise and specific chemicals. Additional efforts are needed to reduce occupational noise exposure in China.","Most of the Chinese occupational population are becoming at risk of noise-induced hearing loss (NIHL). However, there is a limited number of literature reviews on occupational NIHL in China. This study aimed to analyse the prevalence and characteristics of occupational NIHL in the Chinese population using data from relevant studies. Systematic review and meta-analysis. From December 2019 to February 2020, we searched the literature through databases, including Web of Science, PubMed, MEDLINE, Scopus, the China National Knowledge Internet, Chinese Sci-Tech Journal Database (weip.com), WanFang Database and China United Library Database, for studies on NIHL in China published in 1993-2019 and analysed the correlation between NIHL and occupational exposure to noise, including exposure to complex noise and coexposure to noise and chemicals. A total of 71 865 workers aged 33.5Â±8.7 years were occupationally exposed to 98.6Â±7.2 dB(A) (A-weighted decibels) noise for a duration of 9.9Â±8.4 years in the transportation, mining and typical manufacturing industries. The prevalence of occupational NIHL in China was 21.3%, of which 30.2% was related to high-frequency NIHL (HFNIHL), 9.0% to speech-frequency NIHL and 5.8% to noise-induced deafness. Among manufacturing workers, complex noise contributed to greater HFNIHL than Gaussian noise (overall weighted OR (OR)=1.95). Coexposure to noise and chemicals such as organic solvents, welding fumes, carbon monoxide and hydrogen sulfide led to greater HFNIHL than noise exposure alone (overall weighted OR=2.36). Male workers were more likely to experience HFNIHL than female workers (overall weighted OR=2.26). Age, noise level and exposure duration were also risk factors for HFNIHL (overall weighted OR=1.35, 5.63 and 1.75, respectively). The high prevalence of occupational NIHL in China was related to the wide distribution of noise in different industries as well as high-level and long-term noise exposure. The prevalence was further aggravated by exposure to complex noise or coexposure to noise and specific chemicals. Additional efforts are needed to reduce occupational noise exposure in China.","Zhou, Shi, Zhou, Hu, Zhang","Zhou, Shi, Zhou, Hu, Zhang",https://doi.org/10.1136/bmjopen-2020-039576,https://doi.org/10.1136/bmjopen-2020-039576,2021-08-03
16106.0,pubmed,pubmed,A validation study revealed differences in design and performance of search filters for qualitative research in PsycINFO and CINAHL,A validation study revealed differences in design and performance of search filters for qualitative research in PsycINFO and CINAHL,"Search filters can support qualitative evidence of information retrieval. Various search filters are available for the bibliographic databases PsycINFO and CINAHL. To date, no comparative overview of validation results of search filters verified with an independent gold standard exists. Identified search filters for PsycINFO and CINAHL were tested for plausibility. Gold standards were generated according to the relative recall approach using references included in an overview of systematic reviews of qualitative studies. All included references were collected and checked for indexing in PsycINFO and CINAHL. Validation tests for each search filter were conducted in both databases to determine whether the references of the gold standards could be retrieved or not. Twelve search filters for PsycINFO and fifteen for CINAHL were validated. The complexity and design of these search filters vary, as well as the validation results for the databases. When locating primary studies of qualitative research, the best sensitivity and precision ratio (among filters with a sensitivity of &gt;80%) was achieved with a filter by McKibbon etÃ‚Â al. for PsycINFO and a filter by Wilczynski etÃ‚Â al. for CINAHL. Project-specific requirements and resources influence the choice of a specific search filter for PsycINFO and CINAHL.","Search filters can support qualitative evidence of information retrieval. Various search filters are available for the bibliographic databases PsycINFO and CINAHL. To date, no comparative overview of validation results of search filters verified with an independent gold standard exists. Identified search filters for PsycINFO and CINAHL were tested for plausibility. Gold standards were generated according to the relative recall approach using references included in an overview of systematic reviews of qualitative studies. All included references were collected and checked for indexing in PsycINFO and CINAHL. Validation tests for each search filter were conducted in both databases to determine whether the references of the gold standards could be retrieved or not. Twelve search filters for PsycINFO and fifteen for CINAHL were validated. The complexity and design of these search filters vary, as well as the validation results for the databases. When locating primary studies of qualitative research, the best sensitivity and precision ratio (among filters with a sensitivity of &gt;80%) was achieved with a filter by McKibbon etÂ al. for PsycINFO and a filter by Wilczynski etÂ al. for CINAHL. Project-specific requirements and resources influence the choice of a specific search filter for PsycINFO and CINAHL.","Rosumeck, Wagner, Wallraf, Euler","Rosumeck, Wagner, Wallraf, Euler",https://doi.org/10.1016/j.jclinepi.2020.09.031,https://doi.org/10.1016/j.jclinepi.2020.09.031,2021-08-03
16109.0,pubmed,pubmed,Application of artificial intelligence models and optimization algorithms in plant cell and tissue culture,Application of artificial intelligence models and optimization algorithms in plant cell and tissue culture,"Artificial intelligence (AI) models and optimization algorithms (OA) are broadly employed in different fields of technology and science and have recently been applied to improve different stages of plant tissue culture. The usefulness of the application of AI-OA has been demonstrated in the prediction and optimization of length and number of microshoots or roots, biomass in plant cell cultures or hairy root culture, and optimization of environmental conditions to achieve maximum productivity and efficiency, as well as classification of microshoots and somatic embryos. Despite its potential, the use of AI and OA in this field has been limited due to complex definition terms and computational algorithms. Therefore, a systematic review to unravel modeling and optimizing methods is important for plant researchers and has been acknowledged in this study. First, the main steps for AI-OA development (from data selection to evaluation of prediction and classification models), as well as several AI models such as artificial neural networks (ANNs), neurofuzzy logic, support vector machines (SVMs), decision trees, random forest (FR), and genetic algorithms (GA), have been represented. Then, the application of AI-OA models in different steps of plant tissue culture has been discussed and highlighted. This review also points out limitations in the application of AI-OA in different plant tissue culture processes and provides a new view for future study objectives. KEY POINTS: Ã¢â‚¬Â¢ Artificial intelligence models and optimization algorithms can be considered a novel and reliable computational method in plant tissue culture. Ã¢â‚¬Â¢ This review provides the main steps and concepts for model development. Ã¢â‚¬Â¢ The application of machine learning algorithms in different steps of plant tissue culture has been discussed and highlighted.","Artificial intelligence (AI) models and optimization algorithms (OA) are broadly employed in different fields of technology and science and have recently been applied to improve different stages of plant tissue culture. The usefulness of the application of AI-OA has been demonstrated in the prediction and optimization of length and number of microshoots or roots, biomass in plant cell cultures or hairy root culture, and optimization of environmental conditions to achieve maximum productivity and efficiency, as well as classification of microshoots and somatic embryos. Despite its potential, the use of AI and OA in this field has been limited due to complex definition terms and computational algorithms. Therefore, a systematic review to unravel modeling and optimizing methods is important for plant researchers and has been acknowledged in this study. First, the main steps for AI-OA development (from data selection to evaluation of prediction and classification models), as well as several AI models such as artificial neural networks (ANNs), neurofuzzy logic, support vector machines (SVMs), decision trees, random forest (FR), and genetic algorithms (GA), have been represented. Then, the application of AI-OA models in different steps of plant tissue culture has been discussed and highlighted. This review also points out limitations in the application of AI-OA in different plant tissue culture processes and provides a new view for future study objectives. KEY POINTS: â€¢ Artificial intelligence models and optimization algorithms can be considered a novel and reliable computational method in plant tissue culture. â€¢ This review provides the main steps and concepts for model development. â€¢ The application of machine learning algorithms in different steps of plant tissue culture has been discussed and highlighted.","Hesami, Jones","Hesami, Jones",https://doi.org/10.1007/s00253-020-10888-2,https://doi.org/10.1007/s00253-020-10888-2,2021-08-03
16110.0,pubmed,pubmed,A machine learning-based clinical decision support system to identify prescriptions with a high risk of medication error,A machine learning-based clinical decision support system to identify prescriptions with a high risk of medication error,"To improve patient safety and clinical outcomes by reducing the risk of prescribing errors, we tested the accuracy of a hybrid clinical decision support system in prioritizing prescription checks. Data from electronic health records were collated over a period of 18 months. Inferred scores at a patient level (probability of a patient's set of active orders to require a pharmacist review) were calculated using a hybrid approach (machine learning and a rule-based expert system). A clinical pharmacist analyzed randomly selected prescription orders over a 2-week period to corroborate our findings. Predicted scores were compared with the pharmacist's review using the area under the receiving-operating characteristic curve and area under the precision-recall curve. These metrics were compared with existing tools: computerized alerts generated by a clinical decision support (CDS) system and a literature-based multicriteria query prioritization technique. Data from 10Ã‚Â 716 individual patients (133Ã‚Â 179 prescription orders) were used to train the algorithm on the basis of 25 features in a development dataset. While the pharmacist analyzed 412 individual patients (3364 prescription orders) in an independent validation dataset, the areas under the receiving-operating characteristic and precision-recall curves of our digital system were 0.81 and 0.75, respectively, thus demonstrating greater accuracy than the CDS system (0.65 and 0.56, respectively) and multicriteria query techniques (0.68 and 0.56, respectively). Our innovative digital tool was notably more accurate than existing techniques (CDS system and multicriteria query) at intercepting potential prescription errors. By primarily targeting high-risk patients, this novel hybrid decision support system improved the accuracy and reliability of prescription checks in a hospital setting.","To improve patient safety and clinical outcomes by reducing the risk of prescribing errors, we tested the accuracy of a hybrid clinical decision support system in prioritizing prescription checks. Data from electronic health records were collated over a period of 18 months. Inferred scores at a patient level (probability of a patient's set of active orders to require a pharmacist review) were calculated using a hybrid approach (machine learning and a rule-based expert system). A clinical pharmacist analyzed randomly selected prescription orders over a 2-week period to corroborate our findings. Predicted scores were compared with the pharmacist's review using the area under the receiving-operating characteristic curve and area under the precision-recall curve. These metrics were compared with existing tools: computerized alerts generated by a clinical decision support (CDS) system and a literature-based multicriteria query prioritization technique. Data from 10Â 716 individual patients (133Â 179 prescription orders) were used to train the algorithm on the basis of 25 features in a development dataset. While the pharmacist analyzed 412 individual patients (3364 prescription orders) in an independent validation dataset, the areas under the receiving-operating characteristic and precision-recall curves of our digital system were 0.81 and 0.75, respectively, thus demonstrating greater accuracy than the CDS system (0.65 and 0.56, respectively) and multicriteria query techniques (0.68 and 0.56, respectively). Our innovative digital tool was notably more accurate than existing techniques (CDS system and multicriteria query) at intercepting potential prescription errors. By primarily targeting high-risk patients, this novel hybrid decision support system improved the accuracy and reliability of prescription checks in a hospital setting.","Corny, Rajkumar, Martin, Dode, LajonchÃƒÂ¨re, Billuart, BÃƒÂ©zie, Buronfosse","Corny, Rajkumar, Martin, Dode, LajonchÃ¨re, Billuart, BÃ©zie, Buronfosse",https://doi.org/10.1093/jamia/ocaa154,https://doi.org/10.1093/jamia/ocaa154,2021-08-03
16115.0,pubmed,pubmed,Systematic review of sarcomas radiomics studies: Bridging the gap between concepts and clinical applications?,Systematic review of sarcomas radiomics studies: Bridging the gap between concepts and clinical applications?,"Sarcomas are a model for intra- and inter-tumoral heterogeneities making them particularly suitable for radiomics analyses. Our purposes were to review the aims, methods and results of radiomics studies involving sarcomas METHODS: Pubmed and Web of Sciences databases were searched for radiomics or textural studies involving bone, soft-tissues and visceral sarcomas until June 2020. Two radiologists evaluated their objectives, results and quality of their methods, imaging pre-processing and machine-learning workflow helped by the items of the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2), Image Biomarker Standardization Initiative (IBSI) and 'Radiomics Quality Score' (RQS). Statistical analyses included inter-reader agreements, correlations between methodological assessments, scientometrics indices, and their changes over years, and between RQS, number of patients and models performance. Fifty-two studies were included involving: soft-tissue sarcomas (29/52, 55.8 %), bone sarcomas (15/52, 28.8 %), gynecological sarcomas (6/52, 11.5 %) and mixed sarcomas (2/52, 3.8 %), mostly imaged with MRI (36/52, 69.2 %), for a total of distinct patients. Median RQS was 4.5 (28.4 % of the maximum, range: -7 - 17). Performances of predictive models and number of patients negatively correlated (pÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.027). None of the studies detailed all the items from the IBSI guidelines. There was a significant increase in studies' impact factors since the establishing of the RQS in 2017 (pÃ¢â‚¬Â¯=Ã¢â‚¬Â¯0.038). Although showing promising results, further efforts are needed to make sarcoma radiomics studies reproducible with an acceptable level of evidence. A better knowledge of the RQS and IBSI reporting guidelines could improve the quality of sarcoma radiomics studies and accelerate clinical applications.","Sarcomas are a model for intra- and inter-tumoral heterogeneities making them particularly suitable for radiomics analyses. Our purposes were to review the aims, methods and results of radiomics studies involving sarcomas METHODS: Pubmed and Web of Sciences databases were searched for radiomics or textural studies involving bone, soft-tissues and visceral sarcomas until June 2020. Two radiologists evaluated their objectives, results and quality of their methods, imaging pre-processing and machine-learning workflow helped by the items of the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2), Image Biomarker Standardization Initiative (IBSI) and 'Radiomics Quality Score' (RQS). Statistical analyses included inter-reader agreements, correlations between methodological assessments, scientometrics indices, and their changes over years, and between RQS, number of patients and models performance. Fifty-two studies were included involving: soft-tissue sarcomas (29/52, 55.8 %), bone sarcomas (15/52, 28.8 %), gynecological sarcomas (6/52, 11.5 %) and mixed sarcomas (2/52, 3.8 %), mostly imaged with MRI (36/52, 69.2 %), for a total of distinct patients. Median RQS was 4.5 (28.4 % of the maximum, range: -7 - 17). Performances of predictive models and number of patients negatively correlated (pâ€¯=â€¯0.027). None of the studies detailed all the items from the IBSI guidelines. There was a significant increase in studies' impact factors since the establishing of the RQS in 2017 (pâ€¯=â€¯0.038). Although showing promising results, further efforts are needed to make sarcoma radiomics studies reproducible with an acceptable level of evidence. A better knowledge of the RQS and IBSI reporting guidelines could improve the quality of sarcoma radiomics studies and accelerate clinical applications.","CrombÃƒÂ©, Fadli, Italiano, Saut, Buy, Kind","CrombÃ©, Fadli, Italiano, Saut, Buy, Kind",https://doi.org/10.1016/j.ejrad.2020.109283,https://doi.org/10.1016/j.ejrad.2020.109283,2021-08-03
16121.0,pubmed,pubmed,Machine Learning in Nuclear Medicine: Part 2-Neural Networks and Clinical Aspects,Machine Learning in Nuclear Medicine: Part 2-Neural Networks and Clinical Aspects,"This article is the second part in our machine learning series. Part 1 provided a general overview of machine learning in nuclear medicine. Part 2 focuses on neural networks. We start with an example illustrating how neural networks work and a discussion of potential applications. Recognizing there is a spectrum of applications, we focus on recent publications in the areas of image reconstruction, low-dose PET, disease detection and models used for diagnosis and outcome prediction. Finally, since the way machine learning algorithms are reported in the literature is extremely variable, we conclude with a call to arms regarding the need for standardized reporting of design and outcome metrics and we propose a basic checklist our community might follow going forward.","This article is the second part in our machine learning series. Part 1 provided a general overview of machine learning in nuclear medicine. Part 2 focuses on neural networks. We start with an example illustrating how neural networks work and a discussion of potential applications. Recognizing that there is a spectrum of applications, we focus on recent publications in the areas of image reconstruction, low-dose PET, disease detection, and models used for diagnosis and outcome prediction. Finally, since the way machine learning algorithms are reported in the literature is extremely variable, we conclude with a call to arms regarding the need for standardized reporting of design and outcome metrics and we propose a basic checklist our community might follow going forward.","Zukotynski, Gaudet, Uribe, Mathotaarachchi, Smith, Rosa-Neto, Benard, Black","Zukotynski, Gaudet, Uribe, Mathotaarachchi, Smith, Rosa-Neto, BÃ©nard, Black",https://doi.org/10.2967/jnumed.119.231837,https://doi.org/10.2967/jnumed.119.231837,2021-08-03
16122.0,pubmed,pubmed,Preventing the transmission of COVID-19 and other coronaviruses in older adults aged 60Ã¢â‚¬â€°years and above living in long-term care: a rapid review,Preventing the transmission of COVID-19 and other coronaviruses in older adults aged 60â€‰years and above living in long-term care: a rapid review,"The objective of this review was to examine the current guidelines for infection prevention and control (IPAC) of coronavirus disease-19 (COVID-19) or other coronaviruses in adults 60Ã¢â‚¬â€°years or older living in long-term care facilities (LTCF). EMBASE, MEDLINE, Cochrane library, pre-print servers, clinical trial registries, and relevant grey literature sources were searched until July 31, 2020, using database searching and an automated method called Continuous Active LearningÃ‚Â® (CALÃ‚Â®). All search results were processed using CALÃ‚Â® to identify the most likely relevant citations that were then screened by a single human reviewer. Full-text screening, data abstraction, and quality appraisal were completed by a single reviewer and verified by a second. Nine clinical practice guidelines (CPGs) were included. The most common recommendation in the CPGs was establishing surveillance and monitoring systems followed by mandating the use of PPE; physically distancing or cohorting residents; environmental cleaning and disinfection; promoting hand and respiratory hygiene among residents, staff, and visitors; and providing sick leave compensation for staff. Current evidence suggests robust surveillance and monitoring along with support for IPAC initiatives are key to preventing the spread of COVID-19 in LTCF. However, there are significant gaps in the current recommendations especially with regard to the movement of staff between LTCF and their role as possible transmission vectors. PROSPERO CRD42020181993.","The objective of this review was to examine the current guidelines for infection prevention and control (IPAC) of coronavirus disease-19 (COVID-19) or other coronaviruses in adults 60â€‰years or older living in long-term care facilities (LTCF). EMBASE, MEDLINE, Cochrane library, pre-print servers, clinical trial registries, and relevant grey literature sources were searched until July 31, 2020, using database searching and an automated method called Continuous Active LearningÂ® (CALÂ®). All search results were processed using CALÂ® to identify the most likely relevant citations that were then screened by a single human reviewer. Full-text screening, data abstraction, and quality appraisal were completed by a single reviewer and verified by a second. Nine clinical practice guidelines (CPGs) were included. The most common recommendation in the CPGs was establishing surveillance and monitoring systems followed by mandating the use of PPE; physically distancing or cohorting residents; environmental cleaning and disinfection; promoting hand and respiratory hygiene among residents, staff, and visitors; and providing sick leave compensation for staff. Current evidence suggests robust surveillance and monitoring along with support for IPAC initiatives are key to preventing the spread of COVID-19 in LTCF. However, there are significant gaps in the current recommendations especially with regard to the movement of staff between LTCF and their role as possible transmission vectors. PROSPERO CRD42020181993.","Rios, Radhakrishnan, Williams, Ramkissoon, Pham, Cormack, Grossman, Muller, Straus, Tricco","Rios, Radhakrishnan, Williams, Ramkissoon, Pham, Cormack, Grossman, Muller, Straus, Tricco",https://doi.org/10.1186/s13643-020-01486-4,https://doi.org/10.1186/s13643-020-01486-4,2021-08-03
16124.0,pubmed,pubmed,One Algorithm May Not Fit All: How Selection Bias Affects Machine Learning Performance,One Algorithm May Not Fit All: How Selection Bias Affects Machine Learning Performance,"Machine learning (ML) algorithms have demonstrated high diagnostic accuracy in identifying and categorizing disease on radiologic images. Despite the results of initial research studies that report ML algorithm diagnostic accuracy similar to or exceeding that of radiologists, the results are less impressive when the algorithms are installed at new hospitals and are presented with new images. This phenomenon is potentially the result of selection bias in the data that were used to develop the ML algorithm. Selection bias has long been described by clinical epidemiologists as a key consideration when designing a clinical research study, but this concept has largely been unaddressed in the medical imaging ML literature. The authors discuss the importance of selection bias and its relevance to ML algorithm development to prepare the radiologist to critically evaluate ML literature for potential selection bias and understand how it might affect the applicability of ML algorithms in real clinical environments. <sup>Ã‚Â©</sup>RSNA, 2020.","Machine learning (ML) algorithms have demonstrated high diagnostic accuracy in identifying and categorizing disease on radiologic images. Despite the results of initial research studies that report ML algorithm diagnostic accuracy similar to or exceeding that of radiologists, the results are less impressive when the algorithms are installed at new hospitals and are presented with new images. This phenomenon is potentially the result of selection bias in the data that were used to develop the ML algorithm. Selection bias has long been described by clinical epidemiologists as a key consideration when designing a clinical research study, but this concept has largely been unaddressed in the medical imaging ML literature. The authors discuss the importance of selection bias and its relevance to ML algorithm development to prepare the radiologist to critically evaluate ML literature for potential selection bias and understand how it might affect the applicability of ML algorithms in real clinical environments. <sup>Â©</sup>RSNA, 2020.","Yu, Eng","Yu, Eng",https://doi.org/10.1148/rg.2020200040,https://doi.org/10.1148/rg.2020200040,2021-08-03
16131.0,pubmed,pubmed,Diagnosis of Rare Diseases: a scoping review of clinical decision support systems,Diagnosis of Rare Diseases: a scoping review of clinical decision support systems,"Rare Diseases (RDs), which are defined as diseases affecting no more than 5 out of 10,000 people, are often severe, chronic and life-threatening. A main problem is the delay in diagnosing RDs. Clinical decision support systems (CDSSs) for RDs are software systems to support clinicians in the diagnosis of patients with RDs. Due to their clinical importance, we conducted a scoping review to determine which CDSSs are available to support the diagnosis of RDs patients, whether the CDSSs are available to be used by clinicians and which functionalities and data are used to provide decision support. We searched PubMed for CDSSs in RDs published between December 16, 2008 and December 16, 2018. Only English articles, original peer reviewed journals and conference papers describing a clinical prototype or a routine use of CDSSs were included. For data charting, we used the data items &quot;Objective and background of the publication/project&quot;, &quot;System or project name&quot;, &quot;Functionality&quot;, &quot;Type of clinical data&quot;, &quot;Rare Diseases covered&quot;, &quot;Development status&quot;, &quot;System availability&quot;, &quot;Data entry and integration&quot;, &quot;Last software update&quot; and &quot;Clinical usage&quot;. The search identified 636 articles. After title and abstracting screening, as well as assessing the eligibility criteria for full-text screening, 22 articles describing 19 different CDSSs were identified. Three types of CDSSs were classified: &quot;Analysis or comparison of genetic and phenotypic data,&quot; &quot;machine learning&quot; and &quot;information retrieval&quot;. Twelve of nineteen CDSSs use phenotypic and genetic data, followed by clinical data, literature databases and patient questionnaires. Fourteen of nineteen CDSSs are fully developed systems and therefore publicly available. Data can be entered or uploaded manually in six CDSSs, whereas for four CDSSs no information for data integration was available. Only seven CDSSs allow further ways of data integration. thirteen CDSS do not provide information about clinical usage. Different CDSS for various purposes are available, yet clinicians have to determine which is best for their patient. To allow a more precise usage, future research has to focus on CDSSs RDs data integration, clinical usage and updating clinical knowledge. It remains interesting which of the CDSSs will be used and maintained in the future.","Rare Diseases (RDs), which are defined as diseases affecting no more than 5 out of 10,000 people, are often severe, chronic and life-threatening. A main problem is the delay in diagnosing RDs. Clinical decision support systems (CDSSs) for RDs are software systems to support clinicians in the diagnosis of patients with RDs. Due to their clinical importance, we conducted a scoping review to determine which CDSSs are available to support the diagnosis of RDs patients, whether the CDSSs are available to be used by clinicians and which functionalities and data are used to provide decision support. We searched PubMed for CDSSs in RDs published between December 16, 2008 and December 16, 2018. Only English articles, original peer reviewed journals and conference papers describing a clinical prototype or a routine use of CDSSs were included. For data charting, we used the data items ""Objective and background of the publication/project"", ""System or project name"", ""Functionality"", ""Type of clinical data"", ""Rare Diseases covered"", ""Development status"", ""System availability"", ""Data entry and integration"", ""Last software update"" and ""Clinical usage"". The search identified 636 articles. After title and abstracting screening, as well as assessing the eligibility criteria for full-text screening, 22 articles describing 19 different CDSSs were identified. Three types of CDSSs were classified: ""Analysis or comparison of genetic and phenotypic data,"" ""machine learning"" and ""information retrieval"". Twelve of nineteen CDSSs use phenotypic and genetic data, followed by clinical data, literature databases and patient questionnaires. Fourteen of nineteen CDSSs are fully developed systems and therefore publicly available. Data can be entered or uploaded manually in six CDSSs, whereas for four CDSSs no information for data integration was available. Only seven CDSSs allow further ways of data integration. thirteen CDSS do not provide information about clinical usage. Different CDSS for various purposes are available, yet clinicians have to determine which is best for their patient. To allow a more precise usage, future research has to focus on CDSSs RDs data integration, clinical usage and updating clinical knowledge. It remains interesting which of the CDSSs will be used and maintained in the future.","Schaaf, Sedlmayr, Schaefer, Storf","Schaaf, Sedlmayr, Schaefer, Storf",https://doi.org/10.1186/s13023-020-01536-z,https://doi.org/10.1186/s13023-020-01536-z,2021-08-03
16133.0,pubmed,pubmed,Domain Classification and Analysis of National Institutes of Health Funded Medical Physics Research,Domain classification and analysis of national institutes of health-funded medical physics research,"The American Association of Physicists in Medicine (AAPM) previously developed a research database consisting of the National Institutes of Health (NIH) grants that were awarded to its members. The purpose of this report is to classify these NIH grants into various medical physics subdisciplines and analyze the scope of AAPM member research. For this report, an algorithm classified grant topics into medical physics research subdisciplines (grants from 2002 to 2019 were analyzed). This algorithm utilized a search for common words and phrases within grant titles, keywords, abstracts, and activity codes to perform the classification. AAPM member grants were compared with non-AAPM member grants in various relevant subcategories to assess what percentage of these grants were held by AAPM members. The percentage of AAPM member grants that included words relating to both imaging and therapy (image-guided therapy grants) increased from 13% (27/207) in 2002 to 27% (79/293) in 2019. The percentage of AAPM member grants utilizing words relating to artificial intelligence increased from 8% in 2002 to 20% in 2019. From 2002 to 2019, AAPM member grants referenced cancer more than all other diseases combined. The majority of AAPM member grants included words relating to clinical research (81% of grants in 2002 and 99% in 2019). When comparing AAPM member with non-AAPM member grants it was found that in 2019 AAPM members held a substantial fraction of all NIH grants that referenced stereotactic radiation therapies (41%), radionuclide therapies (10%), brachytherapies (35%), intensity-modulated radiation therapies (45%), and external beam particle therapies (55%). From 2002 to 2019, the percentage of AAPM membership holding NIH grants decreased for males (3.2% down to 2.3%) and increased for females (0.8% up to 1.3%) CONCLUSIONS: The majority of grants awarded to AAPM members focus on clinical research, which underlies the translational aspect of medical physics and suggests medical physicists are uniquely positioned to help translate new technologies such as artificial intelligence into the clinic. Since 2002, NIH grants awarded to AAPM members have increasingly referenced some form of image-guided therapy, suggesting opportunities for continued innovation of imaging technologies. A substantial fraction of all radiotherapy-related research grants were awarded to AAPM members, emphasizing the important role physicists have in developing radiotherapy-related treatments.","The American Association of Physicists in Medicine (AAPM) previously developed a research database consisting of the National Institutes of Health (NIH) grants that were awarded to its members. The purpose of this report is to classify these NIH grants into various medical physics subdisciplines and analyze the scope of AAPM member research. For this report, an algorithm classified grant topics into medical physics research subdisciplines (grants from 2002 to 2019 were analyzed). This algorithm utilized a search for common words and phrases within grant titles, keywords, abstracts, and activity codes to perform the classification. AAPM member grants were compared with non-AAPM member grants in various relevant subcategories to assess what percentage of these grants was held by AAPM members. The percentage of AAPM member grants that included words relating to both imaging and therapy (image-guided therapy grants) increased from 13% (27/207) in 2002 to 27% (79/293) in 2019. The percentage of AAPM member grants utilizing words relating to artificial intelligence increased from 8% in 2002 to 20% in 2019. From 2002 to 2019, AAPM member grants referenced cancer more than all other diseases combined. The majority of AAPM member grants included words relating to clinical research (81% of grants in 2002 and 99% in 2019). When comparing AAPM member with non-AAPM member grants it was found that in 2019 AAPM members held a substantial fraction of all NIH grants that referenced stereotactic radiation therapies (41%), radionuclide therapies (10%), brachytherapies (35%), intensity-modulated radiation therapies (45%), and external beam particle therapies (55%). From 2002 to 2019, the percentage of AAPM membership holding NIH grants decreased for males (3.2% down to 2.3%) and increased for females (0.8% up to 1.3%) CONCLUSIONS: The majority of grants awarded to AAPM members focus on clinical research, which underlies the translational aspect of medical physics and suggests medical physicists are uniquely positioned to help translate new technologies such as artificial intelligence into the clinic. Since 2002, NIH grants awarded to AAPM members have increasingly referenced some form of image-guided therapy, suggesting opportunities for continued innovation of imaging technologies. A substantial fraction of all radiotherapy-related research grants were awarded to AAPM members, emphasizing the important role physicists have in developing radiotherapy-related treatments.","Scarpelli, Whelan, Farahani","Scarpelli, Whelan, Farahani",https://doi.org/10.1002/mp.14469,https://doi.org/10.1002/mp.14469,2021-08-03
16138.0,pubmed,pubmed,Neuroimaging advances regarding subjective cognitive decline in preclinical Alzheimer's disease,Neuroimaging advances regarding subjective cognitive decline in preclinical Alzheimer's disease,"Subjective cognitive decline (SCD) is regarded as the first clinical manifestation in the Alzheimer's disease (AD) continuum. Investigating populations with SCD is important for understanding the early pathological mechanisms of AD and identifying SCD-related biomarkers, which are critical for the early detection of AD. With the advent of advanced neuroimaging techniques, such as positron emission tomography (PET) and magnetic resonance imaging (MRI), accumulating evidence has revealed structural and functional brain alterations related to the symptoms of SCD. In this review, we summarize the main imaging features and key findings regarding SCD related to AD, from local and regional data to connectivity-based imaging measures, with the aim of delineating a multimodal imaging signature of SCD due to AD. Additionally, the interaction of SCD with other risk factors for dementia due to AD, such as age and the Apolipoprotein E (ApoE) Ã‰â€º4 status, has also been described. Finally, the possible explanations for the inconsistent and heterogeneous neuroimaging findings observed in individuals with SCD are discussed, along with future directions. Overall, the literature reveals a preferential vulnerability of AD signature regions in SCD in the context of AD, supporting the notion that individuals with SCD share a similar pattern of brain alterations with patients with mild cognitive impairment (MCI) and dementia due to AD. We conclude that these neuroimaging techniques, particularly multimodal neuroimaging techniques, have great potential for identifying the underlying pathological alterations associated with SCD. More longitudinal studies with larger sample sizes combined with more advanced imaging modeling approaches such as artificial intelligence are still warranted to establish their clinical utility.","Subjective cognitive decline (SCD) is regarded as the first clinical manifestation in the Alzheimer's disease (AD) continuum. Investigating populations with SCD is important for understanding the early pathological mechanisms of AD and identifying SCD-related biomarkers, which are critical for the early detection of AD. With the advent of advanced neuroimaging techniques, such as positron emission tomography (PET) and magnetic resonance imaging (MRI), accumulating evidence has revealed structural and functional brain alterations related to the symptoms of SCD. In this review, we summarize the main imaging features and key findings regarding SCD related to AD, from local and regional data to connectivity-based imaging measures, with the aim of delineating a multimodal imaging signature of SCD due to AD. Additionally, the interaction of SCD with other risk factors for dementia due to AD, such as age and the Apolipoprotein E (ApoE) É›4 status, has also been described. Finally, the possible explanations for the inconsistent and heterogeneous neuroimaging findings observed in individuals with SCD are discussed, along with future directions. Overall, the literature reveals a preferential vulnerability of AD signature regions in SCD in the context of AD, supporting the notion that individuals with SCD share a similar pattern of brain alterations with patients with mild cognitive impairment (MCI) and dementia due to AD. We conclude that these neuroimaging techniques, particularly multimodal neuroimaging techniques, have great potential for identifying the underlying pathological alterations associated with SCD. More longitudinal studies with larger sample sizes combined with more advanced imaging modeling approaches such as artificial intelligence are still warranted to establish their clinical utility.","Wang, Huang, Su, Xing, Jessen, Sun, Shu, Han","Wang, Huang, Su, Xing, Jessen, Sun, Shu, Han",https://doi.org/10.1186/s13024-020-00395-3,https://doi.org/10.1186/s13024-020-00395-3,2021-08-03
16139.0,pubmed,pubmed,Improvement on Hypertension Management with Pharmacological and Non-Pharmacological Approaches: Current Perspectives,Improvement in Hypertension Management with Pharmacological and Non- Pharmacological Approaches: Current Perspectives,"Improving hypertension management is still one of the severest challenges in public health worldwide. Existing guidelines do not reach a consensus on the optimal Blood Pressure (BP) target. Therefore, how to effectively manage hypertension based on individual characteristics of patients, combined with pharmacological and nonpharmacological approach, has become a problem to be urgently considered. Reports published in PubMed that covered Pharmacological and Non-Pharmacological Approaches in subjects taking hypertension management were reviewed by the group independently and collectively. Practical recommendations for hypertension management were established by the panel. Pharmacological mechanism, action characteristics and main adverse reactions varied across different pharmacological agents, and patients with hypertension often require a combination of antihypertensive medications to achieve target BP range. Non-pharmacological treatment provides an additional effective method for improving therapy adherence and long-term BP control, reducing Cardiovascular Diseases risk and slowing down the progression of the disease. This review summarizes the available literature on the most convincing Guideline-principles, pharmacological treatment, biotechnology interference, interventional surgical treatment, managing hypertension with technical means of big data, Artificial Intelligence and Behavioral Intervention, as well as providing future directions, for facilitating Current and Developing knowledge into clinical implementation.","Improving hypertension management is still one of the biggest challenges in public health worldwide. Existing guidelines do not reach a consensus on the optimal Blood Pressure (BP) target. Therefore, how to effectively manage hypertension based on individual characteristics of patients, combined with the pharmacological and non-pharmacological approach, has become a problem to be urgently considered. Reports published in PubMed that covered Pharmacological and Non-Pharmacological Approaches in subjects taking hypertension management were reviewed by the group independently and collectively. Practical recommendations for hypertension management were established by the panel. Pharmacological mechanism, action characteristics, and main adverse reactions varied across different pharmacological agents, and patients with hypertension often require a combination of antihypertensive medications to achieve the target BP range. Non-pharmacological treatment provides an additional effective method for improving therapy adherence and long-term BP control, thus reducing the risk of cardiovascular diseases, and slowing down the progression of the disease. This review summarizes the available literature on the most convincing guideline principles, pharmacological treatment, biotechnology interference, interventional surgical treatment, managing hypertension with technical means of big data, Artificial Intelligence and Behavioral Intervention, as well as providing future directions, for facilitating Current and Developing knowledge into clinical implementation.","Hong, Shan","Hong, Shan",https://doi.org/10.2174/1381612826666200922153045,https://doi.org/10.2174/1381612826666200922153045,2021-08-03
16144.0,pubmed,pubmed,Modeling texture in deep 3D CNN for survival analysis,Modeling Texture in Deep 3D CNN for Survival Analysis,"Radiomics has shown remarkable potential for predicting the survival outcome for various types of cancers such as pancreatic ductal adenocarcinoma (PDAC). However, to date, there has been limited research using convolutional neural networks (CNN) with radiomic methods for this task, due to their requirement for large training sets. To overcome this issue, we propose a new type of radiomic descriptor modeling the distribution of learned features with a Gaussian mixture model (GMM). These parametric features (GMM-CNN) are computed from gross tumor volumes of PDAC patients defined semiautomatically in pre-operative computed tomography (CT) scans. We use the proposed GMM-CNN features as input to a robust classifier based on random forests (RF) to predict the survival outcome of patients with PDAC. Our experiments assess the advantage of GMM-CNN compared to employing the same 3D CNN model directly, standard radiomic (i.e., histogram, texture and shape), conditional entropy (CENT) based on 3DCNN, and clinical features (i.e., serum carbohydrate antigen 19-9 and chemotherapy neoadjuvant). Using the RF model (100 samples for training; 59 samples for validation), GMM-CNN features provided the highest area under the ROC curve (AUC) of 72.0% (p = 6.4105) compared to 64.0% (p = 0.01) for the 3D CNN model output, 66.8% (p = 0.01) for standard radiomic features, 64.2% (p = 0.003) for CENT, and 57.6% (p = 0.3) for clinical variables. Our results suggest that the proposed GMM-CNN features used with a RF classifier can significantly improve the capacity to prognosticate PDAC patients prior to surgery via routinely-acquired imaging data.","Radiomics has shown remarkable potential for predicting the survival outcome for various types of cancers such as pancreatic ductal adenocarcinoma (PDAC). However, to date, there has been limited research using convolutional neural networks (CNN) with radiomic methods for this task, due to their requirement for large training sets. To overcome this issue, we propose a new type of radiomic descriptor modeling the distribution of learned features with a Gaussian mixture model (GMM). These parametric features (GMM-CNN) are computed from gross tumor volumes of PDAC patients defined semi-automatically in pre-operative computed tomography (CT) scans. We use the proposed GMM-CNN features as input to a robust classifier based on random forests (RF) to predict the survival outcome of patients with PDAC. Our experiments assess the advantage of GMM-CNN compared to employing the same 3D CNN model directly, standard radiomic (i.e., histogram, texture and shape), conditional entropy (CENT) based on 3DCNN, and clinical features (i.e., serum carbohydrate antigen 19-9 and chemotherapy neoadjuvant). Using the RF model (100 samples for training; 59 samples for validation), GMM-CNN features provided the highest area under the ROC curve (AUC) of 72.0% (pÂ =Â 6.4Ã—10<sup>-5</sup>) compared to 64.0% (pÂ =Â 0.01) for the 3D CNN model output, 66.8% (pÂ =Â 0.01) for standard radiomic features, 64.2% (pÂ =Â 0.003) for CENT, and 57.6% (pÂ =Â 0.3) for clinical variables. Our results suggest that the proposed GMM-CNN features used with a RF classifier can significantly improve the capacity to prognosticate PDAC patients prior to surgery via routinely-acquired imaging data.","Chaddad, Sargos, Desrosiers","Chaddad, Sargos, Desrosiers",https://doi.org/10.1109/JBHI.2020.3025901,https://doi.org/10.1109/JBHI.2020.3025901,2021-08-03
16146.0,pubmed,pubmed,"A Quantitative Retrospective Exposure Assessment for Former Chrysotile Asbestos Miners and Millers from Baie Verte, NL, Canada","A Quantitative Retrospective Exposure Assessment for Former Chrysotile Asbestos Miners and Millers from Baie Verte, NL, Canada","Despite numerous studies of asbestos workers in the epidemiologic literature, there are very few cohort studies of chrysotile asbestos miners/millers that include high-quality retrospective exposure assessments. As part of the creation of the Baie Verte Miners' Registry in 2008, a two-dimensional job exposure matrix (JEM) was developed for estimating asbestos exposures for former chrysotile asbestos miners/millers. Industrial hygiene data collected between 1963 and 1994 were analysed to assess validity for use in a retrospective exposure assessment and epidemiologic study. Registered former employees were divided into 52 exposure groups (EGs) based on job title and department and mean asbestos concentrations were calculated for each EG. The resulting exposure estimates were linked to individual registrants' work histories allowing for the calculation of cumulative asbestos exposure for each registrant. The distribution of exposure for most EGs (82.6%) could be described as fitting a log-normal distribution, although variability within some EGs (55%) exceeded a geometric standard deviation (GSD) of 2.5. Overall, the data used to create EGs in the development of the JEM were deemed to be of adequate quality for estimating cumulative asbestos exposures for the former employees of the Baie Verte asbestos mine/mill. The variability between workers in the same job was often high and is an important factor to be considered when using estimates of cumulative asbestos exposure to adjudicate compensation claims. The exposures experienced in this cohort were comparable to those of other chrysotile asbestos miners/millers cohorts, specifically Italian and QuÃƒÂ©bec cohorts.","Despite numerous studies of asbestos workers in the epidemiologic literature, there are very few cohort studies of chrysotile asbestos miners/millers that include high-quality retrospective exposure assessments. As part of the creation of the Baie Verte Miners' Registry in 2008, a two-dimensional job exposure matrix (JEM) was developed for estimating asbestos exposures for former chrysotile asbestos miners/millers. Industrial hygiene data collected between 1963 and 1994 were analysed to assess validity for use in a retrospective exposure assessment and epidemiologic study. Registered former employees were divided into 52 exposure groups (EGs) based on job title and department and mean asbestos concentrations were calculated for each EG. The resulting exposure estimates were linked to individual registrants' work histories allowing for the calculation of cumulative asbestos exposure for each registrant. The distribution of exposure for most EGs (82.6%) could be described as fitting a log-normal distribution, although variability within some EGs (55%) exceeded a geometric standard deviation (GSD) of 2.5. Overall, the data used to create EGs in the development of the JEM were deemed to be of adequate quality for estimating cumulative asbestos exposures for the former employees of the Baie Verte asbestos mine/mill. The variability between workers in the same job was often high and is an important factor to be considered when using estimates of cumulative asbestos exposure to adjudicate compensation claims. The exposures experienced in this cohort were comparable to those of other chrysotile asbestos miners/millers cohorts, specifically Italian and QuÃ©bec cohorts.","Giles Murphy, Bornstein, Oudyk, Demers","Giles Murphy, Bornstein, Oudyk, Demers",https://doi.org/10.1093/annweh/wxaa092,https://doi.org/10.1093/annweh/wxaa092,2021-08-03
16147.0,pubmed,pubmed,Machine Learning Maps Research Needs in COVID-19 Literature,Machine Learning Maps Research Needs in COVID-19 Literature,"As of August 2020, thousands of COVID-19 (coronavirus disease 2019) publications have been produced. Manual assessment of their scope is an overwhelming task, and shortcuts through metadata analysis (e.g., keywords) assume that studies are properly tagged. However, machine learning approaches can rapidly survey the actual text of publication abstracts to identify research overlap between COVID-19 and other coronaviruses, research hotspots, and areas warranting exploration. We propose a fast, scalable, and reusable framework to parse novel disease literature. When applied to the COVID-19 Open Research Dataset (CORD-19), dimensionality reduction suggests that COVID-19 studies to date are primarily clinical-, modeling- or field-based, in contrast to the vast quantity of laboratory-driven research for other (non-COVID-19) coronavirus diseases. Furthermore, topic modeling indicates that COVID-19 publications have focused on public health, outbreak reporting, clinical care, and testing for coronaviruses, as opposed to the more limited number focused on basic microbiology, including pathogenesis and transmission.","As of August 2020, thousands of COVID-19 (coronavirus disease 2019) publications have been produced. Manual assessment of their scope is an overwhelming task, and shortcuts through metadata analysis (e.g., keywords) assume that studies are properly tagged. However, machine learning approaches can rapidly survey the actual text of publication abstracts to identify research overlap between COVID-19 and other coronaviruses, research hotspots, and areas warranting exploration. We propose a fast, scalable, and reusable framework to parse novel disease literature. When applied to the COVID-19 Open Research Dataset, dimensionality reduction suggests that COVID-19 studies to date are primarily clinical, modeling, or field based, in contrast to the vast quantity of laboratory-driven research for other (non-COVID-19) coronavirus diseases. Furthermore, topic modeling indicates that COVID-19 publications have focused on public health, outbreak reporting, clinical care, and testing for coronaviruses, as opposed to the more limited number focused on basic microbiology, including pathogenesis and transmission.","Doanvo, Qian, Ramjee, Piontkivska, Desai, Majumder","Doanvo, Qian, Ramjee, Piontkivska, Desai, Majumder",https://doi.org/10.1016/j.patter.2020.100123,https://doi.org/10.1016/j.patter.2020.100123,2021-08-03
16148.0,pubmed,pubmed,Deep learning to estimate RECIST in patients with NSCLC treated with PD-1 blockade,Deep Learning to Estimate RECIST in Patients with NSCLC Treated with PD-1 Blockade,"Real-world evidence (RWE) - conclusions derived from analysis of patients not treated in clinical trials - is increasingly recognized as an opportunity for discovery, to reduce disparities, and to contribute to regulatory approval. Maximal value of RWE may be facilitated through machine learning techniques to integrate and interrogate large and otherwise underutilized data sets. In cancer research, an ongoing challenge for RWE is the lack of reliable, reproducible, scalable assessment of treatment-specific outcomes. We hypothesized a deep learning model could be trained to use radiology text reports to estimate gold-standard Response Evaluation Criteria in Solid Tumors (RECIST)-defined outcomes. Using text reports from patients with non-small cell lung cancer treated with PD-1 blockade in a training cohort and two test cohorts, we developed a deep learning model to accurately estimate best overall response and progression-free survival. Our model may be a tool to determine outcomes at scale, enabling analyses of large clinical databases.","Real-world evidence (RWE), conclusions derived from analysis of patients not treated in clinical trials, is increasingly recognized as an opportunity for discovery, to reduce disparities, and to contribute to regulatory approval. Maximal value of RWE may be facilitated through machine-learning techniques to integrate and interrogate large and otherwise underutilized datasets. In cancer research, an ongoing challenge for RWE is the lack of reliable, reproducible, scalable assessment of treatment-specific outcomes. We hypothesized a deep-learning model could be trained to use radiology text reports to estimate gold-standard RECIST-defined outcomes. Using text reports from patients with non-small cell lung cancer treated with PD-1 blockade in a training cohort and two test cohorts, we developed a deep-learning model to accurately estimate best overall response and progression-free survival. Our model may be a tool to determine outcomes at scale, enabling analyses of large clinical databases. SIGNIFICANCE: We developed and validated a deep-learning model trained on radiology text reports to estimate gold-standard objective response categories used in clinical trial assessments. This tool may facilitate analysis of large real-world oncology datasets using objective outcome metrics determined more reliably and at greater scale than currently possible.<i>This article is highlighted in the In This Issue feature, p. 1</i>.","Arbour, Luu, Luo, Rizvi, Plodkowski, Sakhi, Huang, Digumarthy, Ginsberg, Girshman, Kris, Riely, Yala, Gainor, Barzilay, Hellmann","Arbour, Luu, Luo, Rizvi, Plodkowski, Sakhi, Huang, Digumarthy, Ginsberg, Girshman, Kris, Riely, Yala, Gainor, Barzilay, Hellmann",https://doi.org/10.1158/2159-8290.CD-20-0419,https://doi.org/10.1158/2159-8290.CD-20-0419,2021-08-03
16152.0,pubmed,pubmed,Diagnostic Performance of Artificial Intelligence for Detection of Anterior Cruciate Ligament and Meniscus Tears: A Systematic Review,Diagnostic Performance of Artificial Intelligence for Detection of Anterior Cruciate Ligament and Meniscus Tears: A Systematic Review,"To (1) determine the diagnostic efficacy of artificial intelligence (AI) methods for detecting anterior cruciate ligament (ACL) and meniscus tears and to (2) compare the efficacy to human clinical experts. PubMed, OVID/Medline, and Cochrane libraries were queried in November 2019 for research articles pertaining to AI use for detection of ACL and meniscus tears. Information regarding AI model, prediction accuracy/area under the curve (AUC), sample sizes of testing/training sets, and imaging modalities were recorded. A total of 11 AI studies were identified: 5 investigated ACL tears, 5 investigated meniscal tears, and 1 investigated both. The AUC of AI models for detecting ACL tears ranged from 0.895 to 0.980, and the prediction accuracy ranged from 86.7% to 100%. Of these studies, 3 compared AI models to clinical experts. Two found no significant differences in diagnostic capability, whereas one found that radiologists had a significantly greater sensitivity for detecting ACL tears (PÃ‚Â = .002) and statistically similar specificity and accuracy. Of the 5 studies investigating the meniscus, the AUC for AI models ranged from 0.847 to 0.910 and prediction accuracy ranged from 75.0% to 90.0%. Of these studies, 2 compared AI models with clinical experts. One found no significant differences in diagnostic accuracy, whereas one found that the AI model had a significantly lower specificity (PÃ‚Â = .003) and accuracy (PÃ‚Â = .015) than radiologists. Two studies reported that the addition of AI models significantly increased the diagnostic performance of clinicians compared to their efforts without these models. AI prediction capabilities were excellent and may enhance the diagnosis of ACL and meniscal pathology; however, AI did not outperform clinical experts. AI models promise to improve diagnosing certain pathologies as well as or better than human experts, are excellent for detecting ACL and meniscus tears, and may enhance the diagnostic capabilities of human experts; however, when compared with these experts, they may not offer any significant advantage.","To (1) determine the diagnostic efficacy of artificial intelligence (AI) methods for detecting anterior cruciate ligament (ACL) and meniscus tears and to (2) compare the efficacy to human clinical experts. PubMed, OVID/Medline, and Cochrane libraries were queried in November 2019 for research articles pertaining to AI use for detection of ACL and meniscus tears. Information regarding AI model, prediction accuracy/area under the curve (AUC), sample sizes of testing/training sets, and imaging modalities were recorded. A total of 11 AI studies were identified: 5 investigated ACL tears, 5 investigated meniscal tears, and 1 investigated both. The AUC of AI models for detecting ACL tears ranged from 0.895 to 0.980, and the prediction accuracy ranged from 86.7% to 100%. Of these studies, 3 compared AI models to clinical experts. Two found no significant differences in diagnostic capability, whereas one found that radiologists had a significantly greater sensitivity for detecting ACL tears (PÂ = .002) and statistically similar specificity and accuracy. Of the 5 studies investigating the meniscus, the AUC for AI models ranged from 0.847 to 0.910 and prediction accuracy ranged from 75.0% to 90.0%. Of these studies, 2 compared AI models with clinical experts. One found no significant differences in diagnostic accuracy, whereas one found that the AI model had a significantly lower specificity (PÂ = .003) and accuracy (PÂ = .015) than radiologists. Two studies reported that the addition of AI models significantly increased the diagnostic performance of clinicians compared to their efforts without these models. AI prediction capabilities were excellent and may enhance the diagnosis of ACL and meniscal pathology; however, AI did not outperform clinical experts. AI models promise to improve diagnosing certain pathologies as well as or better than human experts, are excellent for detecting ACL and meniscus tears, and may enhance the diagnostic capabilities of human experts; however, when compared with these experts, they may not offer any significant advantage.","Kunze, Rossi, White, Karhade, Deng, Williams, Chahla","Kunze, Rossi, White, Karhade, Deng, Williams, Chahla",https://doi.org/10.1016/j.arthro.2020.09.012,https://doi.org/10.1016/j.arthro.2020.09.012,2021-08-03
16154.0,pubmed,pubmed,Novel Method to Flag Cardiac Implantable Device Infections by Integrating Text Mining With Structured Data in the Veterans Health Administration's Electronic Medical Record,Novel Method to Flag Cardiac Implantable Device Infections by Integrating Text Mining With Structured Data in the Veterans Health Administration's Electronic Medical Record,"Health care-associated infections (HAIs) are preventable, harmful, and costly; however, few resources are dedicated to infection surveillance of nonsurgical procedures, particularly cardiovascular implantable electronic device (CIED) procedures. To develop a method that includes text mining of electronic clinical notes to reliably and efficiently measure HAIs for CIED procedures. In this multicenter, national cohort study using electronic medical record data for patients undergoing CIED procedures in Veterans Health Administration (VA) facilities for fiscal years (FYs) 2016 and 2017, an algorithm to flag cases with a true CIED-related infection based on structured (eg, microbiology orders, vital signs) and free text diagnostic and therapeutic data (eg, procedure notes, discharge summaries, microbiology results) was developed and validated. Procedure data were divided into development and validation data sets. Criterion validity (ie, positive predictive validity [PPV], sensitivity, and specificity) was assessed via criterion-standard manual medical record review. CIED procedure. The concordance between medical record review and the study algorithm with respect to the presence or absence of a CIED infection. CIED infection in the algorithm included 90-day mortality, congestive heart failure and nonmetastatic tumor comorbidities, CIED or surgical site infection International Statistical Classification of Diseases and Related Health Problems, Tenth Revision, Clinical Modification (ICD-10-CM) diagnosis codes, antibiotic treatment of Staphylococci, a microbiology test of a cardiac specimen, and text documentation of infection in specific clinical notes (eg, cardiology, infectious diseases, inpatient discharge summaries). The algorithm sample consisted of 19Ã¢â‚¬Â¯212 CIED procedures; 15Ã¢â‚¬Â¯077 patients (78.5%) were White individuals, 1487 (15.5%) were African American; 18Ã¢â‚¬Â¯766 (97.7%) were men. The mean (SD) age in our sample was 71.8 (10.6) years. The infection detection threshold of predicted probability was set to greater than 0.10 and the algorithm flagged 276 of 9606 (2.9%) cases in the development data set (9606 procedures); PPV in this group was 41.4% (95% CI, 31.6%-51.8%). In the validation set (9606 procedures), at predicted probability 0.10 or more the algorithm PPV was 43.5% (95% CI, 37.1%-50.2%), and overall sensitivity and specificity were 94.4% (95% CI, 88.2%-97.9%) and 48.8% (95% CI, 42.6%-55.1%), respectively. The findings of this study suggest that the method of combining structured and text data in VA electronic medical records can be used to expand infection surveillance beyond traditional boundaries to include outpatient and procedural areas.","Health care-associated infections (HAIs) are preventable, harmful, and costly; however, few resources are dedicated to infection surveillance of nonsurgical procedures, particularly cardiovascular implantable electronic device (CIED) procedures. To develop a method that includes text mining of electronic clinical notes to reliably and efficiently measure HAIs for CIED procedures. In this multicenter, national cohort study using electronic medical record data for patients undergoing CIED procedures in Veterans Health Administration (VA) facilities for fiscal years (FYs) 2016 and 2017, an algorithm to flag cases with a true CIED-related infection based on structured (eg, microbiology orders, vital signs) and free text diagnostic and therapeutic data (eg, procedure notes, discharge summaries, microbiology results) was developed and validated. Procedure data were divided into development and validation data sets. Criterion validity (ie, positive predictive validity [PPV], sensitivity, and specificity) was assessed via criterion-standard manual medical record review. CIED procedure. The concordance between medical record review and the study algorithm with respect to the presence or absence of a CIED infection. CIED infection in the algorithm included 90-day mortality, congestive heart failure and nonmetastatic tumor comorbidities, CIED or surgical site infection International Statistical Classification of Diseases and Related Health Problems, Tenth Revision, Clinical Modification (ICD-10-CM) diagnosis codes, antibiotic treatment of Staphylococci, a microbiology test of a cardiac specimen, and text documentation of infection in specific clinical notes (eg, cardiology, infectious diseases, inpatient discharge summaries). The algorithm sample consisted of 19â€¯212 CIED procedures; 15â€¯077 patients (78.5%) were White individuals, 1487 (15.5%) were African American; 18â€¯766 (97.7%) were men. The mean (SD) age in our sample was 71.8 (10.6) years. The infection detection threshold of predicted probability was set to greater than 0.10 and the algorithm flagged 276 of 9606 (2.9%) cases in the development data set (9606 procedures); PPV in this group was 41.4% (95% CI, 31.6%-51.8%). In the validation set (9606 procedures), at predicted probability 0.10 or more the algorithm PPV was 43.5% (95% CI, 37.1%-50.2%), and overall sensitivity and specificity were 94.4% (95% CI, 88.2%-97.9%) and 48.8% (95% CI, 42.6%-55.1%), respectively. The findings of this study suggest that the method of combining structured and text data in VA electronic medical records can be used to expand infection surveillance beyond traditional boundaries to include outpatient and procedural areas.","Mull, Stolzmann, Shin, Kalver, Schweizer, Branch-Elliman","Mull, Stolzmann, Shin, Kalver, Schweizer, Branch-Elliman",https://doi.org/10.1001/jamanetworkopen.2020.12264,https://doi.org/10.1001/jamanetworkopen.2020.12264,2021-08-03
16155.0,pubmed,pubmed,Evidence-based strategies for interdental cleaning: a practical decision tree and review of the literature,Evidence-based strategies for interdental cleaning: a practical decision tree and review of the literature,"Although several studies have investigated the effectiveness of various interdental cleaning devices, there is a need for an evidence-based synopsis for clinicians to customize interdental hygiene instructions and provide specific devices for each patient. This literature review aims to establish an evidence-based decision-making tree recommending individualized approaches to interdental cleaning based on embrasure size and patient-specific conditions. Specific keywords related to interdental cleaning were used to search and identify the existing literature in PubMed and the Cochrane Library. Through a series of review processes, qualifying studies were identified and assessed with respect to the inclusion criteria to establish the decision tree. A total of 27 studies were included to support a decision tree. Traditional dental floss continues to remain the first choice for individuals of high motivation and good manual dexterity with type I closed embrasures. For individuals with closed embrasures, but lack of motivation and/or dexterity, the use of easy flossers, soft picks, oral irrigation, and small (0.6 to 0.7 mm) interdental brushes are alternatives. For individuals with type II and type III open embrasure spaces, an interdental brush has the highest evidence for its effectiveness to remove interdental plaque. However, two studies showed that residual plaque could be found over lingual embrasures and thus lingual approach of the interdental brush is sometimes needed. The use of gum stimulators and/or woodsticks continues to be supported when significant gingival inflammation is present. Each patient should be individually assessed and given tailored oral hygiene home care instructions for the most effective outcomes. The proposed decision tree provides clinicians with an evidence-based guideline to help customize the use of interdental cleaning devices for each patient. (Quintessence Int 2020;51:2-13; doi: ##.####/j.qi.a#####).","&lt;p&gt;Objectives: Although several studies have investigated the effectiveness of various interdental cleaning devices, there is a need for an evidence-based synopsis for clinicians to customize interdental hygiene instructions and provide specific devices for each patient. This literature review aims to establish an evidence-based decision-making tree recommending individualized approaches to interdental cleaning based on embrasure size and patient-specific conditions.&lt;br /&gt; Data sources: Specific keywords related to interdental cleaning were used to search and identify the existing literature in PubMed and the Cochrane Library. Through a series of review processes, qualifying studies were identified and assessed with respect to the inclusion criteria to establish the decision tree.&lt;br /&gt; Results: A total of 27 studies were included to support a decision tree. Traditional dental floss continues to remain the first choice for individuals of high motivation and good ?manual dexterity with type I closed embrasures. For individuals with closed embrasures, but lack of motivation and/or dexterity, the use of easy flossers, soft picks, oral irrigation, and small (0.6 to 0.7 mm) interdental brushes are alternatives. For individuals with type II and type III open embrasure spaces, an interdental brush has the highest evidence for its effectiveness to remove interdental plaque. However, two studies showed that residual plaque could be found over lingual embrasur?es and thus lingual approach of the interdental brush is sometimes needed. The use of gum stimulators and/or woodsticks continues to be supported when significant gingival inflammation is present.&lt;br /&gt; Conclusion: Each patient should be individually assessed and given tailored oral hygiene home care instructions for the most effective outcomes. The proposed decision tree provides clinicians with an evidence-based guideline to help customize the use of interdental cleaning devices for each patient.&lt;/p&gt;.","Liang, Ye, McComas, Kwon, Wang","Liang, Ye, McComas, Kwon, Wang",https://doi.org/10.3290/j.qi.a45268,https://doi.org/10.3290/j.qi.a45268,2021-08-03
16161.0,pubmed,pubmed,Expanding Data Collection for the MDSGene Database: X-Linked Dystonia-Parkinsonism as Use Case Example,Expanding Data Collection for the MDSGene Database: X-linked Dystonia-Parkinsonism as Use Case Example,"MDSGene is an online database on movement disorders that collates genetic and clinical knowledge using a standardized published literature abstraction strategy. This review is dedicated to X-linked dystonia-parkinsonism (XDP). We screened 233 citations and curated phenotypic and genotypic data for 414 cases. To reduce data missingness, we (1) contacted authors and engaged the research community to provide additional clinical and genetic information, and (2) revisited previously unpublished data from a cohort of XDP patients seen at our institution. Using these approaches, we expanded the cohort to 577 cases and increased information available for important clinical and genetic features such as age at onset, initial manifestation, predominant motor symptoms, functional impairments, and repeat size information. We established the use of mining unpublished data to expand the MDSGene workflow and present an up-to-date description of the phenomenology of XDP using an extensive collection of previously reported and unreported data. Ã‚Â© 2020 International Parkinson and Movement Disorder Society.","MDSGene is an online database on movement disorders that collates genetic and clinical knowledge using a standardized published literature abstraction strategy. This review is dedicated to X-linked dystonia-parkinsonism (XDP). We screened 233 citations and curated phenotypic and genotypic data for 414 cases. To reduce data missingness, we (1) contacted authors and engaged the research community to provide additional clinical and genetic information, and (2) revisited previously unpublished data from a cohort of XDP patients seen at our institution. Using these approaches, we expanded the cohort to 577 cases and increased information available for important clinical and genetic features such as age at onset, initial manifestation, predominant motor symptoms, functional impairments, and repeat size information. We established the use of mining unpublished data to expand the MDSGene workflow and present an up-to-date description of the phenomenology of XDP using an extensive collection of previously reported and unreported data. Â© 2020 The Authors. Movement Disorders published by Wiley Periodicals LLC on behalf of International Parkinson and Movement Disorder Society.","Pauly, Ruiz LÃƒÂ³pez, Westenberger, Saranza, BrÃƒÂ¼ggemann, Weissbach, Rosales, Diesta, Jamora, Reyes, Madoev, Petkovic, Ozelius, Klein, Domingo","Pauly, Ruiz LÃ³pez, Westenberger, Saranza, BrÃ¼ggemann, Weissbach, Rosales, Diesta, Jamora, Reyes, Madoev, Petkovic, Ozelius, Klein, Domingo",https://doi.org/10.1002/mds.28289,https://doi.org/10.1002/mds.28289,2021-08-03
16162.0,pubmed,pubmed,Lifetime quality of life and cost consequences of delays in endovascular treatment for acute ischaemic stroke: a cost-effectiveness analysis from a Singapore healthcare perspective,Lifetime quality of life and cost consequences of delays in endovascular treatment for acute ischaemic stroke: a cost-effectiveness analysis from a Singapore healthcare perspective,"Endovascular therapy (EVT) significantly improves clinical outcomes in patients with acute ischaemic stroke (AIS), while the time of EVT initiation after stroke onset influences both patient clinical outcomes and healthcare costs. This study determined the impact of EVT treatment delay on cost effectiveness of EVT in the Singapore healthcare setting. A short-term decision tree and long-term Markov health state transition model was constructed. For each time window of symptom onset to EVT, the probability of receiving EVT or non-EVT treatment was varied, thereby varying clinical outcomes (modified Rankin Scale scores), short-term costs and long-term modelled (lifetime) costs; all of which were used in calculating an incremental cost-effectiveness ratio of EVT vs non-EVT treatment. Clinical outcomes and cost data were derived from clinical trials, literature, expert opinion, electronic medical records and community-based surveys from Singapore. Deterministic one-way and probabilistic sensitivity analyses were performed to assess the uncertainty of the model. The willingness to pay for per quality-adjusted life-year (QALY) was set to Singapore $50Ã¢â‚¬â€°000 (US$36 500). Singapore healthcare perspective. The model included patients with AIS in Singapore. EVT performed within 6Ã¢â‚¬â€°hours of stroke onset. The model estimated incremental cost-effectiveness ratios (ICERs) and net monetary benefits (NMB) for EVT versus non-EVT treatment, varied by time from symptom onset to time of treatment. EVT performed between 61 min and 120Ã¢â‚¬â€°min after the stroke onset was most cost-effective time window to perform EVT in the Singapore population, with an ICER of Singapore $7197 per QALY (US$5254) for performing EVT at 61-120Ã¢â‚¬â€°min versus 121-180Ã¢â‚¬â€°min. The resulting incremental NMB associated with receipt of EVT at the earlier time point is Singapore $39Ã¢â‚¬â€°827 (US$29 074) per patient at the willingness-to-pay threshold of Singapore $50Ã¢â‚¬â€°000. Each hour delay in EVT resulted in an average loss of 0.54 QALYs and 195.35 healthy days, with an average net monetary loss of Singapore $26Ã¢â‚¬â€°255 (US$19 166). From the Singapore healthcare perspective, although EVT is more expensive than alternative treatments in the short term, the lifetime ICER is below the willingness-to-pay threshold. Thus, healthcare policies and procedures should aim to improve efficiency of pre-hospital and in-hospital workflow processes to reduce the onset-to-puncture duration.","Endovascular therapy (EVT) significantly improves clinical outcomes in patients with acute ischaemic stroke (AIS), while the time of EVT initiation after stroke onset influences both patient clinical outcomes and healthcare costs. This study determined the impact of EVT treatment delay on cost effectiveness of EVT in the Singapore healthcare setting. A short-term decision tree and long-term Markov health state transition model was constructed. For each time window of symptom onset to EVT, the probability of receiving EVT or non-EVT treatment was varied, thereby varying clinical outcomes (modified Rankin Scale scores), short-term costs and long-term modelled (lifetime) costs; all of which were used in calculating an incremental cost-effectiveness ratio of EVT vs non-EVT treatment. Clinical outcomes and cost data were derived from clinical trials, literature, expert opinion, electronic medical records and community-based surveys from Singapore. Deterministic one-way and probabilistic sensitivity analyses were performed to assess the uncertainty of the model. The willingness to pay for per quality-adjusted life-year (QALY) was set to Singapore $50â€‰000 (US$36 500). Singapore healthcare perspective. The model included patients with AIS in Singapore. EVT performed within 6â€‰hours of stroke onset. The model estimated incremental cost-effectiveness ratios (ICERs) and net monetary benefits (NMB) for EVT versus non-EVT treatment, varied by time from symptom onset to time of treatment. EVT performed between 61 min and 120â€‰min after the stroke onset was most cost-effective time window to perform EVT in the Singapore population, with an ICER of Singapore $7197 per QALY (US$5254) for performing EVT at 61-120â€‰min versus 121-180â€‰min. The resulting incremental NMB associated with receipt of EVT at the earlier time point is Singapore $39â€‰827 (US$29 074) per patient at the willingness-to-pay threshold of Singapore $50â€‰000. Each hour delay in EVT resulted in an average loss of 0.54 QALYs and 195.35 healthy days, with an average net monetary loss of Singapore $26â€‰255 (US$19 166). From the Singapore healthcare perspective, although EVT is more expensive than alternative treatments in the short term, the lifetime ICER is below the willingness-to-pay threshold. Thus, healthcare policies and procedures should aim to improve efficiency of pre-hospital and in-hospital workflow processes to reduce the onset-to-puncture duration.","Ni, Kunz, Goyal, Ng, Tan, De Silva","Ni, Kunz, Goyal, Ng, Tan, De Silva",https://doi.org/10.1136/bmjopen-2019-036517,https://doi.org/10.1136/bmjopen-2019-036517,2021-08-03
16163.0,pubmed,pubmed,Treatment of hepatitis B virus infection in chronic infection with HBeAg-positive adult patients (immunotolerant patients): a systematic review,Treatment of hepatitis B virus infection in chronic infection with HBeAg-positive adult patients (immunotolerant patients): a systematic review,"Recently, a controversial approach suggesting the early treatment of chronic infection with hepatitis B &quot;e&quot; antigen-positive patients with hepatitis B virus (HBV) infection, has been proposed. The objective of this study is to systematically review medical literature regarding treatment of HBV infection in adult chronic infection with HBeAg-positive patients. A systematic review was performed according to the recommendations of the Preferred Reporting Items for Systematic Reviews and Meta-Analysis statement. Original studies that evaluated the effect of antivirals in adult chronic infection with HBeAg-positive patients were included. The outcomes of interest were viral load suppression, the loss/seroconversion of HBeAg, the loss/seroconversion of hepatitis B surface antigen, and the development of cirrhosis or hepatocellular carcinoma. The search for eligible studies was performed in Excerpta Medica dataBASE, PubMed and Cochrane databases until January 2020, without language or date restriction. The risk of bias was evaluated using the Newcastle-Ottawa Scale for observational studies and the Revised Cochrane Risk-of-Bias Tool for randomized controlled trials. Two hundred ninety-six articles were retrieved. After analyzing titles and abstracts, 287 articles were excluded and nine were considered potentially eligible. From these, five were excluded after full-text analysis. Finally, four articles were included. Only two were randomized controlled trials. All studies were carried out in Asian patients. Results were variable with regard to viral load, negativation/seroconversion of HBeAg and HBsAg. One study demonstrated that treated patients developed cirrhosis or hepatocellular carcinoma less frequently than untreated individuals. Overall, the studies were of poor quality. In conclusion, the present systematic review demonstrated that, at present, there is not enough evidence to recommend treating this population of patients.","Recently, a controversial approach suggesting the early treatment of chronic infection with hepatitis B ""e"" antigen-positive patients with hepatitis B virus (HBV) infection, has been proposed. The objective of this study is to systematically review medical literature regarding treatment of HBV infection in adult chronic infection with HBeAg-positive patients. A systematic review was performed according to the recommendations of the Preferred Reporting Items for Systematic Reviews and Meta-Analysis statement. Original studies that evaluated the effect of antivirals in adult chronic infection with HBeAg-positive patients were included. The outcomes of interest were viral load suppression, the loss/seroconversion of HBeAg, the loss/seroconversion of hepatitis B surface antigen, and the development of cirrhosis or hepatocellular carcinoma. The search for eligible studies was performed in Excerpta Medica dataBASE, PubMed and Cochrane databases until January 2020, without language or date restriction. The risk of bias was evaluated using the Newcastle-Ottawa Scale for observational studies and the Revised Cochrane Risk-of-Bias Tool for randomized controlled trials. Two hundred ninety-six articles were retrieved. After analyzing titles and abstracts, 287 articles were excluded and nine were considered potentially eligible. From these, five were excluded after full-text analysis. Finally, four articles were included. Only two were randomized controlled trials. All studies were carried out in Asian patients. Results were variable with regard to viral load, negativation/seroconversion of HBeAg and HBsAg. One study demonstrated that treated patients developed cirrhosis or hepatocellular carcinoma less frequently than untreated individuals. Overall, the studies were of poor quality. In conclusion, the present systematic review demonstrated that, at present, there is not enough evidence to recommend treating this population of patients.","Tovo, Ahlert, Panke, de Mattos, de Mattos","Tovo, Ahlert, Panke, de Mattos, de Mattos",https://doi.org/10.1097/MEG.0000000000001907,https://doi.org/10.1097/MEG.0000000000001907,2021-08-03
16167.0,pubmed,pubmed,Phenotypic clustering of heart failure with preserved ejection fraction reveals different rates of hospitalization,Phenotypic clustering of heart failure with preserved ejection fraction reveals different rates of hospitalization,"Approximately 50% of patients with heart failure have preserved (Ã¢â€°Â¥50%) ejection fraction (HFpEF). Improved understanding of the phenotypic heterogeneity of HFpEF might facilitate development of targeted therapies and interventions. This retrospective study characterized a cohort of patients with HFpEF based on similar clinical profiles and evaluated 1-year heart failure related hospitalization. Enrolment, medical and pharmacy data were used to identify patients newly diagnosed with heart failure enrolled in a Medicare Advantage Prescription Drug or commercial healthcare plan. To identify only those patients with HFpEF, we used natural language processing techniques of ejection fraction values abstracted from a linked free-text clinical notes data source. The study population comprised 1515 patients newly identified with HFpEF between 1 January 2011 and 31 December 2015. Using unsupervised machine learning, we identified three distinguishable patient clusters representing different phenotypes: cluster-1 patients had the lowest prevalence of heart failure comorbidities and highest mean age; cluster-2 patients had higher prevalence of metabolic syndrome and pulmonary disease, despite younger mean age; and cluster-3 patients had higher prevalence of cardiac arrhythmia and renal disease. Cluster-3 had the highest 1-year heart failure related hospitalization rates. Within-cluster analysis, prior use of diuretics (cluster-1 and cluster-2) and age (cluster-2 and cluster-3) was associated with 1-year heart failure related hospitalization. Combination therapy was associated with decreased 1-year heart failure related hospitalization in cluster-1. This study demonstrated that clustering can be used to characterize subgroups of patients with newly identified HFpEF, assess differences in heart failure related hospitalization rates at 1 year and suggest patient subtypes may respond differently to treatments or interventions.","Approximately 50% of patients with heart failure have preserved (â‰¥50%) ejection fraction (HFpEF). Improved understanding of the phenotypic heterogeneity of HFpEF might facilitate development of targeted therapies and interventions. This retrospective study characterized a cohort of patients with HFpEF based on similar clinical profiles and evaluated 1-year heart failure related hospitalization. Enrolment, medical and pharmacy data were used to identify patients newly diagnosed with heart failure enrolled in a Medicare Advantage Prescription Drug or commercial healthcare plan. To identify only those patients with HFpEF, we used natural language processing techniques of ejection fraction values abstracted from a linked free-text clinical notes data source. The study population comprised 1515 patients newly identified with HFpEF between 1 January 2011 and 31 December 2015. Using unsupervised machine learning, we identified three distinguishable patient clusters representing different phenotypes: cluster-1 patients had the lowest prevalence of heart failure comorbidities and highest mean age; cluster-2 patients had higher prevalence of metabolic syndrome and pulmonary disease, despite younger mean age; and cluster-3 patients had higher prevalence of cardiac arrhythmia and renal disease. Cluster-3 had the highest 1-year heart failure related hospitalization rates. Within-cluster analysis, prior use of diuretics (cluster-1 and cluster-2) and age (cluster-2 and cluster-3) was associated with 1-year heart failure related hospitalization. Combination therapy was associated with decreased 1-year heart failure related hospitalization in cluster-1. This study demonstrated that clustering can be used to characterize subgroups of patients with newly identified HFpEF, assess differences in heart failure related hospitalization rates at 1 year and suggest patient subtypes may respond differently to treatments or interventions.","Casebeer, Horter, Hayden, Simmons, Evers","Casebeer, Horter, Hayden, Simmons, Evers",https://doi.org/10.2459/JCM.0000000000001116,https://doi.org/10.2459/JCM.0000000000001116,2021-08-03
16180.0,pubmed,pubmed,A Magnetic Resonance Imaging Radiomics Signature to Distinguish Benign From Malignant Orbital Lesions,A Magnetic Resonance Imaging Radiomics Signature to Distinguish Benign From Malignant Orbital Lesions,"Distinguishing benign from malignant orbital lesions remains challenging both clinically and with imaging, leading to risky biopsies. The objective was to differentiate benign from malignant orbital lesions using radiomics on 3 T magnetic resonance imaging (MRI) examinations. This institutional review board-approved prospective single-center study enrolled consecutive patients presenting with an orbital lesion undergoing a 3 T MRI prior to surgery from December 2015 to July 2019. Radiomics features were extracted from 6 MRI sequences (T1-weighted images [WIs], DIXON-T2-WI, diffusion-WI, postcontrast DIXON-T1-WI) using the Pyradiomics software. Features were selected based on their intraobserver and interobserver reproducibility, nonredundancy, and with a sequential step forward feature selection method. Selected features were used to train and optimize a Random Forest algorithm on the training set (75%) with 5-fold cross-validation. Performance metrics were computed on a held-out test set (25%) with bootstrap 95% confidence intervals (95% CIs). Five residents, 4 general radiologists, and 3 expert neuroradiologists were evaluated on their ability to visually distinguish benign from malignant lesions on the test set. Performance comparisons between reader groups and the model were performed using McNemar test. The impact of clinical and categorizable imaging data on algorithm performance was also assessed. A total of 200 patients (116 [58%] women and 84 [42%] men; mean age, 53.0 Ã‚Â± 17.9 years) with 126 of 200 (63%) benign and 74 of 200 (37%) malignant orbital lesions were included in the study. A total of 606 radiomics features were extracted. The best performing model on the training set was composed of 8 features including apparent diffusion coefficient mean value, maximum diameter on T1-WIs, and texture features. Area under the receiver operating characteristic curve, accuracy, sensitivity, and specificity on the test set were respectively 0.869 (95% CI, 0.834-0.898), 0.840 (95% CI, 0.806-0.874), 0.684 (95% CI, 0.615-0.751), and 0.935 (95% CI, 0.905-0.961). The radiomics model outperformed all reader groups, including expert neuroradiologists (P &lt; 0.01). Adding clinical and categorizable imaging data did not significantly impact the algorithm performance (P = 0.49). An MRI radiomics signature is helpful in differentiating benign from malignant orbital lesions and may outperform expert radiologists.","Distinguishing benign from malignant orbital lesions remains challenging both clinically and with imaging, leading to risky biopsies. The objective was to differentiate benign from malignant orbital lesions using radiomics on 3 T magnetic resonance imaging (MRI) examinations. This institutional review board-approved prospective single-center study enrolled consecutive patients presenting with an orbital lesion undergoing a 3 T MRI prior to surgery from December 2015 to July 2019. Radiomics features were extracted from 6 MRI sequences (T1-weighted images [WIs], DIXON-T2-WI, diffusion-WI, postcontrast DIXON-T1-WI) using the Pyradiomics software. Features were selected based on their intraobserver and interobserver reproducibility, nonredundancy, and with a sequential step forward feature selection method. Selected features were used to train and optimize a Random Forest algorithm on the training set (75%) with 5-fold cross-validation. Performance metrics were computed on a held-out test set (25%) with bootstrap 95% confidence intervals (95% CIs). Five residents, 4 general radiologists, and 3 expert neuroradiologists were evaluated on their ability to visually distinguish benign from malignant lesions on the test set. Performance comparisons between reader groups and the model were performed using McNemar test. The impact of clinical and categorizable imaging data on algorithm performance was also assessed. A total of 200 patients (116 [58%] women and 84 [42%] men; mean age, 53.0 Â± 17.9 years) with 126 of 200 (63%) benign and 74 of 200 (37%) malignant orbital lesions were included in the study. A total of 606 radiomics features were extracted. The best performing model on the training set was composed of 8 features including apparent diffusion coefficient mean value, maximum diameter on T1-WIs, and texture features. Area under the receiver operating characteristic curve, accuracy, sensitivity, and specificity on the test set were respectively 0.869 (95% CI, 0.834-0.898), 0.840 (95% CI, 0.806-0.874), 0.684 (95% CI, 0.615-0.751), and 0.935 (95% CI, 0.905-0.961). The radiomics model outperformed all reader groups, including expert neuroradiologists (P &lt; 0.01). Adding clinical and categorizable imaging data did not significantly impact the algorithm performance (P = 0.49). An MRI radiomics signature is helpful in differentiating benign from malignant orbital lesions and may outperform expert radiologists.","Duron, Heraud, Charbonneau, Zmuda, Savatovsky, Fournier, Lecler","Duron, Heraud, Charbonneau, Zmuda, Savatovsky, Fournier, Lecler",https://doi.org/10.1097/RLI.0000000000000722,https://doi.org/10.1097/RLI.0000000000000722,2021-08-03
16190.0,pubmed,pubmed,The utility of automated volume analysis of renal stones before and after shockwave lithotripsy treatment,The utility of automated volume analysis of renal stones before and after shockwave lithotripsy treatment,"This study aimed to evaluate the additional utility of an automated method of estimating volume for stones being treated with shockwave lithotripsy (SWL) using computed tomography (CT) images compared to manual measurement. Utility was assessed as the ability to accurately measure stone burden before andÃ‚Â after SWL treatment, and whether stone volume is a better predictor of SWL outcome than stone diameter. 72 patients treated with SWL for a renal stone with available CT scans before and after treatment were included. Stone axesÃ‚Â measurement and volume estimation using ellipsoid equations were compared to volume estimationÃ‚Â using software usingÃ‚Â CT textural analysis (CTTA) of stone images. There was strong correlation (rÃ¢â‚¬â€°&gt;Ã¢â‚¬â€°0.8) between manual and CTTA estimated stone volume. CTTA measured stone volume showed the highest predictive value (r<sup>2</sup>Ã¢â‚¬â€°=Ã¢â‚¬â€°0.217) for successful SWL outcome on binary logistic regression analysis. Three cases that were originally classified as 'stone-free with clinically insignificant residual fragments' based on manual axis measurements actuallyÃ‚Â had a larger stone volume based on CTTA estimation than the smallest fragments remaining for cases with an outcome of 'not stone-free'. This study suggestsÃ‚Â objective measurement of total stone volume could improve estimation of stone burden before and after treatment. Current definitions of stone-free status based on manual measurements of residual fragment sizes are not accurate and may underestimate remaining stone burden after treatment. Future studies reporting on the efficacy of different stone treatments should consider using objective stone volume measurements based on CT image analysis as an outcome measure of stone-free state.","This study aimed to evaluate the additional utility of an automated method of estimating volume for stones being treated with shockwave lithotripsy (SWL) using computed tomography (CT) images compared to manual measurement. Utility was assessed as the ability to accurately measure stone burden before andÂ after SWL treatment, and whether stone volume is a better predictor of SWL outcome than stone diameter. 72 patients treated with SWL for a renal stone with available CT scans before and after treatment were included. Stone axesÂ measurement and volume estimation using ellipsoid equations were compared to volume estimationÂ using software usingÂ CT textural analysis (CTTA) of stone images. There was strong correlation (râ€‰&gt;â€‰0.8) between manual and CTTA estimated stone volume. CTTA measured stone volume showed the highest predictive value (r<sup>2</sup>â€‰=â€‰0.217) for successful SWL outcome on binary logistic regression analysis. Three cases that were originally classified as 'stone-free with clinically insignificant residual fragments' based on manual axis measurements actuallyÂ had a larger stone volume based on CTTA estimation than the smallest fragments remaining for cases with an outcome of 'not stone-free'. This study suggestsÂ objective measurement of total stone volume could improve estimation of stone burden before and after treatment. Current definitions of stone-free status based on manual measurements of residual fragment sizes are not accurate and may underestimate remaining stone burden after treatment. Future studies reporting on the efficacy of different stone treatments should consider using objective stone volume measurements based on CT image analysis as an outcome measure of stone-free state.","Cui, Tan, Christiansen, Osther, Turney","Cui, Tan, Christiansen, Osther, Turney",https://doi.org/10.1007/s00240-020-01212-8,https://doi.org/10.1007/s00240-020-01212-8,2021-08-03
16193.0,pubmed,pubmed,Artificial Intelligence-Based Conversational Agents for Chronic Conditions: Systematic Literature Review,Artificial Intelligence-Based Conversational Agents for Chronic Conditions: Systematic Literature Review,"A rising number of conversational agents or chatbots are equipped with artificial intelligence (AI) architecture. They are increasingly prevalent in health care applications such as those providing education and support to patients with chronic diseases, one of the leading causes of death in the 21st century. AI-based chatbots enable more effective and frequent interactions with such patients. The goal of this systematic literature review is to review the characteristics, health care conditions, and AI architectures of AI-based conversational agents designed specifically for chronic diseases. We conducted a systematic literature review using PubMed MEDLINE, EMBASE, PyscInfo, CINAHL, ACM Digital Library, ScienceDirect, and Web of Science. We applied a predefined search strategy using the terms &quot;conversational agent,&quot; &quot;healthcare,&quot; &quot;artificial intelligence,&quot; and their synonyms. We updated the search results using Google alerts, and screened reference lists for other relevant articles. We included primary research studies that involved the prevention, treatment, or rehabilitation of chronic diseases, involved a conversational agent, and included any kind of AI architecture. Two independent reviewers conducted screening and data extraction, and Cohen kappa was used to measure interrater agreement.A narrative approach was applied for data synthesis. The literature search found 2052 articles, out of which 10 papers met the inclusion criteria. The small number of identified studies together with the prevalence of quasi-experimental studies (n=7) and prevailing prototype nature of the chatbots (n=7) revealed the immaturity of the field. The reported chatbots addressed a broad variety of chronic diseases (n=6), showcasing a tendency to develop specialized conversational agents for individual chronic conditions. However, there lacks comparison of these chatbots within and between chronic diseases. In addition, the reported evaluation measures were not standardized, and the addressed health goals showed a large range. Together, these study characteristics complicated comparability and open room for future research. While natural language processing represented the most used AI technique (n=7) and the majority of conversational agents allowed for multimodal interaction (n=6), the identified studies demonstrated broad heterogeneity, lack of depth of reported AI techniques and systems, and inconsistent usage of taxonomy of the underlying AI software, further aggravating comparability and generalizability of study results. The literature on AI-based conversational agents for chronic conditions is scarce and mostly consists of quasi-experimental studies with chatbots in prototype stage that use natural language processing and allow for multimodal user interaction. Future research could profit from evidence-based evaluation of the AI-based conversational agents and comparison thereof within and between different chronic health conditions. Besides increased comparability, the quality of chatbots developed for specific chronic conditions and their subsequent impact on the target patients could be enhanced by more structured development and standardized evaluation processes.","A rising number of conversational agents or chatbots are equipped with artificial intelligence (AI) architecture. They are increasingly prevalent in health care applications such as those providing education and support to patients with chronic diseases, one of the leading causes of death in the 21st century. AI-based chatbots enable more effective and frequent interactions with such patients. The goal of this systematic literature review is to review the characteristics, health care conditions, and AI architectures of AI-based conversational agents designed specifically for chronic diseases. We conducted a systematic literature review using PubMed MEDLINE, EMBASE, PyscInfo, CINAHL, ACM Digital Library, ScienceDirect, and Web of Science. We applied a predefined search strategy using the terms ""conversational agent,"" ""healthcare,"" ""artificial intelligence,"" and their synonyms. We updated the search results using Google alerts, and screened reference lists for other relevant articles. We included primary research studies that involved the prevention, treatment, or rehabilitation of chronic diseases, involved a conversational agent, and included any kind of AI architecture. Two independent reviewers conducted screening and data extraction, and Cohen kappa was used to measure interrater agreement.A narrative approach was applied for data synthesis. The literature search found 2052 articles, out of which 10 papers met the inclusion criteria. The small number of identified studies together with the prevalence of quasi-experimental studies (n=7) and prevailing prototype nature of the chatbots (n=7) revealed the immaturity of the field. The reported chatbots addressed a broad variety of chronic diseases (n=6), showcasing a tendency to develop specialized conversational agents for individual chronic conditions. However, there lacks comparison of these chatbots within and between chronic diseases. In addition, the reported evaluation measures were not standardized, and the addressed health goals showed a large range. Together, these study characteristics complicated comparability and open room for future research. While natural language processing represented the most used AI technique (n=7) and the majority of conversational agents allowed for multimodal interaction (n=6), the identified studies demonstrated broad heterogeneity, lack of depth of reported AI techniques and systems, and inconsistent usage of taxonomy of the underlying AI software, further aggravating comparability and generalizability of study results. The literature on AI-based conversational agents for chronic conditions is scarce and mostly consists of quasi-experimental studies with chatbots in prototype stage that use natural language processing and allow for multimodal user interaction. Future research could profit from evidence-based evaluation of the AI-based conversational agents and comparison thereof within and between different chronic health conditions. Besides increased comparability, the quality of chatbots developed for specific chronic conditions and their subsequent impact on the target patients could be enhanced by more structured development and standardized evaluation processes.","Schachner, Keller, V Wangenheim","Schachner, Keller, V Wangenheim",https://doi.org/10.2196/20701,https://doi.org/10.2196/20701,2021-08-03
16194.0,pubmed,pubmed,Natural language processing for automated quantification of bone metastases reported in free-text bone scintigraphy reports,Natural language processing for automated quantification of bone metastases reported in free-text bone scintigraphy reports,"The widespread use of electronic patient-generated health data has led to unprecedented opportunities for automated extraction of clinical features from free-text medical notes. However, processing this rich resource of data for clinical and research purposes, depends on labor-intensive and potentially error-prone manual review. The aim of this study was to develop a natural language processing (NLP) algorithm for binary classification (single metastasis versus two or more metastases) in bone scintigraphy reports of patients undergoing surgery for bone metastases. Bone scintigraphy reports of patients undergoing surgery for bone metastases were labeled each by three independent reviewers using a binary classification (single metastasis versus two or more metastases) to establish a ground truth. A stratified 80:20 split was used to develop and test an extreme-gradient boosting supervised machine learning NLP algorithm. A total of 704 free-text bone scintigraphy reports from 704 patients were included in this study and 617 (88%) had multiple bone metastases. In the independent test set (<i>n</i>Ã¢â‚¬â€°=Ã¢â‚¬â€°141) not used for model development, the NLP algorithm achieved an 0.97 AUC-ROC (95% confidence interval [CI], 0.92-0.99) for classification of multiple bone metastases and an 0.99 AUC-PRC (95% CI, 0.99-0.99). At a threshold of 0.90, NLP algorithm correctly identified multiple bone metastases in 117 of the 124 who had multiple bone metastases in the testing cohort (sensitivity 0.94) and yielded 3 false positives (specificity 0.82). At the same threshold, the NLP algorithm had a positive predictive value of 0.97 and F1-score of 0.96. NLP has the potential to automate clinical data extraction from free text radiology notes in orthopedics, thereby optimizing the speed, accuracy, and consistency of clinical chart review. Pending external validation, the NLP algorithm developed in this study may be implemented as a means to aid researchers in tackling large amounts of data.","The widespread use of electronic patient-generated health data has led to unprecedented opportunities for automated extraction of clinical features from free-text medical notes. However, processing this rich resource of data for clinical and research purposes, depends on labor-intensive and potentially error-prone manual review. The aim of this study was to develop a natural language processing (NLP) algorithm for binary classification (single metastasis versus two or more metastases) in bone scintigraphy reports of patients undergoing surgery for bone metastases. Bone scintigraphy reports of patients undergoing surgery for bone metastases were labeled each by three independent reviewers using a binary classification (single metastasis versus two or more metastases) to establish a ground truth. A stratified 80:20 split was used to develop and test an extreme-gradient boosting supervised machine learning NLP algorithm. A total of 704 free-text bone scintigraphy reports from 704 patients were included in this study and 617 (88%) had multiple bone metastases. In the independent test set (<i>n</i>â€‰=â€‰141) not used for model development, the NLP algorithm achieved an 0.97 AUC-ROC (95% confidence interval [CI], 0.92-0.99) for classification of multiple bone metastases and an 0.99 AUC-PRC (95% CI, 0.99-0.99). At a threshold of 0.90, NLP algorithm correctly identified multiple bone metastases in 117 of the 124 who had multiple bone metastases in the testing cohort (sensitivity 0.94) and yielded 3 false positives (specificity 0.82). At the same threshold, the NLP algorithm had a positive predictive value of 0.97 and F1-score of 0.96. NLP has the potential to automate clinical data extraction from free text radiology notes in orthopedics, thereby optimizing the speed, accuracy, and consistency of clinical chart review. Pending external validation, the NLP algorithm developed in this study may be implemented as a means to aid researchers in tackling large amounts of data.","Groot, Bongers, Karhade, Kapoor, Fenn, Kim, Verlaan, Schwab","Groot, Bongers, Karhade, Kapoor, Fenn, Kim, Verlaan, Schwab",https://doi.org/10.1080/0284186X.2020.1819563,https://doi.org/10.1080/0284186X.2020.1819563,2021-08-03
16196.0,pubmed,pubmed,Review of medical image recognition technologies to detect melanomas using neural networks,Review of medical image recognition technologies to detect melanomas using neural networks,"Melanoma is one of the most aggressive types of cancer that has become a world-class problem. According to the World Health Organization estimates, 132,000 cases of the disease and 66,000 deaths from malignant melanoma and other forms of skin cancer are reported annually worldwide ( https://apps.who.int/gho/data/?theme=main ) and those numbers continue to grow. In our opinion, due to the increasing incidence of the disease, it is necessary to find new, easy to use and sensitive methods for the early diagnosis of melanoma in a large number of people around the world. Over the last decade, neural networks show highly sensitive, specific, and accurate results. This study presents a review of PubMed papers including requests Ã‚Â«melanoma neural networkÃ‚Â» and Ã‚Â«melanoma neural network dermatoscopyÃ‚Â». We review recent researches and discuss their opportunities acceptable in clinical practice. We searched the PubMed database for systematic reviews and original research papers on the requests Ã‚Â«melanoma neural networkÃ‚Â» and Ã‚Â«melanoma neural network dermatoscopyÃ‚Â» published in English. Only papers that reported results, progress and outcomes are included in this review. We found 11 papers that match our requests that observed convolutional and deep-learning neural networks combined with fuzzy clustering or World Cup Optimization algorithms in analyzing dermatoscopic images. All of them require an ABCD (asymmetry, border, color, and differential structures) algorithm and its derivates (in combination with ABCD algorithm or separately). Also, they require a large dataset of dermatoscopic images and optimized estimation parameters to provide high specificity, accuracy and sensitivity. According to the analyzed papers, neural networks show higher specificity, accuracy and sensitivity than dermatologists. Neural networks are able to evaluate features that might be unavailable to the naked human eye. Despite that, we need more datasets to confirm those statements. Nowadays machine learning becomes a helpful tool in early diagnosing skin diseases, especially melanoma.","Melanoma is one of the most aggressive types of cancer that has become a world-class problem. According to the World Health Organization estimates, 132,000 cases of the disease and 66,000 deaths from malignant melanoma and other forms of skin cancer are reported annually worldwide ( https://apps.who.int/gho/data/?theme=main ) and those numbers continue to grow. In our opinion, due to the increasing incidence of the disease, it is necessary to find new, easy to use and sensitive methods for the early diagnosis of melanoma in a large number of people around the world. Over the last decade, neural networks show highly sensitive, specific, and accurate results. This study presents a review of PubMed papers including requests Â«melanoma neural networkÂ» and Â«melanoma neural network dermatoscopyÂ». We review recent researches and discuss their opportunities acceptable in clinical practice. We searched the PubMed database for systematic reviews and original research papers on the requests Â«melanoma neural networkÂ» and Â«melanoma neural network dermatoscopyÂ» published in English. Only papers that reported results, progress and outcomes are included in this review. We found 11 papers that match our requests that observed convolutional and deep-learning neural networks combined with fuzzy clustering or World Cup Optimization algorithms in analyzing dermatoscopic images. All of them require an ABCD (asymmetry, border, color, and differential structures) algorithm and its derivates (in combination with ABCD algorithm or separately). Also, they require a large dataset of dermatoscopic images and optimized estimation parameters to provide high specificity, accuracy and sensitivity. According to the analyzed papers, neural networks show higher specificity, accuracy and sensitivity than dermatologists. Neural networks are able to evaluate features that might be unavailable to the naked human eye. Despite that, we need more datasets to confirm those statements. Nowadays machine learning becomes a helpful tool in early diagnosing skin diseases, especially melanoma.","Efimenko, Ignatev, Koshechkin","Efimenko, Ignatev, Koshechkin",https://doi.org/10.1186/s12859-020-03615-1,https://doi.org/10.1186/s12859-020-03615-1,2021-08-03
16201.0,pubmed,pubmed,ADHD and ADHD-related neural networks in benign epilepsy with centrotemporal spikes: A systematic review,ADHD and ADHD-related neural networks in benign epilepsy with centrotemporal spikes: A systematic review,"Attention-deficit/hyperactivity disorder (ADHD) and benign epilepsy with centrotemporal spikes (BECTS or rolandic epilepsy) present with a very high level of comorbidity. We aimed to review the existing literature focusing on two aspects: the possible role of epileptic activity in the damage of ADHD-related neural networks and the clinical approach to patients presenting with both conditions. A systematic review was performed using Sapienza Library System and PubMed. The following search terms have been considered: attention networks, ADHD, attention systems, rolandic epilepsy, benign epilepsy with centrotemporal spikes, centrotemporal spikes epilepsy, and focal epilepsy in children. The target population consisted of patients under 18Ã¢â‚¬Â¯years of age diagnosed with either BECTS and ADHD or healthy controls. Nine case-control and cohort studies have been selected. The reported prevalence of ADHD in patients with BECTS was around 60%. No clinical correlation was found between the medical records and the presence of ADHD in patients with BECTS, if not due to febrile convulsion (FC). One study showed higher levels of bilateral discharges in patients with severe ADHD. The negative influence of the age at onset of seizures was demonstrated on attention but not on intelligence quotient (IQ). Moreover, the frequency of seizures and the occurrence of discharges during nonrapid eye movement (NREM) sleep were correlated to attention impairment. From a neurobiological point of view, functional connectivity in patients with BECTS and ADHD appears to be disrupted. Two studies reported a specific impairment in selective visual attention, while one study underlined a decreased activation of the dorsal attention network (DAN). Two different studies found that patients with BECTS and comorbid ADHD presented with altered thickness in their magnetic resonance imaging (MRI) scans in the cortical and subcortical regions (including the frontal lobes, lingual-fusiform cortex, cuneus and precuneus, limbic area and pericalcarine cortex among others). This might explain the cognitive and behavioral symptoms such as poor selective visual attention, speech disturbance, and impulsivity. Despite BECTS being considered to have a relative benign course, many studies have documented cognitive and/or behavioral problems in patients diagnosed with this type of epilepsy. In particular, children affected by rolandic epilepsy should receive a complete neuropsychological evaluation at seizure onset considering the high rate of comorbidity with ADHD. A further investigation of the common pathogenic substrate is desirable to better orientate the clinical and therapeutic interventions applied.","Attention-deficit/hyperactivity disorder (ADHD) and benign epilepsy with centrotemporal spikes (BECTS or rolandic epilepsy) present with a very high level of comorbidity. We aimed to review the existing literature focusing on two aspects: the possible role of epileptic activity in the damage of ADHD-related neural networks and the clinical approach to patients presenting with both conditions. A systematic review was performed using Sapienza Library System and PubMed. The following search terms have been considered: attention networks, ADHD, attention systems, rolandic epilepsy, benign epilepsy with centrotemporal spikes, centrotemporal spikes epilepsy, and focal epilepsy in children. The target population consisted of patients under 18â€¯years of age diagnosed with either BECTS and ADHD or healthy controls. Nine case-control and cohort studies have been selected. The reported prevalence of ADHD in patients with BECTS was around 60%. No clinical correlation was found between the medical records and the presence of ADHD in patients with BECTS, if not due to febrile convulsion (FC). One study showed higher levels of bilateral discharges in patients with severe ADHD. The negative influence of the age at onset of seizures was demonstrated on attention but not on intelligence quotient (IQ). Moreover, the frequency of seizures and the occurrence of discharges during nonrapid eye movement (NREM) sleep were correlated to attention impairment. From a neurobiological point of view, functional connectivity in patients with BECTS and ADHD appears to be disrupted. Two studies reported a specific impairment in selective visual attention, while one study underlined a decreased activation of the dorsal attention network (DAN). Two different studies found that patients with BECTS and comorbid ADHD presented with altered thickness in their magnetic resonance imaging (MRI) scans in the cortical and subcortical regions (including the frontal lobes, lingual-fusiform cortex, cuneus and precuneus, limbic area and pericalcarine cortex among others). This might explain the cognitive and behavioral symptoms such as poor selective visual attention, speech disturbance, and impulsivity. Despite BECTS being considered to have a relative benign course, many studies have documented cognitive and/or behavioral problems in patients diagnosed with this type of epilepsy. In particular, children affected by rolandic epilepsy should receive a complete neuropsychological evaluation at seizure onset considering the high rate of comorbidity with ADHD. A further investigation of the common pathogenic substrate is desirable to better orientate the clinical and therapeutic interventions applied.","AricÃƒÂ², Arigliani, Giannotti, Romani","AricÃ², Arigliani, Giannotti, Romani",https://doi.org/10.1016/j.yebeh.2020.107448,https://doi.org/10.1016/j.yebeh.2020.107448,2021-08-03
16203.0,pubmed,pubmed,Inconsistency in the use of the term &quot;validation&quot; in studies reporting the performance of deep learning algorithms in providing diagnosis from medical imaging,"Inconsistency in the use of the term ""validation"" in studies reporting the performance of deep learning algorithms in providing diagnosis from medical imaging","The development of deep learning (DL) algorithms is a three-step process-training, tuning, and testing. Studies are inconsistent in the use of the term &quot;validation&quot;, with some using it to refer to tuning and others testing, which hinders accurate delivery of information and may inadvertently exaggerate the performance of DL algorithms. We investigated the extent of inconsistency in usage of the term &quot;validation&quot; in studies on the accuracy of DL algorithms in providing diagnosis from medical imaging. We analyzed the full texts of research papers cited in two recent systematic reviews. The papers were categorized according to whether the term &quot;validation&quot; was used to refer to tuning alone, both tuning and testing, or testing alone. We analyzed whether paper characteristics (i.e., journal category, field of study, year of print publication, journal impact factor [JIF], and nature of test data) were associated with the usage of the terminology using multivariable logistic regression analysis with generalized estimating equations. Of 201 papers published in 125 journals, 118 (58.7%), 9 (4.5%), and 74 (36.8%) used the term to refer to tuning alone, both tuning and testing, and testing alone, respectively. A weak association was noted between higher JIF and using the term to refer to testing (i.e., testing alone or both tuning and testing) instead of tuning alone (vs. JIF &lt;5; JIF 5 to 10: adjusted odds ratio 2.11, P = 0.042; JIF &gt;10: adjusted odds ratio 2.41, P = 0.089). Journal category, field of study, year of print publication, and nature of test data were not significantly associated with the terminology usage. Existing literature has a significant degree of inconsistency in using the term &quot;validation&quot; when referring to the steps in DL algorithm development. Efforts are needed to improve the accuracy and clarity in the terminology usage.","The development of deep learning (DL) algorithms is a three-step process-training, tuning, and testing. Studies are inconsistent in the use of the term ""validation"", with some using it to refer to tuning and others testing, which hinders accurate delivery of information and may inadvertently exaggerate the performance of DL algorithms. We investigated the extent of inconsistency in usage of the term ""validation"" in studies on the accuracy of DL algorithms in providing diagnosis from medical imaging. We analyzed the full texts of research papers cited in two recent systematic reviews. The papers were categorized according to whether the term ""validation"" was used to refer to tuning alone, both tuning and testing, or testing alone. We analyzed whether paper characteristics (i.e., journal category, field of study, year of print publication, journal impact factor [JIF], and nature of test data) were associated with the usage of the terminology using multivariable logistic regression analysis with generalized estimating equations. Of 201 papers published in 125 journals, 118 (58.7%), 9 (4.5%), and 74 (36.8%) used the term to refer to tuning alone, both tuning and testing, and testing alone, respectively. A weak association was noted between higher JIF and using the term to refer to testing (i.e., testing alone or both tuning and testing) instead of tuning alone (vs. JIF &lt;5; JIF 5 to 10: adjusted odds ratio 2.11, P = 0.042; JIF &gt;10: adjusted odds ratio 2.41, P = 0.089). Journal category, field of study, year of print publication, and nature of test data were not significantly associated with the terminology usage. Existing literature has a significant degree of inconsistency in using the term ""validation"" when referring to the steps in DL algorithm development. Efforts are needed to improve the accuracy and clarity in the terminology usage.","Kim, Jang, Ko, Son, Kim, Kim, Lim, Park","Kim, Jang, Ko, Son, Kim, Kim, Lim, Park",https://doi.org/10.1371/journal.pone.0238908,https://doi.org/10.1371/journal.pone.0238908,2021-08-03
16214.0,pubmed,pubmed,Motivations for performing scholarly prepublication peer review: A scoping review,Motivations for performing scholarly prepublication peer review: A scoping review,"Prepublication peer review is a cornerstone of science. Overburdened reviewers invest millions of hours in this voluntary activity. In this scoping review, we aimed at identifying motivations for performing prepublication peer review of scholarly manuscripts. Original research studies investigating actual peer reviewers' motivations were included. We excluded modeling studies, studies related to other types of peer review, guidelines, peer review processes in particular journals. Medline, WoS, and Scopus were searched in February 2016, with no language or time limitations, and the search was updated in July 2019. The search yielded 5,250 records, and 382 were chosen for full text analysis, out of which 10 were appropriate for synthesis. Reference snowballing identified one eligible study. Eleven studies were appropriate for synthesis: four qualitative, four mixed qualitative/quantitative, and three qualitative studies, published from 1998 to 2018, involving 6,667 respondents. Major internal incentive was &quot;communal obligations and reciprocity.&quot; Major external incentives were &quot;career advancement,&quot; &quot;being recognized as an expert,&quot; and &quot;building relationships with journals and editors.&quot; Major disincentive was the &quot;lack of time.&quot; Editors could incentivize peer review process by choosing highest quality articles, improving communication with peer reviewers, in order to make the process of peer review as short and efficient as possible. The gaps in research concern disincentives to review.","Prepublication peer review is a cornerstone of science. Overburdened reviewers invest millions of hours in this voluntary activity. In this scoping review, we aimed at identifying motivations for performing prepublication peer review of scholarly manuscripts. Original research studies investigating actual peer reviewers' motivations were included. We excluded modeling studies, studies related to other types of peer review, guidelines, peer review processes in particular journals. Medline, WoS, and Scopus were searched in February 2016, with no language or time limitations, and the search was updated in July 2019. The search yielded 5,250 records, and 382 were chosen for full text analysis, out of which 10 were appropriate for synthesis. Reference snowballing identified one eligible study. Eleven studies were appropriate for synthesis: four qualitative, four mixed qualitative/quantitative, and three qualitative studies, published from 1998 to 2018, involving 6,667 respondents. Major internal incentive was ""communal obligations and reciprocity."" Major external incentives were ""career advancement,"" ""being recognized as an expert,"" and ""building relationships with journals and editors."" Major disincentive was the ""lack of time."" Editors could incentivize peer review process by choosing highest quality articles, improving communication with peer reviewers, in order to make the process of peer review as short and efficient as possible. The gaps in research concern disincentives to review.","MahmiÃ„â€¡-Kaknjo, UtrobiÃ„ÂiÃ„â€¡, MaruÃ…Â¡iÃ„â€¡","MahmiÄ‡-Kaknjo, UtrobiÄiÄ‡, MaruÅ¡iÄ‡",https://doi.org/10.1080/08989621.2020.1822170,https://doi.org/10.1080/08989621.2020.1822170,2021-08-03
16216.0,pubmed,pubmed,The evolution of social health research topics: A data-driven analysis,The evolution of social health research topics: A data-driven analysis,"The realm of social health has not yet been properly established in terms of fixed definitions, concepts, and research areas. This study attempts to define social health using macro and micro perspectives and explores trends in social health research by mapping their topics and fields. We used Latent Dirichlet allocation (LDA) topic modeling, which allows the extraction of key terms and topics derived from a large volume of literature. We traced the evolution of research topics from past (the literature that &quot;present&quot; articles cited), present (existing journal articles on social health), to future (the literature which cited the articles) studies based on connections between citations. The datasets were collected by the query terms &quot;social health&quot; in the Scopus database, including title, abstract, and keywords of journal articles. We collected a total of 443 articles from recent social health literature, 6588 articles from past literature that the recent articles on social health cited, and 2680 articles from future literature in which recent social health articles were cited. We defined social health as positive interaction that increases individual engagement in social life at the micro level, and the high degree of social integration that deals with collective problems in society at the macro level. The results of LDA showed that social health research has developed into seven fields: Health Care Delivery; Vulnerable Groups; Measurement; Health Inequality; Social Network and Empowerment; Clinical/Physical Health; and Mental/Behavioral Health. Based on citation relationships, topics grounded in an individual/micro perspective have grown increasingly specialized and productive, while topics grounded in a social/macro perspective have stagnated or was underexplored. Our findings imply that social health studies should follow a more interdisciplinary approach to integrate current health models of individual-centered treatments with social science concerns on building collective capacity for social well-being.","The realm of social health has not yet been properly established in terms of fixed definitions, concepts, and research areas. This study attempts to define social health using macro and micro perspectives and explores trends in social health research by mapping their topics and fields. We used Latent Dirichlet allocation (LDA) topic modeling, which allows the extraction of key terms and topics derived from a large volume of literature. We traced the evolution of research topics from past (the literature that ""present"" articles cited), present (existing journal articles on social health), to future (the literature which cited the articles) studies based on connections between citations. The datasets were collected by the query terms ""social health"" in the Scopus database, including title, abstract, and keywords of journal articles. We collected a total of 443 articles from recent social health literature, 6588 articles from past literature that the recent articles on social health cited, and 2680 articles from future literature in which recent social health articles were cited. We defined social health as positive interaction that increases individual engagement in social life at the micro level, and the high degree of social integration that deals with collective problems in society at the macro level. The results of LDA showed that social health research has developed into seven fields: Health Care Delivery; Vulnerable Groups; Measurement; Health Inequality; Social Network and Empowerment; Clinical/Physical Health; and Mental/Behavioral Health. Based on citation relationships, topics grounded in an individual/micro perspective have grown increasingly specialized and productive, while topics grounded in a social/macro perspective have stagnated or was underexplored. Our findings imply that social health studies should follow a more interdisciplinary approach to integrate current health models of individual-centered treatments with social science concerns on building collective capacity for social well-being.","Cho, Park, Song","Cho, Park, Song",https://doi.org/10.1016/j.socscimed.2020.113299,https://doi.org/10.1016/j.socscimed.2020.113299,2021-08-03
16220.0,pubmed,pubmed,Tumor Control Probability Modeling and Systematic Review of the Literature of Stereotactic Body Radiation Therapy for Prostate Cancer,Tumor Control Probability Modeling and Systematic Review of the Literature of Stereotactic Body Radiation Therapy for Prostate Cancer,"Dose escalation improves localized prostate cancer disease control, and moderately hypofractionated external beam radiation is noninferior to conventional fractionation. The evolving treatment approach of ultrahypofractionation with stereotactic body radiation therapy (SBRT) allows possible further biological dose escalation (biologically equivalent dose [BED]) and shortened treatment time. The American Association of Physicists in Medicine Working Group on Biological Effects of Hypofractionated Radiation Therapy/SBRT included a subgroup to study the prostate tumor control probability (TCP) with SBRT. We performed a systematic review of the available literature and created a dose-response TCP model for the endpoint of freedom from biochemical relapse. Results were stratified by prostate cancer risk group. Twenty-five published cohorts were identified for inclusion, with a total of 4821 patients (2235 with low-risk, 1894 with intermediate-risk, and 446 with high-risk disease, when reported) treated with a variety of dose/fractionation schemes, permitting dose-response modeling. Five studies had a median follow-up of more than 5 years. Dosing regimens ranged from 32 to 50 Gy in 4 to 5 fractions, with total BED (ÃŽÂ±/ÃŽÂ² = 1.5 Gy) between 183.1 and 383.3 Gy. At 5 years, we found that in patients with low-intermediate risk disease, an equivalent doses of 2 Gy per fraction (EQD2) of 71 Gy (31.7 Gy in 5 fractions) achieved a TCP of 90% and an EQD2 of 90 Gy (36.1 Gy in 5 fractions) achieved a TCP of 95%. In patients with high-risk disease, an EQD2 of 97 Gy (37.6 Gy in 5 fractions) can achieve a TCP of 90% and an EQD2 of 102 Gy (38.7 Gy in 5 fractions) can achieve a TCP of 95%. We found significant variation in the published literature on target delineation, margins used, dose/fractionation, and treatment schedule. Despite this variation, TCP was excellent. Most prescription doses range from 35 to 40 Gy, delivered in 4 to 5 fractions. The literature did not provide detailed dose-volume data, and our dosimetric analysis was constrained to prescription doses. There are many areas in need of continued research as SBRT continues to evolve as a treatment modality for prostate cancer, including the durability of local control with longer follow-up across risk groups, the efficacy and safety of SBRT as a boost to intensity modulated radiation therapy (IMRT), and the impact of incorporating novel imaging techniques into treatment planning.","Dose escalation improves localized prostate cancer disease control, and moderately hypofractionated external beam radiation is noninferior to conventional fractionation. The evolving treatment approach of ultrahypofractionation with stereotactic body radiation therapy (SBRT) allows possible further biological dose escalation (biologically equivalent dose [BED]) and shortened treatment time. The American Association of Physicists in Medicine Working Group on Biological Effects of Hypofractionated Radiation Therapy/SBRT included a subgroup to study the prostate tumor control probability (TCP) with SBRT. We performed a systematic review of the available literature and created a dose-response TCP model for the endpoint of freedom from biochemical relapse. Results were stratified by prostate cancer risk group. Twenty-five published cohorts were identified for inclusion, with a total of 4821 patients (2235 with low-risk, 1894 with intermediate-risk, and 446 with high-risk disease, when reported) treated with a variety of dose/fractionation schemes, permitting dose-response modeling. Five studies had a median follow-up of more than 5 years. Dosing regimens ranged from 32 to 50 Gy in 4 to 5 fractions, with total BED (Î±/Î² = 1.5 Gy) between 183.1 and 383.3 Gy. At 5 years, we found that in patients with low-intermediate risk disease, an equivalent doses of 2 Gy per fraction (EQD2) of 71 Gy (31.7 Gy in 5 fractions) achieved a TCP of 90% and an EQD2 of 90 Gy (36.1 Gy in 5 fractions) achieved a TCP of 95%. In patients with high-risk disease, an EQD2 of 97 Gy (37.6 Gy in 5 fractions) can achieve a TCP of 90% and an EQD2 of 102 Gy (38.7 Gy in 5 fractions) can achieve a TCP of 95%. We found significant variation in the published literature on target delineation, margins used, dose/fractionation, and treatment schedule. Despite this variation, TCP was excellent. Most prescription doses range from 35 to 40 Gy, delivered in 4 to 5 fractions. The literature did not provide detailed dose-volume data, and our dosimetric analysis was constrained to prescription doses. There are many areas in need of continued research as SBRT continues to evolve as a treatment modality for prostate cancer, including the durability of local control with longer follow-up across risk groups, the efficacy and safety of SBRT as a boost to intensity modulated radiation therapy (IMRT), and the impact of incorporating novel imaging techniques into treatment planning.","Royce, Mavroidis, Wang, Falchook, Sheets, Fuller, Collins, El Naqa, Song, Ding, Nahum, Jackson, Grimm, Yorke, Chen","Royce, Mavroidis, Wang, Falchook, Sheets, Fuller, Collins, El Naqa, Song, Ding, Nahum, Jackson, Grimm, Yorke, Chen",https://doi.org/10.1016/j.ijrobp.2020.08.014,https://doi.org/10.1016/j.ijrobp.2020.08.014,2021-08-03
16221.0,pubmed,pubmed,Machine learning improves mortality risk prediction after cardiac surgery: Systematic review and meta-analysis,Machine learning improves mortality risk prediction after cardiac surgery: Systematic review and meta-analysis,"Interest in the usefulness of machine learning (ML) methods for outcomes prediction has continued to increase in recent years. However, the advantage of advanced ML model over traditional logistic regression (LR) remains controversial. We performed a systematic review and meta-analysis of studies comparing the discrimination accuracy between ML models versus LR in predicting operative mortality following cardiac surgery. The present systematic review followed the Preferred Reporting Items for Systematic Reviews and Meta-Analysis statement. Discrimination ability was assessed using the C-statistic. Pooled C-statistics and its 95% credibility interval for ML models and LR were obtained were obtained using a Bayesian framework. Pooled estimates for ML models and LR were compared to inform on difference between the 2 approaches. We identified 459 published citations of which 15 studies met inclusion criteria and were used for the quantitative and qualitative analysis. When the best ML model from individual study was used, meta-analytic estimates showed that ML were associated with a significantly higher C-statistic (ML, 0.88; 95% credibility interval, 0.83-0.93 vs LR, 0.81; 95% credibility interval, 0.77-0.85; PÃ‚Â =Ã‚Â .03). When individual ML algorithms were instead selected, we found a nonsignificant trend toward better prediction with each of ML algorithms. We found no evidence of publication bias (PÃ‚Â =Ã‚Â .70). The present findings suggest that when compared with LR, ML models provide better discrimination in mortality prediction after cardiac surgery. However, the magnitude and clinical influence of such an improvement remains uncertain.","Interest in the usefulness of machine learning (ML) methods for outcomes prediction has continued to increase in recent years. However, the advantage of advanced ML model over traditional logistic regression (LR) remains controversial. We performed a systematic review and meta-analysis of studies comparing the discrimination accuracy between ML models versus LR in predicting operative mortality following cardiac surgery. The present systematic review followed the Preferred Reporting Items for Systematic Reviews and Meta-Analysis statement. Discrimination ability was assessed using the C-statistic. Pooled C-statistics and its 95% credibility interval for ML models and LR were obtained were obtained using a Bayesian framework. Pooled estimates for ML models and LR were compared to inform on difference between the 2 approaches. We identified 459 published citations of which 15 studies met inclusion criteria and were used for the quantitative and qualitative analysis. When the best ML model from individual study was used, meta-analytic estimates showed that ML were associated with a significantly higher C-statistic (ML, 0.88; 95% credibility interval, 0.83-0.93 vs LR, 0.81; 95% credibility interval, 0.77-0.85; PÂ =Â .03). When individual ML algorithms were instead selected, we found a nonsignificant trend toward better prediction with each of ML algorithms. We found no evidence of publication bias (PÂ =Â .70). The present findings suggest that when compared with LR, ML models provide better discrimination in mortality prediction after cardiac surgery. However, the magnitude and clinical influence of such an improvement remains uncertain.","Benedetto, Dimagli, Sinha, Cocomello, Gibbison, Caputo, Gaunt, Lyon, Holmes, Angelini","Benedetto, Dimagli, Sinha, Cocomello, Gibbison, Caputo, Gaunt, Lyon, Holmes, Angelini",https://doi.org/10.1016/j.jtcvs.2020.07.105,https://doi.org/10.1016/j.jtcvs.2020.07.105,2021-08-03
16228.0,pubmed,pubmed,Normal appearing brain white matter changes in relapsing multiple sclerosis: Texture image and classification analysis in serial MRI scans,Normal appearing brain white matter changes in relapsing multiple sclerosis: Texture image and classification analysis in serial MRI scans,"There is a clinical interest in identifying normal appearing white matter (NAWM) areas in brain T2-weighted (T<sub>2</sub>W) MRI scans in multiple sclerosis (MS) subjects. These areas are susceptible to disease development and areas need to be studied in order to find potential associations between texture feature changes and disease progression. The subjects investigated had a first demyelinating event (Clinically Isolated Syndrome-CIS) at baseline (Time<sub>0</sub>), and the NAWM<sub>0</sub> (i.e. NAWM at Time<sub>0</sub>) of the brain tissue was subsequently converted to demyelinating plaques (as evaluated in a follow up MRI at Time<sub>6</sub><sub>-</sub><sub>12</sub>). 38 untreated subjects that had developed a CIS, had brain MRI scans within an interval of 6-12Ã‚Â months (Time<sub>6</sub><sub>-</sub><sub>12</sub> at follow-up). An experienced MS neurologist manually delineated the demyelinating lesions at Time<sub>0</sub> (L<sub>0</sub>) and at Time<sub>6</sub><sub>-</sub><sub>12</sub> (L<sub>6</sub><sub>-</sub><sub>12</sub>). Areas in the Time<sub>6</sub><sub>-</sub><sub>12</sub> MRI scans, where new lesions had been developed, were mapped back to their corresponding NAWM areas on the Time<sub>0</sub> MR scans (ROIS<sub>0</sub>). In addition, contralateral ROIs of similar size and shape were segmented on the same images at Time<sub>0</sub> (ROIS<sub>C0</sub>) to form an intra-subject control group. Following that, texture features were extracted from all prescribed areas and MS lesions. Texture features were used as input into Support Vector Machine (SVM) models to differentiate between the following: NAWM<sub>0</sub> vs ROIS<sub>C0</sub>, NAWM<sub>0</sub> vs NAWM<sub>6</sub><sub>-</sub><sub>12</sub>, NAWM<sub>0</sub> vs L<sub>0</sub>, NAWM<sub>6</sub><sub>-</sub><sub>12</sub> vs L<sub>6</sub><sub>-</sub><sub>12</sub>, ROIS<sub>0</sub> vs L<sub>0</sub>, ROIS<sub>0</sub> vs L<sub>6</sub><sub>-</sub><sub>12</sub> and ROIS<sub>0</sub> vs ROIS<sub>C0</sub>, where the corresponding % correct classifications scores were 89%, 95%, 98%, 92%, 85%, 90% and 65% respectively. Texture features may provide complementary information for following up the development and progression of MS disease. Future work will investigate the proposed method on more subjects.","There is a clinical interest in identifying normal appearing white matter (NAWM) areas in brain T2-weighted (T<sub>2</sub>W) MRI scans in multiple sclerosis (MS) subjects. These areas are susceptible to disease development and areas need to be studied in order to find potential associations between texture feature changes and disease progression. The subjects investigated had a first demyelinating event (Clinically Isolated Syndrome-CIS) at baseline (Time<sub>0</sub>), and the NAWM<sub>0</sub> (i.e. NAWM at Time<sub>0</sub>) of the brain tissue was subsequently converted to demyelinating plaques (as evaluated in a follow up MRI at Time<sub>6</sub><sub>-</sub><sub>12</sub>). 38 untreated subjects that had developed a CIS, had brain MRI scans within an interval of 6-12Â months (Time<sub>6</sub><sub>-</sub><sub>12</sub> at follow-up). An experienced MS neurologist manually delineated the demyelinating lesions at Time<sub>0</sub> (L<sub>0</sub>) and at Time<sub>6</sub><sub>-</sub><sub>12</sub> (L<sub>6</sub><sub>-</sub><sub>12</sub>). Areas in the Time<sub>6</sub><sub>-</sub><sub>12</sub> MRI scans, where new lesions had been developed, were mapped back to their corresponding NAWM areas on the Time<sub>0</sub> MR scans (ROIS<sub>0</sub>). In addition, contralateral ROIs of similar size and shape were segmented on the same images at Time<sub>0</sub> (ROIS<sub>C0</sub>) to form an intra-subject control group. Following that, texture features were extracted from all prescribed areas and MS lesions. Texture features were used as input into Support Vector Machine (SVM) models to differentiate between the following: NAWM<sub>0</sub> vs ROIS<sub>C0</sub>, NAWM<sub>0</sub> vs NAWM<sub>6</sub><sub>-</sub><sub>12</sub>, NAWM<sub>0</sub> vs L<sub>0</sub>, NAWM<sub>6</sub><sub>-</sub><sub>12</sub> vs L<sub>6</sub><sub>-</sub><sub>12</sub>, ROIS<sub>0</sub> vs L<sub>0</sub>, ROIS<sub>0</sub> vs L<sub>6</sub><sub>-</sub><sub>12</sub> and ROIS<sub>0</sub> vs ROIS<sub>C0</sub>, where the corresponding % correct classifications scores were 89%, 95%, 98%, 92%, 85%, 90% and 65% respectively. Texture features may provide complementary information for following up the development and progression of MS disease. Future work will investigate the proposed method on more subjects.","Loizou, Pantzaris, Pattichis","Loizou, Pantzaris, Pattichis",https://doi.org/10.1016/j.mri.2020.08.022,https://doi.org/10.1016/j.mri.2020.08.022,2021-08-03
16237.0,pubmed,pubmed,ISAC Probe Tag Dictionary: Standardized Nomenclature for Detection and Visualization Labels Used in Cytometry and Microscopy Imaging,ISAC Probe Tag Dictionary: Standardized Nomenclature for Detection and Visualization Labels Used in Cytometry and Microscopy Imaging,"Since the advent of microscopy imaging and flow cytometry, there has been an explosion in the number of probes, consisting of a component binding to an analyte and a detectable tag, to mark areas of interest in or on cells and tissue. Probe tags have been created to detect and/or visualize probes. Over time, these probe tags have increased in number. The expansion has resulted in arbitrarily created synonyms of probe tags used in publications and software. The synonyms are problematic for readability of publications, accuracy of text/data mining, and bridging data from multiple platforms, protocols, and databases for Big Data analysis. Development and implementation of a universal language for probe tags will ensure equivalent quality and level of data being reported or extracted for clinical/scientific evaluation as well as help connect data from many platforms. The International Society for Advancement of Cytometry Data Standards Task Force composed of academic scientists and industry hardware/software/reagent manufactures have developed recommendations for a standardized nomenclature for probe tags used in cytometry and microscopy imaging. These recommendations are shared in this technical note in the form of a Probe Tag Dictionary. Ã‚Â© 2020 International Society for Advancement of Cytometry.","Since the advent of microscopy imaging and flow cytometry, there has been an explosion in the number of probes, consisting of a component binding to an analyte and a detectable tag, to mark areas of interest in or on cells and tissue. Probe tags have been created to detect and/or visualize probes. Over time, these probe tags have increased in number. The expansion has resulted in arbitrarily created synonyms of probe tags used in publications and software. The synonyms are problematic for readability of publications, accuracy of text/data mining, and bridging data from multiple platforms, protocols, and databases for Big Data analysis. Development and implementation of a universal language for probe tags will ensure equivalent quality and level of data being reported or extracted for clinical/scientific evaluation as well as help connect data from many platforms. The International Society for Advancement of Cytometry Data Standards Task Force composed of academic scientists and industry hardware/software/reagent manufactures have developed recommendations for a standardized nomenclature for probe tags used in cytometry and microscopy imaging. These recommendations are shared in this technical note in the form of a Probe Tag Dictionary. Â© 2020 International Society for Advancement of Cytometry.","Blenman, Spidlen, Parks, Moore, Treister, Leif, Bray, Goldberg, Brinkman","Blenman, Spidlen, Parks, Moore, Treister, Leif, Bray, Goldberg, Brinkman",https://doi.org/10.1002/cyto.a.24224,https://doi.org/10.1002/cyto.a.24224,2021-08-03
16238.0,pubmed,pubmed,A systematic review of radiomics in osteosarcoma: utilizing radiomics quality score as a tool promoting clinical translation,A systematic review of radiomics in osteosarcoma: utilizing radiomics quality score as a tool promoting clinical translation,"To assess the methodological quality and risk of bias in radiomics studies investigating diagnosis, therapy response, and survival of patients with osteosarcoma. In this systematic review, literatures on radiomics in osteosarcoma were included and assessed for methodological quality through the radiomics quality score (RQS). The risk of bias and concern of application was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool. A meta-analysis of studies focusing on predicting osteosarcoma response to neoadjuvant chemotherapy was performed. Twelve radiomics studies exploring osteosarcoma were identified, and five were included in meta-analysis. The RQS reached an average of 20.4% (6.92 of 36) with good inter-rater agreement (ICC 0.95, 95% CI 0.85-0.99). Four studies validated results with an internal dataset, none of which used external dataset; one study was prospectively designed, and another one shared part of the dataset. The risk of bias and concern of application were mainly related to index test aspect. The meta-analysis showed a diagnostic odds ratio of 43.68 (95%CI 13.5-141.31) for predicting response to neoadjuvant chemotherapy with high heterogeneity and low methodological quality. The overall scientific quality of included studies is insufficient; however, radiomics remains a promising technology for predicting treatment response, which might guide therapeutic decision-making and related to prognosis. Improvements in study design, validation, and open science needs to be made to demonstrate the generalizability of findings and to achieve clinical applications. Widespread application of RQS, pre-trained RQS scoring procedure, and modification of RQS in response to clinical needs are necessary. Ã¢â‚¬Â¢ Limited radiomics studies were established in osteosarcoma with mean RQS of 20.4%, commonly due to unvalidated results, retrospective study design, and absence of open science. Ã¢â‚¬Â¢ Meta-analysis of radiomics studies predicting osteosarcoma response to neoadjuvant chemotherapy showed high diagnostic odds ratio 43.68, while high heterogeneity and low methodological quality were the main concerns. Ã¢â‚¬Â¢ A previously trained data extraction instrument allowed reaching moderate inter-rater agreement in RQS applications, while RQS still needs improvement to become a wide adaptive tool in reviews of radiomics studies, in routine self-check before manuscript submitting and in study design.","To assess the methodological quality and risk of bias in radiomics studies investigating diagnosis, therapy response, and survival of patients with osteosarcoma. In this systematic review, literatures on radiomics in osteosarcoma were included and assessed for methodological quality through the radiomics quality score (RQS). The risk of bias and concern of application was assessed using the Quality Assessment of Diagnostic Accuracy Studies tool. A meta-analysis of studies focusing on predicting osteosarcoma response to neoadjuvant chemotherapy was performed. Twelve radiomics studies exploring osteosarcoma were identified, and five were included in meta-analysis. The RQS reached an average of 20.4% (6.92 of 36) with good inter-rater agreement (ICC 0.95, 95% CI 0.85-0.99). Four studies validated results with an internal dataset, none of which used external dataset; one study was prospectively designed, and another one shared part of the dataset. The risk of bias and concern of application were mainly related to index test aspect. The meta-analysis showed a diagnostic odds ratio of 43.68 (95%CI 13.5-141.31) for predicting response to neoadjuvant chemotherapy with high heterogeneity and low methodological quality. The overall scientific quality of included studies is insufficient; however, radiomics remains a promising technology for predicting treatment response, which might guide therapeutic decision-making and related to prognosis. Improvements in study design, validation, and open science needs to be made to demonstrate the generalizability of findings and to achieve clinical applications. Widespread application of RQS, pre-trained RQS scoring procedure, and modification of RQS in response to clinical needs are necessary. â€¢ Limited radiomics studies were established in osteosarcoma with mean RQS of 20.4%, commonly due to unvalidated results, retrospective study design, and absence of open science. â€¢ Meta-analysis of radiomics studies predicting osteosarcoma response to neoadjuvant chemotherapy showed high diagnostic odds ratio 43.68, while high heterogeneity and low methodological quality were the main concerns. â€¢ A previously trained data extraction instrument allowed reaching moderate inter-rater agreement in RQS applications, while RQS still needs improvement to become a wide adaptive tool in reviews of radiomics studies, in routine self-check before manuscript submitting and in study design.","Zhong, Hu, Si, Jia, Xing, Zhang, Yao","Zhong, Hu, Si, Jia, Xing, Zhang, Yao",https://doi.org/10.1007/s00330-020-07221-w,https://doi.org/10.1007/s00330-020-07221-w,2021-08-03
16242.0,pubmed,pubmed,Literature-Wide Association Studies (LWAS) for a Rare Disease: Drug Repurposing for Inflammatory Breast Cancer,Literature-Wide Association Studies (LWAS) for a Rare Disease: Drug Repurposing for Inflammatory Breast Cancer,"Drug repurposing is an effective means for rapid drug discovery. The aim of this study was to develop and validate a computational methodology based on Literature-Wide Association Studies (LWAS) of PubMed to repurpose existing drugs for a rare inflammatory breast cancer (IBC). We have developed a methodology that conducted LWAS based on the text mining technology Word2Vec. 3.80 million &quot;cancer&quot;-related PubMed abstracts were processed as the corpus for Word2Vec to derive vector representation of biological concepts. These vectors for drugs and diseases served as the foundation for creating similarity maps of drugs and diseases, respectively, which were then employed to find potential therapy for IBC. Three hundred and thirty-six (336) known drugs and three hundred and seventy (370) diseases were expressed as vectors in this study. Nine hundred and seventy (970) previously known drug-disease association pairs among these drugs and diseases were used as the reference set. Based on the hypothesis that similar drugs can be used against similar diseases, we have identified 18 diseases similar to IBC, with 24 corresponding known drugs proposed to be the repurposing therapy for IBC. The literature search confirmed most known drugs tested for IBC, with four of them being novel candidates. We conclude that LWAS based on the Word2Vec technology is a novel approach to drug repurposing especially useful for rare diseases.","Drug repurposing is an effective means for rapid drug discovery. The aim of this study was to develop and validate a computational methodology based on Literature-Wide Association Studies (LWAS) of PubMed to repurpose existing drugs for a rare inflammatory breast cancer (IBC). We have developed a methodology that conducted LWAS based on the text mining technology Word2Vec. 3.80 million ""cancer""-related PubMed abstracts were processed as the corpus for Word2Vec to derive vector representation of biological concepts. These vectors for drugs and diseases served as the foundation for creating similarity maps of drugs and diseases, respectively, which were then employed to find potential therapy for IBC. Three hundred and thirty-six (336) known drugs and three hundred and seventy (370) diseases were expressed as vectors in this study. Nine hundred and seventy (970) previously known drug-disease association pairs among these drugs and diseases were used as the reference set. Based on the hypothesis that similar drugs can be used against similar diseases, we have identified 18 diseases similar to IBC, with 24 corresponding known drugs proposed to be the repurposing therapy for IBC. The literature search confirmed most known drugs tested for IBC, with four of them being novel candidates. We conclude that LWAS based on the Word2Vec technology is a novel approach to drug repurposing especially useful for rare diseases.","Ji, Jin, Dong, Dixon, Williams, Zheng","Ji, Jin, Dong, Dixon, Williams, Zheng",https://doi.org/10.3390/molecules25173933,https://doi.org/10.3390/molecules25173933,2021-08-03
16246.0,pubmed,pubmed,Association of Cardiorespiratory Fitness Levels During Youth With Health Risk Later in Life: A Systematic Review and Meta-analysis,Association of Cardiorespiratory Fitness Levels During Youth With Health Risk Later in Life: A Systematic Review and Meta-analysis,"Although the associations between cardiorespiratory fitness (CRF) and health in adults are well understood, to date, no systematic review has quantitatively examined the association between CRF during youth and health parameters later in life. To examine the prospective association between CRF in childhood and adolescence and future health status and to assess whether changes in CRF are associated with future health status at least 1 year later. For this systematic review and meta-analysis, MEDLINE, Embase, and SPORTDiscus electronic databases were searched for relevant articles published from database inception to January 30, 2020. The following inclusion criteria were used: CRF measured using a validated test and assessed at baseline and/or its change from baseline to the end of follow-up, healthy population with a mean age of 3 to 18 years at baseline, and prospective cohort design with a follow-up period of at least 1 year. Data were processed according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). Random-effects models were used to estimate the pooled effect size. Anthropometric and adiposity measurements and cardiometabolic health parameters. Fifty-five studies were included with a total of 37Ã¢â‚¬Â¯563 youths (46% female). Weak-moderate associations were found between CRF at baseline and body mass index (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.11; 95% CI, -0.18 to -0.04; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°59.03), waist circumference (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.29; 95% CI, -0.42 to -0.14; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°69.42), skinfold thickness (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.34; 95% CI, -0.41 to -0.26; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°83.87), obesity (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.15; 95% CI, -0.23 to -0.06; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°86.75), total cholesterol level (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.12; 95% CI, -0.19 to -0.05; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°75.81), high-density lipoprotein cholesterol (HDL-C) level (rÃ¢â‚¬â€°=Ã¢â‚¬â€°0.11; 95% CI, 0.05-0.18; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°69.06), total cholesterol to HDL-C ratio (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.19; 95% CI, -0.26 to -0.13; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°67.07), triglyceride levels (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.10; 95% CI, -0.18 to -0.02; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°73.43), homeostasis model assessment for insulin resistance (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.12; 95% CI, -0.18 to -0.06; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°68.26), fasting insulin level (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.07; 95% CI, -0.11 to -0.03; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°0), and cardiometabolic risk (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.18; 95% CI, -0.29 to -0.07; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°90.61) at follow-up. Meta-regression analyses found that early associations in waist circumference (ÃŽÂ²Ã¢â‚¬â€°=Ã¢â‚¬â€°0.014; 95% CI, 0.002-0.026), skinfold thickness (ÃŽÂ²Ã¢â‚¬â€°=Ã¢â‚¬â€°0.006; 95% CI, 0.002-0.011), HDL-C level (ÃŽÂ²Ã¢â‚¬â€°=Ã¢â‚¬â€°-0.006; 95% CI, -0.011 to -0.001), triglyceride levels (ÃŽÂ²Ã¢â‚¬â€°=Ã¢â‚¬â€°0.009; 95% CI, 0.004-0.014), and cardiometabolic risk (ÃŽÂ²Ã¢â‚¬â€°=Ã¢â‚¬â€°0.007; 95% CI, 0.003-0.011) from baseline to follow-up dissipated over time. Weak-moderate associations were found between change in CRF and body mass index (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.17; 95% CI, -0.24 to -0.11; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°39.65), skinfold thickness (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.36; 95% CI, -0.58 to -0.09; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°96.84), obesity (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.21; 95% CI, -0.35 to -0.06; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°91.08), HDL-C level (rÃ¢â‚¬â€°=Ã¢â‚¬â€°0.05; 95% CI, 0.02-0.08; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°0), low-density lipoprotein cholesterol level (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.06; 95% CI, -0.11 to -0.01; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°58.94), and cardiometabolic risk (rÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.08; 95% CI, -0.15 to -0.02; I2Ã¢â‚¬â€°=Ã¢â‚¬â€°69.53) later in life. This study suggests that early intervention and prevention strategies that target youth CRF may be associated with maintaining health parameters in later life.","Although the associations between cardiorespiratory fitness (CRF) and health in adults are well understood, to date, no systematic review has quantitatively examined the association between CRF during youth and health parameters later in life. To examine the prospective association between CRF in childhood and adolescence and future health status and to assess whether changes in CRF are associated with future health status at least 1 year later. For this systematic review and meta-analysis, MEDLINE, Embase, and SPORTDiscus electronic databases were searched for relevant articles published from database inception to January 30, 2020. The following inclusion criteria were used: CRF measured using a validated test and assessed at baseline and/or its change from baseline to the end of follow-up, healthy population with a mean age of 3 to 18 years at baseline, and prospective cohort design with a follow-up period of at least 1 year. Data were processed according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). Random-effects models were used to estimate the pooled effect size. Anthropometric and adiposity measurements and cardiometabolic health parameters. Fifty-five studies were included with a total of 37â€¯563 youths (46% female). Weak-moderate associations were found between CRF at baseline and body mass index (râ€‰=â€‰-0.11; 95% CI, -0.18 to -0.04; I2â€‰=â€‰59.03), waist circumference (râ€‰=â€‰-0.29; 95% CI, -0.42 to -0.14; I2â€‰=â€‰69.42), skinfold thickness (râ€‰=â€‰-0.34; 95% CI, -0.41 to -0.26; I2â€‰=â€‰83.87), obesity (râ€‰=â€‰-0.15; 95% CI, -0.23 to -0.06; I2â€‰=â€‰86.75), total cholesterol level (râ€‰=â€‰-0.12; 95% CI, -0.19 to -0.05; I2â€‰=â€‰75.81), high-density lipoprotein cholesterol (HDL-C) level (râ€‰=â€‰0.11; 95% CI, 0.05-0.18; I2â€‰=â€‰69.06), total cholesterol to HDL-C ratio (râ€‰=â€‰-0.19; 95% CI, -0.26 to -0.13; I2â€‰=â€‰67.07), triglyceride levels (râ€‰=â€‰-0.10; 95% CI, -0.18 to -0.02; I2â€‰=â€‰73.43), homeostasis model assessment for insulin resistance (râ€‰=â€‰-0.12; 95% CI, -0.18 to -0.06; I2â€‰=â€‰68.26), fasting insulin level (râ€‰=â€‰-0.07; 95% CI, -0.11 to -0.03; I2â€‰=â€‰0), and cardiometabolic risk (râ€‰=â€‰-0.18; 95% CI, -0.29 to -0.07; I2â€‰=â€‰90.61) at follow-up. Meta-regression analyses found that early associations in waist circumference (Î²â€‰=â€‰0.014; 95% CI, 0.002-0.026), skinfold thickness (Î²â€‰=â€‰0.006; 95% CI, 0.002-0.011), HDL-C level (Î²â€‰=â€‰-0.006; 95% CI, -0.011 to -0.001), triglyceride levels (Î²â€‰=â€‰0.009; 95% CI, 0.004-0.014), and cardiometabolic risk (Î²â€‰=â€‰0.007; 95% CI, 0.003-0.011) from baseline to follow-up dissipated over time. Weak-moderate associations were found between change in CRF and body mass index (râ€‰=â€‰-0.17; 95% CI, -0.24 to -0.11; I2â€‰=â€‰39.65), skinfold thickness (râ€‰=â€‰-0.36; 95% CI, -0.58 to -0.09; I2â€‰=â€‰96.84), obesity (râ€‰=â€‰-0.21; 95% CI, -0.35 to -0.06; I2â€‰=â€‰91.08), HDL-C level (râ€‰=â€‰0.05; 95% CI, 0.02-0.08; I2â€‰=â€‰0), low-density lipoprotein cholesterol level (râ€‰=â€‰-0.06; 95% CI, -0.11 to -0.01; I2â€‰=â€‰58.94), and cardiometabolic risk (râ€‰=â€‰-0.08; 95% CI, -0.15 to -0.02; I2â€‰=â€‰69.53) later in life. This study suggests that early intervention and prevention strategies that target youth CRF may be associated with maintaining health parameters in later life.","GarcÃƒÂ­a-Hermoso, RamÃƒÂ­rez-VÃƒÂ©lez, GarcÃƒÂ­a-Alonso, Alonso-MartÃƒÂ­nez, Izquierdo","GarcÃ­a-Hermoso, RamÃ­rez-VÃ©lez, GarcÃ­a-Alonso, Alonso-MartÃ­nez, Izquierdo",https://doi.org/10.1001/jamapediatrics.2020.2400,https://doi.org/10.1001/jamapediatrics.2020.2400,2021-08-03
16247.0,pubmed,pubmed,Coronavirus disease (COVID-19) detection in Chest X-Ray images using majority voting based classifier ensemble,Coronavirus disease (COVID-19) detection in Chest X-Ray images using majority voting based classifier ensemble,"Novel coronavirus disease (nCOVID-19) is the most challenging problem for the world. The disease is caused by severe acute respiratory syndrome coronavirus-2 (SARS-COV-2), leading to high morbidity and mortality worldwide. The study reveals that infected patients exhibit distinct radiographic visual characteristics along with fever, dry cough, fatigue, dyspnea, etc. Chest X-Ray (CXR) is one of the important, non-invasive clinical adjuncts that play an essential role in the detection of such visual responses associated with SARS-COV-2 infection. However, the limited availability of expert radiologists to interpret the CXR images and subtle appearance of disease radiographic responses remains the biggest bottlenecks in manual diagnosis. In this study, we present an automatic COVID screening (ACoS) system that uses radiomic texture descriptors extracted from CXR images to identify the normal, suspected, and nCOVID-19 infected patients. The proposed system uses two-phase classification approach (normal vs. abnormal and nCOVID-19 vs. pneumonia) using majority vote based classifier ensemble of five benchmark supervised classification algorithms. The training-testing and validation of the ACoS system are performed using 2088 (696 normal, 696 pneumonia and 696 nCOVID-19) and 258 (86 images of each category) CXR images, respectively. The obtained validation results for phase-I (accuracy (ACC)Ã‚Â =Ã‚Â 98.062%, area under curve (AUC)Ã‚Â =Ã‚Â 0.956) and phase-II (ACCÃ‚Â =Ã‚Â 91.329% and AUCÃ‚Â =Ã‚Â 0.831) show the promising performance of the proposed system. Further, the Friedman post-hoc multiple comparisons and z-test statistics reveals that the results of ACoS system are statistically significant. Finally, the obtained performance is compared with the existing state-of-the-art methods.","Novel coronavirus disease (nCOVID-19) is the most challenging problem for the world. The disease is caused by severe acute respiratory syndrome coronavirus-2 (SARS-COV-2), leading to high morbidity and mortality worldwide. The study reveals that infected patients exhibit distinct radiographic visual characteristics along with fever, dry cough, fatigue, dyspnea, etc. Chest X-Ray (CXR) is one of the important, non-invasive clinical adjuncts that play an essential role in the detection of such visual responses associated with SARS-COV-2 infection. However, the limited availability of expert radiologists to interpret the CXR images and subtle appearance of disease radiographic responses remains the biggest bottlenecks in manual diagnosis. In this study, we present an automatic COVID screening (ACoS) system that uses radiomic texture descriptors extracted from CXR images to identify the normal, suspected, and nCOVID-19 infected patients. The proposed system uses two-phase classification approach (normal vs. abnormal and nCOVID-19 vs. pneumonia) using majority vote based classifier ensemble of five benchmark supervised classification algorithms. The training-testing and validation of the ACoS system are performed using 2088 (696 normal, 696 pneumonia and 696 nCOVID-19) and 258 (86 images of each category) CXR images, respectively. The obtained validation results for phase-I (accuracy (ACC)Â =Â 98.062%, area under curve (AUC)Â =Â 0.956) and phase-II (ACCÂ =Â 91.329% and AUCÂ =Â 0.831) show the promising performance of the proposed system. Further, the Friedman post-hoc multiple comparisons and z-test statistics reveals that the results of ACoS system are statistically significant. Finally, the obtained performance is compared with the existing state-of-the-art methods.","Chandra, Verma, Singh, Jain, Netam","Chandra, Verma, Singh, Jain, Netam",https://doi.org/10.1016/j.eswa.2020.113909,https://doi.org/10.1016/j.eswa.2020.113909,2021-08-03
16249.0,pubmed,pubmed,Machine learning techniques for detecting electrode misplacement and interchanges when recording ECGs: A systematic review and meta-analysis,Machine learning techniques for detecting electrode misplacement and interchanges when recording ECGs: A systematic review and meta-analysis,"Electrode misplacement and interchange errors are known problems when recording the 12Ã¢â‚¬â€˜lead electrocardiogram (ECG). Automatic detection of these errors could play an important role for improving clinical decision making and outcomes in cardiac care. The objectives of this systematic review and meta-analysis is to 1) study the impact of electrode misplacement on ECG signals and ECG interpretation, 2) to determine the most challenging electrode misplacements to detect using machine learning (ML), 3) to analyse the ML performance of algorithms that detect electrode misplacement or interchange according to sensitivity and specificity and 4) to identify the most commonly used ML technique for detecting electrode misplacement/interchange. This review analysed the current literature regarding electrode misplacement/interchange recognition accuracy using machine learning techniques. A search of three online databases including IEEE, PubMed and ScienceDirect identified 228 articles, while 3 articles were included from additional sources from co-authors. According to the eligibility criteria, 14 articles were selected. The selected articles were considered for qualitative analysis and meta-analysis. The articles showed the effect of lead interchange on ECG morphology and as a consequence on patient diagnoses. Statistical analysis of the included articles found that machine learning performance is high in detecting electrode misplacement/interchange except left arm/left leg interchange. This review emphasises the importance of detecting electrode misplacement detection in ECG diagnosis and the effects on decision making. Machine learning shows promise in detecting lead misplacement/interchange and highlights an opportunity for developing and operationalising deep learning algorithms such as convolutional neural network (CNN) to detect electrode misplacement/interchange.","Electrode misplacement and interchange errors are known problems when recording the 12â€‘lead electrocardiogram (ECG). Automatic detection of these errors could play an important role for improving clinical decision making and outcomes in cardiac care. The objectives of this systematic review and meta-analysis is to 1) study the impact of electrode misplacement on ECG signals and ECG interpretation, 2) to determine the most challenging electrode misplacements to detect using machine learning (ML), 3) to analyse the ML performance of algorithms that detect electrode misplacement or interchange according to sensitivity and specificity and 4) to identify the most commonly used ML technique for detecting electrode misplacement/interchange. This review analysed the current literature regarding electrode misplacement/interchange recognition accuracy using machine learning techniques. A search of three online databases including IEEE, PubMed and ScienceDirect identified 228 articles, while 3 articles were included from additional sources from co-authors. According to the eligibility criteria, 14 articles were selected. The selected articles were considered for qualitative analysis and meta-analysis. The articles showed the effect of lead interchange on ECG morphology and as a consequence on patient diagnoses. Statistical analysis of the included articles found that machine learning performance is high in detecting electrode misplacement/interchange except left arm/left leg interchange. This review emphasises the importance of detecting electrode misplacement detection in ECG diagnosis and the effects on decision making. Machine learning shows promise in detecting lead misplacement/interchange and highlights an opportunity for developing and operationalising deep learning algorithms such as convolutional neural network (CNN) to detect electrode misplacement/interchange.","Rjoob, Bond, Finlay, McGilligan, Leslie, Rababah, Guldenring, Iftikhar, Knoery, McShane, Peace","Rjoob, Bond, Finlay, McGilligan, Leslie, Rababah, Guldenring, Iftikhar, Knoery, McShane, Peace",https://doi.org/10.1016/j.jelectrocard.2020.08.013,https://doi.org/10.1016/j.jelectrocard.2020.08.013,2021-08-03
16250.0,pubmed,pubmed,"Periodontitis, Edentulism, and Risk of Mortality: A Systematic Review with Meta-analyses","Periodontitis, Edentulism, and Risk of Mortality: A Systematic Review with Meta-analyses","Periodontitis has been independently associated with the chronic noncommunicable diseases that most frequently lead to death worldwide. The aim of the present systematic review was to study whether people with periodontitis/edentulism are at increased risk of all-cause and cause-specific mortality as compared with those without periodontitis/edentulism. Cohort studies were included that 1) evaluated periodontitis or edentulism as exposures in relation to all-cause or cause-specific mortality as an outcome and 2) reported effect estimates as hazard ratios, risk ratios, or odds ratios with 95% CIs or crude numbers. Two review authors independently searched for eligible studies, screened the titles and abstracts, did full-text analysis, extracted the data from the published reports, and performed the risk-of-bias assessment. In case of disagreement, a third review author was consulted. Study results were summarized through random effects meta-analyses. A total of 57 studies were included, involving 48 cohorts and 5.71Ã¢â‚¬â€°million participants. Periodontitis was associated with increased risk of all-cause mortality (risk ratio, 1.46 [95% CI, 1.15 to 1.85]) and mortality due to cardiovascular diseases (1.47 [1.14 to 1.90]), cancer (1.38 [1.24 to 1.53]), coronary heart disease (2.58 [2.20 to 3.03]), cerebrovascular diseases (3.11 [2.42 to 3.98]), but not pneumonia (0.98 [0.69 to 1.38]). Edentulism (all types) was associated with increased risk of all-cause mortality (1.66 [1.46 to 1.88]) and mortality due to cardiovascular diseases (2.03 [1.50 to 2.74]), cancer (1.55 [1.24 to 1.94]), pneumonia (1.72 [1.07 to 2.78]), coronary heart disease (2.98 [2.43 to 3.65]), and cerebrovascular diseases (3.18 [2.24 to 4.51]). Periodontitis and its ultimate sequela (edentulism) are associated with an increased risk of all-cause and cause-specific mortality (PROSPERO CRD42018100095).","Periodontitis has been independently associated with the chronic noncommunicable diseases that most frequently lead to death worldwide. The aim of the present systematic review was to study whether people with periodontitis/edentulism are at increased risk of all-cause and cause-specific mortality as compared with those without periodontitis/edentulism. Cohort studies were included that 1) evaluated periodontitis or edentulism as exposures in relation to all-cause or cause-specific mortality as an outcome and 2) reported effect estimates as hazard ratios, risk ratios, or odds ratios with 95% CIs or crude numbers. Two review authors independently searched for eligible studies, screened the titles and abstracts, did full-text analysis, extracted the data from the published reports, and performed the risk-of-bias assessment. In case of disagreement, a third review author was consulted. Study results were summarized through random effects meta-analyses. A total of 57 studies were included, involving 48 cohorts and 5.71â€‰million participants. Periodontitis was associated with increased risk of all-cause mortality (risk ratio, 1.46 [95% CI, 1.15 to 1.85]) and mortality due to cardiovascular diseases (1.47 [1.14 to 1.90]), cancer (1.38 [1.24 to 1.53]), coronary heart disease (2.58 [2.20 to 3.03]), cerebrovascular diseases (3.11 [2.42 to 3.98]), but not pneumonia (0.98 [0.69 to 1.38]). Edentulism (all types) was associated with increased risk of all-cause mortality (1.66 [1.46 to 1.88]) and mortality due to cardiovascular diseases (2.03 [1.50 to 2.74]), cancer (1.55 [1.24 to 1.94]), pneumonia (1.72 [1.07 to 2.78]), coronary heart disease (2.98 [2.43 to 3.65]), and cerebrovascular diseases (3.18 [2.24 to 4.51]). Periodontitis and its ultimate sequela (edentulism) are associated with an increased risk of all-cause and cause-specific mortality (PROSPERO CRD42018100095).","Romandini, Baima, Antonoglou, Bueno, Figuero, Sanz","Romandini, Baima, Antonoglou, Bueno, Figuero, Sanz",https://doi.org/10.1177/0022034520952401,https://doi.org/10.1177/0022034520952401,2021-08-03
16255.0,pubmed,pubmed,Myocardial Infarction Associates With a Distinct Pericoronary Adipose Tissue Radiomic Phenotype: A Prospective Case-Control Study,Myocardial Infarction Associates With a Distinct Pericoronary Adipose Tissue Radiomic Phenotype: A Prospective Case-Control Study,"This study sought to determine whether coronary computed tomography angiography (CCTA)-based radiomic analysis of pericoronary adipose tissue (PCAT) could distinguish patients with acute myocardial infarction (MI) from patients with stable or no coronary artery disease (CAD). Imaging of PCAT with CCTA enables detection of coronary inflammation. Radiomics involves extracting quantitative features from medical images to create big data and identify novel imaging biomarkers. In a prospective case-control study, 60 patients with acute MI underwent CCTA within 48Ã‚Â h of admission, before invasive angiography. These subjects were matched to patients with stable CAD (nÃ‚Â =Ã‚Â 60) and controls with no CAD (nÃ‚Â =Ã‚Â 60) by age, sex, risk factors, medications, and CT tube voltage. PCAT was segmented around the proximal right coronary artery (RCA) in all patients and around culprit and nonculprit lesions in patients with MI. PCAT segmentations were analyzed using Radiomics Image Analysis software. Of 1,103 calculated radiomic parameters, 20.3% differed significantly between MI patients and controls, and 16.5% differed between patients with MI and stable CAD (critical pÃ‚Â &lt;Ã‚Â 0.0006); whereas none differed between patients with stable CAD and controls. On cluster analysis, the most significant radiomic parameters were texture or geometry based. At 6Ã‚Â months post-MI, there was no significant change in the PCAT radiomic profile around the proximal RCA or nonculprit lesions. Using machine learning (XGBoost), a model integrating clinical features (risk factors, serum lipids, high-sensitivity C-reactive protein), PCAT attenuation, and radiomic parameters provided superior discrimination of acute MI (area under the receiver operator characteristic curve [AUC]: 0.87) compared with a model with clinical features and PCAT attenuation (AUC: 0.77; pÃ‚Â =Ã‚Â 0.001) or clinical features alone (AUC: 0.76; pÃ‚Â &lt;Ã‚Â 0.001). Patients with acute MI have a distinct PCAT radiomic phenotype compared with patients with stable or no CAD. Using machine learning, a radiomics-based model outperforms a PCAT attenuation-based model in accurately identifying patients with MI.","This study sought to determine whether coronary computed tomography angiography (CCTA)-based radiomic analysis of pericoronary adipose tissue (PCAT) could distinguish patients with acute myocardial infarction (MI) from patients with stable or no coronary artery disease (CAD). Imaging of PCAT with CCTA enables detection of coronary inflammation. Radiomics involves extracting quantitative features from medical images to create big data and identify novel imaging biomarkers. In a prospective case-control study, 60 patients with acute MI underwent CCTA within 48Â h of admission, before invasive angiography. These subjects were matched to patients with stable CAD (nÂ =Â 60) and controls with no CAD (nÂ =Â 60) by age, sex, risk factors, medications, and CT tube voltage. PCAT was segmented around the proximal right coronary artery (RCA) in all patients and around culprit and nonculprit lesions in patients with MI. PCAT segmentations were analyzed using Radiomics Image Analysis software. Of 1,103 calculated radiomic parameters, 20.3% differed significantly between MI patients and controls, and 16.5% differed between patients with MI and stable CAD (critical pÂ &lt;Â 0.0006); whereas none differed between patients with stable CAD and controls. On cluster analysis, the most significant radiomic parameters were texture or geometry based. At 6Â months post-MI, there was no significant change in the PCAT radiomic profile around the proximal RCA or nonculprit lesions. Using machine learning (XGBoost), a model integrating clinical features (risk factors, serum lipids, high-sensitivity C-reactive protein), PCAT attenuation, and radiomic parameters provided superior discrimination of acute MI (area under the receiver operator characteristic curve [AUC]: 0.87) compared with a model with clinical features and PCAT attenuation (AUC: 0.77; pÂ =Â 0.001) or clinical features alone (AUC: 0.76; pÂ &lt;Â 0.001). Patients with acute MI have a distinct PCAT radiomic phenotype compared with patients with stable or no CAD. Using machine learning, a radiomics-based model outperforms a PCAT attenuation-based model in accurately identifying patients with MI.","Lin, KolossvÃƒÂ¡ry, Yuvaraj, Cadet, McElhinney, Jiang, Nerlekar, Nicholls, Slomka, Maurovich-Horvat, Wong, Dey","Lin, KolossvÃ¡ry, Yuvaraj, Cadet, McElhinney, Jiang, Nerlekar, Nicholls, Slomka, Maurovich-Horvat, Wong, Dey",https://doi.org/10.1016/j.jcmg.2020.06.033,https://doi.org/10.1016/j.jcmg.2020.06.033,2021-08-03
16264.0,pubmed,pubmed,Home Healthcare Clinical Notes Predict Patient Hospitalization and Emergency Department Visits,Home Healthcare Clinical Notes Predict Patient Hospitalization and Emergency Department Visits,"About 30% of home health care patients are hospitalized or visit an emergency department (ED) during a home health care (HHC) episode. Novel data science methods are increasingly used to improve identification of patients at risk for negative outcomes. To identify patients at heightened risk hospitalization or ED visits using HHC narrative data (clinical notes). This study used a large database of HHC visit notes (n = 727,676) documented for 112,237 HHC episodes (89,459 unique patients) by clinicians of the largest nonprofit home health care agency in the United States. Text mining and machine learning algorithms (NaÃƒÂ¯ve Bayes, decision tree, random forest) were implemented to predict patient hospitalization or ED visits using the content of clinical notes. Risk factors associated with hospitalization or ED visits were identified using a feature selection technique (gain ratio attribute evaluation). Best performing text mining method (random forest) achieved good predictive performance. Seven risk factors categories were identified, with clinical factors, coordination/communication, and service use being the most frequent categories. This study was the first to explore the potential contribution of HHC clinical notes to identifying patients at risk for hospitalization or an ED visit. Our results suggest that HHC visit notes are highly informative and can contribute significantly to identification of patients at risk. Further studies are needed to explore ways to improve risk prediction by adding more data elements from additional data sources.","About 30% of home healthcare patients are hospitalized or visit an emergency department (ED) during a home healthcare (HHC) episode. Novel data science methods are increasingly used to improve identification of patients at risk for negative outcomes. The aim of the study was to identify patients at heightened risk hospitalization or ED visits using HHC narrative data (clinical notes). This study used a large database of HHC visit notes (n = 727,676) documented for 112,237 HHC episodes (89,459 unique patients) by clinicians of the largest nonprofit HHC agency in the United States. Text mining and machine learning algorithms (NaÃ¯ve Bayes, decision tree, random forest) were implemented to predict patient hospitalization or ED visits using the content of clinical notes. Risk factors associated with hospitalization or ED visits were identified using a feature selection technique (gain ratio attribute evaluation). Best performing text mining method (random forest) achieved good predictive performance. Seven risk factors categories were identified, with clinical factors, coordination/communication, and service use being the most frequent categories. This study was the first to explore the potential contribution of HHC clinical notes to identifying patients at risk for hospitalization or an ED visit. Our results suggest that HHC visit notes are highly informative and can contribute significantly to identification of patients at risk. Further studies are needed to explore ways to improve risk prediction by adding more data elements from additional data sources.","Topaz, Woo, Ryvicker, Zolnoori, Cato","Topaz, Woo, Ryvicker, Zolnoori, Cato",https://doi.org/10.1097/NNR.0000000000000470,https://doi.org/10.1097/NNR.0000000000000470,2021-08-03
16272.0,pubmed,pubmed,Comparing clotting factors attributes across different methods of preference elicitation in haemophilia patients,Comparing clotting factors attributes across different methods of preference elicitation in haemophilia patients,"Emerging, systematic approaches for capturing patient input, such as preference elicitation, can provide valuable information for the benefit-risk assessment of medical products for treating bleeding disorders, such as haemophilia. This study aims to identify existing and develop new methods to capture, rank and summarize preference scores for clotting factor therapies. Haemophilia patient preference data were compiled from studies identified through literature review and publicly available US FDA patient-focused drug development meeting documents. Text mining was performed to identify major themes across studies. A standardized preference score was estimated and aggregated. Ten preference studies that employed qualitative (nÃ‚Â =Ã‚Â 3), and quantitative methods (nÃ‚Â =Ã‚Â 7) met the inclusion criteria. Text mining of qualitative and quantitative studies revealed similar themes as the standardized preference attribute importance. We found that seven quantitative studies employed discrete choice experiments (DCE)/conjoint analysis (CA) and examined a range of 5-12 attributes. For DCE/CA studies published prior to 2014 (nÃ‚Â =Ã‚Â 4), safety attributes (inhibitor and viral safety) were among the most important attributes, accounting for ~46% of the total utility measured. DCE/CA studies published after 2014 (nÃ‚Â =Ã‚Â 3) focused on frequency of infusion and reduction of bleeding risk, accounting for ~67% of the total utility. Interestingly, two studies that used different preference elicitation approaches (DCE and a monadic conjoint approach) both ranked infusion frequency as the most important attribute. Although there are few published patient preference studies for haemophilia, the results of this study can be viewed in the larger context of enhancing scientific methods of incorporating patient input in medical product development.","Emerging, systematic approaches for capturing patient input, such as preference elicitation, can provide valuable information for the benefit-risk assessment of medical products for treating bleeding disorders, such as haemophilia. This study aims to identify existing and develop new methods to capture, rank and summarize preference scores for clotting factor therapies. Haemophilia patient preference data were compiled from studies identified through literature review and publicly available US FDA patient-focused drug development meeting documents. Text mining was performed to identify major themes across studies. A standardized preference score was estimated and aggregated. Ten preference studies that employed qualitative (nÂ =Â 3), and quantitative methods (nÂ =Â 7) met the inclusion criteria. Text mining of qualitative and quantitative studies revealed similar themes as the standardized preference attribute importance. We found that seven quantitative studies employed discrete choice experiments (DCE)/conjoint analysis (CA) and examined a range of 5-12 attributes. For DCE/CA studies published prior to 2014 (nÂ =Â 4), safety attributes (inhibitor and viral safety) were among the most important attributes, accounting for ~46% of the total utility measured. DCE/CA studies published after 2014 (nÂ =Â 3) focused on frequency of infusion and reduction of bleeding risk, accounting for ~67% of the total utility. Interestingly, two studies that used different preference elicitation approaches (DCE and a monadic conjoint approach) both ranked infusion frequency as the most important attribute. Although there are few published patient preference studies for haemophilia, the results of this study can be viewed in the larger context of enhancing scientific methods of incorporating patient input in medical product development.","Tegenge, Belov, Moncur, Forshee, Irony","Tegenge, Belov, Moncur, Forshee, Irony",https://doi.org/10.1111/hae.14119,https://doi.org/10.1111/hae.14119,2021-08-03
16274.0,pubmed,pubmed,Point-of-care creatinine tests to assess kidney function for outpatients requiring contrast-enhanced CT imaging: systematic reviews and economic evaluation,Point-of-care creatinine tests to assess kidney function for outpatients requiring contrast-enhanced CT imaging: systematic reviews and economic evaluation,"Patients with low estimated glomerular filtration rates may be at higher risk of post-contrast acute kidney injury following contrast-enhanced computed tomography imaging. Point-of-care devices allow rapid measurement of estimated glomerular filtration rates for patients referred without a recent estimated glomerular filtration rate result. To assess the clinical effectiveness and cost-effectiveness of point-of-care creatinine tests for outpatients without a recent estimated glomerular filtration rate measurement who need contrast-enhanced computed tomography imaging. Three systematic reviews of test accuracy, implementation and clinical outcomes, and economic analyses were carried out. Bibliographic databases were searched from inception to November 2018. Studies comparing the accuracy of point-of-care creatinine tests with laboratory reference tests to assess kidney function in adults in a non-emergency setting and studies reporting implementation and clinical outcomes were included. Risk of bias of diagnostic accuracy studies was assessed using a modified version of the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) tool. Probabilities of individuals having their estimated glomerular filtration rates correctly classified were estimated within a Bayesian framework and pooled using a fixed-effects model. A de novo probabilistic decision tree cohort model was developed to characterise the decision problem from an NHS and a Personal Social Services perspective. A range of alternative point-of-care testing approaches were considered. Scenario analyses were conducted. Fifty-four studies were included in the clinical reviews. Twelve studies reported diagnostic accuracy for estimated glomerular filtration rates; half were rated as being at low risk of bias, but there were applicability concerns for most. i-STAT (Abbott Point of Care, Inc., Princeton, NJ, USA) and ABL (Radiometer Ltd, Crawley, UK) devices had higher probabilities of correctly classifying individuals in the same estimated glomerular filtration rate categories as the reference laboratory test than StatSensor<sup>Ã‚Â®</sup> devices (Nova Biomedical, Runcorn, UK). There was limited evidence for epoc<sup>Ã‚Â®</sup> (Siemens Healthineers AG, Erlangen, Germany) and Piccolo Xpress<sup>Ã‚Â®</sup> (Abaxis, Inc., Union City, CA, USA) devices and no studies of DRI-CHEM NX 500 (Fujifilm Corporation, Tokyo, Japan). The review of implementation and clinical outcomes included six studies showing practice variation in the management decisions when a point-of-care device indicated an abnormal estimated glomerular filtration rate. The review of cost-effectiveness evidence identified no relevant studies. The de novo decision model that was developed included a total of 14 strategies. Owing to limited data, the model included only i-STAT, ABL800 FLEX and StatSensor. In the base-case analysis, the cost-effective strategy appeared to be a three-step testing sequence involving initially screening all individuals for risk factors, point-of-care testing for those individuals with at least one risk factor, and including a final confirmatory laboratory test for individuals with a point-of-care-positive test result. Within this testing approach, the specific point-of-care device with the highest net benefit was i-STAT, although differences in net benefit with StatSensor were very small. There was insufficient evidence for patients with estimated glomerular filtration rates &lt;Ã¢â‚¬â€°30Ã¢â‚¬â€°ml/minute/1.73Ã¢â‚¬â€°m<sup>2</sup>, and on the full potential health impact of delayed or rescheduled computed tomography scans or the use of alternative imaging modalities. A three-step testing sequence combining a risk factor questionnaire with a point-of-care test and confirmatory laboratory testing appears to be a cost-effective use of NHS resources compared with current practice. The risk of contrast causing acute kidney injury to patients with an estimated glomerular filtration rate of &lt;Ã¢â‚¬â€°30Ã¢â‚¬â€°ml/minute/1.73Ã¢â‚¬â€°m<sup>2</sup> is uncertain. Cost-effectiveness of point-of-care testing appears largely driven by the potential of point-of-care tests to minimise delays within the current computed tomography pathway. Studies evaluating the impact of risk-stratifying questionnaires on workflow outcomes in computed tomography patients without recent estimated glomerular filtration rate results are needed. This study is registered as PROSPERO CRD42018115818. This project was funded by the National Institute for Health Research (NIHR) Health Technology Assessment programme and will be published in full in <i>Health Technology Assessment</i>; Vol. 24, No. 39. See the NIHR Journals Library website for further project information. Before computed tomography scans are done, a contrast agent is usually needed to improve the visibility of internal body structures. After receiving a contrast agent (through a vein), some patientsÃ¢â‚¬â„¢ kidneys may be affected, especially if their kidneys already do not work well. A blood test can identify these patients before a computed tomography scan, to reduce the risk of kidney harm. The blood test measures creatinine, which is a marker of how well the kidneys work. Before a contrast-enhanced computed tomography scan, some patients have a recent creatinine result from an earlier blood test. Blood tests are normally done in a central laboratory, and usually take at least 1 hour. Other patients do not have a recent creatinine result, so their computed tomography scan may be delayed or rearranged. Sometimes, to avoid risking kidney harm, patients may have scans without contrast. Ã¢â‚¬ËœPoint-of-careÃ¢â‚¬â„¢ (handheld, tabletop or portable) devices can quickly measure creatinine (usually in patients with risk factors), often from a finger-prick blood sample. Many point-of-care devices are available but they may not be as exact as laboratory tests, so their benefit is unclear. This study reviewed all available evidence on the benefits and harms of point-of-care creatinine tests before computed tomography scans and assessed whether or not they are a cost-effective use of NHS resources. The study found that some devices [i.e. i-STAT (Abbott Point of Care, Inc., Princeton, NJ, USA) and ABL (Radiometer Ltd, Crawley, UK)] were more accurate than others [i.e. StatSensor<sup>Ã‚Â®</sup> (Nova Biomedical, Runcorn, UK)]. There was insufficient evidence for other devices. The study found that, for outpatients, doing a point-of-care test in patients who are at a higher risk of kidney harm (according to a questionnaire) and then confirming this with a laboratory test appeared to be a cost-effective use of NHS resources. The study found that the risk of kidney harm as a result of contrast agents appears very low. The main benefit of point-of-care testing may be to reduce needless delays or rearranged computed tomography scan appointments.","Patients with low estimated glomerular filtration rates may be at higher risk of post-contrast acute kidney injury following contrast-enhanced computed tomography imaging. Point-of-care devices allow rapid measurement of estimated glomerular filtration rates for patients referred without a recent estimated glomerular filtration rate result. To assess the clinical effectiveness and cost-effectiveness of point-of-care creatinine tests for outpatients without a recent estimated glomerular filtration rate measurement who need contrast-enhanced computed tomography imaging. Three systematic reviews of test accuracy, implementation and clinical outcomes, and economic analyses were carried out. Bibliographic databases were searched from inception to November 2018. Studies comparing the accuracy of point-of-care creatinine tests with laboratory reference tests to assess kidney function in adults in a non-emergency setting and studies reporting implementation and clinical outcomes were included. Risk of bias of diagnostic accuracy studies was assessed using a modified version of the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) tool. Probabilities of individuals having their estimated glomerular filtration rates correctly classified were estimated within a Bayesian framework and pooled using a fixed-effects model. A de novo probabilistic decision tree cohort model was developed to characterise the decision problem from an NHS and a Personal Social Services perspective. A range of alternative point-of-care testing approaches were considered. Scenario analyses were conducted. Fifty-four studies were included in the clinical reviews. Twelve studies reported diagnostic accuracy for estimated glomerular filtration rates; half were rated as being at low risk of bias, but there were applicability concerns for most. i-STAT (Abbott Point of Care, Inc., Princeton, NJ, USA) and ABL (Radiometer Ltd, Crawley, UK) devices had higher probabilities of correctly classifying individuals in the same estimated glomerular filtration rate categories as the reference laboratory test than StatSensor<sup>Â®</sup> devices (Nova Biomedical, Runcorn, UK). There was limited evidence for epoc<sup>Â®</sup> (Siemens Healthineers AG, Erlangen, Germany) and Piccolo Xpress<sup>Â®</sup> (Abaxis, Inc., Union City, CA, USA) devices and no studies of DRI-CHEM NX 500 (Fujifilm Corporation, Tokyo, Japan). The review of implementation and clinical outcomes included six studies showing practice variation in the management decisions when a point-of-care device indicated an abnormal estimated glomerular filtration rate. The review of cost-effectiveness evidence identified no relevant studies. The de novo decision model that was developed included a total of 14 strategies. Owing to limited data, the model included only i-STAT, ABL800 FLEX and StatSensor. In the base-case analysis, the cost-effective strategy appeared to be a three-step testing sequence involving initially screening all individuals for risk factors, point-of-care testing for those individuals with at least one risk factor, and including a final confirmatory laboratory test for individuals with a point-of-care-positive test result. Within this testing approach, the specific point-of-care device with the highest net benefit was i-STAT, although differences in net benefit with StatSensor were very small. There was insufficient evidence for patients with estimated glomerular filtration rates &lt;â€‰30â€‰ml/minute/1.73â€‰m<sup>2</sup>, and on the full potential health impact of delayed or rescheduled computed tomography scans or the use of alternative imaging modalities. A three-step testing sequence combining a risk factor questionnaire with a point-of-care test and confirmatory laboratory testing appears to be a cost-effective use of NHS resources compared with current practice. The risk of contrast causing acute kidney injury to patients with an estimated glomerular filtration rate of &lt;â€‰30â€‰ml/minute/1.73â€‰m<sup>2</sup> is uncertain. Cost-effectiveness of point-of-care testing appears largely driven by the potential of point-of-care tests to minimise delays within the current computed tomography pathway. Studies evaluating the impact of risk-stratifying questionnaires on workflow outcomes in computed tomography patients without recent estimated glomerular filtration rate results are needed. This study is registered as PROSPERO CRD42018115818. This project was funded by the National Institute for Health Research (NIHR) Health Technology Assessment programme and will be published in full in <i>Health Technology Assessment</i>; Vol. 24, No. 39. See the NIHR Journals Library website for further project information. Before computed tomography scans are done, a contrast agent is usually needed to improve the visibility of internal body structures. After receiving a contrast agent (through a vein), some patientsâ€™ kidneys may be affected, especially if their kidneys already do not work well. A blood test can identify these patients before a computed tomography scan, to reduce the risk of kidney harm. The blood test measures creatinine, which is a marker of how well the kidneys work. Before a contrast-enhanced computed tomography scan, some patients have a recent creatinine result from an earlier blood test. Blood tests are normally done in a central laboratory, and usually take at least 1 hour. Other patients do not have a recent creatinine result, so their computed tomography scan may be delayed or rearranged. Sometimes, to avoid risking kidney harm, patients may have scans without contrast. â€˜Point-of-careâ€™ (handheld, tabletop or portable) devices can quickly measure creatinine (usually in patients with risk factors), often from a finger-prick blood sample. Many point-of-care devices are available but they may not be as exact as laboratory tests, so their benefit is unclear. This study reviewed all available evidence on the benefits and harms of point-of-care creatinine tests before computed tomography scans and assessed whether or not they are a cost-effective use of NHS resources. The study found that some devices [i.e. i-STAT (Abbott Point of Care, Inc., Princeton, NJ, USA) and ABL (Radiometer Ltd, Crawley, UK)] were more accurate than others [i.e. StatSensor<sup>Â®</sup> (Nova Biomedical, Runcorn, UK)]. There was insufficient evidence for other devices. The study found that, for outpatients, doing a point-of-care test in patients who are at a higher risk of kidney harm (according to a questionnaire) and then confirming this with a laboratory test appeared to be a cost-effective use of NHS resources. The study found that the risk of kidney harm as a result of contrast agents appears very low. The main benefit of point-of-care testing may be to reduce needless delays or rearranged computed tomography scan appointments.","Corbett, Duarte, Llewellyn, Altunkaya, Harden, Harris, Walker, Palmer, Dias, Soares","Corbett, Duarte, Llewellyn, Altunkaya, Harden, Harris, Walker, Palmer, Dias, Soares",https://doi.org/10.3310/hta24390,https://doi.org/10.3310/hta24390,2021-08-03
16275.0,pubmed,pubmed,"Recommendations for the surveillance of cancer-related fatigue in childhood, adolescent, and young adult cancer survivors: a report from the International Late Effects of Childhood Cancer Guideline Harmonization Group","Recommendations for the surveillance of cancer-related fatigue in childhood, adolescent, and young adult cancer survivors: a report from the International Late Effects of Childhood Cancer Guideline Harmonization Group","Cancer-related fatigue (CRF) negatively affects the lives of childhood, adolescent, and young adult (CAYA) cancer survivors. We aimed to provide an evidence-based clinical practice guideline (CPG) with internationally harmonized CRF surveillance recommendations for CAYA cancer survivors diagnosed &lt;Ã¢â‚¬â€°30Ã‚Â years. This CPG was developed by a multidisciplinary panel under the umbrella of the International Late Effects of Childhood Cancer Guideline Harmonization Group. After evaluating concordances and discordances of four existing CPGs, we performed systematic literature searches. We screened articles for eligibility, assessed quality, extracted, and summarized the data from included articles. We formulated recommendations based on the evidence and clinical judgment. Of 3647 articles identified, 70 articles from 14 countries were included. The prevalence of CRF in CAYA cancer survivors ranged from 10-85%. We recommend that healthcare providers are aware of the risk of CRF, implement regular screening with validated measures, and recommend effective interventions to fatigued survivors. A considerable proportion of CAYA cancer survivors suffers from CRF even years after the end of treatment. We recommend that healthcare providers adopt regular screening to detect and treat CRF early and positively influence survivors' health and quality of life.","Cancer-related fatigue (CRF) negatively affects the lives of childhood, adolescent, and young adult (CAYA) cancer survivors. We aimed to provide an evidence-based clinical practice guideline (CPG) with internationally harmonized CRF surveillance recommendations for CAYA cancer survivors diagnosed &lt;â€‰30Â years. This CPG was developed by a multidisciplinary panel under the umbrella of the International Late Effects of Childhood Cancer Guideline Harmonization Group. After evaluating concordances and discordances of four existing CPGs, we performed systematic literature searches. We screened articles for eligibility, assessed quality, extracted, and summarized the data from included articles. We formulated recommendations based on the evidence and clinical judgment. Of 3647 articles identified, 70 articles from 14 countries were included. The prevalence of CRF in CAYA cancer survivors ranged from 10-85%. We recommend that healthcare providers are aware of the risk of CRF, implement regular screening with validated measures, and recommend effective interventions to fatigued survivors. A considerable proportion of CAYA cancer survivors suffers from CRF even years after the end of treatment. We recommend that healthcare providers adopt regular screening to detect and treat CRF early and positively influence survivors' health and quality of life.","Christen, Roser, Mulder, Ilic, Lie, Loonen, Mellblom, Kremer, Hudson, Constine, Skinner, Scheinemann, Gilleland Marchak, Michel","Christen, Roser, Mulder, Ilic, Lie, Loonen, Mellblom, Kremer, Hudson, Constine, Skinner, Scheinemann, Gilleland Marchak, Michel",https://doi.org/10.1007/s11764-020-00904-9,https://doi.org/10.1007/s11764-020-00904-9,2021-08-03
16288.0,pubmed,pubmed,Improving Intranasal Naloxone Prescribing Through EMR Modification and Automation,Improving Intranasal Naloxone Prescribing Through EMR Modification and Automation,"In 2017, approximately 11.4 million Americans used opioids inappropriately. Nearly 47,600 deaths in 2017 were attributable to overdose on opioids. Intranasal naloxone was approved by the Food and Drug Administration in 2015 as a rescue medication for opioid overdose. New York State launched a prescription drug monitoring program in 2012, the Internet System for Tracking Over-Prescribing (I-STOP), that required completion before dispensing any controlled substance. Currently, prescribing naloxone at our institution requires 10 clicks and 2 free text boxes. The goal of this project was to increase the prescribing of intranasal naloxone by utilizing EMR automation and visualization tools. Our intervention embedded a section within the required I-STOP note, displaying the last date naloxone was prescribed and an option to &quot;prescribe intranasal naloxone.&quot; If checked, a prepopulated order dialog box was generated. Intranasal naloxone orders for the institution totaled 65 for 2 months before the intervention and 203 for 2 months after the intervention, with 112 (55%) coming directly from the I-STOP note modification. Ease of prescribing improved as total clicks were reduced from 10 to 2, and free text boxes from 2 to 0. Our findings suggest that a clinical decision support system can be an effective way to increase hospital-wide naloxone prescribing rates. We were able to increase prescribing rates by more than three-fold, significantly increasing the availability of a rescue medication to individuals at high-risk for overdose. Intranasal naloxone prescribing increased with the implementation of a visual reminder and a more intuitive ordering experience while preserving provider autonomy.","In 2017, approximately 11.4 million Americans used opioids inappropriately. Nearly 47,600 deaths in 2017 were attributable to overdose on opioids. Intranasal naloxone was approved by the Food and Drug Administration in 2015 as a rescue medication for opioid overdose. New York State launched a prescription drug monitoring program in 2012, the Internet System for Tracking Over-Prescribing (I-STOP), that required completion before dispensing any controlled substance. Currently, prescribing naloxone at our institution requires 10 clicks and 2 free text boxes. The goal of this project was to increase the prescribing of intranasal naloxone by utilizing EMR automation and visualization tools. Our intervention embedded a section within the required I-STOP note, displaying the last date naloxone was prescribed and an option to ""prescribe intranasal naloxone."" If checked, a prepopulated order dialog box was generated. Intranasal naloxone orders for the institution totaled 65 for 2 months before the intervention and 203 for 2 months after the intervention, with 112 (55%) coming directly from the I-STOP note modification. Ease of prescribing improved as total clicks were reduced from 10 to 2, and free text boxes from 2 to 0. Our findings suggest that a clinical decision support system can be an effective way to increase hospital-wide naloxone prescribing rates. We were able to increase prescribing rates by more than three-fold, significantly increasing the availability of a rescue medication to individuals at high-risk for overdose. Intranasal naloxone prescribing increased with the implementation of a visual reminder and a more intuitive ordering experience while preserving provider autonomy.","Crusco, Smith, Rajupet","Crusco, Smith, Rajupet",https://doi.org/10.1097/ADM.0000000000000724,https://doi.org/10.1097/ADM.0000000000000724,2021-08-03
16291.0,pubmed,pubmed,Efficient Deep Learning Architecture for Detection and Recognition of Thyroid Nodules,Efficient Deep Learning Architecture for Detection and Recognition of Thyroid Nodules,"Ultrasonography is widely used in the clinical diagnosis of thyroid nodules. Ultrasound images of thyroid nodules have different appearances, interior features, and blurred borders that are difficult for a physician to diagnose into malignant or benign types merely through visual recognition. The development of artificial intelligence, especially deep learning, has led to great advances in the field of medical image diagnosis. However, there are some challenges to achieve precision and efficiency in the recognition of thyroid nodules. In this work, we propose a deep learning architecture, you only look once v3 dense multireceptive fields convolutional neural network (YOLOv3-DMRF), based on YOLOv3. It comprises a DMRF-CNN and multiscale detection layers. In DMRF-CNN, we integrate dilated convolution with different dilation rates to continue passing the edge and the texture features to deeper layers. Two different scale detection layers are deployed to recognize the different sizes of the thyroid nodules. We used two datasets to train and evaluate the YOLOv3-DMRF during the experiments. One dataset includes 699 original ultrasound images of thyroid nodules collected from a local health physical center. We obtained 10,485 images after data augmentation. Another dataset is an open-access dataset that includes ultrasound images of 111 malignant and 41 benign thyroid nodules. Average precision (AP) and mean average precision (mAP) are used as the metrics for quantitative and qualitative evaluations. We compared the proposed YOLOv3-DMRF with some state-of-the-art deep learning networks. The experimental results show that YOLOv3-DMRF outperforms others on mAP and detection time on both the datasets. Specifically, the values of mAP and detection time were 90.05 and 95.23% and 3.7 and 2.2Ã¢â‚¬â€°s, respectively, on the two test datasets. Experimental results demonstrate that the proposed YOLOv3-DMRF is efficient for detection and recognition of thyroid nodules for ultrasound images.","Ultrasonography is widely used in the clinical diagnosis of thyroid nodules. Ultrasound images of thyroid nodules have different appearances, interior features, and blurred borders that are difficult for a physician to diagnose into malignant or benign types merely through visual recognition. The development of artificial intelligence, especially deep learning, has led to great advances in the field of medical image diagnosis. However, there are some challenges to achieve precision and efficiency in the recognition of thyroid nodules. In this work, we propose a deep learning architecture, you only look once v3 dense multireceptive fields convolutional neural network (YOLOv3-DMRF), based on YOLOv3. It comprises a DMRF-CNN and multiscale detection layers. In DMRF-CNN, we integrate dilated convolution with different dilation rates to continue passing the edge and the texture features to deeper layers. Two different scale detection layers are deployed to recognize the different sizes of the thyroid nodules. We used two datasets to train and evaluate the YOLOv3-DMRF during the experiments. One dataset includes 699 original ultrasound images of thyroid nodules collected from a local health physical center. We obtained 10,485 images after data augmentation. Another dataset is an open-access dataset that includes ultrasound images of 111 malignant and 41 benign thyroid nodules. Average precision (AP) and mean average precision (mAP) are used as the metrics for quantitative and qualitative evaluations. We compared the proposed YOLOv3-DMRF with some state-of-the-art deep learning networks. The experimental results show that YOLOv3-DMRF outperforms others on mAP and detection time on both the datasets. Specifically, the values of mAP and detection time were 90.05 and 95.23% and 3.7 and 2.2â€‰s, respectively, on the two test datasets. Experimental results demonstrate that the proposed YOLOv3-DMRF is efficient for detection and recognition of thyroid nodules for ultrasound images.","Ma, Duan, Zhang, Wang, Wang, Li, Li, Zhang, Ma","Ma, Duan, Zhang, Wang, Wang, Li, Li, Zhang, Ma",https://doi.org/10.1155/2020/1242781,https://doi.org/10.1155/2020/1242781,2021-08-03
16294.0,pubmed,pubmed,Review of hexachlorocyclohexane (HCH) and dichlorodiphenyltrichloroethane (DDT) contamination in Chinese soils,Review of hexachlorocyclohexane (HCH) and dichlorodiphenyltrichloroethane (DDT) contamination in Chinese soils,"Despite a ban on the production and use of organochlorine pesticides (OCPs) after 1983, serious OCP pollution still exists in the soil in certain areas of China because OCPs degrade very slowly. Based on a systematic review, we identified 136 relevant papers focusing on soil contamination from hexachlorocyclohexane (HCH) and dichlorodiphenyltrichloroethane (DDT) in China (published from 2001 to 2019). We compiled scientific data, extracted and analyzed relevant information, and summarized the pollution characteristics of HCH and DDT in Chinese soils found in two land use types: agricultural land and land for construction. Related studies on HCH and DDT in Chinese soils focus on the Beijing-Tianjin-Hebei region and the Yangtze and Pearl River Deltas, where agricultural soils are predominant. The average concentrations of both HCH and DDT in agricultural soils were generally lower than the risk screening value (100Ã‚Â ÃŽÂ¼g/kg) in most provinces in China, except for DDT concentrations in the Inner Mongolia autonomous region. However, in certain central and eastern regions, mean or maximum recorded DDT concentrations approaching or exceeding 100Ã‚Â ÃŽÂ¼g/kg were recorded. Regarding land for construction, soils with excessive concentrations of HCH and DDT were primarily observed at sites of operational or defunct pesticide factories. According to isomer and metabolite compositions, HCH and DDT at most sites originated from historical residues, but others may have been new inputs after 1983. Since 2015, the concentrations of HCH and DDT in agricultural soils in China have been decreasing, and those in the soils of land for construction (except for sites of operational or defunct pesticide factories) have not exceeded the standard after 2005. This indicates that the measures to prohibit the production and use of OCPs in China have been effective. However, the management of operational or defunct pesticide factories polluted by OCPs requires further improvement.","Despite a ban on the production and use of organochlorine pesticides (OCPs) after 1983, serious OCP pollution still exists in the soil in certain areas of China because OCPs degrade very slowly. Based on a systematic review, we identified 136 relevant papers focusing on soil contamination from hexachlorocyclohexane (HCH) and dichlorodiphenyltrichloroethane (DDT) in China (published from 2001 to 2019). We compiled scientific data, extracted and analyzed relevant information, and summarized the pollution characteristics of HCH and DDT in Chinese soils found in two land use types: agricultural land and land for construction. Related studies on HCH and DDT in Chinese soils focus on the Beijing-Tianjin-Hebei region and the Yangtze and Pearl River Deltas, where agricultural soils are predominant. The average concentrations of both HCH and DDT in agricultural soils were generally lower than the risk screening value (100Â Î¼g/kg) in most provinces in China, except for DDT concentrations in the Inner Mongolia autonomous region. However, in certain central and eastern regions, mean or maximum recorded DDT concentrations approaching or exceeding 100Â Î¼g/kg were recorded. Regarding land for construction, soils with excessive concentrations of HCH and DDT were primarily observed at sites of operational or defunct pesticide factories. According to isomer and metabolite compositions, HCH and DDT at most sites originated from historical residues, but others may have been new inputs after 1983. Since 2015, the concentrations of HCH and DDT in agricultural soils in China have been decreasing, and those in the soils of land for construction (except for sites of operational or defunct pesticide factories) have not exceeded the standard after 2005. This indicates that the measures to prohibit the production and use of OCPs in China have been effective. However, the management of operational or defunct pesticide factories polluted by OCPs requires further improvement.","Ma, Yun, Ruan, Lu, Shi, Qin, Men, Zou, Du, Xing, Xie","Ma, Yun, Ruan, Lu, Shi, Qin, Men, Zou, Du, Xing, Xie",https://doi.org/10.1016/j.scitotenv.2020.141212,https://doi.org/10.1016/j.scitotenv.2020.141212,2021-08-03
16301.0,pubmed,pubmed,Review of Clinical Research Informatics,Review of Clinical Research Informatics,"Clinical Research Informatics (CRI) declares its scope in its name, but its content, both in terms of the clinical research it supports-and sometimes initiates-and the methods it has developed over time, reach much further than the name suggests. The goal of this review is to celebrate the extraordinary diversity of activity and of results, not as a prize-giving pageant, but in recognition of the field, the community that both serves and is sustained by it, and of its interdisciplinarity and its international dimension. Beyond personal awareness of a range of work commensurate with the author's own research, it is clear that, even with a thorough literature search, a comprehensive review is impossible. Moreover, the field has grown and subdivided to an extent that makes it very hard for one individual to be familiar with every branch or with more than a few branches in any depth. A literature survey was conducted that focused on informatics-related terms in the general biomedical and healthcare literature, and specific concerns (&quot;artificial intelligence&quot;, &quot;data models&quot;, &quot;analytics&quot;, etc.) in the biomedical informatics (BMI) literature. In addition to a selection from the results from these searches, suggestive references within them were also considered. The substantive sections of the paper-Artificial Intelligence, Machine Learning, and &quot;Big Data&quot; Analytics; Common Data Models, Data Quality, and Standards; Phenotyping and Cohort Discovery; Privacy: Deidentification, Distributed Computation, Blockchain; Causal Inference and Real-World Evidence-provide broad coverage of these active research areas, with, no doubt, a bias towards this reviewer's interests and preferences, landing on a number of papers that stood out in one way or another, or, alternatively, exemplified a particular line of work. CRI is thriving, not only in the familiar major centers of research, but more widely, throughout the world. This is not to pretend that the distribution is uniform, but to highlight the potential for this domain to play a prominent role in supporting progress in medicine, healthcare, and wellbeing everywhere. We conclude with the observation that CRI and its practitioners would make apt stewards of the new medical knowledge that their methods will bring forward.","Clinical Research Informatics (CRI) declares its scope in its name, but its content, both in terms of the clinical research it supports-and sometimes initiates-and the methods it has developed over time, reach much further than the name suggests. The goal of this review is to celebrate the extraordinary diversity of activity and of results, not as a prize-giving pageant, but in recognition of the field, the community that both serves and is sustained by it, and of its interdisciplinarity and its international dimension. Beyond personal awareness of a range of work commensurate with the author's own research, it is clear that, even with a thorough literature search, a comprehensive review is impossible. Moreover, the field has grown and subdivided to an extent that makes it very hard for one individual to be familiar with every branch or with more than a few branches in any depth. A literature survey was conducted that focused on informatics-related terms in the general biomedical and healthcare literature, and specific concerns (""artificial intelligence"", ""data models"", ""analytics"", etc.) in the biomedical informatics (BMI) literature. In addition to a selection from the results from these searches, suggestive references within them were also considered. The substantive sections of the paper-Artificial Intelligence, Machine Learning, and ""Big Data"" Analytics; Common Data Models, Data Quality, and Standards; Phenotyping and Cohort Discovery; Privacy: Deidentification, Distributed Computation, Blockchain; Causal Inference and Real-World Evidence-provide broad coverage of these active research areas, with, no doubt, a bias towards this reviewer's interests and preferences, landing on a number of papers that stood out in one way or another, or, alternatively, exemplified a particular line of work. CRI is thriving, not only in the familiar major centers of research, but more widely, throughout the world. This is not to pretend that the distribution is uniform, but to highlight the potential for this domain to play a prominent role in supporting progress in medicine, healthcare, and wellbeing everywhere. We conclude with the observation that CRI and its practitioners would make apt stewards of the new medical knowledge that their methods will bring forward.",Solomonides,Solomonides,https://doi.org/10.1055/s-0040-1701988,https://doi.org/10.1055/s-0040-1701988,2021-08-03
16318.0,pubmed,pubmed,Effects of reading media on reading comprehension in health professional education: a systematic review protocol,Effects of reading media on reading comprehension in health professional education: a systematic review protocol,"To evaluate the effect of digital-based reading versus paper-based reading on reading comprehension among students, trainees, and residents participating in health professional education. Several reviews have examined the effects of reading media on reading comprehension; however, none have considered health professional education specifically. The growing use of electronic media in health professional education, as well as recent data on the consequences of digital-based reading on learning, justify the necessity to review the current literature to provide research and educational recommendations. Studies conducted with health professions students, trainees, and residents individually receiving educational material written in their first language in a paper-based or a digital-based format will be considered. Studies conducted among participants with cognitive impairment or reading difficulties will be excluded. Observational, experimental and quasi-experimental studies that assess reading comprehension measured by previously validated or researcher-generated tests will be considered. Relevant studies will be sought from CINAHL, Embase, ERIC, Google Scholar, MEDLINE, PsycINFO, and Web of Science (SCI and SSCI), without date or language restrictions. Two independent reviewers will perform title and abstract screening, full-text review, critical appraisal, and data extraction. Disagreements will be resolved through discussion or with a third independent reviewer. Synthesis will occur at four levels (i.e., study, participant, intervention, and outcome levels) in a table format. Data will be synthesized descriptively and with meta-analyses if appropriate. CRD42020154519.","To evaluate the effect of digital-based reading versus paper-based reading on reading comprehension among students, trainees, and residents participating in health professional education. Several reviews have examined the effects of reading media on reading comprehension; however, none have considered health professional education specifically. The growing use of electronic media in health professional education, as well as recent data on the consequences of digital-based reading on learning, justify the necessity to review the current literature to provide research and educational recommendations. Studies conducted with health professions students, trainees, and residents individually receiving educational material written in their first language in a paper-based or a digital-based format will be considered. Studies conducted among participants with cognitive impairment or reading difficulties will be excluded. Observational, experimental and quasi-experimental studies that assess reading comprehension measured by previously validated or researcher-generated tests will be considered. Relevant studies will be sought from CINAHL, Embase, ERIC, Google Scholar, MEDLINE, PsycINFO, and Web of Science (SCI and SSCI), without date or language restrictions. Two independent reviewers will perform title and abstract screening, full-text review, critical appraisal, and data extraction. Disagreements will be resolved through discussion or with a third independent reviewer. Synthesis will occur at four levels (i.e., study, participant, intervention, and outcome levels) in a table format. Data will be synthesized descriptively and with meta-analyses if appropriate. PROSPERO CRD42020154519.","Fontaine, Zagury-Orly, de Denus, LordkipanidzÃƒÂ©, Beauchesne, Maheu-Cadotte, White, Thibodeau-Jarry, Lavoie","Fontaine, Zagury-Orly, de Denus, LordkipanidzÃ©, Beauchesne, Maheu-Cadotte, White, Thibodeau-Jarry, Lavoie",https://doi.org/10.11124/JBISRIR-D-19-00348,https://doi.org/10.11124/JBISRIR-D-19-00348,2021-08-03
16320.0,pubmed,pubmed,Automated Detection of Interictal Epileptiform Discharges from Scalp Electroencephalograms by Convolutional Neural Networks,Automated Detection of Interictal Epileptiform Discharges from Scalp Electroencephalograms by Convolutional Neural Networks,"Visual evaluation of electroencephalogram (EEG) for Interictal Epileptiform Discharges (IEDs) as distinctive biomarkers of epilepsy has various limitations, including time-consuming reviews, steep learning curves, interobserver variability, and the need for specialized experts. The development of an automated IED detector is necessary to provide a faster and reliable diagnosis of epilepsy. In this paper, we propose an automated IED detector based on Convolutional Neural Networks (CNNs). We have evaluated the proposed IED detector on a sizable database of 554 scalp EEG recordings (84 epileptic patients and 461 nonepileptic subjects) recorded at Massachusetts General Hospital (MGH), Boston. The proposed CNN IED detector has achieved superior performance in comparison with conventional methods with a mean cross-validation area under the precision-recall curve (AUPRC) of 0.838[Formula: see text]Ã‚Â±[Formula: see text]0.040 and false detection rate of 0.2[Formula: see text]Ã‚Â±[Formula: see text]0.11 per minute for a sensitivity of 80%. We demonstrated the proposed system to be noninferior to 30 neurologists on a dataset from the Medical University of South Carolina (MUSC). Further, we clinically validated the system at National University Hospital (NUH), Singapore, with an agreement accuracy of 81.41% with a clinical expert. Moreover, the proposed system can be applied to EEG recordings with any arbitrary number of channels.","Visual evaluation of electroencephalogram (EEG) for Interictal Epileptiform Discharges (IEDs) as distinctive biomarkers of epilepsy has various limitations, including time-consuming reviews, steep learning curves, interobserver variability, and the need for specialized experts. The development of an automated IED detector is necessary to provide a faster and reliable diagnosis of epilepsy. In this paper, we propose an automated IED detector based on Convolutional Neural Networks (CNNs). We have evaluated the proposed IED detector on a sizable database of 554 scalp EEG recordings (84 epileptic patients and 461 nonepileptic subjects) recorded at Massachusetts General Hospital (MGH), Boston. The proposed CNN IED detector has achieved superior performance in comparison with conventional methods with a mean cross-validation area under the precision-recall curve (AUPRC) of 0.838[Formula: see text]Â±[Formula: see text]0.040 and false detection rate of 0.2[Formula: see text]Â±[Formula: see text]0.11 per minute for a sensitivity of 80%. We demonstrated the proposed system to be noninferior to 30 neurologists on a dataset from the Medical University of South Carolina (MUSC). Further, we clinically validated the system at National University Hospital (NUH), Singapore, with an agreement accuracy of 81.41% with a clinical expert. Moreover, the proposed system can be applied to EEG recordings with any arbitrary number of channels.","Thomas, Jin, Thangavel, Bagheri, Yuvaraj, Dauwels, Rathakrishnan, Halford, Cash, Westover","Thomas, Jin, Thangavel, Bagheri, Yuvaraj, Dauwels, Rathakrishnan, Halford, Cash, Westover",https://doi.org/10.1142/S0129065720500306,https://doi.org/10.1142/S0129065720500306,2021-08-03
16322.0,pubmed,pubmed,Effect of monoclonal antibody drug therapy on mucosal biomarkers in airway disease: A systematic review,Effect of monoclonal antibody drug therapy on mucosal biomarkers in airway disease: A systematic review,"Monoclonal antibody therapies have a growing role in treating refractory airway disease. The review aimed to summarize the response of respiratory mucosa to monoclonal antibody treatments in inflammatory airway conditions. We conducted a systematic review including risk of bias assessment. MEDLINE, EMBASE and PubMed from 1 January 2000 to 16 November 2019 were searched. Eligible studies assessed the immunological and histological response of airway mucosa to monoclonal antibody therapy compared with baseline or a comparison group in patients with respiratory diseases (asthma, chronic rhinosinusitis and allergic rhinitis). Any prospective interventional studies, including randomized controlled trials (RCTs) and single-arm trials, were eligible. There were 4195 articles screened, and full-text analysis produced nÃ‚Â =Ã‚Â 11 studies with extractable data. Nine were RCTs, and two were single-arm trials. These studies focused on asthma (nÃ‚Â =Ã‚Â 9 articles), chronic rhinosinusitis (nÃ‚Â =Ã‚Â 1) and allergic rhinitis (nÃ‚Â =Ã‚Â 1). Five monoclonal antibody drugs were assessed (omalizumab, mepolizumab, dupilumab, benralizumab and tralokinumab). Risk of bias was low (nÃ‚Â =Ã‚Â 6) or unclear (nÃ‚Â =Ã‚Â 3) in the RCTs and moderate in the single-arm trials. Omalizumab reduced the mucosal concentration of its target, IgE. Dupilumab reduced the concentration of one of its targets, IL-13, but not IL-4. Omalizumab, mepolizumab and benralizumab reduced tissue eosinophil cell density. Dupilumab decreased mucosal eosinophil granule proteins. Tralokinumab did not affect airway mucosa. Knowledge of the expected biological response of monoclonal antibody therapy on biomarkers in disease tissue provides an important supplement to data about clinical outcomes. An understanding of the biological effect is essential to identify likely responders, reasons for treatment failure and necessary adjustments to monoclonal antibody treatment. Further investigation into the effect of monoclonal antibody therapy on disease mucosa and more precise endotyping are required to move closer to achieving personalized medicine.","Monoclonal antibody therapies have a growing role in treating refractory airway disease. The review aimed to summarize the response of respiratory mucosa to monoclonal antibody treatments in inflammatory airway conditions. We conducted a systematic review including risk of bias assessment. MEDLINE, EMBASE and PubMed from 1 January 2000 to 16 November 2019 were searched. Eligible studies assessed the immunological and histological response of airway mucosa to monoclonal antibody therapy compared with baseline or a comparison group in patients with respiratory diseases (asthma, chronic rhinosinusitis and allergic rhinitis). Any prospective interventional studies, including randomized controlled trials (RCTs) and single-arm trials, were eligible. There were 4195 articles screened, and full-text analysis produced nÂ =Â 11 studies with extractable data. Nine were RCTs, and two were single-arm trials. These studies focused on asthma (nÂ =Â 9 articles), chronic rhinosinusitis (nÂ =Â 1) and allergic rhinitis (nÂ =Â 1). Five monoclonal antibody drugs were assessed (omalizumab, mepolizumab, dupilumab, benralizumab and tralokinumab). Risk of bias was low (nÂ =Â 6) or unclear (nÂ =Â 3) in the RCTs and moderate in the single-arm trials. Omalizumab reduced the mucosal concentration of its target, IgE. Dupilumab reduced the concentration of one of its targets, IL-13, but not IL-4. Omalizumab, mepolizumab and benralizumab reduced tissue eosinophil cell density. Dupilumab decreased mucosal eosinophil granule proteins. Tralokinumab did not affect airway mucosa. Knowledge of the expected biological response of monoclonal antibody therapy on biomarkers in disease tissue provides an important supplement to data about clinical outcomes. An understanding of the biological effect is essential to identify likely responders, reasons for treatment failure and necessary adjustments to monoclonal antibody treatment. Further investigation into the effect of monoclonal antibody therapy on disease mucosa and more precise endotyping are required to move closer to achieving personalized medicine.","Walter, Ho, Alvarado, Rimmer, Campbell, Kalish, Sacks, Harvey","Walter, Ho, Alvarado, Rimmer, Campbell, Kalish, Sacks, Harvey",https://doi.org/10.1111/cea.13721,https://doi.org/10.1111/cea.13721,2021-08-03
16323.0,pubmed,pubmed,Toward automatic evaluation of medical abstracts: The current value of sentiment analysis and machine learning for classification of the importance of PubMed abstracts of randomized trials for stroke,Toward automatic evaluation of medical abstracts: The current value of sentiment analysis and machine learning for classification of the importance of PubMed abstracts of randomized trials for stroke,"Text mining with automatic extraction of key features is gaining increasing importance in science and particularly medicine due to the rapidly increasing number of publications. Here we evaluate the current potential of sentiment analysis and machine learning to extract the importance of the reported results and conclusions of randomized trials on stroke. PubMed abstracts of 200 recent reports of randomized trials were reviewed and manually classified according to the estimated importance of the studies. Importance of the papers was classified as &quot;game changer&quot;, &quot;suggestive&quot;, &quot;maybe&quot; &quot;negative result&quot;. Algorithmic sentiment analysis was subsequently used on both the &quot;Results&quot; and the &quot;Conclusions&quot; paragraphs, resulting in a numerical output for polarity and subjectivity. The result of the human assessment was then compared to polarity and subjectivity. In addition, a neural network using the Keras platform built on Tensorflow and Python was trained to map the &quot;Results&quot; and &quot;Conclusions&quot; to the dichotomized human assessment (1: &quot;game changer&quot; or &quot;suggestive&quot;; 0:&quot;maybe&quot; or &quot;negative&quot;, or no results reported). 120 abstracts were used as the training set and 80 as the test set. 9 out of the 200 reports were classified manually as &quot;game changer&quot;, 40 as &quot;suggestive&quot;, 73 as &quot;maybe&quot; and 32 and &quot;negative&quot;; 46 abstracts did not contain any results. Polarity was generally higher for the &quot;Conclusions&quot; than for the &quot;Results&quot;. Polarity was highest for the &quot;Conclusions&quot; classified as &quot;suggestive&quot;. Subjectivity was also higher in the classes &quot;suggestive&quot; and &quot;maybe&quot; than in the classes &quot;game changer&quot; and &quot;negative&quot;. The trained neural network provided a correct dichotomized output with an accuracy of 71% based on the &quot;Results&quot; and 73% based on &quot;Conclusions&quot; . Current statistical approaches to text analysis can grasp the impact of scientific medical abstracts to a certain degree. Sentiment analysis showed that mediocre results are apparently written in more enthusiastic words than clearly positive or negative results.","Text mining with automatic extraction of key features is gaining increasing importance in science and particularly medicine due to the rapidly increasing number of publications. Here we evaluate the current potential of sentiment analysis and machine learning to extract the importance of the reported results and conclusions of randomized trials on stroke. PubMed abstracts of 200 recent reports of randomized trials were reviewed and manually classified according to the estimated importance of the studies. Importance of the papers was classified as ""game changer"", ""suggestive"", ""maybe"" ""negative result"". Algorithmic sentiment analysis was subsequently used on both the ""Results"" and the ""Conclusions"" paragraphs, resulting in a numerical output for polarity and subjectivity. The result of the human assessment was then compared to polarity and subjectivity. In addition, a neural network using the Keras platform built on Tensorflow and Python was trained to map the ""Results"" and ""Conclusions"" to the dichotomized human assessment (1: ""game changer"" or ""suggestive""; 0:""maybe"" or ""negative"", or no results reported). 120 abstracts were used as the training set and 80 as the test set. 9 out of the 200 reports were classified manually as ""game changer"", 40 as ""suggestive"", 73 as ""maybe"" and 32 and ""negative""; 46 abstracts did not contain any results. Polarity was generally higher for the ""Conclusions"" than for the ""Results"". Polarity was highest for the ""Conclusions"" classified as ""suggestive"". Subjectivity was also higher in the classes ""suggestive"" and ""maybe"" than in the classes ""game changer"" and ""negative"". The trained neural network provided a correct dichotomized output with an accuracy of 71% based on the ""Results"" and 73% based on ""Conclusions"" . Current statistical approaches to text analysis can grasp the impact of scientific medical abstracts to a certain degree. Sentiment analysis showed that mediocre results are apparently written in more enthusiastic words than clearly positive or negative results.","Fischer, Steiger","Fischer, Steiger",https://doi.org/10.1016/j.jstrokecerebrovasdis.2020.105042,https://doi.org/10.1016/j.jstrokecerebrovasdis.2020.105042,2021-08-03
16327.0,pubmed,pubmed,Semixup: In-and Out-of-Manifold Regularization for Deep Semi-Supervised Knee Osteoarthritis Severity Grading from Plain Radiographs,Semixup: In- and Out-of-Manifold Regularization for Deep Semi-Supervised Knee Osteoarthritis Severity Grading From Plain Radiographs,"Knee osteoarthritis (OA) is one of the highest disability factors in the world. This musculoskeletal disorder is assessed from clinical symptoms, and typically confirmed via radiographic assessment. This visual assessment done by a radiologist requires experience, and suffers from moderate to high inter-observer variability. The recent literature has shown that deep learning methods can reliably perform the OA severity assessment according to the gold standard Kellgren-Lawrence (KL) grading system. However, these methods require large amounts of labeled data, which are costly to obtain. In this study, we propose the Semixup algorithm, a semi-supervised learning (SSL) approach to leverage unlabeled data. Semixup relies on consistency regularization using in- and out-of-manifold samples, together with interpolated consistency. On an independent test set, our method significantly outperformed other state-of-the-art SSL methods in most cases. Finally, when compared to a well-tuned fully supervised baseline that yielded a balanced accuracy (BA) of 70.9 Ã‚Â± 0.8% on the test set, Semixup had comparable performance - BA of 71Ã‚Â± 0.8% (p = 0.368) while requiring 6 times less labeled data. These results show that our proposed SSL method allows building fully automatic OA severity assessment tools with datasets that are available outside research settings.","Knee osteoarthritis (OA) is one of the highest disability factors in the world. This musculoskeletal disorder is assessed from clinical symptoms, and typically confirmed via radiographic assessment. This visual assessment done by a radiologist requires experience, and suffers from moderate to high inter-observer variability. The recent literature has shown that deep learning methods can reliably perform the OA severity assessment according to the gold standard Kellgren-Lawrence (KL) grading system. However, these methods require large amounts of labeled data, which are costly to obtain. In this study, we propose the Semixup algorithm, a semi-supervised learning (SSL) approach to leverage unlabeled data. Semixup relies on consistency regularization using in- and out-of-manifold samples, together with interpolated consistency. On an independent test set, our method significantly outperformed other state-of-the-art SSL methods in most cases. Finally, when compared to a well-tuned fully supervised baseline that yielded a balanced accuracy (BA) of 70.9 Â± 0.8% on the test set, Semixup had comparable performance - BA of 71 Â± 0.8% ( p=0.368 ) while requiring 6 times less labeled data. These results show that our proposed SSL method allows building fully automatic OA severity assessment tools with datasets that are available outside research settings.","Nguyen, Saarakkala, Blaschko, Tiulpin","Nguyen, Saarakkala, Blaschko, Tiulpin",https://doi.org/10.1109/TMI.2020.3017007,https://doi.org/10.1109/TMI.2020.3017007,2021-08-03
16339.0,pubmed,pubmed,"Social, Behavioral, and Cultural factors of HIV in Malawi: Semi-Automated Systematic Review","Social, Behavioral, and Cultural factors of HIV in Malawi: Semi-Automated Systematic Review","Demographic and sociobehavioral factors are strong drivers of HIV infection rates in sub-Saharan Africa. These factors are often studied in qualitative research but ignored in quantitative analyses. However, they provide in-depth insight into the local behavior and may help to improve HIV prevention. To obtain a comprehensive overview of the sociobehavioral factors influencing HIV prevalence and incidence in Malawi, we systematically reviewed the literature using a newly programmed tool for automatizing part of the systematic review process. Due to the choice of broad search terms (&quot;HIV AND Malawi&quot;), our preliminary search revealed many thousands of articles. We, therefore, developed a Python tool to automatically extract, process, and categorize open-access articles published from January 1, 1987 to October 1, 2019 in the PubMed, PubMed Central, JSTOR, Paperity, and arXiV databases. We then used a topic modelling algorithm to classify and identify publications of interest. Our tool extracted 22,709 unique articles; 16,942 could be further processed. After topic modelling, 519 of these were clustered into relevant topics, of which 20 were kept after manual screening. We retrieved 7 more publications after examining the references so that 27 publications were finally included in the review. Reducing the 16,942 articles to 519 potentially relevant articles using the software took 5 days. Several factors contributing to the risk of HIV infection were identified, including religion, gender and relationship dynamics, beliefs, and sociobehavioral attitudes. Our software does not replace traditional systematic reviews, but it returns useful results to broad queries of open-access literature in under a week, without a priori knowledge. This produces a &quot;seed dataset&quot; of relevance that could be further developed. It identified known factors and factors that may be specific to Malawi. In the future, we aim to expand the tool by adding more social science databases and applying it to other sub-Saharan African countries.","Demographic and sociobehavioral factors are strong drivers of HIV infection rates in sub-Saharan Africa. These factors are often studied in qualitative research but ignored in quantitative analyses. However, they provide in-depth insight into the local behavior and may help to improve HIV prevention. To obtain a comprehensive overview of the sociobehavioral factors influencing HIV prevalence and incidence in Malawi, we systematically reviewed the literature using a newly programmed tool for automatizing part of the systematic review process. Due to the choice of broad search terms (""HIV AND Malawi""), our preliminary search revealed many thousands of articles. We, therefore, developed a Python tool to automatically extract, process, and categorize open-access articles published from January 1, 1987 to October 1, 2019 in the PubMed, PubMed Central, JSTOR, Paperity, and arXiV databases. We then used a topic modelling algorithm to classify and identify publications of interest. Our tool extracted 22,709 unique articles; 16,942 could be further processed. After topic modelling, 519 of these were clustered into relevant topics, of which 20 were kept after manual screening. We retrieved 7 more publications after examining the references so that 27 publications were finally included in the review. Reducing the 16,942 articles to 519 potentially relevant articles using the software took 5 days. Several factors contributing to the risk of HIV infection were identified, including religion, gender and relationship dynamics, beliefs, and sociobehavioral attitudes. Our software does not replace traditional systematic reviews, but it returns useful results to broad queries of open-access literature in under a week, without a priori knowledge. This produces a ""seed dataset"" of relevance that could be further developed. It identified known factors and factors that may be specific to Malawi. In the future, we aim to expand the tool by adding more social science databases and applying it to other sub-Saharan African countries.","Thiabaud, Triulzi, Orel, Tal, Keiser","Thiabaud, Triulzi, Orel, Tal, Keiser",https://doi.org/10.2196/18747,https://doi.org/10.2196/18747,2021-08-03
16348.0,pubmed,pubmed,The cost-utility of intravenous magnesium sulfate for treating asthma exacerbations in children,The cost-utility of intravenous magnesium sulfate for treating asthma exacerbations in children,"Although evidence supports the use of intravenous magnesium sulfate (MS) in asthma exacerbations, MS continues to be considered a second-line drug for managing pediatric asthma exacerbations. This study aimed to evaluate the cost-utility of MS in asthma exacerbations. We used a decision tree model to estimate the cost-utility of MS compared to treatment without MS (control group) in children with asthma exacerbations. Cost data were obtained from a retrospective study from tertiary centers in Rionegro, Colombia, while utilities were collected from the literature. Probabilistic sensitivity analysis was carried out using the Monte Carlo technique with a simulation of a hypothetical cohort of 10Ã¢â‚¬â€°000 patients to generate expected cost utilities with 95% confidence intervals. We used a cost-effectiveness acceptability curve to evaluate the uncertainty surrounding the cost-utility of MS. The model showed that MS had a lower total cost than the control group (US $1149 vs US $1598 average cost per patient) and higher quality-adjusted life years (0.60 vs 0.52 average per patient), showing dominance. The probability that MS provides a more cost-effective use of resources compared with standard therapy exceeds 99% for all willingness-to-pay thresholds. Intravenous MS was less expensive and more effective than treatment without intravenous MS in children with asthma exacerbations. Our study provides evidence that should be used by decision-makers to improve clinical practice guidelines and should be replicated to validate its results in other middle-income countries.","Although evidence supports the use of intravenous magnesium sulfate (MS) in asthma exacerbations, MS continues to be considered a second-line drug for managing pediatric asthma exacerbations. This study aimed to evaluate the cost-utility of MS in asthma exacerbations. We used a decision tree model to estimate the cost-utility of MS compared to treatment without MS (control group) in children with asthma exacerbations. Cost data were obtained from a retrospective study from tertiary centers in Rionegro, Colombia, while utilities were collected from the literature. Probabilistic sensitivity analysis was carried out using the Monte Carlo technique with a simulation of a hypothetical cohort of 10â€‰000 patients to generate expected cost utilities with 95% confidence intervals. We used a cost-effectiveness acceptability curve to evaluate the uncertainty surrounding the cost-utility of MS. The model showed that MS had a lower total cost than the control group (US $1149 vs US $1598 average cost per patient) and higher quality-adjusted life years (0.60 vs 0.52 average per patient), showing dominance. The probability that MS provides a more cost-effective use of resources compared with standard therapy exceeds 99% for all willingness-to-pay thresholds. Intravenous MS was less expensive and more effective than treatment without intravenous MS in children with asthma exacerbations. Our study provides evidence that should be used by decision-makers to improve clinical practice guidelines and should be replicated to validate its results in other middle-income countries.","Buendia, AcuÃƒÂ±a-Cordero, Rodriguez-Martinez","Buendia, AcuÃ±a-Cordero, Rodriguez-Martinez",https://doi.org/10.1002/ppul.25024,https://doi.org/10.1002/ppul.25024,2021-08-03
16349.0,pubmed,pubmed,Bag-of-features-based radiomics for differentiation of ocular adnexal lymphoma and idiopathic orbital inflammation from contrast-enhanced MRI,Bag-of-features-based radiomics for differentiation of ocular adnexal lymphoma and idiopathic orbital inflammation from contrast-enhanced MRI,"To evaluate the effectiveness of bag-of-features (BOF)-based radiomics for differentiating ocular adnexal lymphoma (OAL) and idiopathic orbital inflammation (IOI) from contrast-enhanced MRI (CE-MRI). Fifty-six patients with pathologically confirmed IOI (28 patients) and OAL (28 patients) were randomly divided into training (nÃ¢â‚¬â€°=Ã¢â‚¬â€°42) and testing (nÃ¢â‚¬â€°=Ã¢â‚¬â€°14) groups. One hundred sixty texture features extracted from the CE-MR image were encoded into the BOF representation with fewer features. The support vector machine (SVM) with a linear kernel was used as the classifier. Data augmented was performed by cropping orbital lesions in different directions to alleviate the over-fitting problem. Student's t test and the Holm-Bonferroni method were employed to compare the performance of different analysis methods. The chi-square test was used to compare the analysis with MRI and human radiological diagnosis. In the independent testing group, the differentiation by the BOF features with augmentation achieved an area under the curve (AUC) of 0.803 (95% CI: 0.725-0.880), which was significantly higher than that of the BOF features without augmentation and that of the texture features (pÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°0.05). In addition, the same radiomic analysis with pre-contrast MRI obtained an AUC of 0.618 (95% CI: 0.560-0.677), which was significantly lower than that with CE-MRI. The diagnostic performance of the analysis with CE-MRI was significantly better than the radiology resident (pÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°0.05) but had no significant difference with the experienced radiologist, even though there was less consistency between the radiomic analysis and the human visual diagnosis. The BOF-based radiomics may be helpful for the differentiation between OAL and IOI. Ã¢â‚¬Â¢ It is challenging to differentiate OAL from IOI due to the similar clinical and image features. Ã¢â‚¬Â¢ Radiomics has great potential for the noninvasive diagnosis of orbital diseases. Ã¢â‚¬Â¢ The BOF representation from patch to image may help the differentiation of OAL and IOI.","To evaluate the effectiveness of bag-of-features (BOF)-based radiomics for differentiating ocular adnexal lymphoma (OAL) and idiopathic orbital inflammation (IOI) from contrast-enhanced MRI (CE-MRI). Fifty-six patients with pathologically confirmed IOI (28 patients) and OAL (28 patients) were randomly divided into training (nâ€‰=â€‰42) and testing (nâ€‰=â€‰14) groups. One hundred sixty texture features extracted from the CE-MR image were encoded into the BOF representation with fewer features. The support vector machine (SVM) with a linear kernel was used as the classifier. Data augmented was performed by cropping orbital lesions in different directions to alleviate the over-fitting problem. Student's t test and the Holm-Bonferroni method were employed to compare the performance of different analysis methods. The chi-square test was used to compare the analysis with MRI and human radiological diagnosis. In the independent testing group, the differentiation by the BOF features with augmentation achieved an area under the curve (AUC) of 0.803 (95% CI: 0.725-0.880), which was significantly higher than that of the BOF features without augmentation and that of the texture features (pâ€‰&lt;â€‰0.05). In addition, the same radiomic analysis with pre-contrast MRI obtained an AUC of 0.618 (95% CI: 0.560-0.677), which was significantly lower than that with CE-MRI. The diagnostic performance of the analysis with CE-MRI was significantly better than the radiology resident (pâ€‰&lt;â€‰0.05) but had no significant difference with the experienced radiologist, even though there was less consistency between the radiomic analysis and the human visual diagnosis. The BOF-based radiomics may be helpful for the differentiation between OAL and IOI. â€¢ It is challenging to differentiate OAL from IOI due to the similar clinical and image features. â€¢ Radiomics has great potential for the noninvasive diagnosis of orbital diseases. â€¢ The BOF representation from patch to image may help the differentiation of OAL and IOI.","Hou, Xie, Chen, Lv, Jiang, He, Yang, Zhao","Hou, Xie, Chen, Lv, Jiang, He, Yang, Zhao",https://doi.org/10.1007/s00330-020-07110-2,https://doi.org/10.1007/s00330-020-07110-2,2021-08-03
16350.0,pubmed,pubmed,A guide to writing systematic reviews of rare disease treatments to generate FAIR-compliant datasets: building a Treatabolome,A guide to writing systematic reviews of rare disease treatments to generate FAIR-compliant datasets: building a Treatabolome,"Rare diseases are individually rare but globally affect around 6% of the population, and in over 70% of cases are genetically determined. Their rarity translates into a delayed diagnosis, with 25% of patients waiting 5 to 30Ã¢â‚¬â€°years for one. It is essential to raise awareness of patients and clinicians of existing gene and variant-specific therapeutics at the time of diagnosis to avoid that treatment delays add up to the diagnostic odyssey of rare diseases' patients and their families. This paper aims to provide guidance and give detailed instructions on how to write homogeneous systematic reviews of rare diseases' treatments in a manner that allows the capture of the results in a computer-accessible form. The published results need to comply with the FAIR guiding principles for scientific data management and stewardship to facilitate the extraction of datasets that are easily transposable into machine-actionable information. The ultimate purpose is the creation of a database of rare disease treatments (&quot;Treatabolome&quot;) at gene and variant levels as part of the H2020 research project Solve-RD. Each systematic review follows a written protocol to address one or more rare diseases in which the authors are experts. The bibliographic search strategy requires detailed documentation to allow its replication. Data capture forms should be built to facilitate the filling of a data capture spreadsheet and to record the application of the inclusion and exclusion criteria to each search result. A PRISMA flowchart is required to provide an overview of the processes of search and selection of papers. A separate table condenses the data collected during the Systematic Review, appraised according to their level of evidence. This paper provides a template that includes the instructions for writing FAIR-compliant systematic reviews of rare diseases' treatments that enables the assembly of a Treatabolome database that complement existing diagnostic and management support tools with treatment awareness data.","Rare diseases are individually rare but globally affect around 6% of the population, and in over 70% of cases are genetically determined. Their rarity translates into a delayed diagnosis, with 25% of patients waiting 5 to 30â€‰years for one. It is essential to raise awareness of patients and clinicians of existing gene and variant-specific therapeutics at the time of diagnosis to avoid that treatment delays add up to the diagnostic odyssey of rare diseases' patients and their families. This paper aims to provide guidance and give detailed instructions on how to write homogeneous systematic reviews of rare diseases' treatments in a manner that allows the capture of the results in a computer-accessible form. The published results need to comply with the FAIR guiding principles for scientific data management and stewardship to facilitate the extraction of datasets that are easily transposable into machine-actionable information. The ultimate purpose is the creation of a database of rare disease treatments (""Treatabolome"") at gene and variant levels as part of the H2020 research project Solve-RD. Each systematic review follows a written protocol to address one or more rare diseases in which the authors are experts. The bibliographic search strategy requires detailed documentation to allow its replication. Data capture forms should be built to facilitate the filling of a data capture spreadsheet and to record the application of the inclusion and exclusion criteria to each search result. A PRISMA flowchart is required to provide an overview of the processes of search and selection of papers. A separate table condenses the data collected during the Systematic Review, appraised according to their level of evidence. This paper provides a template that includes the instructions for writing FAIR-compliant systematic reviews of rare diseases' treatments that enables the assembly of a Treatabolome database that complement existing diagnostic and management support tools with treatment awareness data.","Atalaia, Thompson, Corvo, Carmody, Piscia, Matalonga, Macaya, Lochmuller, Fontaine, Zurek, Hernandez-Ferrer, Rheinard, GÃƒÂ³mez-AndrÃƒÂ©s, Desaphy, Schon, Lohmann, Jennings, Synofzik, Riess, Yaou, Evangelista, Ratnaike, Bros-Facer, Gumus, Horvath, Chinnery, Laurie, Graessner, Robinson, Lochmuller, Beltran, Bonne","Atalaia, Thompson, Corvo, Carmody, Piscia, Matalonga, Macaya, Lochmuller, Fontaine, Zurek, Hernandez-Ferrer, Reinhard, GÃ³mez-AndrÃ©s, Desaphy, Schon, Lohmann, Jennings, Synofzik, Riess, Yaou, Evangelista, Ratnaike, Bros-Facer, Gumus, Horvath, Chinnery, Laurie, Graessner, Robinson, Lochmuller, Beltran, Bonne",https://doi.org/10.1186/s13023-020-01493-7,https://doi.org/10.1186/s13023-020-01493-7,2021-08-03
16351.0,pubmed,pubmed,Levofloxacin prophylaxis in hospitalized children with leukemia: A cost-utility analysis,Levofloxacin prophylaxis in hospitalized children with leukemia: A cost-utility analysis,"Infections are common and are a major cause of morbidity and mortality during treatment of childhood leukemia. We evaluated the cost effectiveness of levofloxacin antibiotic prophylaxis, compared to no prophylaxis, in children receiving chemotherapy for acute myeloid leukemia (AML) or relapsed acute lymphoblastic leukemia (ALL). A cost-utility analysis was conducted from the perspective of the single-payer health care system using a lifetime horizon. A comprehensive literature review identified available evidence for effectiveness, safety, costs of antibiotic prophylaxis in children with leukemia, and health utilities associated with the relevant health states. The effects of levofloxacin prophylaxis on health outcomes, quality-adjusted life-years (QALY), and direct health costs were derived from a combined decision tree and state-transition model. One-way deterministic and probabilistic sensitivity analyses were performed to test the sensitivity of results to parameter uncertainty. The literature review revealed one randomized controlled trial on levofloxacin prophylaxis in childhood AML and relapsed ALL, by Alexander etÃ‚Â al, that showed a significant reduction in rates of fever and neutropenia (71.2% vs 82.1%) and bacteremia (21.9% vs 43.4%) with levofloxacin compared to no prophylaxis. In our cost-utility analysis, levofloxacin prophylaxis was dominant over no prophylaxis, resulting in cost savings of $542.44 and increased survival of 0.13 QALY. In probabilistic sensitivity analysis, levofloxacin prophylaxis was dominant in 98.8% of iterations. The present analysis suggests that levofloxacin prophylaxis, compared to no prophylaxis, is cost saving in children receiving intensive chemotherapy for AML or relapsed ALL.","Infections are common and are a major cause of morbidity and mortality during treatment of childhood leukemia. We evaluated the cost effectiveness of levofloxacin antibiotic prophylaxis, compared to no prophylaxis, in children receiving chemotherapy for acute myeloid leukemia (AML) or relapsed acute lymphoblastic leukemia (ALL). A cost-utility analysis was conducted from the perspective of the single-payer health care system using a lifetime horizon. A comprehensive literature review identified available evidence for effectiveness, safety, costs of antibiotic prophylaxis in children with leukemia, and health utilities associated with the relevant health states. The effects of levofloxacin prophylaxis on health outcomes, quality-adjusted life-years (QALY), and direct health costs were derived from a combined decision tree and state-transition model. One-way deterministic and probabilistic sensitivity analyses were performed to test the sensitivity of results to parameter uncertainty. The literature review revealed one randomized controlled trial on levofloxacin prophylaxis in childhood AML and relapsed ALL, by Alexander etÂ al, that showed a significant reduction in rates of fever and neutropenia (71.2% vs 82.1%) and bacteremia (21.9% vs 43.4%) with levofloxacin compared to no prophylaxis. In our cost-utility analysis, levofloxacin prophylaxis was dominant over no prophylaxis, resulting in cost savings of $542.44 and increased survival of 0.13 QALY. In probabilistic sensitivity analysis, levofloxacin prophylaxis was dominant in 98.8% of iterations. The present analysis suggests that levofloxacin prophylaxis, compared to no prophylaxis, is cost saving in children receiving intensive chemotherapy for AML or relapsed ALL.","Maser, Pelland-Marcotte, Alexander, Sung, Gupta","Maser, Pelland-Marcotte, Alexander, Sung, Gupta",https://doi.org/10.1002/pbc.28643,https://doi.org/10.1002/pbc.28643,2021-08-03
16357.0,pubmed,pubmed,Association between diabetic neuropathy and osteoporosis in patients: a systematic review and meta-analysis,Association between diabetic neuropathy and osteoporosis in patients: a systematic review and meta-analysis,"Many studies have explored the association between neuropathy and osteoporosis in patients with diabetes mellitus. However, the results still remain inconsistent and controversial. We aimed to estimate the association between diabetic neuropathy and osteoporosis. Databases, including PubMed, Embase, Web of Science, the Cochrane library, Chinese Biomedical Literature Database (CBM), and Wanfang, were screened from inception to 30 March 2020. Studies were selected and data were extracted by two independent reviewers. Study characteristics and quality sections were reviewed independently. Pooled ORs and 95% CIs were calculated using random effects model when evidence of heterogeneity was present; otherwise, fixed effects model was used. Meta-regression and subgroup analyses were performed to explore the source of heterogeneity. Sensitivity analysis and publication bias were also tested. A total of 11 studies with 27,585 participants were included in this analysis which indicated that there was an increased odd between diabetic neuropathy and osteoporosis (overall OR 2.20, 95% CI 1.71-2.83). In the subgroup analyses and meta-regression, diabetic neuropathy has no significant difference in osteoporosis or fracture (pÃ¢â‚¬â€°=Ã¢â‚¬â€°0.532). And osteoporosis also has no significant difference in type 1 or type 2 diabetic neuropathy (pÃ¢â‚¬â€°=Ã¢â‚¬â€°0.668). This meta-analysis suggests that patients with diabetic neuropathy have a significantly increased chance of developing osteoporosis, even fragility fracture. The clinicians should pay more attention to the patients with diabetic neuropathy. Further studies were still needed to explore the confounding factors among studies and to elucidate the underlying biological mechanisms.","Many studies have explored the association between neuropathy and osteoporosis in patients with diabetes mellitus. However, the results still remain inconsistent and controversial. We aimed to estimate the association between diabetic neuropathy and osteoporosis. Databases, including PubMed, Embase, Web of Science, the Cochrane library, Chinese Biomedical Literature Database (CBM), and Wanfang, were screened from inception to 30 March 2020. Studies were selected and data were extracted by two independent reviewers. Study characteristics and quality sections were reviewed independently. Pooled ORs and 95% CIs were calculated using random effects model when evidence of heterogeneity was present; otherwise, fixed effects model was used. Meta-regression and subgroup analyses were performed to explore the source of heterogeneity. Sensitivity analysis and publication bias were also tested. A total of 11 studies with 27,585 participants were included in this analysis which indicated that there was an increased odd between diabetic neuropathy and osteoporosis (overall OR 2.20, 95% CI 1.71-2.83). In the subgroup analyses and meta-regression, diabetic neuropathy has no significant difference in osteoporosis or fracture (pâ€‰=â€‰0.532). And osteoporosis also has no significant difference in type 1 or type 2 diabetic neuropathy (pâ€‰=â€‰0.668). This meta-analysis suggests that patients with diabetic neuropathy have a significantly increased chance of developing osteoporosis, even fragility fracture. The clinicians should pay more attention to the patients with diabetic neuropathy. Further studies were still needed to explore the confounding factors among studies and to elucidate the underlying biological mechanisms.","Liu, Lv, Niu, Tan, Ma","Liu, Lv, Niu, Tan, Ma",https://doi.org/10.1007/s11657-020-00804-6,https://doi.org/10.1007/s11657-020-00804-6,2021-08-03
16360.0,pubmed,pubmed,Associated Targets of the Antioxidant Cardioprotection of <i>Ganoderma lucidum</i> in Diabetic Cardiomyopathy by Using Open Targets Platform: A Systematic Review,Associated Targets of the Antioxidant Cardioprotection of <i>Ganoderma lucidum</i> in Diabetic Cardiomyopathy by Using Open Targets Platform: A Systematic Review,"Even with substantial advances in cardiovascular therapy, the morbidity and mortality rates of diabetic cardiomyopathy (DCM) continually increase. Hence, a feasible therapeutic approach is urgently needed. <i>Objectives</i>. This work is aimed at systemically reviewing literature and addressing cell targets in DCM through the possible cardioprotection of <i>G</i>. <i>lucidum</i> through its antioxidant effects by using the Open Targets Platform (OTP) website. <i>Methods</i>. The OTP website version of 19.11 was accessed in December 2019 to identify the studies in DCM involving <i>G</i>. <i>lucidum</i>. <i>Results</i>. Among the 157 cell targets associated with DCM, the mammalian target of rapamycin (mTOR) was shared by all evidence, drug, and text mining data with 0.08 score association. mTOR also had the highest score association 0.1 with autophagy in DCM. Among the 1731 studies of indexed PubMed articles on <i>G. lucidum</i> published between 1985 and 2019, 33 addressed the antioxidant effects of <i>G. lucidum</i> and its molecular signal pathways involving oxidative stress and therefore were included in the current work. <i>Conclusion</i>. mTOR is one of the targets by DCM and can be inhibited by the antioxidative properties of <i>G. lucidum</i> directly via scavenging radicals and indirectly via modulating mTOR signal pathways such as Wnt signaling pathway, Erk1/2 signaling, and NF-<i>ÃŽÂº</i>B pathways.","Even with substantial advances in cardiovascular therapy, the morbidity and mortality rates of diabetic cardiomyopathy (DCM) continually increase. Hence, a feasible therapeutic approach is urgently needed. <i>Objectives</i>. This work is aimed at systemically reviewing literature and addressing cell targets in DCM through the possible cardioprotection of <i>G</i>. <i>lucidum</i> through its antioxidant effects by using the Open Targets Platform (OTP) website. <i>Methods</i>. The OTP website version of 19.11 was accessed in December 2019 to identify the studies in DCM involving <i>G</i>. <i>lucidum</i>. <i>Results</i>. Among the 157 cell targets associated with DCM, the mammalian target of rapamycin (mTOR) was shared by all evidence, drug, and text mining data with 0.08 score association. mTOR also had the highest score association 0.1 with autophagy in DCM. Among the 1731 studies of indexed PubMed articles on <i>G. lucidum</i> published between 1985 and 2019, 33 addressed the antioxidant effects of <i>G. lucidum</i> and its molecular signal pathways involving oxidative stress and therefore were included in the current work. <i>Conclusion</i>. mTOR is one of the targets by DCM and can be inhibited by the antioxidative properties of <i>G. lucidum</i> directly via scavenging radicals and indirectly via modulating mTOR signal pathways such as Wnt signaling pathway, Erk1/2 signaling, and NF-<i>Îº</i>B pathways.","Shaher, Qiu, Wang, Hu, Wang, Zhang, Wei, Al-Ward, Abdulghani, Alenezi, Baldi, Zhou","Shaher, Qiu, Wang, Hu, Wang, Zhang, Wei, Al-Ward, Abdulghani, Alenezi, Baldi, Zhou",https://doi.org/10.1155/2020/7136075,https://doi.org/10.1155/2020/7136075,2021-08-03
16365.0,pubmed,pubmed,CORR InsightsÃ‚Â®: Does Artificial Intelligence Outperform Natural Intelligence in Interpretation of Musculoskeletal Radiological Studies? A Systematic Review,CORR InsightsÂ®: Does Artificial Intelligence Outperform Natural Intelligence in Interpretation of Musculoskeletal Radiological Studies? A Systematic Review,,,Porcher,Porcher,https://doi.org/10.1097/CORR.0000000000001415,https://doi.org/10.1097/CORR.0000000000001415,2021-08-03
16368.0,pubmed,pubmed,Predicting Absenteeism and Temporary Disability Using Machine Learning: a Systematic Review and Analysis,Predicting Absenteeism and Temporary Disability Using Machine Learning: a Systematic Review and Analysis,"The main objective of this paper is to present a systematic analysis and review of the state of the art regarding the prediction of absenteeism and temporary incapacity using machine learning techniques. Moreover, the main contribution of this research is to reveal the most successful prediction models available in the literature. A systematic review of research papers published from 2010 to the present, related to the prediction of temporary disability and absenteeism in available in different research databases, is presented in this paper. The review focuses primarily on scientific databases such as Google Scholar, Science Direct, IEEE Xplore, Web of Science, and ResearchGate. A total of 58 articles were obtained from which, after removing duplicates and applying the search criteria, 18 have been included in the review. In total, 44% of the articles were published in 2019, representing a significant growth in scientific work regarding these indicators. This study also evidenced the interest of several countries. In addition, 56% of the articles were found to base their study on regression methods, 33% in classification, and 11% in grouping. After this systematic review, the efficiency and usefulness of artificial neural networks in predicting absenteeism and temporary incapacity are demonstrated. The studies regarding absenteeism and temporary disability at work are mainly conducted in Brazil and India, which are responsible for 44% of the analyzed papers followed by Saudi Arabia, and Australia which represented 22%. ANNs are the most used method in both classification and regression models representing 83% and 80% of the analyzed works, respectively. Only 10% of the literature use SVM, which is the less used method in regression models. Moreover, NaÃƒÂ¯ve Bayes is the less used method in classification models representing 17%.","The main objective of this paper is to present a systematic analysis and review of the state of the art regarding the prediction of absenteeism and temporary incapacity using machine learning techniques. Moreover, the main contribution of this research is to reveal the most successful prediction models available in the literature. A systematic review of research papers published from 2010 to the present, related to the prediction of temporary disability and absenteeism in available in different research databases, is presented in this paper. The review focuses primarily on scientific databases such as Google Scholar, Science Direct, IEEE Xplore, Web of Science, and ResearchGate. A total of 58 articles were obtained from which, after removing duplicates and applying the search criteria, 18 have been included in the review. In total, 44% of the articles were published in 2019, representing a significant growth in scientific work regarding these indicators. This study also evidenced the interest of several countries. In addition, 56% of the articles were found to base their study on regression methods, 33% in classification, and 11% in grouping. After this systematic review, the efficiency and usefulness of artificial neural networks in predicting absenteeism and temporary incapacity are demonstrated. The studies regarding absenteeism and temporary disability at work are mainly conducted in Brazil and India, which are responsible for 44% of the analyzed papers followed by Saudi Arabia, and Australia which represented 22%. ANNs are the most used method in both classification and regression models representing 83% and 80% of the analyzed works, respectively. Only 10% of the literature use SVM, which is the less used method in regression models. Moreover, NaÃ¯ve Bayes is the less used method in classification models representing 17%.","Montano, Marques, Alonso, LÃƒÂ³pez-Coronado, de la Torre DÃƒÂ­ez","Montano, Marques, Alonso, LÃ³pez-Coronado, de la Torre DÃ­ez",https://doi.org/10.1007/s10916-020-01626-2,https://doi.org/10.1007/s10916-020-01626-2,2021-08-03
16369.0,pubmed,pubmed,A review of auditing techniques for the Unified Medical Language System,A review of auditing techniques for the Unified Medical Language System,"The study sought to describe the literature related to the development of methods for auditing the Unified Medical Language System (UMLS), with particular attention to identifying errors and inconsistencies of attributes of the concepts in the UMLS Metathesaurus. We applied the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) approach by searchingÃ‚Â the MEDLINE database and Google Scholar for studies referencing the UMLS and any of several terms related to auditing, error detection, and quality assurance. A qualitative analysis and summarization of articles that met inclusion criteria were performed. Eighty-three studies were reviewed in detail. We first categorized techniques based on various aspects including concepts, concept names, and synonymy (nÃ¢â‚¬â€°=Ã¢â‚¬â€°37), semantic type assignments (nÃ¢â‚¬â€°=Ã¢â‚¬â€°36), hierarchical relationships (nÃ¢â‚¬â€°=Ã¢â‚¬â€°24), lateral relationships (nÃ¢â‚¬â€°=Ã¢â‚¬â€°12), ontology enrichment (nÃ¢â‚¬â€°=Ã¢â‚¬â€°8), and ontology alignment (nÃ¢â‚¬â€°=Ã¢â‚¬â€°18). We also categorized the methods according to their level of automation (ie, automated systematic, automated heuristic, or manual) and the type of knowledge used (ie, intrinsic or extrinsic knowledge). This study is a comprehensive review of the published methods for auditing the various conceptual aspects of the UMLS. Categorizing the auditing techniques according to the various aspects will enable the curators of the UMLS as well as researchers comprehensive easy access to this wealth of knowledge (eg, for auditing lateral relationships in the UMLS). We also reviewed ontology enrichment and alignment techniques due to their critical use of and impact on the UMLS.","The study sought to describe the literature related to the development of methods for auditing the Unified Medical Language System (UMLS), with particular attention to identifying errors and inconsistencies of attributes of the concepts in the UMLS Metathesaurus. We applied the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) approach by searchingÂ the MEDLINE database and Google Scholar for studies referencing the UMLS and any of several terms related to auditing, error detection, and quality assurance. A qualitative analysis and summarization of articles that met inclusion criteria were performed. Eighty-three studies were reviewed in detail. We first categorized techniques based on various aspects including concepts, concept names, and synonymy (nâ€‰=â€‰37), semantic type assignments (nâ€‰=â€‰36), hierarchical relationships (nâ€‰=â€‰24), lateral relationships (nâ€‰=â€‰12), ontology enrichment (nâ€‰=â€‰8), and ontology alignment (nâ€‰=â€‰18). We also categorized the methods according to their level of automation (ie, automated systematic, automated heuristic, or manual) and the type of knowledge used (ie, intrinsic or extrinsic knowledge). This study is a comprehensive review of the published methods for auditing the various conceptual aspects of the UMLS. Categorizing the auditing techniques according to the various aspects will enable the curators of the UMLS as well as researchers comprehensive easy access to this wealth of knowledge (eg, for auditing lateral relationships in the UMLS). We also reviewed ontology enrichment and alignment techniques due to their critical use of and impact on the UMLS.","Zheng, He, Wei, Keloth, Fan, Lindemann, Zhu, Cimino, Perl","Zheng, He, Wei, Keloth, Fan, Lindemann, Zhu, Cimino, Perl",https://doi.org/10.1093/jamia/ocaa108,https://doi.org/10.1093/jamia/ocaa108,2021-08-03
16372.0,pubmed,pubmed,Conversational Agents in Health Care: Scoping Review and Conceptual Analysis,Conversational Agents in Health Care: Scoping Review and Conceptual Analysis,"Conversational agents, also known as chatbots, are computer programs designed to simulate human text or verbal conversations. They are increasingly used in a range of fields, including health care. By enabling better accessibility, personalization, and efficiency, conversational agents have the potential to improve patient care. This study aimed to review the current applications, gaps, and challenges in the literature on conversational agents in health care and provide recommendations for their future research, design, and application. We performed a scoping review. A broad literature search was performed in MEDLINE (Medical Literature Analysis and Retrieval System Online; Ovid), EMBASE (Excerpta Medica database; Ovid), PubMed, Scopus, and Cochrane Central with the search terms &quot;conversational agents,&quot; &quot;conversational AI,&quot; &quot;chatbots,&quot; and associated synonyms. We also searched the gray literature using sources such as the OCLC (Online Computer Library Center) WorldCat database and ResearchGate in April 2019. Reference lists of relevant articles were checked for further articles. Screening and data extraction were performed in parallel by 2 reviewers. The included evidence was analyzed narratively by employing the principles of thematic analysis. The literature search yielded 47 study reports (45 articles and 2 ongoing clinical trials) that matched the inclusion criteria. The identified conversational agents were largely delivered via smartphone apps (n=23) and used free text only as the main input (n=19) and output (n=30) modality. Case studies describing chatbot development (n=18) were the most prevalent, and only 11 randomized controlled trials were identified. The 3 most commonly reported conversational agent applications in the literature were treatment and monitoring, health care service support, and patient education. The literature on conversational agents in health care is largely descriptive and aimed at treatment and monitoring and health service support. It mostly reports on text-based, artificial intelligence-driven, and smartphone app-delivered conversational agents. There is an urgent need for a robust evaluation of diverse health care conversational agents' formats, focusing on their acceptability, safety, and effectiveness.","Conversational agents, also known as chatbots, are computer programs designed to simulate human text or verbal conversations. They are increasingly used in a range of fields, including health care. By enabling better accessibility, personalization, and efficiency, conversational agents have the potential to improve patient care. This study aimed to review the current applications, gaps, and challenges in the literature on conversational agents in health care and provide recommendations for their future research, design, and application. We performed a scoping review. A broad literature search was performed in MEDLINE (Medical Literature Analysis and Retrieval System Online; Ovid), EMBASE (Excerpta Medica database; Ovid), PubMed, Scopus, and Cochrane Central with the search terms ""conversational agents,"" ""conversational AI,"" ""chatbots,"" and associated synonyms. We also searched the gray literature using sources such as the OCLC (Online Computer Library Center) WorldCat database and ResearchGate in April 2019. Reference lists of relevant articles were checked for further articles. Screening and data extraction were performed in parallel by 2 reviewers. The included evidence was analyzed narratively by employing the principles of thematic analysis. The literature search yielded 47 study reports (45 articles and 2 ongoing clinical trials) that matched the inclusion criteria. The identified conversational agents were largely delivered via smartphone apps (n=23) and used free text only as the main input (n=19) and output (n=30) modality. Case studies describing chatbot development (n=18) were the most prevalent, and only 11 randomized controlled trials were identified. The 3 most commonly reported conversational agent applications in the literature were treatment and monitoring, health care service support, and patient education. The literature on conversational agents in health care is largely descriptive and aimed at treatment and monitoring and health service support. It mostly reports on text-based, artificial intelligence-driven, and smartphone app-delivered conversational agents. There is an urgent need for a robust evaluation of diverse health care conversational agents' formats, focusing on their acceptability, safety, and effectiveness.","Tudor Car, Dhinagaran, Kyaw, Kowatsch, Joty, Theng, Atun","Tudor Car, Dhinagaran, Kyaw, Kowatsch, Joty, Theng, Atun",https://doi.org/10.2196/17158,https://doi.org/10.2196/17158,2021-08-03
16376.0,pubmed,pubmed,Laparoscopic versus open subtotal gastrectomy for gastric adenocarcinoma: cost-effectiveness analysis,Laparoscopic versus open subtotal gastrectomy for gastric adenocarcinoma: cost-effectiveness analysis,"Laparoscopic subtotal gastrectomy (LSG) for cancer is associated with good perioperative outcomes and superior quality of life compared with the open approach, albeit at higher cost. An economic evaluation was conducted to compare the two approaches. A cost-effectiveness analysis between LSG and open subtotal gastrectomy (OSG) for gastric cancer was performed using a decision-tree cohort model with a healthcare system perspective and a 12-month time horizon. Model inputs were informed by a meta-analysis of relevant literature, with costs represented in 2016 Canadian dollars (CAD) and outcomes measured in quality-adjusted life-years (QALYs). A secondary analysis was conducted using inputs extracted solely from European and North American studies. Deterministic (DSA) and probabilistic (PSA) sensitivity analyses were performed. In the base-case model, costs of LSG were $935 (Ã¢â€šÂ¬565) greater than those of OSG, with an incremental gain of 0Ã‚Â·050 QALYs, resulting in an incremental cost-effectiveness ratio of $18Ã¢â‚¬â€°846 (Ã¢â€šÂ¬11Ã¢â‚¬â€°398) per additional QALY gained from LSG. In the DSA, results were most sensitive to changes in postoperative utility, operating theatre and equipment costs, as well as duration of surgery and hospital stay. PSA showed that the likelihood of LSG being cost-effective at willingness-to-pay thresholds of $50Ã¢â‚¬â€°000 (Ã¢â€šÂ¬30Ã¢â‚¬â€°240) per QALY and $100Ã¢â‚¬â€°000 (Ã¢â€šÂ¬60Ã¢â‚¬â€°480) per QALY was 64 and 68 per cent respectively. Secondary analysis using European and North American clinical inputs resulted in LSG being dominant (cheaper and more effective) over OSG, largely due to reduced length of stay after LSG. In this decision analysis model, LSG was cost-effective compared with OSG for gastric cancer. Pese a su mayor coste, la gastrectomÃƒÂ­a subtotal laparoscÃƒÂ³pica se asocia con buenos resultados perioperatorios y una mejor calidad de vida en comparaciÃƒÂ³n con la cirugÃƒÂ­a abierta en el tratamiento del cÃƒÂ¡ncer. Se realizÃƒÂ³ una evaluaciÃƒÂ³n econÃƒÂ³mica comparando los dos abordajes. MÃƒâ€°TODOS: Se efectuÃƒÂ³ un anÃƒÂ¡lisis de coste-efectividad de la gastrectomÃƒÂ­a subtotal laparoscÃƒÂ³pica (laparoscopic subtotal gastrectomy, LSG) o de la gastrectomÃƒÂ­a subtotal abierta (open subtotal gastrectomy, OSG) en el cÃƒÂ¡ncer gÃƒÂ¡strico utilizando un modelo de cohortes con ÃƒÂ¡rbol de decisiÃƒÂ³n desde la perspectiva del sistema de salud y con un horizonte temporal de 12 meses. Los gastos del modelo fueron evaluados tras un metaanÃƒÂ¡lisis de literatura relevante y expresados en dÃƒÂ³lares canadienses (Canadian dollars, CAD) del 2016. Los resultados se midieron en aÃƒÂ±os de vida ajustados por su calidad (quality-adjusted life years, QALYs). Se realizÃƒÂ³ un anÃƒÂ¡lisis secundario utilizando los datos extraÃƒÂ­dos ÃƒÂºnicamente de estudios europeos y norteamericanos. AdemÃƒÂ¡s, se realizaron anÃƒÂ¡lisis de sensibilidad determinÃƒÂ­stico y probabilÃƒÂ­stico (deterministic and probabilistic sensitivity analyses, DSA y PSA). En el modelo del caso base, los costes de la LSG fueron de 934,78$ (565Ã¢â€šÂ¬) mÃƒÂ¡s que en la OSG, con una ganancia incremental de 0,050 QALYs, que supuso una relaciÃƒÂ³n coste-efectividad incremental (incremental cost-effectiveness ratio, ICER) de 18.846,12$ (11.398Ã¢â€šÂ¬) por QALY adicional en la LSG. En el DSA, los resultados fueron mÃƒÂ¡s sensibles a cambios en el postoperatorio, quirÃƒÂ³fano y coste de los equipos, asÃƒÂ­ como en la duraciÃƒÂ³n de la intervenciÃƒÂ³n y la hospitalizaciÃƒÂ³n. El PSA demostrÃƒÂ³ que la probabilidad de que la LSG fuera rentable en tÃƒÂ©rminos de disposiciÃƒÂ³n de pago (willingness-to-pay, WTP) para dos umbrales, de 50.000$ (30.240Ã¢â€šÂ¬) y 100.000$ (60.480Ã¢â€šÂ¬) por QALY fue del 64% y del 68%, respectivamente. En el anÃƒÂ¡lisis secundario utilizando los datos europeos y norteamericanos se demostrÃƒÂ³ que la LSG era claramente dominante (mÃƒÂ¡s barata y mÃƒÂ¡s efectiva) que la OSG, en gran parte debido a la reducciÃƒÂ³n de la estancia hospitalaria de la LSG. CONCLUSIÃƒâ€œN: En este modelo de anÃƒÂ¡lisis de decisiÃƒÂ³n, la LSG fue coste-efectiva en comparaciÃƒÂ³n con la OSG para el cÃƒÂ¡ncer gÃƒÂ¡strico.","Laparoscopic subtotal gastrectomy (LSG) for cancer is associated with good perioperative outcomes and superior quality of life compared with the open approach, albeit at higher cost. An economic evaluation was conducted to compare the two approaches. A cost-effectiveness analysis between LSG and open subtotal gastrectomy (OSG) for gastric cancer was performed using a decision-tree cohort model with a healthcare system perspective and a 12-month time horizon. Model inputs were informed by a meta-analysis of relevant literature, with costs represented in 2016 Canadian dollars (CAD) and outcomes measured in quality-adjusted life-years (QALYs). A secondary analysis was conducted using inputs extracted solely from European and North American studies. Deterministic (DSA) and probabilistic (PSA) sensitivity analyses were performed. In the base-case model, costs of LSG were $935 (â‚¬565) greater than those of OSG, with an incremental gain of 0Â·050 QALYs, resulting in an incremental cost-effectiveness ratio of $18â€‰846 (â‚¬11â€‰398) per additional QALY gained from LSG. In the DSA, results were most sensitive to changes in postoperative utility, operating theatre and equipment costs, as well as duration of surgery and hospital stay. PSA showed that the likelihood of LSG being cost-effective at willingness-to-pay thresholds of $50â€‰000 (â‚¬30â€‰240) per QALY and $100â€‰000 (â‚¬60â€‰480) per QALY was 64 and 68 per cent respectively. Secondary analysis using European and North American clinical inputs resulted in LSG being dominant (cheaper and more effective) over OSG, largely due to reduced length of stay after LSG. In this decision analysis model, LSG was cost-effective compared with OSG for gastric cancer. Pese a su mayor coste, la gastrectomÃ­a subtotal laparoscÃ³pica se asocia con buenos resultados perioperatorios y una mejor calidad de vida en comparaciÃ³n con la cirugÃ­a abierta en el tratamiento del cÃ¡ncer. Se realizÃ³ una evaluaciÃ³n econÃ³mica comparando los dos abordajes. MÃ‰TODOS: Se efectuÃ³ un anÃ¡lisis de coste-efectividad de la gastrectomÃ­a subtotal laparoscÃ³pica (laparoscopic subtotal gastrectomy, LSG) o de la gastrectomÃ­a subtotal abierta (open subtotal gastrectomy, OSG) en el cÃ¡ncer gÃ¡strico utilizando un modelo de cohortes con Ã¡rbol de decisiÃ³n desde la perspectiva del sistema de salud y con un horizonte temporal de 12 meses. Los gastos del modelo fueron evaluados tras un metaanÃ¡lisis de literatura relevante y expresados en dÃ³lares canadienses (Canadian dollars, CAD) del 2016. Los resultados se midieron en aÃ±os de vida ajustados por su calidad (quality-adjusted life years, QALYs). Se realizÃ³ un anÃ¡lisis secundario utilizando los datos extraÃ­dos Ãºnicamente de estudios europeos y norteamericanos. AdemÃ¡s, se realizaron anÃ¡lisis de sensibilidad determinÃ­stico y probabilÃ­stico (deterministic and probabilistic sensitivity analyses, DSA y PSA). En el modelo del caso base, los costes de la LSG fueron de 934,78$ (565â‚¬) mÃ¡s que en la OSG, con una ganancia incremental de 0,050 QALYs, que supuso una relaciÃ³n coste-efectividad incremental (incremental cost-effectiveness ratio, ICER) de 18.846,12$ (11.398â‚¬) por QALY adicional en la LSG. En el DSA, los resultados fueron mÃ¡s sensibles a cambios en el postoperatorio, quirÃ³fano y coste de los equipos, asÃ­ como en la duraciÃ³n de la intervenciÃ³n y la hospitalizaciÃ³n. El PSA demostrÃ³ que la probabilidad de que la LSG fuera rentable en tÃ©rminos de disposiciÃ³n de pago (willingness-to-pay, WTP) para dos umbrales, de 50.000$ (30.240â‚¬) y 100.000$ (60.480â‚¬) por QALY fue del 64% y del 68%, respectivamente. En el anÃ¡lisis secundario utilizando los datos europeos y norteamericanos se demostrÃ³ que la LSG era claramente dominante (mÃ¡s barata y mÃ¡s efectiva) que la OSG, en gran parte debido a la reducciÃ³n de la estancia hospitalaria de la LSG. CONCLUSIÃ“N: En este modelo de anÃ¡lisis de decisiÃ³n, la LSG fue coste-efectiva en comparaciÃ³n con la OSG para el cÃ¡ncer gÃ¡strico.","Gosselin-Tardif, Abou-Khalil, Mata, Guigui, Cools-Lartigue, Ferri, Lee, Mueller","Gosselin-Tardif, Abou-Khalil, Mata, Guigui, Cools-Lartigue, Ferri, Lee, Mueller",https://doi.org/10.1002/bjs5.50327,https://doi.org/10.1002/bjs5.50327,2021-08-03
16377.0,pubmed,pubmed,Machine learning for predicting long-term kidney allograft survival: a scoping review,Machine learning for predicting long-term kidney allograft survival: a scoping review,"Supervised machine learning (ML) is a class of algorithms that &quot;learn&quot; from existing input-output pairs, which is gaining popularity in pattern recognition for classification and prediction problems. In this scoping review, we examined the use of supervised ML algorithms for the prediction of long-term allograft survival in kidney transplant recipients. Data sources included PubMed, the Cumulative Index to Nursing and Allied Health Literature, and the Institute for Electrical and Electronics Engineers (IEEE) Xplore libraries from inception to November 2019. We screened titles and abstracts and potentially eligible full-text reports to select studies and subsequently abstracted the data. Eleven studies were identified. Decision trees were the most commonly used method (nÃ¢â‚¬â€°=Ã¢â‚¬â€°8), followed by artificial neural networks (ANN) (nÃ¢â‚¬â€°=Ã¢â‚¬â€°4) and Bayesian belief networks (nÃ¢â‚¬â€°=Ã¢â‚¬â€°2). The area under receiver operating curve (AUC) was the most common measure of discrimination (nÃ¢â‚¬â€°=Ã¢â‚¬â€°7), followed by sensitivity (nÃ¢â‚¬â€°=Ã¢â‚¬â€°5) and specificity (nÃ¢â‚¬â€°=Ã¢â‚¬â€°4). Model calibration examining the reliability in risk prediction was performed using either the Pearson r or the Hosmer-Lemeshow test in four studies. One study showed that logistic regression had comparable performance to ANN, while another study demonstrated that ANN performed better in terms of sensitivity, specificity, and accuracy, as compared with a Cox proportional hazards model. We synthesized the evidence related to the comparison of ML techniques with traditional statistical approaches for prediction of long-term allograft survival in patients with a kidney transplant. The methodological and reporting quality of included studies was poor. Our study also demonstrated mixed results in terms of the predictive potential of the models.","Supervised machine learning (ML) is a class of algorithms that ""learn"" from existing input-output pairs, which is gaining popularity in pattern recognition for classification and prediction problems. In this scoping review, we examined the use of supervised ML algorithms for the prediction of long-term allograft survival in kidney transplant recipients. Data sources included PubMed, the Cumulative Index to Nursing and Allied Health Literature, and the Institute for Electrical and Electronics Engineers (IEEE) Xplore libraries from inception to November 2019. We screened titles and abstracts and potentially eligible full-text reports to select studies and subsequently abstracted the data. Eleven studies were identified. Decision trees were the most commonly used method (nâ€‰=â€‰8), followed by artificial neural networks (ANN) (nâ€‰=â€‰4) and Bayesian belief networks (nâ€‰=â€‰2). The area under receiver operating curve (AUC) was the most common measure of discrimination (nâ€‰=â€‰7), followed by sensitivity (nâ€‰=â€‰5) and specificity (nâ€‰=â€‰4). Model calibration examining the reliability in risk prediction was performed using either the Pearson r or the Hosmer-Lemeshow test in four studies. One study showed that logistic regression had comparable performance to ANN, while another study demonstrated that ANN performed better in terms of sensitivity, specificity, and accuracy, as compared with a Cox proportional hazards model. We synthesized the evidence related to the comparison of ML techniques with traditional statistical approaches for prediction of long-term allograft survival in patients with a kidney transplant. The methodological and reporting quality of included studies was poor. Our study also demonstrated mixed results in terms of the predictive potential of the models.","Sekercioglu, Fu, Kim, Mitsakakis","Sekercioglu, Fu, Kim, Mitsakakis",https://doi.org/10.1007/s11845-020-02332-1,https://doi.org/10.1007/s11845-020-02332-1,2021-08-03
16385.0,pubmed,pubmed,Towards User-friendly Wearable Platforms for Monitoring Unconstrained Indoor and Outdoor Activities,Towards User-Friendly Wearable Platforms for Monitoring Unconstrained Indoor and Outdoor Activities,"Developing wearable platforms for unconstrained monitoring of limb movements has been an active recent topic of research due to potential applications such as clinical and athletic performance evaluation. However, practicality of these platforms might be affected by the dynamic and complexity of movements as well as characteristics of the surrounding environment. This paper addresses such issues by proposing a novel method for obtaining kinematic information of joints using a custom-designed wearable platform. The proposed method uses data from two gyroscopes and an array of textile stretch sensors to accurately track three-dimensional movements, including extension, flexion, and rotation, of a joint. More specifically, gyroscopes provide angular velocity data of two sides of a joint, while their relative orientation is estimated by a machine learning algorithm. An unscented Kalman filter (UKF) algorithm is applied to directly fuse angular velocity/relative orientation data and estimate the kinematic orientation of the joint. Experimental evaluations were carried out using data from 10 volunteers performing a series of predefined as well as unconstrained random three-dimensional trunk movements. Results show that the proposed sensor setup and the UKF-based data fusion algorithm can accurately estimate the orientation of the trunk relative to pelvis with an average error of less than 1.72 degrees in predefined movements and a comparable accuracy of 3.00 degrees in random movements. Moreover, the proposed platform is easy to setup, does not restrict body motion, and is not affected by environmental disturbances. This study is a further step towards developing user-friendly wearable sensor systems than can be readily used in indoor and outdoor settings without requiring bulky equipment or a tedious calibration phase.","Developing wearable platforms for unconstrained monitoring of limb movements has been an active recent topic of research due to potential applications such as clinical and athletic performance evaluation. However, practicality of these platforms might be affected by the dynamic and complexity of movements as well as characteristics of the surrounding environment. This paper addresses such issues by proposing a novel method for obtaining kinematic information of joints using a custom-designed wearable platform. The proposed method uses data from two gyroscopes and an array of textile stretch sensors to accurately track three-dimensional movements, including extension, flexion, and rotation, of a joint. More specifically, gyroscopes provide angular velocity data of two sides of a joint, while their relative orientation is estimated by a machine learning algorithm. An Unscented Kalman Filter (UKF) algorithm is applied to directly fuse angular velocity/relative orientation data and estimate the kinematic orientation of the joint. Experimental evaluations were carried out using data from 10 volunteers performing a series of predefined as well as unconstrained random three-dimensional trunk movements. Results show that the proposed sensor setup and the UKF-based data fusion algorithm can accurately estimate the orientation of the trunk relative to pelvis with an average error of less than 1.72 degrees in predefined movements and a comparable accuracy of 3.00 degrees in random movements. Moreover, the proposed platform is easy to setup, does not restrict body motion, and is not affected by environmental disturbances. This study is a further step towards developing user-friendly wearable sensor systems than can be readily used in indoor and outdoor settings without requiring bulky equipment or a tedious calibration phase.","Rezaei, Khoshnam, Menon","Rezaei, Khoshnam, Menon",https://doi.org/10.1109/JBHI.2020.3004319,https://doi.org/10.1109/JBHI.2020.3004319,2021-08-03
16386.0,pubmed,pubmed,Single Volume Image Generator and Deep Learning-based ASD Classification,Single Volume Image Generator and Deep Learning-Based ASD Classification,"Autism spectrum disorder (ASD) is an intricate neuropsychiatric brain disorder characterized by social deficits and repetitive behaviors. Deep learning approaches have been applied in clinical or behavioral identification of ASD; most erstwhile models are inadequate in their capacity to exploit the data richness. On the other hand, classification techniques often solely rely on region-based summary and/or functional connectivity analysis of functional magnetic resonance imaging (fMRI). Besides, biomedical data modeling to analyze big data related to ASD is still perplexing due to its complexity and heterogeneity. Single volume image consideration has not been previously investigated in classification purposes. By deeming these challenges, in this work, firstly, we design an image generator to generate single volume brain images from the whole-brain image by considering the voxel time point of each subject separately. Then, to classify ASD and typical control participants, we evaluate four deep learning approaches with their corresponding ensemble classifiers comprising one amended Convolutional Neural Network (CNN). Finally, to check out the data variability, we apply the proposed CNN classifier with leave-one-site-out 5-fold cross-validation across the sites and validate our findings by comparing with literature reports. We showcase our approach on large-scale multi-site brain imaging dataset (ABIDE) by considering four preprocessing pipelines, which outperforms the state-of-the-art methods. Hence, it is robust and consistent.","Autism spectrum disorder (ASD) is an intricate neuropsychiatric brain disorder characterized by social deficits and repetitive behaviors. Deep learning approaches have been applied in clinical or behavioral identification of ASD; most erstwhile models are inadequate in their capacity to exploit the data richness. On the other hand, classification techniques often solely rely on region-based summary and/or functional connectivity analysis of functional magnetic resonance imaging (fMRI). Besides, biomedical data modeling to analyze big data related to ASD is still perplexing due to its complexity and heterogeneity. Single volume image consideration has not been previously investigated in classification purposes. By deeming these challenges, in this work, firstly, we design an image generator to generate single volume brain images from the whole-brain image by considering the voxel time point of each subject separately. Then, to classify ASD and typical control participants, we evaluate four deep learning approaches with their corresponding ensemble classifiers comprising one amended Convolutional Neural Network (CNN). Finally, to check out the data variability, we apply the proposed CNN classifier with leave-one-site-out 5-fold cross-validation across the sites and validate our findings by comparing with literature reports. We showcase our approach on large-scale multi-site brain imaging dataset (ABIDE) by considering four preprocessing pipelines, which outperforms the state-of-the-art methods. Hence, it is robust and consistent.","Ahmed, Zhang, Liu, Liao","Ahmed, Zhang, Liu, Liao",https://doi.org/10.1109/JBHI.2020.2998603,https://doi.org/10.1109/JBHI.2020.2998603,2021-08-03
16393.0,pubmed,pubmed,Machine Learning for Clinical Outcome Prediction,Machine Learning for Clinical Outcome Prediction,"Clinical decision-making in healthcare is already being influenced by predictions or recommendations made by data-driven machines. Numerous machine learning applications have appeared in the latest clinical literature, especially for outcome prediction models, with outcomes ranging from mortality and cardiac arrest to acute kidney injury and arrhythmia. In this review article, we summarize the stateof- the-art in related works covering data processing, inference, and model evaluation, in the context of outcome prediction models developed using data extracted from electronic health records.We also discuss limitations of prominent modeling assumptions and highlight opportunities for future research.","Clinical decision-making in healthcare is already being influenced by predictions or recommendations made by data-driven machines. Numerous machine learning applications have appeared in the latest clinical literature, especially for outcome prediction models, with outcomes ranging from mortality and cardiac arrest to acute kidney injury and arrhythmia. In this review article, we summarize the state-of-the-art in related works covering data processing, inference, and model evaluation, in the context of outcome prediction models developed using data extracted from electronic health records. We also discuss limitations of prominent modeling assumptions and highlight opportunities for future research.","Shamout, Zhu, Clifton","Shamout, Zhu, Clifton",https://doi.org/10.1109/RBME.2020.3007816,https://doi.org/10.1109/RBME.2020.3007816,2021-08-03
16403.0,pubmed,pubmed,"Sarcopenia, frailty and cachexia patients detected in a multisystem electronic health record database","Sarcopenia, frailty and cachexia patients detected in a multisystem electronic health record database","Sarcopenia, cachexia and frailty have overlapping features and clinical consequences, but often go unrecognized. The objective was to detect patients described by clinicians as having sarcopenia, cachexia or frailty within electronic health records (EHR) and compare clinical variables between cases and matched controls. We conducted a case-control study using retrospective data from the Indiana Network for Patient Care multi-health system database from 2016 to 2017. The computable phenotype combined ICD codes for sarcopenia, cachexia and frailty, with clinical note text terms for sarcopenia, cachexia and frailty detected using natural language processing. Cases with these codes or text terms were matched to controls without these codes or text terms matched on birth year, sex and race. Two physicians reviewed EHR for all cases and a subset of controls. Comorbidity codes, laboratory values, and other coded clinical variables were compared between groups using Wilcoxon matched-pair sign-rank test for continuous variables and conditional logistic regression for binary variables. Cohorts of 9594 cases and 9594 matched controls were generated. Cases were 59% female, 69% white, and a median (1st, 3rd quartiles) age 74.9 (62.2, 84.8) years. Most cases were detected by text terms without ICD codes nÃ‚Â =Ã¢â‚¬â€°8285 (86.4%). All cases detected by ICD codes (total nÃ‚Â =Ã¢â‚¬â€°1309) also had supportive text terms. Overall 1496 (15.6%) had concurrent terms or codes for two or more of the three conditions (sarcopenia, cachexia or frailty). Of text term occurrence, 97% were used positively for sarcopenia, 90% for cachexia, and 95% for frailty. The remaining occurrences were negative uses of the terms or applied to someone other than the patient. Cases had lower body mass index, albumin and prealbumin, and significantly higher odds ratios for diabetes, hypertension, cardiovascular and peripheral vascular diseases, chronic kidney disease, liver disease, malignancy, osteoporosis and fractures (all pÃ‚Â &lt;Ã¢â‚¬â€°0.05). Cases were more likely to be prescribed appetite stimulants and caloric supplements. Patients detected with a computable phenotype for sarcopenia, cachexia and frailty differed from controls in several important clinical variables. Potential uses include detection among clinical cohorts for targeting recruitment for research and interventions.","Sarcopenia, cachexia and frailty have overlapping features and clinical consequences, but often go unrecognized. The objective was to detect patients described by clinicians as having sarcopenia, cachexia or frailty within electronic health records (EHR) and compare clinical variables between cases and matched controls. We conducted a case-control study using retrospective data from the Indiana Network for Patient Care multi-health system database from 2016 to 2017. The computable phenotype combined ICD codes for sarcopenia, cachexia and frailty, with clinical note text terms for sarcopenia, cachexia and frailty detected using natural language processing. Cases with these codes or text terms were matched to controls without these codes or text terms matched on birth year, sex and race. Two physicians reviewed EHR for all cases and a subset of controls. Comorbidity codes, laboratory values, and other coded clinical variables were compared between groups using Wilcoxon matched-pair sign-rank test for continuous variables and conditional logistic regression for binary variables. Cohorts of 9594 cases and 9594 matched controls were generated. Cases were 59% female, 69% white, and a median (1st, 3rd quartiles) age 74.9 (62.2, 84.8) years. Most cases were detected by text terms without ICD codes nÂ =â€‰8285 (86.4%). All cases detected by ICD codes (total nÂ =â€‰1309) also had supportive text terms. Overall 1496 (15.6%) had concurrent terms or codes for two or more of the three conditions (sarcopenia, cachexia or frailty). Of text term occurrence, 97% were used positively for sarcopenia, 90% for cachexia, and 95% for frailty. The remaining occurrences were negative uses of the terms or applied to someone other than the patient. Cases had lower body mass index, albumin and prealbumin, and significantly higher odds ratios for diabetes, hypertension, cardiovascular and peripheral vascular diseases, chronic kidney disease, liver disease, malignancy, osteoporosis and fractures (all pÂ &lt;â€‰0.05). Cases were more likely to be prescribed appetite stimulants and caloric supplements. Patients detected with a computable phenotype for sarcopenia, cachexia and frailty differed from controls in several important clinical variables. Potential uses include detection among clinical cohorts for targeting recruitment for research and interventions.","Moorthi, Liu, El-Azab, Lembcke, Miller, Broyles, Imel","Moorthi, Liu, El-Azab, Lembcke, Miller, Broyles, Imel",https://doi.org/10.1186/s12891-020-03522-9,https://doi.org/10.1186/s12891-020-03522-9,2021-08-03
16405.0,pubmed,pubmed,The Anatomy of the SARS-CoV-2 Biomedical Literature: Introducing the CovidX Network Algorithm for Drug Repurposing Recommendation,The Anatomy of the SARS-CoV-2 Biomedical Literature: Introducing the CovidX Network Algorithm for Drug Repurposing Recommendation,"Driven by the COVID-19 pandemic and the dire need to discover an antiviral drug, we explored the landscape of the SARS-CoV-2 biomedical publications to identify potential treatments. The aims of this study are to identify off-label drugs that may have benefits for the coronavirus disease pandemic, present a novel ranking algorithm called CovidX to recommend existing drugs for potential repurposing, and validate the literature-based outcome with drug knowledge available in clinical trials. To achieve such objectives, we applied natural language processing techniques to identify drugs and linked entities (eg, disease, gene, protein, chemical compounds). When such entities are linked, they form a map that can be further explored using network science tools. The CovidX algorithm was based upon a notion that we called &quot;diversity.&quot; A diversity score for a given drug was calculated by measuring how &quot;diverse&quot; a drug is calculated using various biological entities (regardless of the cardinality of actual instances in each category). The algorithm validates the ranking and awards those drugs that are currently being investigated in open clinical trials. The rationale behind the open clinical trial is to provide a validating mechanism of the PubMed results. This ensures providing up to date evidence of the fast development of this disease. From the analyzed biomedical literature, the algorithm identified 30 possible drug candidates for repurposing, ranked them accordingly, and validated the ranking outcomes against evidence from clinical trials. The top 10 candidates according to our algorithm are hydroxychloroquine, azithromycin, chloroquine, ritonavir, losartan, remdesivir, favipiravir, methylprednisolone, rapamycin, and tilorone dihydrochloride. The ranking shows both consistency and promise in identifying drugs that can be repurposed. We believe, however, the full treatment to be a multifaceted, adjuvant approach where multiple drugs may need to be taken at the same time.","Driven by the COVID-19 pandemic and the dire need to discover an antiviral drug, we explored the landscape of the SARS-CoV-2 biomedical publications to identify potential treatments. The aims of this study are to identify off-label drugs that may have benefits for the coronavirus disease pandemic, present a novel ranking algorithm called CovidX to recommend existing drugs for potential repurposing, and validate the literature-based outcome with drug knowledge available in clinical trials. To achieve such objectives, we applied natural language processing techniques to identify drugs and linked entities (eg, disease, gene, protein, chemical compounds). When such entities are linked, they form a map that can be further explored using network science tools. The CovidX algorithm was based upon a notion that we called ""diversity."" A diversity score for a given drug was calculated by measuring how ""diverse"" a drug is calculated using various biological entities (regardless of the cardinality of actual instances in each category). The algorithm validates the ranking and awards those drugs that are currently being investigated in open clinical trials. The rationale behind the open clinical trial is to provide a validating mechanism of the PubMed results. This ensures providing up to date evidence of the fast development of this disease. From the analyzed biomedical literature, the algorithm identified 30 possible drug candidates for repurposing, ranked them accordingly, and validated the ranking outcomes against evidence from clinical trials. The top 10 candidates according to our algorithm are hydroxychloroquine, azithromycin, chloroquine, ritonavir, losartan, remdesivir, favipiravir, methylprednisolone, rapamycin, and tilorone dihydrochloride. The ranking shows both consistency and promise in identifying drugs that can be repurposed. We believe, however, the full treatment to be a multifaceted, adjuvant approach where multiple drugs may need to be taken at the same time.","Gates, Hamed","Gates, Hamed",https://doi.org/10.2196/21169,https://doi.org/10.2196/21169,2021-08-03
16410.0,pubmed,pubmed,Feasibility and Acceptability of Mobile Phone Self-monitoring and Automated Feedback to Enhance Telephone Coaching for People with Risky Substance Use: The QUIT-Mobile Pilot Study,Feasibility and Acceptability of Mobile Phone Self-monitoring and Automated Feedback to Enhance Telephone Coaching for People With Risky Substance Use: The QUIT-Mobile Pilot Study,"This study evaluates the feasibility, acceptability, and perceived benefits of mobile-phone delivered self-monitoring queries and feedback integrated into the evidence-based Quit Using Drugs Intervention Trial (QUIT) screening and brief telephone health coaching intervention to prevent progression from risky drug use to addiction as the QUIT-Mobile intervention. Participants (nÃ¢â‚¬Å =Ã¢â‚¬Å 20) were primarily Black/African American and Latino men in Los Angeles with risky substance use. Self-monitoring surveys were sent by text-message twice-weekly for 6 weeks and once-weekly from 6 to 12-weeks. Surveys consisted of 10 questions regarding drug and alcohol use (ie, # days of use) and cravings, quality of life, and medication adherence. Feedback messages praised or encouraged drug use reductions. Coaches monitored patient responses and discussed them in QUIT's telephone coaching sessions. Participants' experiences were assessed qualitatively at 3-month follow-up. Nineteen out of 20 participants that completed the qualitative evaluation from the 12-week follow-up reported: (1) self-monitoring surveys helped them adhere to drug use reduction goals and reflect on associations between self-monitoring domains; (2) preference for higher frequency (twice-weekly) self-monitoring during the 6-week coaching period, and then weekly surveys thereafter but not monthly; and (3) self-monitoring and coaching were mutually reinforcing for their drug use reduction goals. Results are consistent with prior similar research suggesting that mobile phone self-monitoring of drug use and related factors is feasible and acceptable among diverse adults with risky drug use. Findings also suggest the potential benefits of integrating electronic self-monitoring and feedback into substance use reduction interventions such as QUIT to enhance patient self-management and coaching or counseling intervention components.","This study evaluates the feasibility, acceptability, and perceived benefits of mobile-phone delivered self-monitoring queries and feedback integrated into the evidence-based Quit Using Drugs Intervention Trial (QUIT) screening and brief telephone health coaching intervention to prevent progression from risky drug use to addiction as the QUIT-Mobile intervention. Participants (nâ€Š=â€Š20) were primarily Black/African American and Latino men in Los Angeles with risky substance use. Self-monitoring surveys were sent by text-message twice-weekly for 6 weeks and once-weekly from 6 to 12-weeks. Surveys consisted of 10 questions regarding drug and alcohol use (ie, # days of use) and cravings, quality of life, and medication adherence. Feedback messages praised or encouraged drug use reductions. Coaches monitored patient responses and discussed them in QUIT's telephone coaching sessions. Participants' experiences were assessed qualitatively at 3-month follow-up. Nineteen out of 20 participants that completed the qualitative evaluation from the 12-week follow-up reported: (1) self-monitoring surveys helped them adhere to drug use reduction goals and reflect on associations between self-monitoring domains; (2) preference for higher frequency (twice-weekly) self-monitoring during the 6-week coaching period, and then weekly surveys thereafter but not monthly; and (3) self-monitoring and coaching were mutually reinforcing for their drug use reduction goals. Results are consistent with prior similar research suggesting that mobile phone self-monitoring of drug use and related factors is feasible and acceptable among diverse adults with risky drug use. Findings also suggest the potential benefits of integrating electronic self-monitoring and feedback into substance use reduction interventions such as QUIT to enhance patient self-management and coaching or counseling intervention components.","Swendeman, Sumstine, Aguilar, Gorbach, Comulada, Gelberg","Swendeman, Sumstine, Aguilar, Gorbach, Comulada, Gelberg",https://doi.org/10.1097/ADM.0000000000000707,https://doi.org/10.1097/ADM.0000000000000707,2021-08-03
16414.0,pubmed,pubmed,[Extraction of features from clinical routine data using text mining],[Extraction of features from clinical routine data using text mining],"Anti-VEGF drugs are currently used to treat macular diseases. This has led to aÃ‚Â wealth of additional data, which could help understand and predict treatment courses; however, this information is usually only available in free text form. AÃ‚Â retrospective study was designed to analyze how far interpretable information can be obtained from clinical texts by automated extraction. The aim was to assess the suitability of aÃ‚Â text mining method that was customized for this purpose. Data on 3683 patients were available, including 40,485 discharge letters. Some of the data of interest, e.g. visual acuity (VA), intraocular pressure (IOP) and accompanying diagnoses, were not only recorded textually but also entered in aÃ‚Â database and could thus serve as aÃ‚Â gold standard for text analysis. The text was analyzed using the Averbis Health Discovery text mining platform. To optimize the extraction task, rule knowledge and aÃ‚Â German language technical vocabulary linked to the international medical terminology standard systematized nomenclature of medicine (SNOMED CT) was manually added. The correspondence between extracted data and the structured database entries is described by the F1 value. There was agreement of 94.7% for VA, 98.3% for IOP and 94.7% for the accompanying diagnoses. Manual analysis of noncorresponding cases showed that in 50% text content did not match the database content for various reasons. After an adjustment, F1 values 1-3% above the previously determined values were obtained. Text mining procedures are very well suited for the considered discharge letter corpus and the problem described in order to extract contents from clinical texts in aÃ‚Â structured manner for further evaluation.","Anti-VEGF drugs are currently used to treat macular diseases. This has led to aÂ wealth of additional data, which could help understand and predict treatment courses; however, this information is usually only available in free text form. AÂ retrospective study was designed to analyze how far interpretable information can be obtained from clinical texts by automated extraction. The aim was to assess the suitability of aÂ text mining method that was customized for this purpose. Data on 3683 patients were available, including 40,485 discharge letters. Some of the data of interest, e.g. visual acuity (VA), intraocular pressure (IOP) and accompanying diagnoses, were not only recorded textually but also entered in aÂ database and could thus serve as aÂ gold standard for text analysis. The text was analyzed using the Averbis Health Discovery text mining platform. To optimize the extraction task, rule knowledge and aÂ German language technical vocabulary linked to the international medical terminology standard systematized nomenclature of medicine (SNOMED CT) was manually added. The correspondence between extracted data and the structured database entries is described by the F1 value. There was agreement of 94.7% for VA, 98.3% for IOP and 94.7% for the accompanying diagnoses. Manual analysis of noncorresponding cases showed that in 50% text content did not match the database content for various reasons. After an adjustment, F1 values 1-3% above the previously determined values were obtained. Text mining procedures are very well suited for the considered discharge letter corpus and the problem described in order to extract contents from clinical texts in aÂ structured manner for further evaluation. HINTERGRUND: Anti-VEGF-Medikamente prÃ¤gen heute die Therapie von Makulaerkrankungen. In diesem Zusammenhang wird eine FÃ¼lle zusÃ¤tzlicher Daten erhoben. Damit lieÃŸen sich BehandlungsverlÃ¤ufe besser verstehen und vorhersagen. Allerdings sind diese Informationen meist nur in freitextlicher Form verfÃ¼gbar. Wie weit auswertbare Information aus Kliniktexten automatisch gewonnen werden kann, sollte in einer retrospektiven Studie analysiert werden. Ziel war die EinschÃ¤tzung der Eignung eines zu diesem Zweck parametrierten Text-Mining-Verfahrens. Es standen Daten zu 3683 Patienten zur VerfÃ¼gung, davon 40.485 Arztbriefe. FÃ¼r einen Teil waren die interessierenden Daten (Visus, Tensio und Begleitdiagnosen) auch strukturiert erfasst worden und konnten so als Goldstandard fÃ¼r die Textanalyse dienen. Diese wurde mit dem System Averbis Health Discovery durchgefÃ¼hrt. Zur Optimierung auf die Extraktionsaufgabe wurde dieses mit Regelwissen sowie mit einem deutschsprachigen Fachvokabular fÃ¼r die internationale Medizinterminologie SNOMED CT angereichert. Die Ãœbereinstimmung der Datenextrakte mit den strukturierten DatenbankeintrÃ¤gen wird durch den F1-Wert beschrieben. Hierbei ergab sich eine Ãœbereinstimmung von 94,7â€¯% fÃ¼r den Visus, 98,3â€¯% fÃ¼r die Tensio und 94,7â€¯% fÃ¼r begleitende Diagnosen. Die manuelle Analyse nicht Ã¼bereinstimmender FÃ¤lle zeigte zur HÃ¤lfte, dass Textinhalte aus verschiedenen GrÃ¼nden von Datenbankinhalten abwichen. Nach einer daraus berechneten Adjustierung lagen die F1-Werte noch 1â€“3â€¯% Ã¼ber den zuvor ermittelten Werten. FÃ¼r den betrachteten Arztbriefkorpus und die beschriebene Fragestellung sind Text-Mining-Verfahren sehr gut geeignet, um Inhalte zur weiteren Auswertung strukturiert aus Kliniktexten zu extrahieren.","Grundel, Bernardeau, Langner, Schmidt, BÃƒÂ¶hringer, Ritter, Rosenthal, Grandjean, Schulz, Daumke, Stahl","Grundel, Bernardeau, Langner, Schmidt, BÃ¶hringer, Ritter, Rosenthal, Grandjean, Schulz, Daumke, Stahl",https://doi.org/10.1007/s00347-020-01177-4,https://doi.org/10.1007/s00347-020-01177-4,2021-08-03
16422.0,pubmed,pubmed,High pooled performance of convolutional neural networks in computer-aided diagnosis of GI ulcers and/or hemorrhage on wireless capsule endoscopy images: a systematic review and meta-analysis,High pooled performance of convolutional neural networks in computer-aided diagnosis of GI ulcers and/or hemorrhage on wireless capsule endoscopy images: a systematic review and meta-analysis,"Diagnosis of GI ulcers and/or hemorrhage by wireless capsule endoscopy (WCE) is limited by the physician-dependent, tedious, time-consuming process of image and/ or video classification. Computer-aided diagnosis (CAD) by convolutional neural network (CNN)-based machine learning may help reduce this burden. Our aim was to conduct a meta-analysis and appraise the reported data. Multiple databases were searched (from inception to November 2019), and studies that reported on the performance of CNN in the diagnosis of GI ulcerations and/or hemorrhage on WCE were selected. A random-effects model was used to calculate the pooled rates. In cases where multiple 2Ã‚Â Ãƒâ€” 2 contingency tables were provided for different thresholds, we assumed the data tables were independent from each other. Heterogeneity was assessed by I<sup>2</sup>% and 95% prediction intervals. Nine studies were included in our final analysis that evaluated the performance of CNN-based CAD of GI ulcers and/or hemorrhage by WCE. The pooled accuracy was 95.4% (95% confidence interval [CI], 94.3-96.3), sensitivity was 95.5% (95% CI, 94-96.5), specificity was 95.8% (95% CI, 94.7-96.6), positive predictive value was 95.8% (95% CI, 90.5-98.2), and negative predictive value was 96.8% (95% CI, 94.9-98.1). I<sup>2</sup>% heterogeneity was negligible except for the pooled positive predictive value. Based on our meta-analysis, CNN-based CAD of GI ulcerations and/or hemorrhage on WCE achieves a high-level performance. The quality of the evidence is robust, and therefore CNN-based CAD has the potential to become the first choice of machine learning to optimize WCE image/video reading.","Diagnosis of GI ulcers and/or hemorrhage by wireless capsule endoscopy (WCE) is limited by the physician-dependent, tedious, time-consuming process of image and/ or video classification. Computer-aided diagnosis (CAD) by convolutional neural network (CNN)-based machine learning may help reduce this burden. Our aim was to conduct a meta-analysis and appraise the reported data. Multiple databases were searched (from inception to November 2019), and studies that reported on the performance of CNN in the diagnosis of GI ulcerations and/or hemorrhage on WCE were selected. A random-effects model was used to calculate the pooled rates. In cases where multiple 2Â Ã— 2 contingency tables were provided for different thresholds, we assumed the data tables were independent from each other. Heterogeneity was assessed by I<sup>2</sup>% and 95% prediction intervals. Nine studies were included in our final analysis that evaluated the performance of CNN-based CAD of GI ulcers and/or hemorrhage by WCE. The pooled accuracy was 95.4% (95% confidence interval [CI], 94.3-96.3), sensitivity was 95.5% (95% CI, 94-96.5), specificity was 95.8% (95% CI, 94.7-96.6), positive predictive value was 95.8% (95% CI, 90.5-98.2), and negative predictive value was 96.8% (95% CI, 94.9-98.1). I<sup>2</sup>% heterogeneity was negligible except for the pooled positive predictive value. Based on our meta-analysis, CNN-based CAD of GI ulcerations and/or hemorrhage on WCE achieves a high-level performance. The quality of the evidence is robust, and therefore CNN-based CAD has the potential to become the first choice of machine learning to optimize WCE image/video reading.","Mohan, Khan, Kassab, Ponnada, Chandan, Ali, Dulai, Adler, Kochhar","Mohan, Khan, Kassab, Ponnada, Chandan, Ali, Dulai, Adler, Kochhar",https://doi.org/10.1016/j.gie.2020.07.038,https://doi.org/10.1016/j.gie.2020.07.038,2021-08-03
16423.0,pubmed,pubmed,Machine learning versus conventional clinical methods in guiding management of heart failure patients-a systematic review,Machine learning versus conventional clinical methods in guiding management of heart failure patients-a systematic review,"Machine learning (ML) algorithms &quot;learn&quot; information directly from data, and their performance improves proportionally with the number of high-quality samples. The aim of our systematic review is to present the state of the art regarding the implementation of ML techniques in the management of heart failure (HF) patients. We manually searched MEDLINE and Cochrane databases as well the reference lists of the relevant review studies and included studies. Our search retrieved 122 relevant studies. These studies mainly refer to (a) the role of ML in the classification of HF patients into distinct categories which may require a different treatment strategy, (b) discrimination of HF patients from the healthy population or other diseases, (c) prediction of HF outcomes, (d) identification of HF patients from electronic records and identification of HF patients with similar characteristics who may benefit form a similar treatment strategy, (e) supporting the extraction of important data from clinical notes, and (f) prediction of outcomes in HF populations with implantable devices (left ventricular assist device, cardiac resynchronization therapy). We concluded that ML techniques may play an important role for the efficient construction of methodologies for diagnosis, management, and prediction of outcomes in HF patients.","Machine learning (ML) algorithms ""learn"" information directly from data, and their performance improves proportionally with the number of high-quality samples. The aim of our systematic review is to present the state of the art regarding the implementation of ML techniques in the management of heart failure (HF) patients. We manually searched MEDLINE and Cochrane databases as well the reference lists of the relevant review studies and included studies. Our search retrieved 122 relevant studies. These studies mainly refer to (a) the role of ML in the classification of HF patients into distinct categories which may require a different treatment strategy, (b) discrimination of HF patients from the healthy population or other diseases, (c) prediction of HF outcomes, (d) identification of HF patients from electronic records and identification of HF patients with similar characteristics who may benefit form a similar treatment strategy, (e) supporting the extraction of important data from clinical notes, and (f) prediction of outcomes in HF populations with implantable devices (left ventricular assist device, cardiac resynchronization therapy). We concluded that ML techniques may play an important role for the efficient construction of methodologies for diagnosis, management, and prediction of outcomes in HF patients.","Bazoukis, Stavrakis, Zhou, Bollepalli, Tse, Zhang, Singh, Armoundas","Bazoukis, Stavrakis, Zhou, Bollepalli, Tse, Zhang, Singh, Armoundas",https://doi.org/10.1007/s10741-020-10007-3,https://doi.org/10.1007/s10741-020-10007-3,2021-08-03
16425.0,pubmed,pubmed,Unified Medical Language System resources improve sieve-based generation and Bidirectional Encoder Representations from Transformers (BERT)-based ranking for concept normalization,Unified Medical Language System resources improve sieve-based generation and Bidirectional Encoder Representations from Transformers (BERT)-based ranking for concept normalization,"Concept normalization, the task of linking phrases in text to concepts in an ontology, is useful for many downstream tasks including relation extraction, information retrieval, etc. We present a generate-and-rank concept normalization system based on our participation in the 2019 National NLP Clinical Challenges Shared Task Track 3 Concept Normalization. The shared task provided 13Ã‚Â 609 concept mentions drawn from 100 discharge summaries. We first design a sieve-based system that uses Lucene indices over the training data, Unified Medical Language System (UMLS) preferred terms, and UMLS synonyms to generate a list of possible concepts for each mention. We then design a listwise classifier based on the BERT (Bidirectional Encoder Representations from Transformers) neural network to rank the candidate concepts, integrating UMLS semantic types through a regularizer. Our generate-and-rank system was third of 33 in the competition, outperforming the candidate generator alone (81.66% vs 79.44%) and the previous state of the art (76.35%). During postevaluation, the model's accuracy was increased to 83.56% via improvements to how training data are generated from UMLS and incorporation of our UMLS semantic type regularizer. Analysis of the model shows that prioritizing UMLS preferred terms yields better performance, that the UMLS semantic type regularizer results in qualitatively better concept predictions, and that the model performs well even on concepts not seen during training. Our generate-and-rank framework for UMLS concept normalization integrates key UMLS features like preferred terms and semantic types with a neural network-based ranking model to accurately link phrases in text to UMLS concepts.","Concept normalization, the task of linking phrases in text to concepts in an ontology, is useful for many downstream tasks including relation extraction, information retrieval, etc. We present a generate-and-rank concept normalization system based on our participation in the 2019 National NLP Clinical Challenges Shared Task Track 3 Concept Normalization. The shared task provided 13Â 609 concept mentions drawn from 100 discharge summaries. We first design a sieve-based system that uses Lucene indices over the training data, Unified Medical Language System (UMLS) preferred terms, and UMLS synonyms to generate a list of possible concepts for each mention. We then design a listwise classifier based on the BERT (Bidirectional Encoder Representations from Transformers) neural network to rank the candidate concepts, integrating UMLS semantic types through a regularizer. Our generate-and-rank system was third of 33 in the competition, outperforming the candidate generator alone (81.66% vs 79.44%) and the previous state of the art (76.35%). During postevaluation, the model's accuracy was increased to 83.56% via improvements to how training data are generated from UMLS and incorporation of our UMLS semantic type regularizer. Analysis of the model shows that prioritizing UMLS preferred terms yields better performance, that the UMLS semantic type regularizer results in qualitatively better concept predictions, and that the model performs well even on concepts not seen during training. Our generate-and-rank framework for UMLS concept normalization integrates key UMLS features like preferred terms and semantic types with a neural network-based ranking model to accurately link phrases in text to UMLS concepts.","Xu, Gopale, Zhang, Brown, Begoli, Bethard","Xu, Gopale, Zhang, Brown, Begoli, Bethard",https://doi.org/10.1093/jamia/ocaa080,https://doi.org/10.1093/jamia/ocaa080,2021-08-03
16426.0,pubmed,pubmed,COVID-19 Coronavirus Vaccine Design Using Reverse Vaccinology and Machine Learning,COVID-19 Coronavirus Vaccine Design Using Reverse Vaccinology and Machine Learning,"To ultimately combat the emerging COVID-19 pandemic, it is desired to develop an effective and safe vaccine against this highly contagious disease caused by the SARS-CoV-2 coronavirus. Our literature and clinical trial survey showed that the whole virus, as well as the spike (S) protein, nucleocapsid (N) protein, and membrane (M) protein, have been tested for vaccine development against SARS and MERS. However, these vaccine candidates might lack the induction of complete protection and have safety concerns. We then applied the Vaxign and the newly developed machine learning-based Vaxign-ML reverse vaccinology tools to predict COVID-19 vaccine candidates. Our Vaxign analysis found that the SARS-CoV-2 N protein sequence is conserved with SARS-CoV and MERS-CoV but not from the other four human coronaviruses causing mild symptoms. By investigating the entire proteome of SARS-CoV-2, six proteins, including the S protein and five non-structural proteins (nsp3, 3CL-pro, and nsp8-10), were predicted to be adhesins, which are crucial to the viral adhering and host invasion. The S, nsp3, and nsp8 proteins were also predicted by Vaxign-ML to induce high protective antigenicity. Besides the commonly used S protein, the nsp3 protein has not been tested in any coronavirus vaccine studies and was selected for further investigation. The nsp3 was found to be more conserved among SARS-CoV-2, SARS-CoV, and MERS-CoV than among 15 coronaviruses infecting human and other animals. The protein was also predicted to contain promiscuous MHC-I and MHC-II T-cell epitopes, and the predicted linear B-cell epitopes were found to be localized on the surface of the protein. Our predicted vaccine targets have the potential for effective and safe COVID-19 vaccine development. We also propose that an &quot;Sp/Nsp cocktail vaccine&quot; containing a structural protein(s) (Sp) and a non-structural protein(s) (Nsp) would stimulate effective complementary immune responses.","To ultimately combat the emerging COVID-19 pandemic, it is desired to develop an effective and safe vaccine against this highly contagious disease caused by the SARS-CoV-2 coronavirus. Our literature and clinical trial survey showed that the whole virus, as well as the spike (S) protein, nucleocapsid (N) protein, and membrane (M) protein, have been tested for vaccine development against SARS and MERS. However, these vaccine candidates might lack the induction of complete protection and have safety concerns. We then applied the Vaxign and the newly developed machine learning-based Vaxign-ML reverse vaccinology tools to predict COVID-19 vaccine candidates. Our Vaxign analysis found that the SARS-CoV-2 N protein sequence is conserved with SARS-CoV and MERS-CoV but not from the other four human coronaviruses causing mild symptoms. By investigating the entire proteome of SARS-CoV-2, six proteins, including the S protein and five non-structural proteins (nsp3, 3CL-pro, and nsp8-10), were predicted to be adhesins, which are crucial to the viral adhering and host invasion. The S, nsp3, and nsp8 proteins were also predicted by Vaxign-ML to induce high protective antigenicity. Besides the commonly used S protein, the nsp3 protein has not been tested in any coronavirus vaccine studies and was selected for further investigation. The nsp3 was found to be more conserved among SARS-CoV-2, SARS-CoV, and MERS-CoV than among 15 coronaviruses infecting human and other animals. The protein was also predicted to contain promiscuous MHC-I and MHC-II T-cell epitopes, and the predicted linear B-cell epitopes were found to be localized on the surface of the protein. Our predicted vaccine targets have the potential for effective and safe COVID-19 vaccine development. We also propose that an ""Sp/Nsp cocktail vaccine"" containing a structural protein(s) (Sp) and a non-structural protein(s) (Nsp) would stimulate effective complementary immune responses.","Ong, Wong, Huffman, He","Ong, Wong, Huffman, He",https://doi.org/10.3389/fimmu.2020.01581,https://doi.org/10.3389/fimmu.2020.01581,2021-08-03
16427.0,pubmed,pubmed,Online resources and apps to aid self-diagnosis and help seeking in the perinatal period: A descriptive survey of women's experiences,Online resources and apps to aid self-diagnosis and help seeking in the perinatal period: A descriptive survey of women's experiences,"Assess the role of online resources and apps for women's help seeking and staff's response to concerns in the perinatal period. Online survey. Descriptive analysis of women's use and experiences of digital resources for self-diagnosis and help seeking, drawing on numerical and free-text responses. Two tertiary referral centres and one district general hospital in two UK geographic locations. 632 postnatal women, surveyed over a 4 month period. Women's access to digital devices; frequency and type of health concerns experienced after 22 weeks' gestation; variability in use and experiences of websites/apps; perceptions of staff's response to concerns after help-seeking. 1254 women were approached over a 4-month period; 632 participated (response rate: 50%). Women reported a 'mix and match' blended use of digital resources to both learn about, and self-diagnose/self-triage for potential complications in pregnancy as an adjunct to care provided by maternity staff. Over half the participants experienced concerns about themselves or their baby after 22 weeks. The top concern was fetal movements, reported by 62%. Women used 91 different digital resources to help with understanding and decision-making, in addition to seeking support from family, friends and healthcare professionals. Enabling features of staff responses were identified from free-text responses (nÃ¢â‚¬Â¯=Ã¢â‚¬Â¯292) by women who sought professional help regarding their health concerns, and influencing factors at clinical, organisational and digital level. Online information retrieval and digital self-monitoring is increasingly integral to women's self-care during pregnancy and offers opportunities to support escalation of care and shared decision-making. Further work should assess optimal inclusion of this 'digital work' into clinical consultations.","Assess the role of online resources and apps for women's help seeking and staff's response to concerns in the perinatal period. Online survey. Descriptive analysis of women's use and experiences of digital resources for self-diagnosis and help seeking, drawing on numerical and free-text responses. Two tertiary referral centres and one district general hospital in two UK geographic locations. 632 postnatal women, surveyed over a 4 month period. Women's access to digital devices; frequency and type of health concerns experienced after 22 weeks' gestation; variability in use and experiences of websites/apps; perceptions of staff's response to concerns after help-seeking. 1254 women were approached over a 4-month period; 632 participated (response rate: 50%). Women reported a 'mix and match' blended use of digital resources to both learn about, and self-diagnose/self-triage for potential complications in pregnancy as an adjunct to care provided by maternity staff. Over half the participants experienced concerns about themselves or their baby after 22 weeks. The top concern was fetal movements, reported by 62%. Women used 91 different digital resources to help with understanding and decision-making, in addition to seeking support from family, friends and healthcare professionals. Enabling features of staff responses were identified from free-text responses (nâ€¯=â€¯292) by women who sought professional help regarding their health concerns, and influencing factors at clinical, organisational and digital level. Online information retrieval and digital self-monitoring is increasingly integral to women's self-care during pregnancy and offers opportunities to support escalation of care and shared decision-making. Further work should assess optimal inclusion of this 'digital work' into clinical consultations.","Mackintosh, Agarwal, Adcock, Armstrong, Briley, Patterson, Sandall, Sarah Gong","Mackintosh, Agarwal, Adcock, Armstrong, Briley, Patterson, Sandall, Sarah Gong",https://doi.org/10.1016/j.midw.2020.102803,https://doi.org/10.1016/j.midw.2020.102803,2021-08-03
16434.0,pubmed,pubmed,Radiomics of Coronary Artery Calcium in the Framingham Heart Study,Radiomics of Coronary Artery Calcium in the Framingham Heart Study,"To extract radiomic features from coronary artery calcium (CAC) on CT images and to determine whether this approach could improve the ability to identify individuals at risk for a composite endpoint of clinical events. Participants from the Offspring and Third Generation cohorts of the community-based Framingham Heart Study underwent noncontrast cardiac CT (2002-2005) and were followed for more than a median of 9.1 years for composite major events. A total of 624 participants with CAC Agatston score (AS) of greater than 0 and good or excellent CT image quality were included for manual CAC segmentation and extraction of a predefined set of radiomic features reflecting intensity, shape, and texture. In a discovery cohort (<i>n</i> = 318), machine learning was used to select the 20 most informative and nonredundant CAC radiomic features, classify features predicting events, and define a radiomic-based score (RS). Performance of the RS was tested independently for the prediction of events in a validation cohort (<i>n</i> = 306). The RS had a median value of 0.08 (interquartile range, 0.007-0.71) and a weak and modest correlation with Framingham risk score (FRS) (<i>r</i> = 0.2) and AS (<i>r</i> = 0.39), respectively. The continuous RS unadjusted, adjusted for age and sex, FRS, AS, and FRS plus AS were significantly associated with events (hazard ratio [HR] = 2.2, <i>P</i> &lt; .001; HR = 1.8, <i>P</i> = .002; HR = 2.0, <i>P</i> &lt; .001; HR = 1.7, <i>P</i> = .02; and HR = 1.8, <i>P</i> = .01, respectively). In participants with AS of less than 300, RS association with events remained significant when unadjusted and adjusted for age and sex, FRS, AS, and FRS plus AS (HR = 2.4, 2.8, 2.8, 2.3, and 2.6; <i>P</i> &lt; .001, respectively). In the same subgroup of participants, adding the RS to AS resulted in a significant improvement in the discriminatory ability for events as compared with the AS (area under the receiver operating curve: 0.80 vs 0.68, respectively; <i>P</i> = .03). A radiomic-based score, including the complex properties of CAC, may constitute an imaging biomarker to be further developed to identify individuals at risk for major adverse cardiovascular events in a community-based cohort. <i>Supplemental material is available for this article.</i> Ã‚Â© RSNA, 2020.","To extract radiomic features from coronary artery calcium (CAC) on CT images and to determine whether this approach could improve the ability to identify individuals at risk for a composite endpoint of clinical events. Participants from the Offspring and Third Generation cohorts of the community-based Framingham Heart Study underwent noncontrast cardiac CT (2002-2005) and were followed for more than a median of 9.1 years for composite major events. A total of 624 participants with CAC Agatston score (AS) of greater than 0 and good or excellent CT image quality were included for manual CAC segmentation and extraction of a predefined set of radiomic features reflecting intensity, shape, and texture. In a discovery cohort (<i>n</i> = 318), machine learning was used to select the 20 most informative and nonredundant CAC radiomic features, classify features predicting events, and define a radiomic-based score (RS). Performance of the RS was tested independently for the prediction of events in a validation cohort (<i>n</i> = 306). The RS had a median value of 0.08 (interquartile range, 0.007-0.71) and a weak and modest correlation with Framingham risk score (FRS) (<i>r</i> = 0.2) and AS (<i>r</i> = 0.39), respectively. The continuous RS unadjusted, adjusted for age and sex, FRS, AS, and FRS plus AS were significantly associated with events (hazard ratio [HR] = 2.2, <i>P</i> &lt; .001; HR = 1.8, <i>P</i> = .002; HR = 2.0, <i>P</i> &lt; .001; HR = 1.7, <i>P</i> = .02; and HR = 1.8, <i>P</i> = .01, respectively). In participants with AS of less than 300, RS association with events remained significant when unadjusted and adjusted for age and sex, FRS, AS, and FRS plus AS (HR = 2.4, 2.8, 2.8, 2.3, and 2.6; <i>P</i> &lt; .001, respectively). In the same subgroup of participants, adding the RS to AS resulted in a significant improvement in the discriminatory ability for events as compared with the AS (area under the receiver operating curve: 0.80 vs 0.68, respectively; <i>P</i> = .03). A radiomic-based score, including the complex properties of CAC, may constitute an imaging biomarker to be further developed to identify individuals at risk for major adverse cardiovascular events in a community-based cohort. <i>Supplemental material is available for this article.</i> Â© RSNA, 2020.","Eslami, Parmar, Foldyna, Scholtz, Ivanov, Zeleznik, Lu, Ferencik, Vasan, Baltrusaitis, Massaro, D'Agostino, Mayrhofer, O'Donnell, Aerts, Hoffmann","Eslami, Parmar, Foldyna, Scholtz, Ivanov, Zeleznik, Lu, Ferencik, Vasan, Baltrusaitis, Massaro, D'Agostino, Mayrhofer, O'Donnell, Aerts, Hoffmann",https://doi.org/10.1148/ryct.2020190119,https://doi.org/10.1148/ryct.2020190119,2021-08-03
16435.0,pubmed,pubmed,Automatic cancer detection on digital histopathology images of mid-gland radical prostatectomy specimens,Automatic cancer detection on digital histopathology images of mid-gland radical prostatectomy specimens,"<b>Purpose:</b> Automatic cancer detection on radical prostatectomy (RP) sections facilitates graphical and quantitative surgical pathology reporting, which can potentially benefit postsurgery follow-up care and treatment planning. It can also support imaging validation studies using a histologic reference standard and pathology research studies. This problem is challenging due to the large sizes of digital histopathology whole-mount whole-slide images (WSIs) of RP sections and staining variability across different WSIs. <b>Approach:</b> We proposed a calibration-free adaptive thresholding algorithm, which compensates for staining variability and yields consistent tissue component maps (TCMs) of the nuclei, lumina, and other tissues. We used and compared three machine learning methods for classifying each cancer versus noncancer region of interest (ROI) throughout each WSI: (1)Ã‚Â conventional machine learning methods and 14 texture features extracted from TCMs, (2)Ã‚Â transfer learning with pretrained AlexNet fine-tuned by TCM ROIs, and (3)Ã‚Â transfer learning with pretrained AlexNet fine-tuned with raw image ROIs. <b>Results:</b> The three methods yielded areas under the receiver operating characteristic curve of 0.96, 0.98, and 0.98, respectively, in leave-one-patient-out cross validation using 1.3Ã‚Â million ROIs from 286 mid-gland whole-mount WSIs from 68 patients. <b>Conclusion:</b> Transfer learning with the use of TCMs demonstrated state-of-the-art overall performance and is more stable with respect to sample size across different tissue types. For the tissue types involving Gleason 5 (most aggressive) cancer, it achieved the best performance compared to the other tested methods. This tool can be translated to clinical workflow to assist graphical and quantitative pathology reporting for surgical specimens upon further multicenter validation.","<b>Purpose:</b> Automatic cancer detection on radical prostatectomy (RP) sections facilitates graphical and quantitative surgical pathology reporting, which can potentially benefit postsurgery follow-up care and treatment planning. It can also support imaging validation studies using a histologic reference standard and pathology research studies. This problem is challenging due to the large sizes of digital histopathology whole-mount whole-slide images (WSIs) of RP sections and staining variability across different WSIs. <b>Approach:</b> We proposed a calibration-free adaptive thresholding algorithm, which compensates for staining variability and yields consistent tissue component maps (TCMs) of the nuclei, lumina, and other tissues. We used and compared three machine learning methods for classifying each cancer versus noncancer region of interest (ROI) throughout each WSI: (1)Â conventional machine learning methods and 14 texture features extracted from TCMs, (2)Â transfer learning with pretrained AlexNet fine-tuned by TCM ROIs, and (3)Â transfer learning with pretrained AlexNet fine-tuned with raw image ROIs. <b>Results:</b> The three methods yielded areas under the receiver operating characteristic curve of 0.96, 0.98, and 0.98, respectively, in leave-one-patient-out cross validation using 1.3Â million ROIs from 286 mid-gland whole-mount WSIs from 68 patients. <b>Conclusion:</b> Transfer learning with the use of TCMs demonstrated state-of-the-art overall performance and is more stable with respect to sample size across different tissue types. For the tissue types involving Gleason 5 (most aggressive) cancer, it achieved the best performance compared to the other tested methods. This tool can be translated to clinical workflow to assist graphical and quantitative pathology reporting for surgical specimens upon further multicenter validation.","Han, Johnson, Warner, Gaed, Gomez, Moussa, Chin, Pautler, Bauman, Ward","Han, Johnson, Warner, Gaed, Gomez, Moussa, Chin, Pautler, Bauman, Ward",https://doi.org/10.1117/1.JMI.7.4.047501,https://doi.org/10.1117/1.JMI.7.4.047501,2021-08-03
16449.0,pubmed,pubmed,A 3D Deep Learning System for Detecting Referable Glaucoma Using Full OCT Macular Cube Scans,A 3D Deep Learning System for Detecting Referable Glaucoma Using Full OCT Macular Cube Scans,"The purpose of this study was to develop a 3D deep learning system from spectral domain optical coherence tomography (SD-OCT) macular cubes to differentiate between referable and nonreferable cases for glaucoma applied to real-world datasets to understand how this would affect the performance. There were 2805 Cirrus optical coherence tomography (OCT) macula volumes (Macula protocol 512 Ãƒâ€” 128) of 1095 eyes from 586 patients at a single site that were used to train a fully 3D convolutional neural network (CNN). Referable glaucoma included true glaucoma, pre-perimetric glaucoma, and high-risk suspects, based on qualitative fundus photographs, visual fields, OCT reports, and clinical examinations, including intraocular pressure (IOP) and treatment history as the binary (two class) ground truth. The curated real-world dataset did not include eyes with retinal disease or nonglaucomatous optic neuropathies. The cubes were first homogenized using layer segmentation with the Orion Software (Voxeleron) to achieve standardization. The algorithm was tested on two separate external validation sets from different glaucoma studies, comprised of Cirrus macular cube scans of 505 and 336 eyes, respectively. The area under the receiver operating characteristic (AUROC) curve for the development dataset for distinguishing referable glaucoma was 0.88 for our CNN using homogenization, 0.82 without homogenization, and 0.81 for a CNN architecture from the existing literature. For the external validation datasets, which had different glaucoma definitions, the AUCs were 0.78 and 0.95, respectively. The performance of the model across myopia severity distribution has been assessed in the dataset from the United States and was found to have an AUC of 0.85, 0.92, and 0.95 in the severe, moderate, and mild myopia, respectively. A 3D deep learning algorithm trained on macular OCT volumes without retinal disease to detect referable glaucoma performs better with retinal segmentation preprocessing and performs reasonably well across all levels of myopia. Interpretation of OCT macula volumes based on normative data color distributions is highly influenced by population demographics and characteristics, such as refractive error, as well as the size of the normative database. Referable glaucoma, in this study, was chosen to include cases that should be seen by a specialist. This study is unique because it uses multimodal patient data for the glaucoma definition, and includes all severities of myopia as well as validates the algorithm with international data to understand generalizability potential.","The purpose of this study was to develop a 3D deep learning system from spectral domain optical coherence tomography (SD-OCT) macular cubes to differentiate between referable and nonreferable cases for glaucoma applied to real-world datasets to understand how this would affect the performance. There were 2805 Cirrus optical coherence tomography (OCT) macula volumes (Macula protocol 512 Ã— 128) of 1095 eyes from 586 patients at a single site that were used to train a fully 3D convolutional neural network (CNN). Referable glaucoma included true glaucoma, pre-perimetric glaucoma, and high-risk suspects, based on qualitative fundus photographs, visual fields, OCT reports, and clinical examinations, including intraocular pressure (IOP) and treatment history as the binary (two class) ground truth. The curated real-world dataset did not include eyes with retinal disease or nonglaucomatous optic neuropathies. The cubes were first homogenized using layer segmentation with the Orion Software (Voxeleron) to achieve standardization. The algorithm was tested on two separate external validation sets from different glaucoma studies, comprised of Cirrus macular cube scans of 505 and 336 eyes, respectively. The area under the receiver operating characteristic (AUROC) curve for the development dataset for distinguishing referable glaucoma was 0.88 for our CNN using homogenization, 0.82 without homogenization, and 0.81 for a CNN architecture from the existing literature. For the external validation datasets, which had different glaucoma definitions, the AUCs were 0.78 and 0.95, respectively. The performance of the model across myopia severity distribution has been assessed in the dataset from the United States and was found to have an AUC of 0.85, 0.92, and 0.95 in the severe, moderate, and mild myopia, respectively. A 3D deep learning algorithm trained on macular OCT volumes without retinal disease to detect referable glaucoma performs better with retinal segmentation preprocessing and performs reasonably well across all levels of myopia. Interpretation of OCT macula volumes based on normative data color distributions is highly influenced by population demographics and characteristics, such as refractive error, as well as the size of the normative database. Referable glaucoma, in this study, was chosen to include cases that should be seen by a specialist. This study is unique because it uses multimodal patient data for the glaucoma definition, and includes all severities of myopia as well as validates the algorithm with international data to understand generalizability potential.","Russakoff, Mannil, Oakley, Ran, Cheung, Dasari, Riyazzuddin, Nagaraj, Rao, Chang, Chang","Russakoff, Mannil, Oakley, Ran, Cheung, Dasari, Riyazzuddin, Nagaraj, Rao, Chang, Chang",https://doi.org/10.1167/tvst.9.2.12,https://doi.org/10.1167/tvst.9.2.12,2021-08-03
16451.0,pubmed,pubmed,CT texture analysis predicts abdominal aortic aneurysm post-endovascular aortic aneurysm repair progression,CT texture analysis predicts abdominal aortic aneurysm post-endovascular aortic aneurysm repair progression,"The aim of this study isÃ‚Â to investigate the role of early postoperative CT texture analysis in aneurysm progression. Ninety-nine patients who had undergone post-endovascular aneurysm repair (EVAR) infra-renal abdominal aortic aneurysm CT serial scans were enrolled from July 2014 to December 2019. The clinical and traditional imaging features were obtained. Aneurysm texture analysis was performed using three methods-the grey-level co-occurrence matrix (GLCM), the grey-level run length matrix (GLRLM), and the grey-level difference method (GLDM). A multilayer perceptron neural network was applied as a classifier, and receiver operating characteristic (ROC) curve analysis and area under the curve (AUC) analysis were employed to illustrate the classification performance. No difference was found in the morphological and clinical features between the expansion (+) and (-) groups. GLCM yielded the best performance with an accuracy of 85.17% and an AUC of 0.90, followed by GLRLM with an accuracy of 87.23% and an AUC of 0.8615, and GLDM with an accuracy of 86.09% and an AUC of 0.8313. All three texture analyses showed superior predictive ability over clinical risk factors (accuracy: 69.41%; AUC: 0.6649), conventional imaging features (accuracy: 69.02%; AUC: 0.6747), and combined (accuracy: 75.29%; AUC: 0.7249). Early post-EVAR arterial phase-derived aneurysm texture analysis is a better predictor of later aneurysm expansion than clinical factors and traditional imaging evaluation combined.","The aim of this study isÂ to investigate the role of early postoperative CT texture analysis in aneurysm progression. Ninety-nine patients who had undergone post-endovascular aneurysm repair (EVAR) infra-renal abdominal aortic aneurysm CT serial scans were enrolled from July 2014 to December 2019. The clinical and traditional imaging features were obtained. Aneurysm texture analysis was performed using three methods-the grey-level co-occurrence matrix (GLCM), the grey-level run length matrix (GLRLM), and the grey-level difference method (GLDM). A multilayer perceptron neural network was applied as a classifier, and receiver operating characteristic (ROC) curve analysis and area under the curve (AUC) analysis were employed to illustrate the classification performance. No difference was found in the morphological and clinical features between the expansion (+) and (-) groups. GLCM yielded the best performance with an accuracy of 85.17% and an AUC of 0.90, followed by GLRLM with an accuracy of 87.23% and an AUC of 0.8615, and GLDM with an accuracy of 86.09% and an AUC of 0.8313. All three texture analyses showed superior predictive ability over clinical risk factors (accuracy: 69.41%; AUC: 0.6649), conventional imaging features (accuracy: 69.02%; AUC: 0.6747), and combined (accuracy: 75.29%; AUC: 0.7249). Early post-EVAR arterial phase-derived aneurysm texture analysis is a better predictor of later aneurysm expansion than clinical factors and traditional imaging evaluation combined.","Ding, Hao, Wang, Xuan, Kong, Xue, Jin","Ding, Hao, Wang, Xuan, Kong, Xue, Jin",https://doi.org/10.1038/s41598-020-69226-1,https://doi.org/10.1038/s41598-020-69226-1,2021-08-03
16453.0,pubmed,pubmed,Long-term exposure to PM and all-cause and cause-specific mortality: A systematic review and meta-analysis,Long-term exposure to PM and all-cause and cause-specific mortality: A systematic review and meta-analysis,"As new scientific evidence on health effects of air pollution is generated, air quality guidelines need to be periodically updated. The objective of this review is to support the derivation of updated guidelines by the World Health Organization (WHO) by performing a systematic review of evidence of associations between long-term exposure to particulate matter with diameter under 2.5Ã‚Â Ã‚Âµm (PM<sub>2.5</sub>) and particulate matter with diameter under 10Ã‚Â Ã‚Âµm (PM<sub>10</sub>), in relation to all-cause and cause-specific mortality. As there is especially uncertainty about the relationship at the low and high end of the exposure range, the review needed to provide an indication of the shape of the concentration-response function (CRF). We systematically searched MEDLINE and EMBASE from database inception to 9 October 2018. Articles were checked for eligibility by two reviewers. We included cohort and case-control studies on outdoor air pollution in human populations using individual level data. In addition to natural-cause mortality, we evaluated mortality from circulatory diseases (ischemic heart disease (IHD) and cerebrovascular disease (stroke) also specifically), respiratory diseases (Chronic Obstructive Pulmonary Disease (COPD) and acute lower respiratory infection (ALRI) also specifically) and lung cancer. A random-effect meta-analysis was performed when at least three studies were available for a specific exposure-outcome pair. Risk of bias was assessed for all included articles using a specifically developed tool coordinated by WHO. Additional analyses were performed to assess consistency across geographic region, explain heterogeneity and explore the shape of the CRF. An adapted GRADE (Grading of Recommendations Assessment, Development and Evaluation) assessment of the body of evidence was made using a specifically developed tool coordinated by WHO. A large number (NÃ‚Â =Ã‚Â 107) of predominantly cohort studies (NÃ‚Â =Ã‚Â 104) were included after screening more than 3000 abstracts. Studies were conducted globally with the majority of studies from North America (NÃ‚Â =Ã‚Â 62) and Europe (NÃ‚Â =Ã‚Â 25). More studies used PM<sub>2.5</sub> (NÃ‚Â =Ã‚Â 71) as the exposure metric than PM<sub>10</sub> (NÃ‚Â =Ã‚Â 42). PM<sub>2.5</sub> was significantly associated with all causes of death evaluated. The combined Risk Ratio (RR) for PM<sub>2.5</sub> and natural-cause mortality was 1.08 (95%CI 1.06, 1.09) per 10Ã‚Â Ã‚Âµg/m<sup>3</sup>. Meta analyses of studies conducted at the low mean PM<sub>2.5</sub> levels (&lt;25, 20, 15, 12, 10Ã‚Â Ã‚Âµg/m<sup>3</sup>) yielded RRs that were similar or higher compared to the overall RR, consistent with the finding of generally linear or supra-linear CRFs in individual studies. Pooled RRs were almost identical for studies conducted in North America, Europe and Western Pacific region. PM<sub>10</sub> was significantly associated with natural-cause and most but not all causes of death. Application of the risk of bias tool showed that few studies were at a high risk of bias in any domain. Application of the adapted GRADE tool resulted in an assessment of &quot;high certainty of evidence&quot; for PM<sub>2.5</sub> with all assessed endpoints except for respiratory mortality (moderate). The evidence was rated as less certain for PM<sub>10</sub> and cause-specific mortality (&quot;moderate&quot; for circulatory, IHD, COPD and &quot;low&quot; for stroke mortality. Compared to the previous global WHO evaluation, the evidence base has increased substantially. However, studies conducted in low- and middle- income countries (LMICs) are still limited. There is clear evidence that both PM<sub>2.5</sub> and PM<sub>10</sub> were associated with increased mortality from all causes, cardiovascular disease, respiratory disease and lung cancer. Associations remained below the current WHO guideline exposure level of 10Ã‚Â Ã‚Âµg/m<sup>3</sup> for PM<sub>2.5</sub>. Systematic review registration number (PROSPERO ID): CRD42018082577.","As new scientific evidence on health effects of air pollution is generated, air quality guidelines need to be periodically updated. The objective of this review is to support the derivation of updated guidelines by the World Health Organization (WHO) by performing a systematic review of evidence of associations between long-term exposure to particulate matter with diameter under 2.5Â Âµm (PM<sub>2.5</sub>) and particulate matter with diameter under 10Â Âµm (PM<sub>10</sub>), in relation to all-cause and cause-specific mortality. As there is especially uncertainty about the relationship at the low and high end of the exposure range, the review needed to provide an indication of the shape of the concentration-response function (CRF). We systematically searched MEDLINE and EMBASE from database inception to 9 October 2018. Articles were checked for eligibility by two reviewers. We included cohort and case-control studies on outdoor air pollution in human populations using individual level data. In addition to natural-cause mortality, we evaluated mortality from circulatory diseases (ischemic heart disease (IHD) and cerebrovascular disease (stroke) also specifically), respiratory diseases (Chronic Obstructive Pulmonary Disease (COPD) and acute lower respiratory infection (ALRI) also specifically) and lung cancer. A random-effect meta-analysis was performed when at least three studies were available for a specific exposure-outcome pair. Risk of bias was assessed for all included articles using a specifically developed tool coordinated by WHO. Additional analyses were performed to assess consistency across geographic region, explain heterogeneity and explore the shape of the CRF. An adapted GRADE (Grading of Recommendations Assessment, Development and Evaluation) assessment of the body of evidence was made using a specifically developed tool coordinated by WHO. A large number (NÂ =Â 107) of predominantly cohort studies (NÂ =Â 104) were included after screening more than 3000 abstracts. Studies were conducted globally with the majority of studies from North America (NÂ =Â 62) and Europe (NÂ =Â 25). More studies used PM<sub>2.5</sub> (NÂ =Â 71) as the exposure metric than PM<sub>10</sub> (NÂ =Â 42). PM<sub>2.5</sub> was significantly associated with all causes of death evaluated. The combined Risk Ratio (RR) for PM<sub>2.5</sub> and natural-cause mortality was 1.08 (95%CI 1.06, 1.09) per 10Â Âµg/m<sup>3</sup>. Meta analyses of studies conducted at the low mean PM<sub>2.5</sub> levels (&lt;25, 20, 15, 12, 10Â Âµg/m<sup>3</sup>) yielded RRs that were similar or higher compared to the overall RR, consistent with the finding of generally linear or supra-linear CRFs in individual studies. Pooled RRs were almost identical for studies conducted in North America, Europe and Western Pacific region. PM<sub>10</sub> was significantly associated with natural-cause and most but not all causes of death. Application of the risk of bias tool showed that few studies were at a high risk of bias in any domain. Application of the adapted GRADE tool resulted in an assessment of ""high certainty of evidence"" for PM<sub>2.5</sub> with all assessed endpoints except for respiratory mortality (moderate). The evidence was rated as less certain for PM<sub>10</sub> and cause-specific mortality (""moderate"" for circulatory, IHD, COPD and ""low"" for stroke mortality. Compared to the previous global WHO evaluation, the evidence base has increased substantially. However, studies conducted in low- and middle- income countries (LMICs) are still limited. There is clear evidence that both PM<sub>2.5</sub> and PM<sub>10</sub> were associated with increased mortality from all causes, cardiovascular disease, respiratory disease and lung cancer. Associations remained below the current WHO guideline exposure level of 10Â Âµg/m<sup>3</sup> for PM<sub>2.5</sub>. Systematic review registration number (PROSPERO ID): CRD42018082577.","Chen, Hoek","Chen, Hoek",https://doi.org/10.1016/j.envint.2020.105974,https://doi.org/10.1016/j.envint.2020.105974,2021-08-03
16455.0,pubmed,pubmed,Application of systematic evidence mapping to assess the impact of new research when updating health reference values: A case example using acrolein,Application of systematic evidence mapping to assess the impact of new research when updating health reference values: A case example using acrolein,"The environmental health community needs transparent, methodologically rigorous, and rapid approaches for updating human health risk assessments. These assessments often contain reference values for cancer and/or noncancer effects. Increasingly, the use of systematic review methods are preferred when developing these assessments. Systematic evidence maps are a type of analysis that has the potential to be very helpful in the update process, especially when combined with machine-learning software advances designed to expedite the process of conducting a review. To evaluate the applicability of evidence mapping to determine whether new evidence is likely to result in a change to an existing health reference value, using inhalation exposure to the air pollutant acrolein as a case example. New literature published since the 2008 California Environmental Protection Agency's Office of Environmental Health Hazard Assessment (OEHHA) Reference Exposure Level (REL) for acrolein was assessed. Systematic review methods were used to search the literature and screening included the use of machine-learning software. The Populations, Exposures, Comparators and Outcomes (PECO) criteria were kept broad to identify studies that characterized acute and chronic exposure and could be informative for hazard characterization. Studies that met the PECO criteria after full-text review were briefly summarized before their suitability for chronic point of departure (POD) derivation and calculation of a reference value was considered. Studies considered potentially suitable underwent a targeted evaluation to determine their suitability for use in dose-response analysis. Over 15,000 studies were identified from scientific databases. Both machine-learning and manual screening processes were used to identify 60 studies considered PECO-relevant after full-text review. Most of these PECO-relevant studies were short-term exposure animal studies (acute or less than 1Ã‚Â month of exposure) and considered less suitable for deriving a chronic reference value when compared to the subchronic study in rats used in the 2008 OEHHA assessment. Thirteen epidemiological studies were identified but had limitations in the exposure assessment that made them less suitable for dose-response compared to the subchronic rat study. Among the 13 studies, there were four controlled trial studies that have the potential to be informative for future acute reference value derivation. Thus, the 2008 subchronic rat study used by OEHHA appears to still be the most appropriate study for chronic reference value derivation. In addition, advances in dosimetric modeling for gases, including new evidence pertinent to acrolein, could be considered when updating existing acrolein toxicity values. Evidence mapping is a very useful tool to assess the need for updating an assessment based on understanding the potential impact of new studies on revising an existing health reference value. In this case example, the focus was to identify studies suitable for chronic exposure dose-response analysis, while also identifying studies that may be important to consider for acute exposure scenarios, hazard identification, or for future research. This allows the evidence map to be a useful resource for a range of decision-making contexts. Specialized systematic review software increased the efficiency of the process in terms of human resources and time to conduct the analysis.","The environmental health community needs transparent, methodologically rigorous, and rapid approaches for updating human health risk assessments. These assessments often contain reference values for cancer and/or noncancer effects. Increasingly, the use of systematic review methods are preferred when developing these assessments. Systematic evidence maps are a type of analysis that has the potential to be very helpful in the update process, especially when combined with machine-learning software advances designed to expedite the process of conducting a review. To evaluate the applicability of evidence mapping to determine whether new evidence is likely to result in a change to an existing health reference value, using inhalation exposure to the air pollutant acrolein as a case example. New literature published since the 2008 California Environmental Protection Agency's Office of Environmental Health Hazard Assessment (OEHHA) Reference Exposure Level (REL) for acrolein was assessed. Systematic review methods were used to search the literature and screening included the use of machine-learning software. The Populations, Exposures, Comparators and Outcomes (PECO) criteria were kept broad to identify studies that characterized acute and chronic exposure and could be informative for hazard characterization. Studies that met the PECO criteria after full-text review were briefly summarized before their suitability for chronic point of departure (POD) derivation and calculation of a reference value was considered. Studies considered potentially suitable underwent a targeted evaluation to determine their suitability for use in dose-response analysis. Over 15,000 studies were identified from scientific databases. Both machine-learning and manual screening processes were used to identify 60 studies considered PECO-relevant after full-text review. Most of these PECO-relevant studies were short-term exposure animal studies (acute or less than 1Â month of exposure) and considered less suitable for deriving a chronic reference value when compared to the subchronic study in rats used in the 2008 OEHHA assessment. Thirteen epidemiological studies were identified but had limitations in the exposure assessment that made them less suitable for dose-response compared to the subchronic rat study. Among the 13 studies, there were four controlled trial studies that have the potential to be informative for future acute reference value derivation. Thus, the 2008 subchronic rat study used by OEHHA appears to still be the most appropriate study for chronic reference value derivation. In addition, advances in dosimetric modeling for gases, including new evidence pertinent to acrolein, could be considered when updating existing acrolein toxicity values. Evidence mapping is a very useful tool to assess the need for updating an assessment based on understanding the potential impact of new studies on revising an existing health reference value. In this case example, the focus was to identify studies suitable for chronic exposure dose-response analysis, while also identifying studies that may be important to consider for acute exposure scenarios, hazard identification, or for future research. This allows the evidence map to be a useful resource for a range of decision-making contexts. Specialized systematic review software increased the efficiency of the process in terms of human resources and time to conduct the analysis.","Keshava, Davis, Stanek, Thayer, Galizia, Keshava, Gift, Vulimiri, Woodall, Gigot, Garcia, Greenhalgh, Schulz, Volkoff, Camargo, Persad","Keshava, Davis, Stanek, Thayer, Galizia, Keshava, Gift, Vulimiri, Woodall, Gigot, Garcia, Greenhalgh, Schulz, Volkoff, Camargo, Persad",https://doi.org/10.1016/j.envint.2020.105956,https://doi.org/10.1016/j.envint.2020.105956,2021-08-03
16458.0,pubmed,pubmed,Statistical inference for natural language processing algorithms with a demonstration using type 2 diabetes prediction from electronic health record notes,Statistical inference for natural language processing algorithms with a demonstration using type 2 diabetes prediction from electronic health record notes,"The pointwise mutual information statistic (PMI), which measures how often two words occur together in a document corpus, is a cornerstone of recently proposed popular natural language processing algorithms such as word2vec. PMI and word2vec reveal semantic relationships between words and can be helpful in a range of applications such as document indexing, topic analysis, or document categorization. We use probability theory to demonstrate the relationship between PMI and word2vec. We use the theoretical results to demonstrate how the PMI can be modeled and estimated in a simple and straight forward manner. We further describe how one can obtain standard error estimates that account for within-patient clustering that arises from patterns of repeated words within a patient's health record due to a unique health history. We then demonstrate the usefulness of PMI on the problem of predictive identification of disease from free text notes of electronic health records. Specifically, we use our methods to distinguish those with and without type 2 diabetes mellitus in electronic health record free text data using over 400Ã¢â‚¬â€°000 clinical notes from an academic medicalÃ‚Â center.","The pointwise mutual information statistic (PMI), which measures how often two words occur together in a document corpus, is a cornerstone of recently proposed popular natural language processing algorithms such as word2vec. PMI and word2vec reveal semantic relationships between words and can be helpful in a range of applications such as document indexing, topic analysis, or document categorization. We use probability theory to demonstrate the relationship between PMI and word2vec. We use the theoretical results to demonstrate how the PMI can be modeled and estimated in a simple and straight forward manner. We further describe how one can obtain standard error estimates that account for within-patient clustering that arises from patterns of repeated words within a patient's health record due to a unique health history. We then demonstrate the usefulness of PMI on the problem of predictive identification of disease from free text notes of electronic health records. Specifically, we use our methods to distinguish those with and without type 2 diabetes mellitus in electronic health record free text data using over 400â€‰000 clinical notes from an academic medicalÂ center.","Egleston, Bai, Bleicher, Taylor, Lutz, Vucetic","Egleston, Bai, Bleicher, Taylor, Lutz, Vucetic",https://doi.org/10.1111/biom.13338,https://doi.org/10.1111/biom.13338,2021-08-03
16459.0,pubmed,pubmed,"An online, interactive, screen-based simulator for learning basic EEG interpretation","An online, interactive, screen-based simulator for learning basic EEG interpretation","Develop and pilot test a simulator that presents ten commonly encountered representative clinical contexts for trainees to learn basic electroencephalogram (EEG) interpretation skills. We created an interactive web-based training simulator that allows self-paced, asynchronous learning and assessment of basic EEG interpretation skills. The simulator uses the information retrieval process via a free-response text box to enhance learning. Ten scenarios were created that present dynamic (scrolling) EEG tracings resembling the clinical setting, followed by questions with free-text answers. The answer was checked against an accepted word/phrase list. The simulator has been used by 76 trainees in total. We report pilot study results from the University of Florida's neurology residents (NÃ¢â‚¬â€°=Ã¢â‚¬â€°24). Total percent correct for each scenario and average percent correct for all scenarios were calculated and correlated with most recent In-training Examination (ITE) and United States Medical License Examination (USMLE) scores. Neurology residents' mean percent correct scenario scores ranged from 27.1-86.0% with an average scenario score of 61.2% Ã‚Â±Ã¢â‚¬â€°7.7. We showed a moderately strong correlation rÃ¢â‚¬â€°=Ã¢â‚¬â€°0.49 between the ITE and the average scenario score. We developed an online interactive EEG interpretation simulator to review basic EEG content and assess interpretation skills using an active retrieval approach. The pilot study showed a moderately strong correlation rÃ¢â‚¬â€°=Ã¢â‚¬â€°0.49 between the ITE and the average scenario score. Since the ITE is a measure of clinical practice, this is evidence that the simulator can provide self-directed instruction and shows promise as a tool for assessment of EEG knowledge.","Develop and pilot test a simulator that presents ten commonly encountered representative clinical contexts for trainees to learn basic electroencephalogram (EEG) interpretation skills. We created an interactive web-based training simulator that allows self-paced, asynchronous learning and assessment of basic EEG interpretation skills. The simulator uses the information retrieval process via a free-response text box to enhance learning. Ten scenarios were created that present dynamic (scrolling) EEG tracings resembling the clinical setting, followed by questions with free-text answers. The answer was checked against an accepted word/phrase list. The simulator has been used by 76 trainees in total. We report pilot study results from the University of Florida's neurology residents (Nâ€‰=â€‰24). Total percent correct for each scenario and average percent correct for all scenarios were calculated and correlated with most recent In-training Examination (ITE) and United States Medical License Examination (USMLE) scores. Neurology residents' mean percent correct scenario scores ranged from 27.1-86.0% with an average scenario score of 61.2% Â±â€‰7.7. We showed a moderately strong correlation râ€‰=â€‰0.49 between the ITE and the average scenario score. We developed an online interactive EEG interpretation simulator to review basic EEG content and assess interpretation skills using an active retrieval approach. The pilot study showed a moderately strong correlation râ€‰=â€‰0.49 between the ITE and the average scenario score. Since the ITE is a measure of clinical practice, this is evidence that the simulator can provide self-directed instruction and shows promise as a tool for assessment of EEG knowledge.","Fahy, Cibula, Johnson, Cooper, Lizdas, Gravenstein, Lampotang","Fahy, Cibula, Johnson, Cooper, Lizdas, Gravenstein, Lampotang",https://doi.org/10.1007/s10072-020-04610-3,https://doi.org/10.1007/s10072-020-04610-3,2021-08-03
16460.0,pubmed,pubmed,Prevalence of ÃŽÂ±<sub>1</sub>-antitrypsin PiZZ genotypes in patients with COPD in Europe: a systematic review,Prevalence of Î±<sub>1</sub>-antitrypsin PiZZ genotypes in patients with COPD in Europe: a systematic review,"The percentage of ÃŽÂ±<sub>1</sub>-antitrypsin protease inhibitor ZZ (PiZZ) genotypes in patients with COPD is controversial, with large differences among various studies. We aimed to estimate the prevalence of PiZZ in COPD patients from 20 European countries with available data, according to the number of PiZZ and COPD individuals in each country.A systematic review was conducted to select European countries with reliable data on the prevalence of PiZZ and COPD. We created a database with the following data: 1) total population and population aged Ã¢â€°Â¥40Ã¢â‚¬â€¦years according to the Eurostat database; 2) number and 95% CI of PiZZ patients aged Ã¢â€°Â¥40Ã¢â‚¬â€¦years; 3) application of a conversion factor of genetic penetrance of 60%; 4) number of COPD individuals, with 95% CI, aged Ã¢â€°Â¥40Ã¢â‚¬â€¦years; and 5) calculation of the PiZZ/COPD ratio. Finally, results were presented using an Inverse Distance Weighted Interpolation map.We found 36Ã¢â‚¬Å 298 (95% CI 23Ã¢â‚¬Å 643-56Ã¢â‚¬Å 594) PiZZ individuals at high risk and 30Ã¢â‚¬Å 849Ã¢â‚¬Å 709 (95% CI 21Ã¢â‚¬Å 411Ã¢â‚¬Å 293-40Ã¢â‚¬Å 344Ã¢â‚¬Å 496) COPD patients, with a PiZZ/COPD ratio of 0.12% (range 0.08-0.24%), and a prevalence of 1 out of 408 in Northern, 1 out of 944 in Western, 1 out of 1051 in Central, 1 out of 711 in Southern, and 1 out of 1274 in Eastern Europe.These data may be useful to plan strategies for future research and diagnosis, and to rationalise the available therapeutic resources.","The percentage of Î±<sub>1</sub>-antitrypsin protease inhibitor ZZ (PiZZ) genotypes in patients with COPD is controversial, with large differences among various studies. We aimed to estimate the prevalence of PiZZ in COPD patients from 20 European countries with available data, according to the number of PiZZ and COPD individuals in each country.A systematic review was conducted to select European countries with reliable data on the prevalence of PiZZ and COPD. We created a database with the following data: 1) total population and population aged â‰¥40â€…years according to the Eurostat database; 2) number and 95% CI of PiZZ patients aged â‰¥40â€…years; 3) application of a conversion factor of genetic penetrance of 60%; 4) number of COPD individuals, with 95% CI, aged â‰¥40â€…years; and 5) calculation of the PiZZ/COPD ratio. Finally, results were presented using an Inverse Distance Weighted Interpolation map.We found 36â€Š298 (95% CI 23â€Š643-56â€Š594) PiZZ individuals at high risk and 30â€Š849â€Š709 (95% CI 21â€Š411â€Š293-40â€Š344â€Š496) COPD patients, with a PiZZ/COPD ratio of 0.12% (range 0.08-0.24%), and a prevalence of 1 out of 408 in Northern, 1 out of 944 in Western, 1 out of 1051 in Central, 1 out of 711 in Southern, and 1 out of 1274 in Eastern Europe.These data may be useful to plan strategies for future research and diagnosis, and to rationalise the available therapeutic resources.","Blanco, Diego, Bueno, PÃƒÂ©rez-Holanda, Casas-Maldonado, Miravitlles","Blanco, Diego, Bueno, PÃ©rez-Holanda, Casas-Maldonado, Miravitlles",https://doi.org/10.1183/16000617.0014-2020,https://doi.org/10.1183/16000617.0014-2020,2021-08-03
16461.0,pubmed,pubmed,Clinician-centric diagnosis of rare genetic diseases: performance of a gene pertinence metric in decision support for clinicians,Clinician-centric diagnosis of rare genetic diseases: performance of a gene pertinence metric in decision support for clinicians,"In diagnosis of rare genetic diseases we face a decision as to the degree to which the sequencing lab offers one or more diagnoses based on clinical input provided by the clinician, or the clinician reaches a diagnosis based on the complete set of variants provided by the lab. We tested a software approach to assist the clinician in making the diagnosis based on clinical findings and an annotated genomic variant table, using cases already solved using less automated processes. For the 81 cases studied (involving 216 individuals), 70 had genetic abnormalities with phenotypes previously described in the literature, and 11 were not described in the literature at the time of analysis (&quot;discovery genes&quot;). These included cases beyond a trio, including ones with different variants in the same gene. In 100% of cases the abnormality was recognized. Of the 70, the abnormality was ranked #1 in 94% of cases, with an average rank 1.1 for all cases. Large CNVs could be analyzed in an integrated analysis, performed in 24 of the cases. The process is rapid enough to allow for periodic reanalysis of unsolved cases. A clinician-friendly environment for clinical correlation can be provided to clinicians who are best positioned to have the clinical information needed for this interpretation.","In diagnosis of rare genetic diseases we face a decision as to the degree to which the sequencing lab offers one or more diagnoses based on clinical input provided by the clinician, or the clinician reaches a diagnosis based on the complete set of variants provided by the lab. We tested a software approach to assist the clinician in making the diagnosis based on clinical findings and an annotated genomic variant table, using cases already solved using less automated processes. For the 81 cases studied (involving 216 individuals), 70 had genetic abnormalities with phenotypes previously described in the literature, and 11 were not described in the literature at the time of analysis (""discovery genes""). These included cases beyond a trio, including ones with different variants in the same gene. In 100% of cases the abnormality was recognized. Of the 70, the abnormality was ranked #1 in 94% of cases, with an average rank 1.1 for all cases. Large CNVs could be analyzed in an integrated analysis, performed in 24 of the cases. The process is rapid enough to allow for periodic reanalysis of unsolved cases. A clinician-friendly environment for clinical correlation can be provided to clinicians who are best positioned to have the clinical information needed for this interpretation.","Segal, George, Waltman, El-Hattab, James, Stanley, Gleeson","Segal, George, Waltman, El-Hattab, James, Stanley, Gleeson",https://doi.org/10.1186/s13023-020-01461-1,https://doi.org/10.1186/s13023-020-01461-1,2021-08-03
16464.0,pubmed,pubmed,Natural language processing and machine learning to enable automatic extraction and classification of patients' smoking status from electronic medical records,Natural language processing and machine learning to enable automatic extraction and classification of patients' smoking status from electronic medical records,"The electronic medical record (EMR) offers unique possibilities for clinical research, but some important patient attributes are not readily available due to its unstructured properties. We applied text mining using machine learning to enable automatic classification of unstructured information on smoking status from Swedish EMR data. Data on patients' smoking status from EMRs were used to develop 32 different predictive models that were trained using Weka, changing sentence frequency, classifier type, tokenization, and attribute selection in a database of 85,000 classified sentences. The models were evaluated using F-score and accuracy based on out-of-sample test data including 8500 sentences. The error weight matrix was used to select the best model, assigning a weight to each type of misclassification and applying it to the model confusion matrices. The best performing model was then compared to a rule-based method. The best performing model was based on the Support Vector Machine (SVM) Sequential Minimal Optimization (SMO) classifier using a combination of unigrams and bigrams as tokens. Sentence frequency and attributes selection did not improve model performance. SMO achieved 98.14% accuracy and 0.981Ã¢â‚¬â€°F-score versus 79.32% and 0.756 for the rule-based model. A model using machine-learning algorithms to automatically classify patients' smoking status was successfully developed. Such algorithms may enable automatic assessment of smoking status and other unstructured data directly from EMRs without manual classification of complete case notes.","The electronic medical record (EMR) offers unique possibilities for clinical research, but some important patient attributes are not readily available due to its unstructured properties. We applied text mining using machine learning to enable automatic classification of unstructured information on smoking status from Swedish EMR data. Data on patients' smoking status from EMRs were used to develop 32 different predictive models that were trained using Weka, changing sentence frequency, classifier type, tokenization, and attribute selection in a database of 85,000 classified sentences. The models were evaluated using F-score and accuracy based on out-of-sample test data including 8500 sentences. The error weight matrix was used to select the best model, assigning a weight to each type of misclassification and applying it to the model confusion matrices. The best performing model was then compared to a rule-based method. The best performing model was based on the Support Vector Machine (SVM) Sequential Minimal Optimization (SMO) classifier using a combination of unigrams and bigrams as tokens. Sentence frequency and attributes selection did not improve model performance. SMO achieved 98.14% accuracy and 0.981â€‰F-score versus 79.32% and 0.756 for the rule-based model. A model using machine-learning algorithms to automatically classify patients' smoking status was successfully developed. Such algorithms may enable automatic assessment of smoking status and other unstructured data directly from EMRs without manual classification of complete case notes.","Caccamisi, JÃƒÂ¸rgensen, Dalianis, Rosenlund","Caccamisi, JÃ¸rgensen, Dalianis, Rosenlund",https://doi.org/10.1080/03009734.2020.1792010,https://doi.org/10.1080/03009734.2020.1792010,2021-08-03
16469.0,pubmed,pubmed,Ginseng and Cancer-Related Fatigue: A Systematic Review of Clinical Trials,Ginseng and Cancer-Related Fatigue: A Systematic Review of Clinical Trials,"The data on the effect of ginseng on general fatigue were previously reviewed. However, there is limited data on the effect of various types of ginseng on cancer-related fatigue (CRF). CRF is one of the most pervasive symptoms of cancer and cancer treatment. The primary objective of the current study was to systematically review trials investigating the safety and efficacy of three different types of ginseng separately used in the treatment protocol for patients with CRF. We searched the available online databases for relevant publications up to October 2019. Data were independently extracted by two reviewers. We assessed the risk of bias using the Cochrane Collaboration Review Manager (RevMan, version 5.3) and reported the results in a narrative summary. A total of 210 studies were identified by the initial search, from which seven clinical trials and one retrospective study were included in this systematic review. A total of two clinical trials and one retrospective review examined the impact of <i>American ginseng</i> on CRF symptoms, three studies tested <i>Asian ginseng</i>, and two trials were conducted using <i>Korean ginseng</i>. The quality of the selected studies varied greatly. All three types of ginseng were tolerated well with few low-grade adverse events. <i>American ginseng</i>, containing more than 5% ginsenosides, consumed at the dosage of 2000Ã¢â‚¬â€°mg/day for up to eight weeks significantly reduced fatigue. <i>Asian ginseng</i>, containing Ã¢â€°Â¥ 7% ginsenosides, relieved symptoms of fatigue at the dosage of 400Ã¢â‚¬â€°mg/day in the majority of patients with CRF. <i>Korean ginseng</i>, consumed at the dosage of 3000Ã¢â‚¬â€°mg/day for 12Ã¢â‚¬â€°weeks, decreased symptoms of CRF. Although our findings support the safety and effectiveness of ginseng in the treatment of CRF, the number of high-quality studies is not adequate to adopt ginseng as a standard treatment option for CRF.","The data on the effect of ginseng on general fatigue were previously reviewed. However, there is limited data on the effect of various types of ginseng on cancer-related fatigue (CRF). CRF is one of the most pervasive symptoms of cancer and cancer treatment. The primary objective of the current study was to systematically review trials investigating the safety and efficacy of three different types of ginseng separately used in the treatment protocol for patients with CRF. We searched the available online databases for relevant publications up to October 2019. Data were independently extracted by two reviewers. We assessed the risk of bias using the Cochrane Collaboration Review Manager (RevMan, version 5.3) and reported the results in a narrative summary. A total of 210 studies were identified by the initial search, from which seven clinical trials and one retrospective study were included in this systematic review. A total of two clinical trials and one retrospective review examined the impact of <i>American ginseng</i> on CRF symptoms, three studies tested <i>Asian ginseng</i>, and two trials were conducted using <i>Korean ginseng</i>. The quality of the selected studies varied greatly. All three types of ginseng were tolerated well with few low-grade adverse events. <i>American ginseng</i>, containing more than 5% ginsenosides, consumed at the dosage of 2000â€‰mg/day for up to eight weeks significantly reduced fatigue. <i>Asian ginseng</i>, containing â‰¥ 7% ginsenosides, relieved symptoms of fatigue at the dosage of 400â€‰mg/day in the majority of patients with CRF. <i>Korean ginseng</i>, consumed at the dosage of 3000â€‰mg/day for 12â€‰weeks, decreased symptoms of CRF. Although our findings support the safety and effectiveness of ginseng in the treatment of CRF, the number of high-quality studies is not adequate to adopt ginseng as a standard treatment option for CRF.","Sadeghian, Rahmani, Zendehdel, Hosseini, Zare Javid","Sadeghian, Rahmani, Zendehdel, Hosseini, Zare Javid",https://doi.org/10.1080/01635581.2020.1795691,https://doi.org/10.1080/01635581.2020.1795691,2021-08-03
16472.0,pubmed,pubmed,Evaluation of discrete orthogonal versus polar Stockwell Transform for local multi-resolution texture analysis using brain MRI of multiple sclerosis patients,Evaluation of discrete orthogonal versus polar Stockwell Transform for local multi-resolution texture analysis using brain MRI of multiple sclerosis patients,"The Stockwell Transform has the potential to perform multi-resolution texture analysis in magnetic resonance imaging (MRI). However, it is computationally intensive and memory demanding. The polar Stockwell Transform (PST) is rotation-invariant and relatively memory efficient, but still computationally demanding. The new Discrete Orthogonal Stockwell Transform (DOST) appears to have addressed both the computation and storage challenges; however, its utility in localized texture analysis remains unclear. Our goal was to investigate the theory and texture analysis ability of the DOST versus PST using both synthetic and MR images, and explore the relative importance of the associated texture features using a simple classification example based on clinical brain MRI of six multiple sclerosis patients. MRI texture analysis focused on FLAIR images, and the classification used a machine learning algorithm, random forest, that differentiated regions of interest (ROIs) into 2 classes: white matter lesions, and the contralateral normal-appearing white matter (control). Our results showed that the PST features had a greater ability in detecting subtle changes in image structure than the DOST and polar-index DOST (PDOST). Quantitatively, based on 187 lesion and 187 control ROIs, both the PST and the rotation-invariant radial PST performed better in the classification than the DOST and PDOST, where the latter were no better than guessing (pÃ‚Â =Ã‚Â 0.65 and 0.98). Further analysis using a hierarchical random forest showed that combining MRI signal intensity with the PST or DOST predictions increased the classification performance, with the accuracy, sensitivity, and specificity all improved to &gt;85% in the tests. Collectively, the DOST is less competitive than the PST in localized image texture analysis. The PST features may help with texture-based lesion classification in MS based on clinical brain MRI scans following further verification.","The Stockwell Transform has the potential to perform multi-resolution texture analysis in magnetic resonance imaging (MRI). However, it is computationally intensive and memory demanding. The polar Stockwell Transform (PST) is rotation-invariant and relatively memory efficient, but still computationally demanding. The new Discrete Orthogonal Stockwell Transform (DOST) appears to have addressed both the computation and storage challenges; however, its utility in localized texture analysis remains unclear. Our goal was to investigate the theory and texture analysis ability of the DOST versus PST using both synthetic and MR images, and explore the relative importance of the associated texture features using a simple classification example based on clinical brain MRI of six multiple sclerosis patients. MRI texture analysis focused on FLAIR images, and the classification used a machine learning algorithm, random forest, that differentiated regions of interest (ROIs) into 2 classes: white matter lesions, and the contralateral normal-appearing white matter (control). Our results showed that the PST features had a greater ability in detecting subtle changes in image structure than the DOST and polar-index DOST (PDOST). Quantitatively, based on 187 lesion and 187 control ROIs, both the PST and the rotation-invariant radial PST performed better in the classification than the DOST and PDOST, where the latter were no better than guessing (pÂ =Â 0.65 and 0.98). Further analysis using a hierarchical random forest showed that combining MRI signal intensity with the PST or DOST predictions increased the classification performance, with the accuracy, sensitivity, and specificity all improved to &gt;85% in the tests. Collectively, the DOST is less competitive than the PST in localized image texture analysis. The PST features may help with texture-based lesion classification in MS based on clinical brain MRI scans following further verification.","Pridham, Oladosu, Zhang","Pridham, Oladosu, Zhang",https://doi.org/10.1016/j.mri.2020.07.007,https://doi.org/10.1016/j.mri.2020.07.007,2021-08-03
16478.0,pubmed,pubmed,Relationship between bariatric surgery and dental erosion: aÃ‚Â systematicÃ‚Â review,Relationship between bariatric surgery and dental erosion: aÂ systematicÂ review,"Bariatric surgery can have several oral repercussions, including tooth erosion due to decreased salivary flow associated with the action of acidic pH and behavioral modifications in the diet that lead to the dissolution of mineralized dental tissues. This systematic review aimed to evaluate whether bariatric surgery presented a greater risk of dental erosion. Dentistry School, Pernambuco University, Camaragibe, Pernambuco, Brazil. This review was based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyzes (PRISMA) and registered at the International Prospective Registry of Systematic Reviews (CRD42019124960). A search was performed in the PubMed/MEDLINE, Web of Science, and Cochrane Library databases using the following descriptors: &quot;dental erosion OR tooth wear OR oral health OR dental wear OR tooth erosion OR salivary flow AND bariatric surgery OR gastrectomy OR obesity surgery&quot;. The review included 553 articles (after exclusion of duplicates) submitted for title and abstract reading, of which 24 were selected for full text analysis. Five articles fulfilling the eligibility criteria were included in the qualitative and quantitative analysis review. Because of high heterogeneity of the studies, meta-analysis could not be performed. Based on the review, we concluded that patients undergoing bariatric surgery had a higher incidence of dental erosion. All studies presented a high degree of dental erosion in patients submitted to bariatric surgery (P &lt; .05).","Bariatric surgery can have several oral repercussions, including tooth erosion due to decreased salivary flow associated with the action of acidic pH and behavioral modifications in the diet that lead to the dissolution of mineralized dental tissues. This systematic review aimed to evaluate whether bariatric surgery presented a greater risk of dental erosion. Dentistry School, Pernambuco University, Camaragibe, Pernambuco, Brazil. This review was based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyzes (PRISMA) and registered at the International Prospective Registry of Systematic Reviews (CRD42019124960). A search was performed in the PubMed/MEDLINE, Web of Science, and Cochrane Library databases using the following descriptors: ""dental erosion OR tooth wear OR oral health OR dental wear OR tooth erosion OR salivary flow AND bariatric surgery OR gastrectomy OR obesity surgery"". The review included 553 articles (after exclusion of duplicates) submitted for title and abstract reading, of which 24 were selected for full text analysis. Five articles fulfilling the eligibility criteria were included in the qualitative and quantitative analysis review. Because of high heterogeneity of the studies, meta-analysis could not be performed. Based on the review, we concluded that patients undergoing bariatric surgery had a higher incidence of dental erosion. All studies presented a high degree of dental erosion in patients submitted to bariatric surgery (P &lt; .05).","Quintella, Farias, SoutoMaior, Casado, LeÃƒÂ£o, Moraes","Quintella, Farias, SoutoMaior, Casado, LeÃ£o, Moraes",https://doi.org/10.1016/j.soard.2020.04.044,https://doi.org/10.1016/j.soard.2020.04.044,2021-08-03
16479.0,pubmed,pubmed,Medical data science in rhinology: Background and implications for clinicians,Medical data science in rhinology: Background and implications for clinicians,"An important challenge of big data is using complex information networks to provide useful clinical information. Recently, machine learning, and particularly deep learning, has enabled rapid advances in clinical practice. The application of artificial intelligence (AI) and machine learning (ML) in rhinology is an increasingly relevant topic. We review the literature and provide a detailed overview of the recent advances in AI and ML as applied to rhinology. Also, we discuss both the significant benefits of this work as well as the challenges in the implementation and acceptance of these methods for clinical purposes. We aimed to identify and explain published studies on the use of AI and ML in rhinology based on PubMed, Scopus, and Google searches. The search string &quot;nasal OR respiratory AND artificial intelligence OR machine learning&quot; was used. Most of the studies covered areas of paranasal sinuses radiology, including allergic rhinitis, chronic rhinitis, computed tomography scans, and nasal cytology. Cluster analysis and convolutional neural networks (CNNs) were mainly used in studies related to rhinology. AI is increasingly affecting healthcare research, and ML technology has been used in studies of chronic rhinitis and allergic rhinitis, providing some exciting new research modalities. AI is especially useful when there is no conclusive evidence to aid decision making. ML can help doctors make clinical decisions, but it does not entirely replace doctors. However, when critically evaluating studies using this technique, rhinologists must take into account the limitations of its applications and use.","An important challenge of big data is using complex information networks to provide useful clinical information. Recently, machine learning, and particularly deep learning, has enabled rapid advances in clinical practice. The application of artificial intelligence (AI) and machine learning (ML) in rhinology is an increasingly relevant topic. We review the literature and provide a detailed overview of the recent advances in AI and ML as applied to rhinology. Also, we discuss both the significant benefits of this work as well as the challenges in the implementation and acceptance of these methods for clinical purposes. We aimed to identify and explain published studies on the use of AI and ML in rhinology based on PubMed, Scopus, and Google searches. The search string ""nasal OR respiratory AND artificial intelligence OR machine learning"" was used. Most of the studies covered areas of paranasal sinuses radiology, including allergic rhinitis, chronic rhinitis, computed tomography scans, and nasal cytology. Cluster analysis and convolutional neural networks (CNNs) were mainly used in studies related to rhinology. AI is increasingly affecting healthcare research, and ML technology has been used in studies of chronic rhinitis and allergic rhinitis, providing some exciting new research modalities. AI is especially useful when there is no conclusive evidence to aid decision making. ML can help doctors make clinical decisions, but it does not entirely replace doctors. However, when critically evaluating studies using this technique, rhinologists must take into account the limitations of its applications and use.","Jun, Jung, Lee","Jun, Jung, Lee",https://doi.org/10.1016/j.amjoto.2020.102627,https://doi.org/10.1016/j.amjoto.2020.102627,2021-08-03
16480.0,pubmed,pubmed,Manual segmentation versus semi-automated segmentation forÃ‚Â quantifying vestibular schwannoma volume on MRI,Manual segmentation versus semi-automated segmentation forÂ quantifying vestibular schwannoma volume on MRI,"Management of vestibular schwannoma (VS) is based on tumour size as observed on T1 MRI scans with contrast agent injection. The current clinical practice is to measure the diameter of the tumour in its largest dimension. It has been shown that volumetric measurement is more accurate and more reliable as a measure of VS size. The reference approach to achieve such volumetry is to manually segment the tumour, which is a time intensive task. We suggest that semi-automated segmentation may be a clinically applicable solution to this problem and that it could replace linear measurements as the clinical standard. Using high-quality software available for academic purposes, we ran a comparative study of manual versus semi-automated segmentation of VS on MRI with 5 clinicians and scientists. We gathered both quantitative and qualitative data to compare the two approaches; including segmentation time, segmentation effort and segmentation accuracy. We found that the selected semi-automated segmentation approach is significantly faster (167Ã‚Â s vs 479Ã‚Â s, [Formula: see text]), less temporally and physically demanding and has approximately equal performance when compared with manual segmentation, with some improvements in accuracy. There were some limitations, including algorithmic unpredictability and error, which produced more frustration and increased mental effort in comparison with manual segmentation. We suggest that semi-automated segmentation could be applied clinically for volumetric measurement of VS on MRI. In future, the generic software could be refined for use specifically for VS segmentation, thereby improving accuracy.","Management of vestibular schwannoma (VS) is based on tumour size as observed on T1 MRI scans with contrast agent injection. The current clinical practice is to measure the diameter of the tumour in its largest dimension. It has been shown that volumetric measurement is more accurate and more reliable as a measure of VS size. The reference approach to achieve such volumetry is to manually segment the tumour, which is a time intensive task. We suggest that semi-automated segmentation may be a clinically applicable solution to this problem and that it could replace linear measurements as the clinical standard. Using high-quality software available for academic purposes, we ran a comparative study of manual versus semi-automated segmentation of VS on MRI with 5 clinicians and scientists. We gathered both quantitative and qualitative data to compare the two approaches; including segmentation time, segmentation effort and segmentation accuracy. We found that the selected semi-automated segmentation approach is significantly faster (167Â s vs 479Â s, [Formula: see text]), less temporally and physically demanding and has approximately equal performance when compared with manual segmentation, with some improvements in accuracy. There were some limitations, including algorithmic unpredictability and error, which produced more frustration and increased mental effort in comparison with manual segmentation. We suggest that semi-automated segmentation could be applied clinically for volumetric measurement of VS on MRI. In future, the generic software could be refined for use specifically for VS segmentation, thereby improving accuracy.","McGrath, Li, Dorent, Bradford, Saeed, Bisdas, Ourselin, Shapey, Vercauteren","McGrath, Li, Dorent, Bradford, Saeed, Bisdas, Ourselin, Shapey, Vercauteren",https://doi.org/10.1007/s11548-020-02222-y,https://doi.org/10.1007/s11548-020-02222-y,2021-08-03
16481.0,pubmed,pubmed,Use of artificial intelligence in diagnosis of head and neck precancerous and cancerous lesions: A systematic review,Use of artificial intelligence in diagnosis of head and neck precancerous and cancerous lesions: A systematic review,"This systematic review analyses and describes the application and diagnostic accuracy of Artificial Intelligence (AI) methods used for detection and grading of potentially malignant (pre-cancerous) and cancerous head and neck lesions using whole slide images (WSI) of human tissue slides. Electronic databases MEDLINE via OVID, Scopus and Web of Science were searched between October 2009 - April 2020. Tailored search-strings were developed using database-specific terms. Studies were selected using a strict inclusion criterion following PRISMA Guidelines. Risk of bias assessment was conducted using a tailored QUADAS-2 tool. Out of 315 records, 11 fulfilled the inclusion criteria. AI-based methods were employed for analysis of specific histological features for oral epithelial dysplasia (nÃ‚Â =Ã‚Â 1), oral submucous fibrosis (nÃ‚Â =Ã‚Â 5), oral squamous cell carcinoma (nÃ‚Â =Ã‚Â 4) and oropharyngeal squamous cell carcinoma (nÃ‚Â =Ã‚Â 1). A combination of heuristics, supervised and unsupervised learning methods were employed, including more than 10 different classification and segmentation techniques. Most studies used uni-centric datasets (range 40-270 images) comprising small sub-images within WSI with accuracy between 79 and 100%. This review provides early evidence to support the potential application of supervised machine learning methods as a diagnostic aid for some oral potentially malignant and malignant lesions; however, there is a paucity of evidence using AI for diagnosis of other head and neck pathologies. Overall, the quality of evidence is low, with most studies showing a high risk of bias which is likely to have overestimated accuracy rates. This review highlights the need for development of state-of-the-art deep learning techniques in future head and neck research.","This systematic review analyses and describes the application and diagnostic accuracy of Artificial Intelligence (AI) methods used for detection and grading of potentially malignant (pre-cancerous) and cancerous head and neck lesions using whole slide images (WSI) of human tissue slides. Electronic databases MEDLINE via OVID, Scopus and Web of Science were searched between October 2009 - April 2020. Tailored search-strings were developed using database-specific terms. Studies were selected using a strict inclusion criterion following PRISMA Guidelines. Risk of bias assessment was conducted using a tailored QUADAS-2 tool. Out of 315 records, 11 fulfilled the inclusion criteria. AI-based methods were employed for analysis of specific histological features for oral epithelial dysplasia (nÂ =Â 1), oral submucous fibrosis (nÂ =Â 5), oral squamous cell carcinoma (nÂ =Â 4) and oropharyngeal squamous cell carcinoma (nÂ =Â 1). A combination of heuristics, supervised and unsupervised learning methods were employed, including more than 10 different classification and segmentation techniques. Most studies used uni-centric datasets (range 40-270 images) comprising small sub-images within WSI with accuracy between 79 and 100%. This review provides early evidence to support the potential application of supervised machine learning methods as a diagnostic aid for some oral potentially malignant and malignant lesions; however, there is a paucity of evidence using AI for diagnosis of other head and neck pathologies. Overall, the quality of evidence is low, with most studies showing a high risk of bias which is likely to have overestimated accuracy rates. This review highlights the need for development of state-of-the-art deep learning techniques in future head and neck research.","Mahmood, Shaban, Indave, Santos-Silva, Rajpoot, Khurram","Mahmood, Shaban, Indave, Santos-Silva, Rajpoot, Khurram",https://doi.org/10.1016/j.oraloncology.2020.104885,https://doi.org/10.1016/j.oraloncology.2020.104885,2021-08-03
16484.0,pubmed,pubmed,Clinical and preclinical evidence of somatosensory involvement in amyotrophic lateral sclerosis,Clinical and preclinical evidence of somatosensory involvement in amyotrophic lateral sclerosis,"Amyotrophic lateral sclerosis (ALS) is the most common motor neuron neurodegenerative disease. Although it has been classically considered as a disease limited to the motor system, there is increasing evidence for the involvement of other neural and non-neuronal systems. In this review, we will discuss currently existing literature regarding the involvement of the sensory system in ALS. Human studies have reported intradermic small fibre loss, sensory axonal predominant neuropathy, as well as somatosensory cortex hyperexcitability. In line with this, ALS animal studies have demonstrated the involvement of several sensory components. Specifically, they have highlighted the impairment of sensory-motor networks as a potential mechanism for the disease. The elucidation of these &quot;non-motor&quot; systems involvement, which might also be part of the degeneration process, should prompt the scientific community to re-consider ALS as a pure motor neuron disease, which may in turn result in more holistic research approaches.","Amyotrophic lateral sclerosis (ALS) is the most common motor neuron neurodegenerative disease. Although it has been classically considered as a disease limited to the motor system, there is increasing evidence for the involvement of other neural and non-neuronal systems. In this review, we will discuss currently existing literature regarding the involvement of the sensory system in ALS. Human studies have reported intradermic small fibre loss, sensory axonal predominant neuropathy, as well as somatosensory cortex hyperexcitability. In line with this, ALS animal studies have demonstrated the involvement of several sensory components. Specifically, they have highlighted the impairment of sensory-motor networks as a potential mechanism for the disease. The elucidation of these ""non-motor"" systems involvement, which might also be part of the degeneration process, should prompt the scientific community to re-consider ALS as a pure motor neuron disease, which may in turn result in more holistic research approaches. LINKED ARTICLES: This article is part of a themed issue on Neurochemistry in Japan. To view the other articles in this section visit http://onlinelibrary.wiley.com/doi/10.1111/bph.v178.6/issuetoc.","Riancho, Paz-Fajardo, LÃƒÂ³pez de MunaÃƒÂ­n","Riancho, Paz-Fajardo, LÃ³pez de MunaÃ­n",https://doi.org/10.1111/bph.15202,https://doi.org/10.1111/bph.15202,2021-08-03
16489.0,pubmed,pubmed,Machine Learning-Based Interpretation and Visualization of Nonlinear Interactions in Prostate Cancer Survival,Machine Learning-Based Interpretation and Visualization of Nonlinear Interactions in Prostate Cancer Survival,"Shapley additive explanation (SHAP) values represent a unified approach to interpreting predictions made by complex machine learning (ML) models, with superior consistency and accuracy compared with prior methods. We describe a novel application of SHAP values to the prediction of mortality risk in prostate cancer. Patients with nonmetastatic, node-negative prostate cancer, diagnosed between 2004 and 2015, were identified using the National Cancer Database. Model features were specified a priori: age, prostate-specific antigen (PSA), Gleason score, percent positive cores (PPC), comorbidity score, and clinical T stage. We trained a gradient-boosted tree model and applied SHAP values to model predictions. Open-source libraries in Python 3.7 were used for all analyses. We identified 372,808 patients meeting the inclusion criteria. When analyzing the interaction between PSA and Gleason score, we demonstrated consistency with the literature using the example of low-PSA, high-Gleason prostate cancer, recently identified as a unique entity with a poor prognosis. When analyzing the PPC-Gleason score interaction, we identified a novel finding of stronger interaction effects in patients with Gleason Ã¢â€°Â¥ 8 disease compared with Gleason 6-7 disease, particularly with PPC Ã¢â€°Â¥ 50%. Subsequent confirmatory linear analyses supported this finding: 5-year overall survival in Gleason Ã¢â€°Â¥ 8 patients was 87.7% with PPC &lt; 50% versus 77.2% with PPC Ã¢â€°Â¥ 50% (<i>P</i> &lt; .001), compared with 89.1% versus 86.0% in Gleason 7 patients (<i>P</i> &lt; .001), with a significant interaction term between PPC Ã¢â€°Â¥ 50% and Gleason Ã¢â€°Â¥ 8 (<i>P</i> &lt; .001). We describe a novel application of SHAP values for modeling and visualizing nonlinear interaction effects in prostate cancer. This ML-based approach is a promising technique with the potential to meaningfully improve risk stratification and staging systems.","Shapley additive explanation (SHAP) values represent a unified approach to interpreting predictions made by complex machine learning (ML) models, with superior consistency and accuracy compared with prior methods. We describe a novel application of SHAP values to the prediction of mortality risk in prostate cancer. Patients with nonmetastatic, node-negative prostate cancer, diagnosed between 2004 and 2015, were identified using the National Cancer Database. Model features were specified a priori: age, prostate-specific antigen (PSA), Gleason score, percent positive cores (PPC), comorbidity score, and clinical T stage. We trained a gradient-boosted tree model and applied SHAP values to model predictions. Open-source libraries in Python 3.7 were used for all analyses. We identified 372,808 patients meeting the inclusion criteria. When analyzing the interaction between PSA and Gleason score, we demonstrated consistency with the literature using the example of low-PSA, high-Gleason prostate cancer, recently identified as a unique entity with a poor prognosis. When analyzing the PPC-Gleason score interaction, we identified a novel finding of stronger interaction effects in patients with Gleason â‰¥ 8 disease compared with Gleason 6-7 disease, particularly with PPC â‰¥ 50%. Subsequent confirmatory linear analyses supported this finding: 5-year overall survival in Gleason â‰¥ 8 patients was 87.7% with PPC &lt; 50% versus 77.2% with PPC â‰¥ 50% (<i>P</i> &lt; .001), compared with 89.1% versus 86.0% in Gleason 7 patients (<i>P</i> &lt; .001), with a significant interaction term between PPC â‰¥ 50% and Gleason â‰¥ 8 (<i>P</i> &lt; .001). We describe a novel application of SHAP values for modeling and visualizing nonlinear interaction effects in prostate cancer. This ML-based approach is a promising technique with the potential to meaningfully improve risk stratification and staging systems.","Li, Shinde, Liu, Glaser, Lyou, Yuh, Wong, Amini","Li, Shinde, Liu, Glaser, Lyou, Yuh, Wong, Amini",https://doi.org/10.1200/CCI.20.00002,https://doi.org/10.1200/CCI.20.00002,2021-08-03
16500.0,pubmed,pubmed,Artificial intelligence to improve back pain outcomes and lessons learnt from clinical classification approaches: three systematic reviews,Artificial intelligence to improve back pain outcomes and lessons learnt from clinical classification approaches: three systematic reviews,"Artificial intelligence and machine learning (AI/ML) could enhance the ability to detect patterns of clinical characteristics in low-back pain (LBP) and guide treatment. We conducted three systematic reviews to address the following aims: (a) review the status of AI/ML research in LBP, (b) compare its status to that of two established LBP classification systems (STarT Back, McKenzie). AI/ML in LBP is in its infancy: 45 of 48 studies assessed sample sizes &lt;1000 people, 19 of 48 studies used Ã¢â€°Â¤5 parameters in models, 13 of 48 studies applied multiple models and attained high accuracy, 25 of 48 studies assessed the binary classification of LBP versus no-LBP only. Beyond the 48 studies using AI/ML for LBP classification, no studies examined use of AI/ML in prognosis prediction of specific sub-groups, and AI/ML techniques are yet to be implemented in guiding LBP treatment. In contrast, the STarT Back tool has been assessed for internal consistency, test-retest reliability, validity, pain and disability prognosis, and influence on pain and disability treatment outcomes. McKenzie has been assessed for inter- and intra-tester reliability, prognosis, and impact on pain and disability outcomes relative to other treatments. For AI/ML methods to contribute to the refinement of LBP (sub-)classification and guide treatment allocation, large data sets containing known and exploratory clinical features should be examined. There is also a need to establish reliability, validity, and prognostic capacity of AI/ML techniques in LBP as well as its ability to inform treatment allocation for improved patient outcomes and/or reduced healthcare costs.","Artificial intelligence and machine learning (AI/ML) could enhance the ability to detect patterns of clinical characteristics in low-back pain (LBP) and guide treatment. We conducted three systematic reviews to address the following aims: (a) review the status of AI/ML research in LBP, (b) compare its status to that of two established LBP classification systems (STarT Back, McKenzie). AI/ML in LBP is in its infancy: 45 of 48 studies assessed sample sizes &lt;1000 people, 19 of 48 studies used â‰¤5 parameters in models, 13 of 48 studies applied multiple models and attained high accuracy, 25 of 48 studies assessed the binary classification of LBP versus no-LBP only. Beyond the 48 studies using AI/ML for LBP classification, no studies examined use of AI/ML in prognosis prediction of specific sub-groups, and AI/ML techniques are yet to be implemented in guiding LBP treatment. In contrast, the STarT Back tool has been assessed for internal consistency, test-retest reliability, validity, pain and disability prognosis, and influence on pain and disability treatment outcomes. McKenzie has been assessed for inter- and intra-tester reliability, prognosis, and impact on pain and disability outcomes relative to other treatments. For AI/ML methods to contribute to the refinement of LBP (sub-)classification and guide treatment allocation, large data sets containing known and exploratory clinical features should be examined. There is also a need to establish reliability, validity, and prognostic capacity of AI/ML techniques in LBP as well as its ability to inform treatment allocation for improved patient outcomes and/or reduced healthcare costs.","Tagliaferri, Angelova, Zhao, Owen, Miller, Wilkin, Belavy","Tagliaferri, Angelova, Zhao, Owen, Miller, Wilkin, Belavy",https://doi.org/10.1038/s41746-020-0303-x,https://doi.org/10.1038/s41746-020-0303-x,2021-08-03
16506.0,pubmed,pubmed,Bone shadow segmentation from ultrasound data for orthopedic surgery using GAN,Bone shadow segmentation from ultrasound data for orthopedic surgery using GAN,"Real-time, two (2D) and three-dimensional (3D) ultrasound (US) has been investigated as a potential alternative to fluoroscopy imaging in various surgical and non-surgical orthopedic procedures. However, low signal to noise ratio, imaging artifacts and bone surfaces appearing several millimeters (mm) in thickness have hindered the wide spread adaptation of this safe imaging modality. Limited field of view and manual data collection cause additional problems during US-based orthopedic procedures. In order to overcome these limitations various bone segmentation and registration methods have been developed. Acoustic bone shadow is an important image artifact used to identify the presence of bone boundaries in the collected US data. Information about bone shadow region can be used (1) to guide the orthopedic surgeon or clinician to a standardized diagnostic viewing plane with minimal artifacts, (2) as a prior feature to improve bone segmentation and registration. In this work, we propose a computational method, based on a novel generative adversarial network (GAN) architecture, to segment bone shadow images from in vivo US scans in real-time. We also show how these segmented shadow images can be incorporated, as a proxy, to a multi-feature guided convolutional neural network (CNN) architecture for real-time and accurate bone surface segmentation. Quantitative and qualitative evaluation studies are performed on 1235 scans collected from 27 subjects using two different US machines. Finally, we provide qualitative and quantitative comparison results against state-of-the-art GANs. We have obtained mean dice coefficient (Ã‚Â± standard deviation) of [Formula: see text] ([Formula: see text]) for bone shadow segmentation, showing that the method is in close range with manual expert annotation. Statistical significant improvements against state-of-the-art GAN methods (paired t-test [Formula: see text]) is also obtained. Using the segmented bone shadow features average bone localization accuracy of 0.11Ã‚Â mm ([Formula: see text]) was achieved. Reported accurate and robust results make the proposed method promising for various orthopedic procedures. Although we did not investigate in this work, the segmented bone shadow images could also be used as an additional feature to improve accuracy of US-based registration methods. Further extensive validations are required in order to fully understand the clinical utility of the proposed method.","Real-time, two (2D) and three-dimensional (3D) ultrasound (US) has been investigated as a potential alternative to fluoroscopy imaging in various surgical and non-surgical orthopedic procedures. However, low signal to noise ratio, imaging artifacts and bone surfaces appearing several millimeters (mm) in thickness have hindered the wide spread adaptation of this safe imaging modality. Limited field of view and manual data collection cause additional problems during US-based orthopedic procedures. In order to overcome these limitations various bone segmentation and registration methods have been developed. Acoustic bone shadow is an important image artifact used to identify the presence of bone boundaries in the collected US data. Information about bone shadow region can be used (1) to guide the orthopedic surgeon or clinician to a standardized diagnostic viewing plane with minimal artifacts, (2) as a prior feature to improve bone segmentation and registration. In this work, we propose a computational method, based on a novel generative adversarial network (GAN) architecture, to segment bone shadow images from in vivo US scans in real-time. We also show how these segmented shadow images can be incorporated, as a proxy, to a multi-feature guided convolutional neural network (CNN) architecture for real-time and accurate bone surface segmentation. Quantitative and qualitative evaluation studies are performed on 1235 scans collected from 27 subjects using two different US machines. Finally, we provide qualitative and quantitative comparison results against state-of-the-art GANs. We have obtained mean dice coefficient (Â± standard deviation) of [Formula: see text] ([Formula: see text]) for bone shadow segmentation, showing that the method is in close range with manual expert annotation. Statistical significant improvements against state-of-the-art GAN methods (paired t-test [Formula: see text]) is also obtained. Using the segmented bone shadow features average bone localization accuracy of 0.11Â mm ([Formula: see text]) was achieved. Reported accurate and robust results make the proposed method promising for various orthopedic procedures. Although we did not investigate in this work, the segmented bone shadow images could also be used as an additional feature to improve accuracy of US-based registration methods. Further extensive validations are required in order to fully understand the clinical utility of the proposed method.","Alsinan, Patel, Hacihaliloglu","Alsinan, Patel, Hacihaliloglu",https://doi.org/10.1007/s11548-020-02221-z,https://doi.org/10.1007/s11548-020-02221-z,2021-08-03
16508.0,pubmed,pubmed,Repurpose Open Data to Discover Therapeutics for COVID-19 Using Deep Learning,Repurpose Open Data to Discover Therapeutics for COVID-19 Using Deep Learning,"There have been more than 2.2 million confirmed cases and over 120Ã¢â‚¬Â¯000 deaths from the human coronavirus disease 2019 (COVID-19) pandemic, caused by the novel severe acute respiratory syndrome coronavirus (SARS-CoV-2), in the United States alone. However, there is currently a lack of proven effective medications against COVID-19. Drug repurposing offers a promising route for the development of prevention and treatment strategies for COVID-19. This study reports an integrative, network-based deep-learning methodology to identify repurposable drugs for COVID-19 (termed CoV-KGE). Specifically, we built a comprehensive knowledge graph that includes 15 million edges across 39 types of relationships connecting drugs, diseases, proteins/genes, pathways, and expression from a large scientific corpus of 24 million PubMed publications. Using Amazon's AWS computing resources and a network-based, deep-learning framework, we identified 41 repurposable drugs (including dexamethasone, indomethacin, niclosamide, and toremifene) whose therapeutic associations with COVID-19 were validated by transcriptomic and proteomics data in SARS-CoV-2-infected human cells and data from ongoing clinical trials. Whereas this study by no means recommends specific drugs, it demonstrates a powerful deep-learning methodology to prioritize existing drugs for further investigation, which holds the potential to accelerate therapeutic development for COVID-19.","There have been more than 2.2 million confirmed cases and over 120â€¯000 deaths from the human coronavirus disease 2019 (COVID-19) pandemic, caused by the novel severe acute respiratory syndrome coronavirus (SARS-CoV-2), in the United States alone. However, there is currently a lack of proven effective medications against COVID-19. Drug repurposing offers a promising route for the development of prevention and treatment strategies for COVID-19. This study reports an integrative, network-based deep-learning methodology to identify repurposable drugs for COVID-19 (termed CoV-KGE). Specifically, we built a comprehensive knowledge graph that includes 15 million edges across 39 types of relationships connecting drugs, diseases, proteins/genes, pathways, and expression from a large scientific corpus of 24 million PubMed publications. Using Amazon's AWS computing resources and a network-based, deep-learning framework, we identified 41 repurposable drugs (including dexamethasone, indomethacin, niclosamide, and toremifene) whose therapeutic associations with COVID-19 were validated by transcriptomic and proteomics data in SARS-CoV-2-infected human cells and data from ongoing clinical trials. Whereas this study by no means recommends specific drugs, it demonstrates a powerful deep-learning methodology to prioritize existing drugs for further investigation, which holds the potential to accelerate therapeutic development for COVID-19.","Zeng, Song, Ma, Pan, Zhou, Hou, Zhang, Li, Karypis, Cheng","Zeng, Song, Ma, Pan, Zhou, Hou, Zhang, Li, Karypis, Cheng",https://doi.org/10.1021/acs.jproteome.0c00316,https://doi.org/10.1021/acs.jproteome.0c00316,2021-08-03
16509.0,pubmed,pubmed,Improving Follow-up Attendance for Discharged Emergency Care Patients Using Automated Phone System to Self-schedule: A Randomized Controlled Trial,Improving Follow-up Attendance for Discharged Emergency Care Patients Using Automated Phone System to Self-schedule: A Randomized Controlled Trial,"Automated phone appointment reminders have improved adherence with follow-up appointments in a variety of hospital settings, but have mixed results in patients discharged from the emergency department (ED). Increasing adherence to follow-up care has been a priority in the ED to improve patient outcomes and reduce unnecessary future visits. We conducted a prospective randomized open, blinded end-point (PROBE) trial of 278 adult patients discharged from the ED and referred to a provider for follow-up care. Participants in the intervention arm received a self-scheduling text or phone message that automatically connected them to their referral provider to schedule a follow-up appointment and sent them appointment reminders. Those in the control arm received standard-of-care written instructions to contact listed referral providers. The primary outcome was time to appointment. The secondary outcome was time to return visit to the ED. The automated reminders increased the cumulative incidence of keeping the referral appointment after ED discharge (pÃ‚Â &lt;Ã‚Â 0.001, Gray's test). Of participants randomized to the automated phone intervention, 49.3% (nÃ‚Â =Ã‚Â 74) kept their follow-up appointment versus 23.4% (nÃ‚Â =Ã‚Â 30) in the control arm, with a hazard ratio (HR) and 95% confidence interval (CI) over the duration of the study period of 2.4 (1.6 to 3.7; pÃ‚Â &lt;Ã‚Â 0.001). In a sensitivity analysis using 30Ã‚Â days of follow-up data, 42.0% (nÃ‚Â =Ã‚Â 63) of participants randomized to the phone intervention kept their follow-up versus 21.1% (nÃ‚Â =Ã‚Â 27) in the control arm, with a HR (95% CI) of 2.2 (1.4 to 3.5; pÃ‚Â &lt;Ã‚Â 0.001). There was no difference in ED revisits between the intervention and control group within 120Ã‚Â days postdischarge. An automated self-scheduling phone system significantly improved follow-up adherence after ED discharge, but did not decrease ED revisits.","Automated phone appointment reminders have improved adherence with follow-up appointments in a variety of hospital settings, but have mixed results in patients discharged from the emergency department (ED). Increasing adherence to follow-up care has been a priority in the ED to improve patient outcomes and reduce unnecessary future visits. We conducted a prospective randomized open, blinded end-point (PROBE) trial of 278 adult patients discharged from the ED and referred to a provider for follow-up care. Participants in the intervention arm received a self-scheduling text or phone message that automatically connected them to their referral provider to schedule a follow-up appointment and sent them appointment reminders. Those in the control arm received standard-of-care written instructions to contact listed referral providers. The primary outcome was time to appointment. The secondary outcome was time to return visit to the ED. The automated reminders increased the cumulative incidence of keeping the referral appointment after ED discharge (pÂ &lt;Â 0.001, Gray's test). Of participants randomized to the automated phone intervention, 49.3% (nÂ =Â 74) kept their follow-up appointment versus 23.4% (nÂ =Â 30) in the control arm, with a hazard ratio (HR) and 95% confidence interval (CI) over the duration of the study period of 2.4 (1.6 to 3.7; pÂ &lt;Â 0.001). In a sensitivity analysis using 30Â days of follow-up data, 42.0% (nÂ =Â 63) of participants randomized to the phone intervention kept their follow-up versus 21.1% (nÂ =Â 27) in the control arm, with a HR (95% CI) of 2.2 (1.4 to 3.5; pÂ &lt;Â 0.001). There was no difference in ED revisits between the intervention and control group within 120Â days postdischarge. An automated self-scheduling phone system significantly improved follow-up adherence after ED discharge, but did not decrease ED revisits.","Bauer, Sogade, Gage, Ruoff, Lewis","Bauer, Sogade, Gage, Ruoff, Lewis",https://doi.org/10.1111/acem.14080,https://doi.org/10.1111/acem.14080,2021-08-03
16512.0,pubmed,pubmed,FDG-PET/CT Radiomics Models for The Early Prediction of Loco-regional Recurrence in Head and Neck Cancer,FDG-PET/CT Radiomics Models for The Early Prediction of Locoregional Recurrence in Head and Neck Cancer,"Purpose Both CT and PET radiomics is considered as a potential prognostic biomarker in head and neck cancer. This study investigate the value of fused pre-treatment functional imaging (18F-FDG PET/CT) radiomics for modeling of local recurrence of head and neck cancers. Firstly, 298 patients is divided into a training set (n = 192) and verification set (n = 106). Secondly, PETs and CTs are fused based on wavelet transform. Thirdly, radiomics features are extracted from the 3D tumor area from PETCT fusion. The training set is used to select the features reduction and predict local recurrence, and the random forest prediction models combining radiomics and clinical variables are constructed. Finally, the ROC curve and K-M analysis are used to evaluate the prediction efficiency of the model on the validation set. Two PET / CT fusion radiomics features and three clinic parameters are extracted to construct the radiomics model. AUC value in the verification set 0.70 is better than no fused sets 0.69. The accuracy of 0.66 is not the highest value (0.67). Either consistency index CI 0.70 (from 0.67 to 0.70) or the p-value 0.025 (from 0.03 to 0.025) get the best result in all four models. The radiomics model based on the fusion of PETCT is better than the model based on PET or CT alone in predicting local recurrence, the inclusion of clinical parameters may result in more accurate predictions, which has certain guiding significance for the development of personalized precise treatment scheme.","Both CT and PET radiomics is considered as a potential prognostic biomarker in head and neck cancer. This study investigates the value of fused pre-treatment functional imaging (18F-FDG PET/CT) radiomics for modeling of local recurrence of head and neck cancers. Firstly, 298 patients have been divided into a training set (n = 192) and verification set (n = 106). Secondly, PETs and CTs are fused based on wavelet transform. Thirdly, radiomics features are extracted from the 3D tumor area from PETCT fusion. The training set is used to select the features reduction and predict local recurrence, and the random forest prediction models combining radiomics and clinical variables are constructed. Finally, the ROC curve and KM analysis are used to evaluate the prediction efficiency of the model on the validation set. Two PET/CT fusion radiomics features and three clinic parameters are extracted to construct the radiomics model. AUC value in the verification set 0.70 is better than no fused sets 0.69. The accuracy of 0.66 is not the highest value (0.67). Either consistency index CI 0.70 (from 0.67 to 0.70) or the p-value 0.025 (from 0.03 to 0.025) get the best result in all four models. The radiomics model based on the fusion of PETCT is better than the model based on PET or CT alone in predicting local recurrence, the inclusion of clinical parameters may result in more accurate predictions, which has certain guiding significance for the development of personalized, precise treatment scheme.","Cong, Peng, Tian, ValliÃƒÂ¨res, Chuanpei, Aijun, Benxin","Cong, Peng, Tian, ValliÃ¨res, Chuanpei, Aijun, Benxin",https://doi.org/10.2174/1573405616666200712181135,https://doi.org/10.2174/1573405616666200712181135,2021-08-03
16514.0,pubmed,pubmed,Data preprocessing for heart disease classification: A systematic literature review,Data preprocessing for heart disease classification: A systematic literature review,"Early detection of heart disease is an important challenge since 17.3 million people yearly lose their lives due to heart diseases. Besides, any error in diagnosis of cardiac disease can be dangerous and risks an individual's life. Accurate diagnosis is therefore critical in cardiology. Data Mining (DM) classification techniques have been used to diagnosis heart diseases but still limited by some challenges of data quality such as inconsistencies, noise, missing data, outliers, high dimensionality and imbalanced data. Data preprocessing (DP) techniques were therefore used to prepare data with the goal of improving the performance of heart disease DM based prediction systems. The purpose of this study is to review and summarize the current evidence on the use of preprocessing techniques in heart disease classification as regards: (1) the DP tasks and techniques most frequently used, (2) the impact of DP tasks and techniques on the performance of classification in cardiology, (3) the overall performance of classifiers when using DP techniques, and (4) comparisons of different combinations classifier-preprocessing in terms of accuracy rate. A systematic literature review is carried out, by identifying and analyzing empirical studies on the application of data preprocessing in heart disease classification published in the period between January 2000 and June 2019. A total of 49 studies were therefore selected and analyzed according to the aforementioned criteria. The review results show that data reduction is the most used preprocessing task in cardiology, followed by data cleaning. In general, preprocessing either maintained or improved the performance of heart disease classifiers. Some combinations such as (ANNÃ‚Â +Ã‚Â PCA), (ANNÃ‚Â +Ã‚Â CHI) and (SVMÃ‚Â +Ã‚Â PCA) are promising terms of accuracy. However the deployment of these models in real-world diagnosis decision support systems is subject to several risks and limitations due to the lack of interpretation.","Early detection of heart disease is an important challenge since 17.3 million people yearly lose their lives due to heart diseases. Besides, any error in diagnosis of cardiac disease can be dangerous and risks an individual's life. Accurate diagnosis is therefore critical in cardiology. Data Mining (DM) classification techniques have been used to diagnosis heart diseases but still limited by some challenges of data quality such as inconsistencies, noise, missing data, outliers, high dimensionality and imbalanced data. Data preprocessing (DP) techniques were therefore used to prepare data with the goal of improving the performance of heart disease DM based prediction systems. The purpose of this study is to review and summarize the current evidence on the use of preprocessing techniques in heart disease classification as regards: (1) the DP tasks and techniques most frequently used, (2) the impact of DP tasks and techniques on the performance of classification in cardiology, (3) the overall performance of classifiers when using DP techniques, and (4) comparisons of different combinations classifier-preprocessing in terms of accuracy rate. A systematic literature review is carried out, by identifying and analyzing empirical studies on the application of data preprocessing in heart disease classification published in the period between January 2000 and June 2019. A total of 49 studies were therefore selected and analyzed according to the aforementioned criteria. The review results show that data reduction is the most used preprocessing task in cardiology, followed by data cleaning. In general, preprocessing either maintained or improved the performance of heart disease classifiers. Some combinations such as (ANNÂ +Â PCA), (ANNÂ +Â CHI) and (SVMÂ +Â PCA) are promising terms of accuracy. However the deployment of these models in real-world diagnosis decision support systems is subject to several risks and limitations due to the lack of interpretation.","Benhar, Idri, FernÃƒÂ¡ndez-AlemÃƒÂ¡n","Benhar, Idri, FernÃ¡ndez-AlemÃ¡n",https://doi.org/10.1016/j.cmpb.2020.105635,https://doi.org/10.1016/j.cmpb.2020.105635,2021-08-03
16518.0,pubmed,pubmed,Validation of the effectiveness of a digital integrated healthcare platform utilizing an AI-based dietary management solution and a real-time continuous glucose monitoring system for diabetes management: a randomized controlled trial,Validation of the effectiveness of a digital integrated healthcare platform utilizing an AI-based dietary management solution and a real-time continuous glucose monitoring system for diabetes management: a randomized controlled trial,"Despite the numerous healthcare smartphone applications for self-management of diabetes, patients often fail to use these applications consistently due to various limitations, including difficulty in inputting dietary information by text search and inconvenient and non-persistent self-glucose measurement by home glucometer. We plan to apply a digital integrated healthcare platform using an artificial intelligence (AI)-based dietary management solution and a continuous glucose monitoring system (CGMS) to overcome those limitations. Furthermore, medical staff will be performing monitoring and intervention to encourage continuous use of the program. The aim of this trial is to examine the efficacy of the program in patients with type 2 diabetes mellitus (T2DM) who have HbA1c 53-69Ã¢â‚¬â€°mmol/mol (7.0-8.5%) and body mass index (BMI)Ã¢â‚¬â€°Ã¢â€°Â¥Ã¢â‚¬â€°23Ã¢â‚¬â€°mg/m<sup>2</sup>. This is a 48-week, open-label, randomized, multicenter trial consisting of patients with type 2 diabetes. The patients will be randomly assigned to three groups: control group A will receive routine diabetes care; experimental group B will use the digital integrated healthcare platform by themselves without feedback; and experimental group C will use the digital integrated healthcare platform with continuous glucose monitoring and feedback from medical staff. There are five follow-up measures: baseline and post-intervention at weeks 12, 24, 36, and 48. The primary end point is change in HbA1c from baseline to six months after the intervention. This trial will verify the effectiveness of a digital integrated healthcare platform with an AI-driven dietary solution and a real-time CGMS in patients with T2DM. Clinicaltrials.gov NCT04161170, registered on 08 November 2019. https://clinicaltrials.gov/ct2/show/NCT04161170?term=NCT04161170&amp;draw=2&amp;rank=1.","Despite the numerous healthcare smartphone applications for self-management of diabetes, patients often fail to use these applications consistently due to various limitations, including difficulty in inputting dietary information by text search and inconvenient and non-persistent self-glucose measurement by home glucometer. We plan to apply a digital integrated healthcare platform using an artificial intelligence (AI)-based dietary management solution and a continuous glucose monitoring system (CGMS) to overcome those limitations. Furthermore, medical staff will be performing monitoring and intervention to encourage continuous use of the program. The aim of this trial is to examine the efficacy of the program in patients with type 2 diabetes mellitus (T2DM) who have HbA1c 53-69â€‰mmol/mol (7.0-8.5%) and body mass index (BMI)â€‰â‰¥â€‰23â€‰mg/m<sup>2</sup>. This is a 48-week, open-label, randomized, multicenter trial consisting of patients with type 2 diabetes. The patients will be randomly assigned to three groups: control group A will receive routine diabetes care; experimental group B will use the digital integrated healthcare platform by themselves without feedback; and experimental group C will use the digital integrated healthcare platform with continuous glucose monitoring and feedback from medical staff. There are five follow-up measures: baseline and post-intervention at weeks 12, 24, 36, and 48. The primary end point is change in HbA1c from baseline to six months after the intervention. This trial will verify the effectiveness of a digital integrated healthcare platform with an AI-driven dietary solution and a real-time CGMS in patients with T2DM. Clinicaltrials.gov NCT04161170, registered on 08 November 2019. https://clinicaltrials.gov/ct2/show/NCT04161170?term=NCT04161170&amp;draw=2&amp;rank=1.","Park, Kim, Hwang, Lee, Park, Kim","Park, Kim, Hwang, Lee, Park, Kim",https://doi.org/10.1186/s12911-020-01179-x,https://doi.org/10.1186/s12911-020-01179-x,2021-08-03
16523.0,pubmed,pubmed,"Radiomics in radiation oncology-basics, methods, and limitations","Radiomics in radiation oncology-basics, methods, and limitations","Over the past years, the quantity and complexity of imaging data available for the clinical management of patients with solid tumors has increased substantially. Without the support of methods from the field of artificial intelligence (AI) and machine learning, aÃ‚Â complete evaluation of the available image information is hardly feasible in clinical routine. Especially in radiotherapy planning, manual detection and segmentation of lesions is laborious, time consuming, and shows significant variability among observers. Here, AI already offers techniques to support radiation oncologists, whereby ultimately, the productivity and the quality are increased, potentially leading to an improved patient outcome. Besides detection and segmentation of lesions, AI allows the extraction of aÃ‚Â vast number of quantitative imaging features from structural or functional imaging data that are typically not accessible by means of human perception. These features can be used alone or in combination with other clinical parameters to generate mathematical models that allow, for example, prediction of the response to radiotherapy. Within the large field of AI, radiomics is the subdiscipline that deals with the extraction of quantitative image features as well as the generation of predictive or prognostic mathematical models. This review gives an overview of the basics, methods, and limitations of radiomics, with aÃ‚Â focus on patients with brain tumors treated by radiation therapy.","Over the past years, the quantity and complexity of imaging data available for the clinical management of patients with solid tumors has increased substantially. Without the support of methods from the field of artificial intelligence (AI) and machine learning, aÂ complete evaluation of the available image information is hardly feasible in clinical routine. Especially in radiotherapy planning, manual detection and segmentation of lesions is laborious, time consuming, and shows significant variability among observers. Here, AI already offers techniques to support radiation oncologists, whereby ultimately, the productivity and the quality are increased, potentially leading to an improved patient outcome. Besides detection and segmentation of lesions, AI allows the extraction of aÂ vast number of quantitative imaging features from structural or functional imaging data that are typically not accessible by means of human perception. These features can be used alone or in combination with other clinical parameters to generate mathematical models that allow, for example, prediction of the response to radiotherapy. Within the large field of AI, radiomics is the subdiscipline that deals with the extraction of quantitative image features as well as the generation of predictive or prognostic mathematical models. This review gives an overview of the basics, methods, and limitations of radiomics, with aÂ focus on patients with brain tumors treated by radiation therapy.","Lohmann, Bousabarah, Hoevels, Treuer","Lohmann, Bousabarah, Hoevels, Treuer",https://doi.org/10.1007/s00066-020-01663-3,https://doi.org/10.1007/s00066-020-01663-3,2021-08-03
16527.0,pubmed,pubmed,Identifying diagnosis evidence of cardiogenic stroke from Chinese echocardiograph reports,Identifying diagnosis evidence of cardiogenic stroke from Chinese echocardiograph reports,"Cardiogenic stroke has increasing morbidity in China and brought economic burden to patient families. In cardiogenic stroke diagnosis, echocardiograph examination is one of the most important examinations. Sonographers will investigate patients' heart via echocardiograph, and describe them in the echocardiograph reports. In this study, we developed a machine learning model to automatically identify diagnosis evidences of cardiogenic stroke providing to neurologist for clinical decision making. We collected 4188 Chinese echocardiograph reports of 4018 patients, with average length 177 Chinese characters in free-text style. Collaborating with neurologists and sonographers, we summarized 149 phrases on diagnosis evidence of cardiogenic stroke such as &quot; (severe mitral stenosis), &quot; (aortic valve degeneration) and so on. Furthermore, we developed an annotated corpus via mapping 149 phrases to the 4188 reports. We selected 11 most frequent diagnosis evidence types such as &quot; (mitral stenosis) for further identifying. The generated corpus is divided into training set and testing set in the ratio of 8:2, which is used to train and validate a machine learning model to identify the evidence of cardiogenic stroke using BiLSTM-CRF algorithm. Our machine learning method achieved the average performance on the diagnosis evidence identification is 98.03, 90.17 and 93.94% respectively. In addition, our method is capable to identify the novel diagnosis evidence of cardiogenic stroke description such as &quot;-&quot; (mitral stenosis), &quot; (aortic valve calcification) et al. CONCLUSIONS: In this study, we analyze the structure of the echocardiograph reports and summarized 149 phrases on diagnosis evidence of cardiogenic stroke. We use the phrases to generate an annotated corpus automatically, which greatly reduces the cost of manual annotation. The model trained based on the corpus also has a good performance on the testing set. The method of automatically identifying diagnosis evidence of cardiogenic stroke proposed in this study will be further refined in the practice.","Cardiogenic stroke has increasing morbidity in China and brought economic burden to patient families. In cardiogenic stroke diagnosis, echocardiograph examination is one of the most important examinations. Sonographers will investigate patients' heart via echocardiograph, and describe them in the echocardiograph reports. In this study, we developed a machine learning model to automatically identify diagnosis evidences of cardiogenic stroke providing to neurologist for clinical decision making. We collected 4188 Chinese echocardiograph reports of 4018 patients, with average length 177 Chinese characters in free-text style. Collaborating with neurologists and sonographers, we summarized 149 phrases on diagnosis evidence of cardiogenic stroke such as """" (severe mitral stenosis), """" (aortic valve degeneration) and so on. Furthermore, we developed an annotated corpus via mapping 149 phrases to the 4188 reports. We selected 11 most frequent diagnosis evidence types such as """" (mitral stenosis) for further identifying. The generated corpus is divided into training set and testing set in the ratio of 8:2, which is used to train and validate a machine learning model to identify the evidence of cardiogenic stroke using BiLSTM-CRF algorithm. Our machine learning method achieved the average performance on the diagnosis evidence identification is 98.03, 90.17 and 93.94% respectively. In addition, our method is capable to identify the novel diagnosis evidence of cardiogenic stroke description such as ""-"" (mitral stenosis), """" (aortic valve calcification) et al. CONCLUSIONS: In this study, we analyze the structure of the echocardiograph reports and summarized 149 phrases on diagnosis evidence of cardiogenic stroke. We use the phrases to generate an annotated corpus automatically, which greatly reduces the cost of manual annotation. The model trained based on the corpus also has a good performance on the testing set. The method of automatically identifying diagnosis evidence of cardiogenic stroke proposed in this study will be further refined in the practice.","Qin, Xu, Ding, Li, Li","Qin, Xu, Ding, Li, Li",https://doi.org/10.1186/s12911-020-1106-3,https://doi.org/10.1186/s12911-020-1106-3,2021-08-03
16531.0,pubmed,pubmed,AI in Radiology: Where are we today in Multiple Sclerosis Imaging?,AI in Radiology: Where are we today in Multiple Sclerosis Imaging?,"Ã¢â‚¬â€šMR imaging is an essential component in managing patients with Multiple sclerosis (MS). This holds true for the initial diagnosis as well as for assessing the clinical course of MS.Ã¢â‚¬Å In recent years, a growing number of computer tools were developed to analyze imaging data in MS.Ã¢â‚¬Å This review gives an overview of the most important applications with special emphasis on artificial intelligence (AI). Ã¢â‚¬â€šRelevant studies were identified through a literature search in recognized databases, and through parsing the references in studies found this way. Literature published as of November 2019 was included with a special focus on recent studies from 2018 and 2019. Ã¢â‚¬â€šThere are a number of studies which focus on optimizing lesion visualization and lesion segmentation. Some of these studies accomplished these tasks with high accuracy, enabling a reproducible quantitative analysis of lesion loads. Some studies took a radiomics approach and aimed at predicting clinical endpoints such as the conversion from a clinically isolated syndrome to definite MS.Ã¢â‚¬Å Moreover, recent studies investigated synthetic imaging, i.Ã¢â‚¬Å e. imaging data that is not measured during an MR scan but generated by a computer algorithm to optimize the contrast between MS lesions and brain parenchyma. Ã¢â‚¬â€šComputer-based image analysis and AI are hot topics in imaging MS.Ã¢â‚¬Å Some applications are ready for use in clinical routine. A major challenge for the future is to improve prediction of expected disease courses and thereby helping to find optimal treatment decisions on an individual level. With technical improvements, more questions arise about the integration of new tools into the radiological workflow. Ã¢â‚¬â€š Ã‚Â· Computer algorithms have a growing impact on analyzing MR imaging in MS.. Ã‚Â· Artificial intelligence is more and more commonly employed in such computer tools.. Ã‚Â· Applications include lesion segmentation, prediction of clinical parameters and image synthesizing.. Ã‚Â· Eichinger P, Zimmer C, Wiestler B. AI in Radiology: Where are we today in Multiple Sclerosis Imaging?. Fortschr RÃƒÂ¶ntgenstr 2020; 192: 847Ã¢â‚¬Å -Ã¢â‚¬Å 853. Ã¢â‚¬â€šMRT-Untersuchungen sind ein zentraler Baustein in der Diagnostik bei Multipler Sklerose (MS). Dies gilt sowohl fÃƒÂ¼r das Erstereignis wie auch fÃƒÂ¼r die Verlaufsbeurteilung. In den vergangenen Jahren wurden zunehmend Algorithmen zur Analyse von MRT-Daten bei MS entwickelt. Diese ÃƒÅ“bersichtsarbeit stellt die wesentlichen Anwendungsfelder unter besonderer BerÃƒÂ¼cksichtigung von Algorithmen aus dem Bereich der KÃƒÂ¼nstlichen Intelligenz (KI) vor. Ã¢â‚¬â€šRelevante Studien wurden durch eine Literatursuche in anerkannten Datenbanken sowie durch Querverweise in so gefundenen Studien identifiziert. Dabei wurde Literatur berÃƒÂ¼cksichtigt, die bis November 2019 erschienen war, ein besonderes Augenmerk lag auf kÃƒÂ¼rzlich erschienenen Studien aus den Jahren 2018 und 2019. Ã¢â‚¬â€šViele Studien haben LÃƒÂ¶sungen zur optimierten LÃƒÂ¤sionsvisualisierung oder der Segmentierung von LÃƒÂ¤sionen entwickelt. Hier liegen bereits Werkzeuge vor, die diese Aufgaben mit hoher Genauigkeit bewerkstelligen kÃƒÂ¶nnen und damit mittelbar eine reproduzierbare, quantitative Auswertung der LÃƒÂ¤sionslast ermÃƒÂ¶glichen. Einige Arbeiten gingen einem Radiomics-Ansatz nach und untersuchten die Vorhersage klinischer Endpunkte, z.Ã¢â‚¬Å B. die Konversion von einem klinisch isolierten Syndrom zu definitiver MS.Ã¢â‚¬Å Zuletzt liegen erste Arbeiten vor, die synthetisch erstellte Bildgebung untersuchen, also solche Bilder, die basierend auf tatsÃƒÂ¤chlich gemessenen MRT-Sequenzen von Maschinenlernalgorithmen generiert werden und die Kontraste zwischen LÃƒÂ¤sionen und normalem Hirnparenchym optimieren. Ã¢â‚¬â€šComputerunterstÃƒÂ¼tzte Bildanalyse und KI sind hochaktuelle Themen in der MS-Bildgebung. Einzelne Anwendungen sind dabei bereits jetzt prinzipiell in der klinischen Routine einsetzbar. Eine wesentliche Herausforderung fÃƒÂ¼r die Zukunft besteht vor allem darin, bessere PrÃƒÂ¤diktionen klinischer VerlÃƒÂ¤ufe und entsprechende Hilfestellungen in der Findung einer optimalen Therapie auf patientenindividueller Ebene bereitzustellen. AuÃƒÅ¸erdem rÃƒÂ¼cken durch die Erfolge auf technologischer Ebene zunehmend Fragen ÃƒÂ¼ber die Integration in klinisch-radiologische AblÃƒÂ¤ufe in den Vordergrund. Ã‚Â· Computeralgorithmen haben einen zunehmenden Einfluss auf die Auswertung von MRT-Bildgebung bei Multipler Sklerose.. Ã‚Â· KÃƒÂ¼nstliche Intelligenz wird zunehmend fÃƒÂ¼r solche Algorithmen verwendet.. Ã‚Â· Wesentliche Anwendungen sind die LÃƒÂ¤sionssegmentierung, die PrÃƒÂ¤diktion klinischer Parameter sowie die Generierung synthetischer Bildgebung.. Ã‚Â· Eichinger P, Zimmer C, Wiestler B. AI in Radiology: Where are we today in Multiple Sclerosis Imaging?. Fortschr RÃƒÂ¶ntgenstr 2020; 192: 847Ã¢â‚¬Å Ã¢â‚¬â€œÃ¢â‚¬Å 853.","â€‚MR imaging is an essential component in managing patients with Multiple sclerosis (MS). This holds true for the initial diagnosis as well as for assessing the clinical course of MS.â€ŠIn recent years, a growing number of computer tools were developed to analyze imaging data in MS.â€ŠThis review gives an overview of the most important applications with special emphasis on artificial intelligence (AI). â€‚Relevant studies were identified through a literature search in recognized databases, and through parsing the references in studies found this way. Literature published as of November 2019 was included with a special focus on recent studies from 2018 and 2019. â€‚There are a number of studies which focus on optimizing lesion visualization and lesion segmentation. Some of these studies accomplished these tasks with high accuracy, enabling a reproducible quantitative analysis of lesion loads. Some studies took a radiomics approach and aimed at predicting clinical endpoints such as the conversion from a clinically isolated syndrome to definite MS.â€ŠMoreover, recent studies investigated synthetic imaging, i.â€Še. imaging data that is not measured during an MR scan but generated by a computer algorithm to optimize the contrast between MS lesions and brain parenchyma. â€‚Computer-based image analysis and AI are hot topics in imaging MS.â€ŠSome applications are ready for use in clinical routine. A major challenge for the future is to improve prediction of expected disease courses and thereby helping to find optimal treatment decisions on an individual level. With technical improvements, more questions arise about the integration of new tools into the radiological workflow. â€‚ Â· Computer algorithms have a growing impact on analyzing MR imaging in MS.. Â· Artificial intelligence is more and more commonly employed in such computer tools.. Â· Applications include lesion segmentation, prediction of clinical parameters and image synthesizing.. Â· Eichinger P, Zimmer C, Wiestler B. AI in Radiology: Where are we today in Multiple Sclerosis Imaging?. Fortschr RÃ¶ntgenstr 2020; 192: 847â€Š-â€Š853. â€‚MRT-Untersuchungen sind ein zentraler Baustein in der Diagnostik bei Multipler Sklerose (MS). Dies gilt sowohl fÃ¼r das Erstereignis wie auch fÃ¼r die Verlaufsbeurteilung. In den vergangenen Jahren wurden zunehmend Algorithmen zur Analyse von MRT-Daten bei MS entwickelt. Diese Ãœbersichtsarbeit stellt die wesentlichen Anwendungsfelder unter besonderer BerÃ¼cksichtigung von Algorithmen aus dem Bereich der KÃ¼nstlichen Intelligenz (KI) vor. â€‚Relevante Studien wurden durch eine Literatursuche in anerkannten Datenbanken sowie durch Querverweise in so gefundenen Studien identifiziert. Dabei wurde Literatur berÃ¼cksichtigt, die bis November 2019 erschienen war, ein besonderes Augenmerk lag auf kÃ¼rzlich erschienenen Studien aus den Jahren 2018 und 2019. â€‚Viele Studien haben LÃ¶sungen zur optimierten LÃ¤sionsvisualisierung oder der Segmentierung von LÃ¤sionen entwickelt. Hier liegen bereits Werkzeuge vor, die diese Aufgaben mit hoher Genauigkeit bewerkstelligen kÃ¶nnen und damit mittelbar eine reproduzierbare, quantitative Auswertung der LÃ¤sionslast ermÃ¶glichen. Einige Arbeiten gingen einem Radiomics-Ansatz nach und untersuchten die Vorhersage klinischer Endpunkte, z.â€ŠB. die Konversion von einem klinisch isolierten Syndrom zu definitiver MS.â€ŠZuletzt liegen erste Arbeiten vor, die synthetisch erstellte Bildgebung untersuchen, also solche Bilder, die basierend auf tatsÃ¤chlich gemessenen MRT-Sequenzen von Maschinenlernalgorithmen generiert werden und die Kontraste zwischen LÃ¤sionen und normalem Hirnparenchym optimieren. â€‚ComputerunterstÃ¼tzte Bildanalyse und KI sind hochaktuelle Themen in der MS-Bildgebung. Einzelne Anwendungen sind dabei bereits jetzt prinzipiell in der klinischen Routine einsetzbar. Eine wesentliche Herausforderung fÃ¼r die Zukunft besteht vor allem darin, bessere PrÃ¤diktionen klinischer VerlÃ¤ufe und entsprechende Hilfestellungen in der Findung einer optimalen Therapie auf patientenindividueller Ebene bereitzustellen. AuÃŸerdem rÃ¼cken durch die Erfolge auf technologischer Ebene zunehmend Fragen Ã¼ber die Integration in klinisch-radiologische AblÃ¤ufe in den Vordergrund. Â· Computeralgorithmen haben einen zunehmenden Einfluss auf die Auswertung von MRT-Bildgebung bei Multipler Sklerose.. Â· KÃ¼nstliche Intelligenz wird zunehmend fÃ¼r solche Algorithmen verwendet.. Â· Wesentliche Anwendungen sind die LÃ¤sionssegmentierung, die PrÃ¤diktion klinischer Parameter sowie die Generierung synthetischer Bildgebung.. Â· Eichinger P, Zimmer C, Wiestler B. AI in Radiology: Where are we today in Multiple Sclerosis Imaging?. Fortschr RÃ¶ntgenstr 2020; 192: 847â€Šâ€“â€Š853.","Eichinger, Zimmer, Wiestler","Eichinger, Zimmer, Wiestler",https://doi.org/10.1055/a-1167-8402,https://doi.org/10.1055/a-1167-8402,2021-08-03
16534.0,pubmed,pubmed,Neural side effect discovery from user credibility and experience-assessed online health discussions,Neural side effect discovery from user credibility and experience-assessed online health discussions,"Health 2.0 allows patients and caregivers to conveniently seek medical information and advice via e-portals and online discussion forums, especially regarding potential drug side effects. Although online health communities are helpful platforms for obtaining non-professional opinions, they pose risks in communicating unreliable and insufficient information in terms of quality and quantity. Existing methods in extracting user-reported adverse drug reactions (ADRs) in online health forums are not only insufficiently accurate as they disregard user credibility and drug experience, but are also expensive as they rely on supervised ground truth annotation of individual statement. We propose a NEural ArchiTecture for Drug side effect prediction (NEAT), which is optimized on the task of drug side effect discovery based on a complete discussion while being attentive to user credibility and experience, thus, addressing the mentioned shortcomings. We train our neural model in a self-supervised fashion using ground truth drug side effects from mayoclinic.org. NEAT learns to assign each user a score that is descriptive of their credibility and highlights the critical textual segments of their post. Experiments show that NEAT improves drug side effect discovery from online health discussion by 3.04% from user-credibility agnostic baselines, and by 9.94% from non-neural baselines in term of F<sub>1</sub>. Additionally, the latent credibility scores learned by the model correlate well with trustworthiness signals, such as the number of &quot;thanks&quot; received by other forum members, and improve credibility heuristics such as number of posts by 0.113 in term of Spearman's rank correlation coefficient. Experience-based self-supervised attention highlights critical phrases such as mentioned side effects, and enhances fully supervised ADR extraction models based on sequence labelling by 5.502% in terms of precision. NEAT considers both user credibility and experience in online health forums, making feasible a self-supervised approach to side effect prediction for mentioned drugs. The derived user credibility and attention mechanism are transferable and improve downstream ADR extraction models. Our approach enhances automatic drug side effect discovery and fosters research in several domains including pharmacovigilance and clinical studies.","Health 2.0 allows patients and caregivers to conveniently seek medical information and advice via e-portals and online discussion forums, especially regarding potential drug side effects. Although online health communities are helpful platforms for obtaining non-professional opinions, they pose risks in communicating unreliable and insufficient information in terms of quality and quantity. Existing methods in extracting user-reported adverse drug reactions (ADRs) in online health forums are not only insufficiently accurate as they disregard user credibility and drug experience, but are also expensive as they rely on supervised ground truth annotation of individual statement. We propose a NEural ArchiTecture for Drug side effect prediction (NEAT), which is optimized on the task of drug side effect discovery based on a complete discussion while being attentive to user credibility and experience, thus, addressing the mentioned shortcomings. We train our neural model in a self-supervised fashion using ground truth drug side effects from mayoclinic.org. NEAT learns to assign each user a score that is descriptive of their credibility and highlights the critical textual segments of their post. Experiments show that NEAT improves drug side effect discovery from online health discussion by 3.04% from user-credibility agnostic baselines, and by 9.94% from non-neural baselines in term of F<sub>1</sub>. Additionally, the latent credibility scores learned by the model correlate well with trustworthiness signals, such as the number of ""thanks"" received by other forum members, and improve credibility heuristics such as number of posts by 0.113 in term of Spearman's rank correlation coefficient. Experience-based self-supervised attention highlights critical phrases such as mentioned side effects, and enhances fully supervised ADR extraction models based on sequence labelling by 5.502% in terms of precision. NEAT considers both user credibility and experience in online health forums, making feasible a self-supervised approach to side effect prediction for mentioned drugs. The derived user credibility and attention mechanism are transferable and improve downstream ADR extraction models. Our approach enhances automatic drug side effect discovery and fosters research in several domains including pharmacovigilance and clinical studies.","Nguyen, Sugiyama, Kan, Halder","Nguyen, Sugiyama, Kan, Halder",https://doi.org/10.1186/s13326-020-00221-1,https://doi.org/10.1186/s13326-020-00221-1,2021-08-03
16540.0,pubmed,pubmed,[The Importance of Different Topics in Medical Psychology/Sociology from Physicians' and Medical Students' View: Similarities and Differences],[The Importance of Different Topics in Medical Psychology/Sociology from Physicians' and Medical Students' View: Similarities and Differences],"The subject of &quot;Medical Psychology/Sociology&quot; is facing various content changes as a result of the currently pending study reforms within the framework of the &quot;Master Plan Medical Studies 2020&quot;. These include the further development of the catalogues of exam-relevant topics (GK) and the National Competence-Based Learning Objectives Catalogue for Undergraduate Medical Education (NKLM) as well as the Medical Licensing Regulations (Ãƒâ€žApprO). In this context, the question arises as to which training contents of &quot;medical psychology/sociology&quot;, that medical students are confronted with, are of particular importance for their future medical work. 332 physicians and 265 students evaluated different contents of the catalogue of exam-relevant topics of medical psychology/sociology (GK-MPS) according to their importance regarding their education or respectively their daily work. In addition, the physicians indicated in free text fields of the questionnaire which situations in their daily work they would have liked to have been better prepared for during medical school. In a combined quantitative-qualitative analysis approach, differences between the 2 groups were identified by t-tests for independent samples with unequal variances (Welch test) and free text information was assigned to the various topics of the GK-MPS and evaluated in terms of content by 3 raters within the framework of a qualitatively oriented category-based text analysis. Both physicians and students considered those topics of the GK-MPS, that involve communication between physician and patient, to be the most important. In addition, physicians rated the topics of physician-patient communication, statistics and prevention as more important than the students did. The physicians would have liked to have been better prepared for special and challenging medical situations in physician-patient interaction. The topics of physician-patient communication are still considered to be of particular importance for the medical training of physicians and students, but physicians consider them to be more important than students. The study results support the current study reforms towards a stronger emphasis on physician-patient communication and scientific basics as well as the linking of pre-clinical and clinical study contents. Das Fach Ã¢â‚¬Å¾Medizinische Psychologie/SoziologieÃ¢â‚¬Å“ steht durch die aktuell anstehenden Studienreformen im Rahmen des Ã¢â‚¬Å¾Masterplans Medizinstudium 2020Ã¢â‚¬Å“ verschiedenen inhaltlichen Ãƒâ€žnderungen gegenÃƒÂ¼ber. Diese beinhalten u.Ã¢â‚¬â€°a. eine Weiterentwicklung des Gegenstandskatalogs (GK) und des Nationalen Kompetenzbasierten Lernzielkatalogs Medizin (NKLM) sowie der Ãƒâ€žrztlichen Approbationsordnung (Ãƒâ€žAppro). Hier stellt sich die Frage, welche Ausbildungsinhalte der Ã¢â‚¬Å¾Medizinische Psychologie/SoziologieÃ¢â‚¬Å“, mit denen Medizinstudierende konfrontiert sind, von besonderer Bedeutung fÃƒÂ¼r die spÃƒÂ¤tere ÃƒÂ¤rztliche TÃƒÂ¤tigkeit sind. 332 Ãƒâ€žrztInnen und 265 Studierende bewerteten im Rahmen der Studie Lehrinhalte des Gegenstandskatalogs der Medizinischen Psychologie/ Soziologie (GK-MPS) nach ihrer Bedeutung fÃƒÂ¼r ihr Studium bzw. ihr Berufsleben. Die Ãƒâ€žrztInnen gaben zudem in Freitextangaben an, auf welche Situationen im Berufsalltag sie durch das Studium gerne besser vorbereitet worden wÃƒÂ¤ren. In einem kombiniert quantitativ-qualitativen Analyseansatz wurden Unterschiede zwischen beiden Gruppen durch t-Tests fÃƒÂ¼r unabhÃƒÂ¤ngige Stichproben bei ungleichen Varianzen (Welch-Test) identifiziert sowie Freitextangaben durch 3 Rater im Rahmen einer qualitativ orientierten kategoriengeleiteten Textanalyse den verschiedenen Themen des GK-MPS zugeordnet und inhaltlich ausgewertet. Sowohl Ãƒâ€žrztInnen als auch Studierende schÃƒÂ¤tzten jene Themenbereiche des GK-MPS als am wichtigsten ein, die die unmittelbare Arzt-Patient-Kommunikation beschreiben. Die Ãƒâ€žrztInnen maÃƒÅ¸en den Themenbereichen der Arzt-Patient-Kommunikation, Statistik sowie PrÃƒÂ¤vention eine hÃƒÂ¶here Wichtigkeit bei als die Studierenden. Die Ãƒâ€žrztInnen beschrieben im Freitext, dass sie durch das Studium gerne besser auf besondere und herausfordernde medizinische Situationen in der Arzt-Patienten-Interaktion vorbereitet worden wÃƒÂ¤ren. Nach wie vor werden den Themen der Arzt-Patienten-Kommunikation bei Ãƒâ€žrztInnen als auch Studierenden eine besondere Bedeutung fÃƒÂ¼r die medizinische Ausbildung beigemessen, jedoch schÃƒÂ¤tzen Ãƒâ€žrztInnen diese Wichtigkeit noch einmal als hÃƒÂ¶her ein als Studierende. Die Studienergebnisse unterstÃƒÂ¼tzen die aktuellen Studienreformen hin zu einer stÃƒÂ¤rkeren Gewichtung der Arzt-Patienten-Kommunikation und Vermittlung wissenschaftlicher Grundlagen sowie die VerknÃƒÂ¼pfung vorklinischer und klinischer Studieninhalte.","The subject of ""Medical Psychology/Sociology"" is facing various content changes as a result of the currently pending study reforms within the framework of the ""Master Plan Medical Studies 2020"". These include the further development of the catalogues of exam-relevant topics (GK) and the National Competence-Based Learning Objectives Catalogue for Undergraduate Medical Education (NKLM) as well as the Medical Licensing Regulations (Ã„ApprO). In this context, the question arises as to which training contents of ""medical psychology/sociology"", that medical students are confronted with, are of particular importance for their future medical work. 332 physicians and 265 students evaluated different contents of the catalogue of exam-relevant topics of medical psychology/sociology (GK-MPS) according to their importance regarding their education or respectively their daily work. In addition, the physicians indicated in free text fields of the questionnaire which situations in their daily work they would have liked to have been better prepared for during medical school. In a combined quantitative-qualitative analysis approach, differences between the 2 groups were identified by t-tests for independent samples with unequal variances (Welch test) and free text information was assigned to the various topics of the GK-MPS and evaluated in terms of content by 3 raters within the framework of a qualitatively oriented category-based text analysis. Both physicians and students considered those topics of the GK-MPS, that involve communication between physician and patient, to be the most important. In addition, physicians rated the topics of physician-patient communication, statistics and prevention as more important than the students did. The physicians would have liked to have been better prepared for special and challenging medical situations in physician-patient interaction. The topics of physician-patient communication are still considered to be of particular importance for the medical training of physicians and students, but physicians consider them to be more important than students. The study results support the current study reforms towards a stronger emphasis on physician-patient communication and scientific basics as well as the linking of pre-clinical and clinical study contents. Das Fach â€žMedizinische Psychologie/Soziologieâ€œ steht durch die aktuell anstehenden Studienreformen im Rahmen des â€žMasterplans Medizinstudium 2020â€œ verschiedenen inhaltlichen Ã„nderungen gegenÃ¼ber. Diese beinhalten u.â€‰a. eine Weiterentwicklung des Gegenstandskatalogs (GK) und des Nationalen Kompetenzbasierten Lernzielkatalogs Medizin (NKLM) sowie der Ã„rztlichen Approbationsordnung (Ã„Appro). Hier stellt sich die Frage, welche Ausbildungsinhalte der â€žMedizinische Psychologie/Soziologieâ€œ, mit denen Medizinstudierende konfrontiert sind, von besonderer Bedeutung fÃ¼r die spÃ¤tere Ã¤rztliche TÃ¤tigkeit sind. 332 Ã„rztInnen und 265 Studierende bewerteten im Rahmen der Studie Lehrinhalte des Gegenstandskatalogs der Medizinischen Psychologie/ Soziologie (GK-MPS) nach ihrer Bedeutung fÃ¼r ihr Studium bzw. ihr Berufsleben. Die Ã„rztInnen gaben zudem in Freitextangaben an, auf welche Situationen im Berufsalltag sie durch das Studium gerne besser vorbereitet worden wÃ¤ren. In einem kombiniert quantitativ-qualitativen Analyseansatz wurden Unterschiede zwischen beiden Gruppen durch t-Tests fÃ¼r unabhÃ¤ngige Stichproben bei ungleichen Varianzen (Welch-Test) identifiziert sowie Freitextangaben durch 3 Rater im Rahmen einer qualitativ orientierten kategoriengeleiteten Textanalyse den verschiedenen Themen des GK-MPS zugeordnet und inhaltlich ausgewertet. Sowohl Ã„rztInnen als auch Studierende schÃ¤tzten jene Themenbereiche des GK-MPS als am wichtigsten ein, die die unmittelbare Arzt-Patient-Kommunikation beschreiben. Die Ã„rztInnen maÃŸen den Themenbereichen der Arzt-Patient-Kommunikation, Statistik sowie PrÃ¤vention eine hÃ¶here Wichtigkeit bei als die Studierenden. Die Ã„rztInnen beschrieben im Freitext, dass sie durch das Studium gerne besser auf besondere und herausfordernde medizinische Situationen in der Arzt-Patienten-Interaktion vorbereitet worden wÃ¤ren. Nach wie vor werden den Themen der Arzt-Patienten-Kommunikation bei Ã„rztInnen als auch Studierenden eine besondere Bedeutung fÃ¼r die medizinische Ausbildung beigemessen, jedoch schÃ¤tzen Ã„rztInnen diese Wichtigkeit noch einmal als hÃ¶her ein als Studierende. Die Studienergebnisse unterstÃ¼tzen die aktuellen Studienreformen hin zu einer stÃ¤rkeren Gewichtung der Arzt-Patienten-Kommunikation und Vermittlung wissenschaftlicher Grundlagen sowie die VerknÃ¼pfung vorklinischer und klinischer Studieninhalte.","Mahal, Amann, Wischmann, Ditzen","Mahal, Amann, Wischmann, Ditzen",https://doi.org/10.1055/a-1153-9262,https://doi.org/10.1055/a-1153-9262,2021-08-03
16545.0,pubmed,pubmed,Conventional Computed Tomographic Calcium Scoring vs full chest CTCS for lung cancer screening: a cost-effectiveness analysis,Conventional Computed Tomographic Calcium Scoring vs full chest CTCS for lung cancer screening: a cost-effectiveness analysis,"Conventional CTCS images the mid/lower chest for coronary artery disease (CAD). Because many CAD patients are also at risk for lung malignancy, CTCS often discovers incidental pulmonary nodules (IPN). CTCS excludes the upper chest, where malignancy is common. Full-chest CTCS (FCT) may be a cost-effective screening tool for IPN. A decision tree was created to compare a FCT to CTCS in a hypothetical patient cohort with suspected CAD. (Figure) The design compares the effects of missed cancers on CTCS with the cost of working up non-malignant nodules on FCT. The model was informed by results of the National Lung Screening Trial and literature review, including the rate of malignancy among patients receiving CTCS and the rate of malignancy in upper vs lower portions of the lung. The analysis outcomes are Quality-Adjusted Life Year (QALY) and incremental cost-effectiveness ratio (ICER), which is generally considered beneficial when &lt;$50,000/QALY. Literature review suggests that rate of IPNs in the upper portion of the lung varied from 47 to 76%. Our model assumed that IPNs occur in upper and lower portions of the lung with equal frequency. The model also assumes an equal malignancy potential in upper lung IPNs despite data that malignancy occurs 61-66% in upper lung fields. In the base case analysis, a FCT will lead to an increase of 0.03 QALYs comparing to conventional CTCS (14.54 vs 14.51 QALY, respectively), which translates into an QALY increase of 16Ã¢â‚¬â€°days. The associated incremental cost for FCT is $278 ($1027 vs $748, FCT vs CTCS respectively. The incremental cost-effectiveness ratio (ICER) is $10,289/QALY, suggesting significant benefit. Sensitivity analysis shows this benefit increases proportional to the rate of malignancy in upper lung fields. Conventional CTCS may be a missed opportunity to screen for upper lung field cancers in high risk patients. The ICER of FCT is better than screening for breast cancer screening (mammograms $80Ã¢â‚¬â€°k/QALY) and colon cancer (colonoscopy $6Ã¢â‚¬â€°k/QALY). Prospective studies are appropriate to define protocols for FCT.","Conventional CTCS images the mid/lower chest for coronary artery disease (CAD). Because many CAD patients are also at risk for lung malignancy, CTCS often discovers incidental pulmonary nodules (IPN). CTCS excludes the upper chest, where malignancy is common. Full-chest CTCS (FCT) may be a cost-effective screening tool for IPN. A decision tree was created to compare a FCT to CTCS in a hypothetical patient cohort with suspected CAD. (Figure) The design compares the effects of missed cancers on CTCS with the cost of working up non-malignant nodules on FCT. The model was informed by results of the National Lung Screening Trial and literature review, including the rate of malignancy among patients receiving CTCS and the rate of malignancy in upper vs lower portions of the lung. The analysis outcomes are Quality-Adjusted Life Year (QALY) and incremental cost-effectiveness ratio (ICER), which is generally considered beneficial when &lt;$50,000/QALY. Literature review suggests that rate of IPNs in the upper portion of the lung varied from 47 to 76%. Our model assumed that IPNs occur in upper and lower portions of the lung with equal frequency. The model also assumes an equal malignancy potential in upper lung IPNs despite data that malignancy occurs 61-66% in upper lung fields. In the base case analysis, a FCT will lead to an increase of 0.03 QALYs comparing to conventional CTCS (14.54 vs 14.51 QALY, respectively), which translates into an QALY increase of 16â€‰days. The associated incremental cost for FCT is $278 ($1027 vs $748, FCT vs CTCS respectively. The incremental cost-effectiveness ratio (ICER) is $10,289/QALY, suggesting significant benefit. Sensitivity analysis shows this benefit increases proportional to the rate of malignancy in upper lung fields. Conventional CTCS may be a missed opportunity to screen for upper lung field cancers in high risk patients. The ICER of FCT is better than screening for breast cancer screening (mammograms $80â€‰k/QALY) and colon cancer (colonoscopy $6â€‰k/QALY). Prospective studies are appropriate to define protocols for FCT.","Jiang, Linden, Gupta, Jarrett, Worrell, Ho, Perry, Towe","Jiang, Linden, Gupta, Jarrett, Worrell, Ho, Perry, Towe",https://doi.org/10.1186/s12890-020-01221-8,https://doi.org/10.1186/s12890-020-01221-8,2021-08-03
16546.0,pubmed,pubmed,Diagnostic performance of artificial intelligence to detect genetic diseases with facial phenotypes: A protocol for systematic review and meta analysis,Diagnostic performance of artificial intelligence to detect genetic diseases with facial phenotypes: A protocol for systematic review and meta analysis,"Many genetic diseases are known to have distinctive facial phenotypes, which are highly informative to provide an opportunity for automated detection. However, the diagnostic performance of artificial intelligence to identify genetic diseases with facial phenotypes requires further investigation. The objectives of this systematic review and meta-analysis are to evaluate the diagnostic accuracy of artificial intelligence to identify the genetic diseases with face phenotypes and then find the best algorithm. The systematic review will be conducted in accordance with the &quot;Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols&quot; guidelines. The following electronic databases will be searched: PubMed, Web of Science, IEEE, Ovid, Cochrane Library, EMBASE and China National Knowledge Infrastructure. Two reviewers will screen and select the titles and abstracts of the studies retrieved independently during the database searches and perform full-text reviews and extract available data. The main outcome measures include diagnostic accuracy, as defined by accuracy, recall, specificity, and precision. The descriptive forest plot and summary receiver operating characteristic curves will be used to represent the performance of diagnostic tests. Subgroup analysis will be performed for different algorithms aided diagnosis tests. The quality of study characteristics and methodology will be assessed using the Quality Assessment of Diagnostic Accuracy Studies 2 tool. Data will be synthesized by RevMan 5.3 and Meta-disc 1.4 software. The findings of this systematic review and meta-analysis will be disseminated in a relevant peer-reviewed journal and academic presentations. To our knowledge, there have not been any systematic review or meta-analysis relating to diagnosis performance of artificial intelligence in identifying the genetic diseases with face phenotypes. The findings would provide evidence to formulate a comprehensive understanding of applications using artificial intelligence in identifying the genetic diseases with face phenotypes and add considerable value in the future of precision medicine. DOI 10.17605/OSF.IO/P9KUH.","Many genetic diseases are known to have distinctive facial phenotypes, which are highly informative to provide an opportunity for automated detection. However, the diagnostic performance of artificial intelligence to identify genetic diseases with facial phenotypes requires further investigation. The objectives of this systematic review and meta-analysis are to evaluate the diagnostic accuracy of artificial intelligence to identify the genetic diseases with face phenotypes and then find the best algorithm. The systematic review will be conducted in accordance with the ""Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols"" guidelines. The following electronic databases will be searched: PubMed, Web of Science, IEEE, Ovid, Cochrane Library, EMBASE and China National Knowledge Infrastructure. Two reviewers will screen and select the titles and abstracts of the studies retrieved independently during the database searches and perform full-text reviews and extract available data. The main outcome measures include diagnostic accuracy, as defined by accuracy, recall, specificity, and precision. The descriptive forest plot and summary receiver operating characteristic curves will be used to represent the performance of diagnostic tests. Subgroup analysis will be performed for different algorithms aided diagnosis tests. The quality of study characteristics and methodology will be assessed using the Quality Assessment of Diagnostic Accuracy Studies 2 tool. Data will be synthesized by RevMan 5.3 and Meta-disc 1.4 software. The findings of this systematic review and meta-analysis will be disseminated in a relevant peer-reviewed journal and academic presentations. To our knowledge, there have not been any systematic review or meta-analysis relating to diagnosis performance of artificial intelligence in identifying the genetic diseases with face phenotypes. The findings would provide evidence to formulate a comprehensive understanding of applications using artificial intelligence in identifying the genetic diseases with face phenotypes and add considerable value in the future of precision medicine. DOI 10.17605/OSF.IO/P9KUH.","Qin, Quan, Wu, Liang, Li","Qin, Quan, Wu, Liang, Li",https://doi.org/10.1097/MD.0000000000020989,https://doi.org/10.1097/MD.0000000000020989,2021-08-03
16552.0,pubmed,pubmed,In-hospital Prognostic Value of Electrocardiographic Parameters Except ST-Segment Changes in Acute Myocardial Infarction: Literature Review and Future Perspectives,In-Hospital Prognostic Value of Electrocardiographic Parameters Other Than ST-Segment Changes in Acute Myocardial Infarction: Literature Review and Future Perspectives,"Electrocardiography (ECG) remains an irreplaceable tool in the management of the patients with myocardial infarction, with evaluation of the QRS and ST segment being the present major focus. Several ECG parameters have already been proposed to have prognostic value with regard to both in-hospital and long-term follow-up of patients. In this review, we discuss various ECG parameters other than ST segment changes, particularly with regard to their in-hospital prognostic importance. Our review not only evaluates the prognostic segments and parts of ECG, but also highlights the need for an integrative approach in a bigÃ‚Â data to re-assess the parameters reported to predict in-hospital prognosis. The evolving importance of artificial intelligence in evaluation of ECG, particularly with regard to predicting prognosis, and the potential integration with other patient characteristics to predict prognosis, are discussed.","Electrocardiography (ECG) remains an irreplaceable tool in the management of the patients with myocardial infarction, with evaluation of the QRS and ST segment being the present major focus. Several ECG parameters have already been proposed to have prognostic value with regard to both in-hospital and long-term follow-up of patients. In this review, we discuss various ECG parameters other than ST segment changes, particularly with regard to their in-hospital prognostic importance. Our review not only evaluates the prognostic segments and parts of ECG, but also highlights the need for an integrative approach in bigÂ data to re-assess the parameters reported to predict in-hospital prognosis. The evolving importance of artificial intelligence in evaluation of ECG, particularly with regard to predicting prognosis, and the potential integration with other patient characteristics to predict prognosis, are discussed.","HayÃ„Â±roÃ„Å¸lu, Lakhani, Tse, Ãƒâ€¡Ã„Â±nar, Ãƒâ€¡inier, TekkeÃ…Å¸in","HayÄ±roÄŸlu, Lakhani, Tse, Ã‡Ä±nar, Ã‡inier, TekkeÅŸin",https://doi.org/10.1016/j.hlc.2020.04.011,https://doi.org/10.1016/j.hlc.2020.04.011,2021-08-03
16560.0,pubmed,pubmed,Understanding the relationship between patient language and outcomes in internet-enabled cognitive behavioural therapy: A deep learning approach to automatic coding of session transcripts,Understanding the relationship between patient language and outcomes in internet-enabled cognitive behavioural therapy: A deep learning approach to automatic coding of session transcripts,"<b>Objective:</b> Understanding patient responses to psychotherapy is important in developing effective interventions. However, coding patient language is a resource-intensive exercise and difficult to perform at scale. Our aim was to develop a deep learning model to automatically identify patient utterances during text-based internet-enabled Cognitive Behavioural Therapy and to determine the association between utterances and clinical outcomes. <b>Method:</b> Using 340 manually annotated transcripts we trained a deep learning model to categorize patient utterances into one or more of five categories. The model was used to automatically code patient utterances from our entire data set of transcripts (Ã¢Ë†Â¼34,000 patients), and logistic regression analyses used to determine the association between both reliable improvement and engagement, and patient responses. <b>Results:</b> Our model reached human-level agreement on three of the five patient categories. Regression analyses revealed that increased counter change-talk (movement away from change) was associated with lower odds of both reliable improvement and engagement, while increased change-talk (movement towards change or self-exploration) was associated with increased odds of improvement and engagement. <b>Conclusions:</b> Deep learning provides an effective means of automatically coding patient utterances at scale. This approach enables the development of a data-driven understanding of the relationship between therapist and patient during therapy.","<b>Objective:</b> Understanding patient responses to psychotherapy is important in developing effective interventions. However, coding patient language is a resource-intensive exercise and difficult to perform at scale. Our aim was to develop a deep learning model to automatically identify patient utterances during text-based internet-enabled Cognitive Behavioural Therapy and to determine the association between utterances and clinical outcomes. <b>Method:</b> Using 340 manually annotated transcripts we trained a deep learning model to categorize patient utterances into one or more of five categories. The model was used to automatically code patient utterances from our entire data set of transcripts (âˆ¼34,000 patients), and logistic regression analyses used to determine the association between both reliable improvement and engagement, and patient responses. <b>Results:</b> Our model reached human-level agreement on three of the five patient categories. Regression analyses revealed that increased counter change-talk (movement away from change) was associated with lower odds of both reliable improvement and engagement, while increased change-talk (movement towards change or self-exploration) was associated with increased odds of improvement and engagement. <b>Conclusions:</b> Deep learning provides an effective means of automatically coding patient utterances at scale. This approach enables the development of a data-driven understanding of the relationship between therapist and patient during therapy.","Ewbank, Cummins, Tablan, Catarino, Buchholz, Blackwell","Ewbank, Cummins, Tablan, Catarino, Buchholz, Blackwell",https://doi.org/10.1080/10503307.2020.1788740,https://doi.org/10.1080/10503307.2020.1788740,2021-08-03
16564.0,pubmed,pubmed,Combined Orthodontic-Orthognathic Approach for Dentofacial Deformities as a Risk Factor for Gingival Recession: A Systematic Review,Combined Orthodontic-Orthognathic Approach for Dentofacial Deformities as a Risk Factor for Gingival Recession: A Systematic Review,"The objective of this systematic review was to evaluate the risk of development of gingival recession (GR) as a result of the combined orthodontic-orthognathic approach. The PubMed, Google Scholar, ClinicalTrials.gov, and Cochrane Library databases were searched. Included articles mentioned gingival parameters in their materials and methods sections; specifically, they evaluated GR, which was measured before and after the surgical procedure. Study parameters such as methodology, evaluation period, sample characteristics, and follow-up were extracted by 2 authors independently. In total, 133 relevant articles were identified from the databases; after screening and full-text analysis, 9 studies were included in this systematic review. Meta-analysis could not be conducted because of considerable heterogeneity in methods. The incidence of GR in the range of 0.5 to 3.0Ã‚Â mm as a significant clinical finding after orthognathic surgery showed statistically significant differences in all included articles. Among patients with GR, the mean age was 23.0 to 29.5Ã‚Â years and the mandibular incisors were the most common site. However,Ã‚Â no case of recession greater than 3.0 mm was associated with surgery. On the basis of the findings of this review, GR of approximately 0.5 to 3.0Ã‚Â mm is a common finding after the combined orthodontic-orthognathic approach. Although periodontal damage up to 3Ã‚Â mm can be observed as an isolated finding in mainly the incisors, true recession is not associated with orthognathic surgery in general.","The objective of this systematic review was to evaluate the risk of development of gingival recession (GR) as a result of the combined orthodontic-orthognathic approach. The PubMed, Google Scholar, ClinicalTrials.gov, and Cochrane Library databases were searched. Included articles mentioned gingival parameters in their materials and methods sections; specifically, they evaluated GR, which was measured before and after the surgical procedure. Study parameters such as methodology, evaluation period, sample characteristics, and follow-up were extracted by 2 authors independently. In total, 133 relevant articles were identified from the databases; after screening and full-text analysis, 9 studies were included in this systematic review. Meta-analysis could not be conducted because of considerable heterogeneity in methods. The incidence of GR in the range of 0.5 to 3.0Â mm as a significant clinical finding after orthognathic surgery showed statistically significant differences in all included articles. Among patients with GR, the mean age was 23.0 to 29.5Â years and the mandibular incisors were the most common site. However,Â no case of recession greater than 3.0 mm was associated with surgery. On the basis of the findings of this review, GR of approximately 0.5 to 3.0Â mm is a common finding after the combined orthodontic-orthognathic approach. Although periodontal damage up to 3Â mm can be observed as an isolated finding in mainly the incisors, true recession is not associated with orthognathic surgery in general.","Mota de Paulo, Herbert de Oliveira Mendes, GonÃƒÂ§alves Filho, MarÃƒÂ§al","Mota de Paulo, Herbert de Oliveira Mendes, GonÃ§alves Filho, MarÃ§al",https://doi.org/10.1016/j.joms.2020.05.040,https://doi.org/10.1016/j.joms.2020.05.040,2021-08-03
16572.0,pubmed,pubmed,Shared and distinct functional networks for empathy and pain processing: a systematic review and meta-analysis of fMRI studies,Shared and distinct functional networks for empathy and pain processing: a systematic review and meta-analysis of fMRI studies,"Empathy for pain is a complex phenomenon incorporating sensory, cognitive and affective processes. Functional neuroimaging studies indicate a rich network of brain activations for empathic processing. However, previous research focused on core activations in bilateral anterior insula (AI) and anterior cingulate/anterior midcingulate cortex (ACC/aMCC) which are also typically present during nociceptive (pain) processing. Theoretical understanding of empathy would benefit from empirical investigation of shared and contrasting brain activations for empathic and nociceptive processing. Thirty-nine empathy for observed pain studies (1112 participants; 527 foci) were selected by systematic review. Coordinate based meta-analysis (activation likelihood estimation) was performed and novel contrast analyses compared neurobiological processing of empathy with a comprehensive meta-analysis of 180 studies of nociceptive processing (Tanasescu etÃ‚Â al., 2016). Conjunction analysis indicated overlapping activations for empathy and nociception in AI, aMCC, somatosensory and inferior frontal regions. Contrast analysis revealed increased likelihood of activation for empathy, relative to nociception, in bilateral supramarginal, inferior frontal and occipitotemporal regions. Nociception preferentially activated bilateral posterior insula, somatosensory cortex and aMCC. Our findings support the likelihood of shared and distinct neural networks for empathic, relative to nociceptive, processing. This offers succinct empirical support for recent tiered or modular theoretical accounts of empathy.","Empathy for pain is a complex phenomenon incorporating sensory, cognitive and affective processes. Functional neuroimaging studies indicate a rich network of brain activations for empathic processing. However, previous research focused on core activations in bilateral anterior insula (AI) and anterior cingulate/anterior midcingulate cortex (ACC/aMCC) which are also typically present during nociceptive (pain) processing. Theoretical understanding of empathy would benefit from empirical investigation of shared and contrasting brain activations for empathic and nociceptive processing. Thirty-nine empathy for observed pain studies (1112 participants; 527 foci) were selected by systematic review. Coordinate based meta-analysis (activation likelihood estimation) was performed and novel contrast analyses compared neurobiological processing of empathy with a comprehensive meta-analysis of 180 studies of nociceptive processing (Tanasescu etÂ al., 2016). Conjunction analysis indicated overlapping activations for empathy and nociception in AI, aMCC, somatosensory and inferior frontal regions. Contrast analysis revealed increased likelihood of activation for empathy, relative to nociception, in bilateral supramarginal, inferior frontal and occipitotemporal regions. Nociception preferentially activated bilateral posterior insula, somatosensory cortex and aMCC. Our findings support the likelihood of shared and distinct neural networks for empathic, relative to nociceptive, processing. This offers succinct empirical support for recent tiered or modular theoretical accounts of empathy.","Fallon, Roberts, Stancak","Fallon, Roberts, Stancak",https://doi.org/10.1093/scan/nsaa090,https://doi.org/10.1093/scan/nsaa090,2021-08-03
16575.0,pubmed,pubmed,A study of MRI-based radiomics biomarkers for sacroiliitis and spondyloarthritis,A study of MRI-based radiomics biomarkers for sacroiliitis and spondyloarthritis,"To evaluate the performance of texture-based biomarkers by radiomic analysis using magnetic resonance imaging (MRI) of patients with sacroiliitis secondary to spondyloarthritis (SpA). The determination of sacroiliac joints inflammatory activity supports the drug management in these diseases. Sacroiliac joints (SIJ) MRI examinations of 47 patients were evaluated. Thirty-seven patients had SpA diagnoses (27 axial SpA and ten peripheral SpA) which was established previously after clinical and laboratory follow-up. To perform the analysis, the SIJ MRI was first segmented and warped. Second, radiomics biomarkers were extracted from the warped MRI images for associative analysis with sacroiliitis and the SpA subtypes. Finally, statistical and machine learning methods were applied to assess the associations of the radiomics texture-based biomarkers with clinical outcomes. All diagnostic performances obtained with individual or combined biomarkers reached areas under the receiver operating characteristic curves Ã¢â€°Â¥Ã¢â‚¬â€°0.80 regarding SpA related sacroiliitis andÃ‚Â and SpA subtypes classification. Radiomics texture-based analysis showed significant differences between the positive and negative SpA groups and differentiated the axial and peripheral subtypes (PÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°0.001). In addition, the radiomics analysis was also able to correctly identify the disease even in the absence of active inflammation. We concluded that the application of the radiomic approach constitutes a potential noninvasive tool to aid the diagnosis of sacroiliitis and for SpA subclassifications based on MRI of sacroiliac joints.","To evaluate the performance of texture-based biomarkers by radiomic analysis using magnetic resonance imaging (MRI) of patients with sacroiliitis secondary to spondyloarthritis (SpA). The determination of sacroiliac joints inflammatory activity supports the drug management in these diseases. Sacroiliac joints (SIJ) MRI examinations of 47 patients were evaluated. Thirty-seven patients had SpA diagnoses (27 axial SpA and ten peripheral SpA) which was established previously after clinical and laboratory follow-up. To perform the analysis, the SIJ MRI was first segmented and warped. Second, radiomics biomarkers were extracted from the warped MRI images for associative analysis with sacroiliitis and the SpA subtypes. Finally, statistical and machine learning methods were applied to assess the associations of the radiomics texture-based biomarkers with clinical outcomes. All diagnostic performances obtained with individual or combined biomarkers reached areas under the receiver operating characteristic curves â‰¥â€‰0.80 regarding SpA related sacroiliitis andÂ and SpA subtypes classification. Radiomics texture-based analysis showed significant differences between the positive and negative SpA groups and differentiated the axial and peripheral subtypes (Pâ€‰&lt;â€‰0.001). In addition, the radiomics analysis was also able to correctly identify the disease even in the absence of active inflammation. We concluded that the application of the radiomic approach constitutes a potential noninvasive tool to aid the diagnosis of sacroiliitis and for SpA subclassifications based on MRI of sacroiliac joints.","TenÃƒÂ³rio, Faleiros, Junior, Dalto, Assad, Louzada-Junior, Yoshida, Nogueira-Barbosa, de Azevedo-Marques","TenÃ³rio, Faleiros, Junior, Dalto, Assad, Louzada-Junior, Yoshida, Nogueira-Barbosa, de Azevedo-Marques",https://doi.org/10.1007/s11548-020-02219-7,https://doi.org/10.1007/s11548-020-02219-7,2021-08-03
16577.0,pubmed,pubmed,Rapid antigen detection and molecular tests for group A streptococcal infections for acute sore throat: systematic reviews and economic evaluation,Rapid antigen detection and molecular tests for group A streptococcal infections for acute sore throat: systematic reviews and economic evaluation,"Sore throat is a common condition caused by an infection of the airway. Most cases are of a viral nature; however, a number of these infections may be caused by the group A <i>Streptococcus</i> bacterium. Most viral and bacterial sore throat infections resolve spontaneously within a few weeks. Point-of-care testing in primary care has been recognised as an emerging technology for aiding targeted antibiotic prescribing for sore throat in cases that do not spontaneously resolve. Systematically review the evidence for 21 point-of-care tests for detecting group A <i>Streptococcus</i> bacteria and develop a de novo economic model to compare the cost-effectiveness of point-of-care tests alongside clinical scoring tools with the cost-effectiveness of clinical scoring tools alone for patients managed in primary care and hospital settings. Multiple electronic databases were searched from inception to March 2019. The following databases were searched in November and December 2018 and searches were updated in March 2019: MEDLINE [via OvidSP (Health First, Rockledge, FL, USA)], MEDLINE In-Process &amp; Other Non-Indexed Citations (via OvidSP), MEDLINE Epub Ahead of Print (via OvidSP), MEDLINE Daily Update (via OvidSP), EMBASE (via OvidSP), Cochrane Database of Systematic Reviews [via Wiley Online Library (John Wiley &amp; Sons, Inc., Hoboken, NJ, USA)], Cochrane Central Register of Controlled Trials (CENTRAL) (via Wiley Online Library), Database of Abstracts of Reviews of Effects (DARE) (via Centre for Reviews and Dissemination), Health Technology Assessment database (via the Centre for Reviews and Dissemination), Science Citation Index and Conference Proceedings [via the Web of ScienceÃ¢â€žÂ¢ (Clarivate Analytics, Philadelphia, PA, USA)] and the PROSPERO International Prospective Register of Systematic Reviews (via the Centre for Reviews and Dissemination). Eligible studies included those of people aged Ã¢â€°Â¥Ã¢â‚¬â€°5 years presenting with sore throat symptoms, studies comparing point-of-care testing with antibiotic-prescribing decisions, studies of test accuracy and studies of cost-effectiveness. Quality assessment of eligible studies was undertaken. Meta-analysis of sensitivity and specificity was carried out for tests with sufficient data. A decision tree model estimated costs and quality-adjusted life-years from an NHS and Personal Social Services perspective. The searches identified 38 studies of clinical effectiveness and three studies of cost-effectiveness. Twenty-six full-text articles and abstracts reported on the test accuracy of point-of-care tests and/or clinical scores with biological culture as a reference standard. In the population of interest (patients with Centor/McIsaac scores of Ã¢â€°Â¥Ã¢â‚¬â€°3 points or FeverPAIN scores of Ã¢â€°Â¥Ã¢â‚¬â€°4 points), point estimates were 0.829 to 0.946 for sensitivity and 0.849 to 0.991 for specificity. There was considerable heterogeneity, even for studies using the same point-of-care test, suggesting that is unlikely that any single study will have accurately captured a test's true performance. There is some randomised controlled trial evidence to suggest that the use of rapid antigen detection tests may help to reduce antibiotic-prescribing rates. Sensitivity and specificity estimates for each test in each age group and care setting combination were obtained using meta-analyses where appropriate. Any apparent differences in test accuracy may not be attributable to the tests, and may have been caused by known differences in the studies, latent characteristics or chance. Fourteen of the 21 tests reviewed were included in the economic modelling, and these tests were not cost-effective within the current National Institute for Health and Care Excellence's cost-effectiveness thresholds. Uncertainties in the cost-effectiveness estimates included model parameter inputs and assumptions that increase the cost of testing, and the penalty for antibiotic overprescriptions. No information was identified for the elderly population or pharmacy setting. It was not possible to identify which test is the most accurate owing to the paucity of evidence. The systematic review and the cost-effectiveness models identified uncertainties around the adoption of point-of-care tests in primary and secondary care settings. Although sensitivity and specificity estimates are promising, we have little information to establish the most accurate point-of-care test. Further research is needed to understand the test accuracy of point-of-care tests in the proposed NHS pathway and in comparable settings and patient groups. The protocol of the review is registered as PROSPERO CRD42018118653. This project was funded by the National Institute for Health Research (NIHR) Health Technology Assessment programme and will be published in full in <i>Health Technology Assessment</i>; Vol. 24, No. 31. See the NIHR Journals Library website for further project information. Sore throat is a common condition caused by an infection of the airway. Most cases are viral; however, a small number may be caused by the group A <i>Streptococcus</i> bacterium. Most viral and bacterial sore throat infections resolve spontaneously within a few weeks; however, some may be more serious and require antibiotics. Currently, National Institute for Health and Care Excellence guidance recommends the use of clinical scoring tools to identify patients for whom antibiotic treatment is appropriate. Ideally, a throat swab culture should be obtained to identify the organism causing the infection in cases in which diagnosis is uncertain. However, this takes time, causing potential delays in administering the correct treatment. Point-of-care tests can be administered at or near the site of the patient; therefore, they are much faster. Our review considered evidence for the test accuracy and cost-effectiveness of 21 point-of-care tests for detecting group A <i>Streptococcus</i> bacteria. We built an economic model, predicting costs and benefits for adults and children in a primary care or hospital setting. The findings will support the National Institute for Health and Care Excellence to make recommendations about the use of these point-of-care tests for detecting group A <i>Streptococcus</i> bacteria in the NHS in England and Wales. The clinical effectiveness review found 38 relevant studies; of these, 26 reported on the accuracy of point-of-care tests. These studies found wide variation in the accuracy of the tests. The quality of the evidence was weak and there was little information on some of the 21 tests. As the studies were all so different, it was not possible to identify which test is the most accurate. The economic model found considerable uncertainty about how costs and benefits would change if point-of-care tests were introduced in different care settings. Further research is needed to see whether or not point-of-care testing provides value for money.","Sore throat is a common condition caused by an infection of the airway. Most cases are of a viral nature; however, a number of these infections may be caused by the group A <i>Streptococcus</i> bacterium. Most viral and bacterial sore throat infections resolve spontaneously within a few weeks. Point-of-care testing in primary care has been recognised as an emerging technology for aiding targeted antibiotic prescribing for sore throat in cases that do not spontaneously resolve. Systematically review the evidence for 21 point-of-care tests for detecting group A <i>Streptococcus</i> bacteria and develop a de novo economic model to compare the cost-effectiveness of point-of-care tests alongside clinical scoring tools with the cost-effectiveness of clinical scoring tools alone for patients managed in primary care and hospital settings. Multiple electronic databases were searched from inception to March 2019. The following databases were searched in November and December 2018 and searches were updated in March 2019: MEDLINE [via OvidSP (Health First, Rockledge, FL, USA)], MEDLINE In-Process &amp; Other Non-Indexed Citations (via OvidSP), MEDLINE Epub Ahead of Print (via OvidSP), MEDLINE Daily Update (via OvidSP), EMBASE (via OvidSP), Cochrane Database of Systematic Reviews [via Wiley Online Library (John Wiley &amp; Sons, Inc., Hoboken, NJ, USA)], Cochrane Central Register of Controlled Trials (CENTRAL) (via Wiley Online Library), Database of Abstracts of Reviews of Effects (DARE) (via Centre for Reviews and Dissemination), Health Technology Assessment database (via the Centre for Reviews and Dissemination), Science Citation Index and Conference Proceedings [via the Web of Scienceâ„¢ (Clarivate Analytics, Philadelphia, PA, USA)] and the PROSPERO International Prospective Register of Systematic Reviews (via the Centre for Reviews and Dissemination). Eligible studies included those of people aged â‰¥â€‰5 years presenting with sore throat symptoms, studies comparing point-of-care testing with antibiotic-prescribing decisions, studies of test accuracy and studies of cost-effectiveness. Quality assessment of eligible studies was undertaken. Meta-analysis of sensitivity and specificity was carried out for tests with sufficient data. A decision tree model estimated costs and quality-adjusted life-years from an NHS and Personal Social Services perspective. The searches identified 38 studies of clinical effectiveness and three studies of cost-effectiveness. Twenty-six full-text articles and abstracts reported on the test accuracy of point-of-care tests and/or clinical scores with biological culture as a reference standard. In the population of interest (patients with Centor/McIsaac scores of â‰¥â€‰3 points or FeverPAIN scores of â‰¥â€‰4 points), point estimates were 0.829 to 0.946 for sensitivity and 0.849 to 0.991 for specificity. There was considerable heterogeneity, even for studies using the same point-of-care test, suggesting that is unlikely that any single study will have accurately captured a test's true performance. There is some randomised controlled trial evidence to suggest that the use of rapid antigen detection tests may help to reduce antibiotic-prescribing rates. Sensitivity and specificity estimates for each test in each age group and care setting combination were obtained using meta-analyses where appropriate. Any apparent differences in test accuracy may not be attributable to the tests, and may have been caused by known differences in the studies, latent characteristics or chance. Fourteen of the 21 tests reviewed were included in the economic modelling, and these tests were not cost-effective within the current National Institute for Health and Care Excellence's cost-effectiveness thresholds. Uncertainties in the cost-effectiveness estimates included model parameter inputs and assumptions that increase the cost of testing, and the penalty for antibiotic overprescriptions. No information was identified for the elderly population or pharmacy setting. It was not possible to identify which test is the most accurate owing to the paucity of evidence. The systematic review and the cost-effectiveness models identified uncertainties around the adoption of point-of-care tests in primary and secondary care settings. Although sensitivity and specificity estimates are promising, we have little information to establish the most accurate point-of-care test. Further research is needed to understand the test accuracy of point-of-care tests in the proposed NHS pathway and in comparable settings and patient groups. The protocol of the review is registered as PROSPERO CRD42018118653. This project was funded by the National Institute for Health Research (NIHR) Health Technology Assessment programme and will be published in full in <i>Health Technology Assessment</i>; Vol. 24, No. 31. See the NIHR Journals Library website for further project information. Sore throat is a common condition caused by an infection of the airway. Most cases are viral; however, a small number may be caused by the group A <i>Streptococcus</i> bacterium. Most viral and bacterial sore throat infections resolve spontaneously within a few weeks; however, some may be more serious and require antibiotics. Currently, National Institute for Health and Care Excellence guidance recommends the use of clinical scoring tools to identify patients for whom antibiotic treatment is appropriate. Ideally, a throat swab culture should be obtained to identify the organism causing the infection in cases in which diagnosis is uncertain. However, this takes time, causing potential delays in administering the correct treatment. Point-of-care tests can be administered at or near the site of the patient; therefore, they are much faster. Our review considered evidence for the test accuracy and cost-effectiveness of 21 point-of-care tests for detecting group A <i>Streptococcus</i> bacteria. We built an economic model, predicting costs and benefits for adults and children in a primary care or hospital setting. The findings will support the National Institute for Health and Care Excellence to make recommendations about the use of these point-of-care tests for detecting group A <i>Streptococcus</i> bacteria in the NHS in England and Wales. The clinical effectiveness review found 38 relevant studies; of these, 26 reported on the accuracy of point-of-care tests. These studies found wide variation in the accuracy of the tests. The quality of the evidence was weak and there was little information on some of the 21 tests. As the studies were all so different, it was not possible to identify which test is the most accurate. The economic model found considerable uncertainty about how costs and benefits would change if point-of-care tests were introduced in different care settings. Further research is needed to see whether or not point-of-care testing provides value for money.","Fraser, Gallacher, Achana, Court, Taylor-Phillips, Nduka, Stinton, Willans, Gill, Mistry","Fraser, Gallacher, Achana, Court, Taylor-Phillips, Nduka, Stinton, Willans, Gill, Mistry",https://doi.org/10.3310/hta24310,https://doi.org/10.3310/hta24310,2021-08-03
16584.0,pubmed,pubmed,COVID-KOP: Integrating Emerging COVID-19 Data with the ROBOKOP Database,COVID-KOP: Integrating Emerging COVID-19 Data with the ROBOKOP Database,"In response to the COVID-19 pandemic, we established COVID-KOP, a new knowledgebase integrating the existing ROBOKOP biomedical knowledge graph with information from recent biomedical literature on COVID-19 annotated in the CORD-19 collection. COVID-KOP can be used effectively to test new hypotheses concerning repurposing of known drugs and clinical drug candidates against COVID-19. COVID-KOP is freely accessible at &lt;a href=&quot;https://covidkop.renci.org/&quot;&gt;https://covidkop.renci.org/&lt;/a&gt;. For code and instructions for the original ROBOKOP, see: https://github.com/NCATS-Gamma/robokop.","In response to the COVID-19 pandemic, we established COVID-KOP, a new knowledgebase integrating the existing ROBOKOP biomedical knowledge graph with information from recent biomedical literature on COVID-19 annotated in the CORD-19 collection. COVID-KOP can be used effectively to test new hypotheses concerning repurposing of known drugs and clinical drug candidates against COVID-19. COVID-KOP is freely accessible at &lt;a href=""https://covidkop.renci.org/""&gt;https://covidkop.renci.org/&lt;/a&gt;. For code and instructions for the original ROBOKOP, see: https://github.com/NCATS-Gamma/robokop.","Korn, Bobrowski, Li, Kebede, Wang, Owen, Vaidya, Muratov, Chirkova, Bizon, Tropsha","Korn, Bobrowski, Li, Kebede, Wang, Owen, Vaidya, Muratov, Chirkova, Bizon, Tropsha",https://doi.org/10.26434/chemrxiv.12462623,https://doi.org/10.26434/chemrxiv.12462623,2021-08-03
16589.0,pubmed,pubmed,Performance of artificial intelligence in colonoscopy for adenoma and polyp detection: a systematic review and meta-analysis,Performance of artificial intelligence in colonoscopy for adenoma and polyp detection: a systematic review and meta-analysis,"One-fourth of colorectal neoplasia are missed at screening colonoscopy, representing the main cause of interval colorectal cancer. Deep learning systems with real-time computer-aided polyp detection (CADe) showed high accuracy in artificial settings, and preliminary randomized controlled trials (RCTs) reported favorable outcomes in the clinical setting. The aim of this meta-analysis was to summarize available RCTs on the performance of CADe systems in colorectal neoplasia detection. We searched MEDLINE, EMBASE, and Cochrane Central databases until March 2020 for RCTs reporting diagnostic accuracy of CADe systems in the detection of colorectal neoplasia. The primary outcome was pooled adenoma detection rate (ADR), and secondary outcomes were adenoma per colonoscopy (APC) according to size, morphology, and location; advanced APC; polyp detection rate; polyps per colonoscopy; and sessile serrated lesions per colonoscopy. We calculated risk ratios (RRs), performed subgroup and sensitivity analyses, and assessed heterogeneity and publication bias. Overall, 5 randomized controlled trials (4354 patients) were included in the final analysis. Pooled ADR was significantly higher in the CADe group than in the control group (791/2163 [36.6%] vs 558/2191 [25.2%]; RR, 1.44; 95% confidence interval [CI], 1.27-1.62; PÃ‚Â &lt; .01; I<sup>2</sup>Ã‚Â = 42%). APC was also higher in the CADe group compared with control (1249/2163 [.58] vs 779/2191 [.36]; RR, 1.70; 95% CI, 1.53-1.89; PÃ‚Â &lt; .01; I<sup>2</sup>Ã‚Â = 33%). APC was higher forÃ‚Â Ã¢â€°Â¤5-mm (RR, 1.69; 95% CI, 1.48-1.84), 6- to 9-mm (RR, 1.44; 95% CI, 1.19-1.75), andÃ‚Â Ã¢â€°Â¥10-mm adenomas (RR, 1.46; 95% CI, 1.04-2.06) and for proximal (RR, 1.59; 95% CI, 1.34-1.88), distal (RR, 1.68; 95% CI, 1.50-1.88), flat (RR, 1.78; 95% CI, 1.47-2.15), and polypoid morphology (RR, 1.54; 95% CI, 1.40-1.68). Regarding histology, CADe resulted in a higher sessile serrated lesion per colonoscopy (RR, 1.52; 95% CI, 1.14-2.02), whereas a nonsignificant trend for advanced ADR was found (RR, 1.35; 95% CI, .74-2.47; PÃ‚Â = .33; I<sup>2</sup>Ã‚Â = 69%). Level of evidence for RCTs was graded as moderate. According to available evidence, the incorporation of artificial intelligence as aid for detection of colorectal neoplasia results in a significant increase in the detection of colorectal neoplasia, and such effect is independent from main adenoma characteristics.","One-fourth of colorectal neoplasia are missed at screening colonoscopy, representing the main cause of interval colorectal cancer. Deep learning systems with real-time computer-aided polyp detection (CADe) showed high accuracy in artificial settings, and preliminary randomized controlled trials (RCTs) reported favorable outcomes in the clinical setting. The aim of this meta-analysis was to summarize available RCTs on the performance of CADe systems in colorectal neoplasia detection. We searched MEDLINE, EMBASE, and Cochrane Central databases until March 2020 for RCTs reporting diagnostic accuracy of CADe systems in the detection of colorectal neoplasia. The primary outcome was pooled adenoma detection rate (ADR), and secondary outcomes were adenoma per colonoscopy (APC) according to size, morphology, and location; advanced APC; polyp detection rate; polyps per colonoscopy; and sessile serrated lesions per colonoscopy. We calculated risk ratios (RRs), performed subgroup and sensitivity analyses, and assessed heterogeneity and publication bias. Overall, 5 randomized controlled trials (4354 patients) were included in the final analysis. Pooled ADR was significantly higher in the CADe group than in the control group (791/2163 [36.6%] vs 558/2191 [25.2%]; RR, 1.44; 95% confidence interval [CI], 1.27-1.62; PÂ &lt; .01; I<sup>2</sup>Â = 42%). APC was also higher in the CADe group compared with control (1249/2163 [.58] vs 779/2191 [.36]; RR, 1.70; 95% CI, 1.53-1.89; PÂ &lt; .01; I<sup>2</sup>Â = 33%). APC was higher forÂ â‰¤5-mm (RR, 1.69; 95% CI, 1.48-1.84), 6- to 9-mm (RR, 1.44; 95% CI, 1.19-1.75), andÂ â‰¥10-mm adenomas (RR, 1.46; 95% CI, 1.04-2.06) and for proximal (RR, 1.59; 95% CI, 1.34-1.88), distal (RR, 1.68; 95% CI, 1.50-1.88), flat (RR, 1.78; 95% CI, 1.47-2.15), and polypoid morphology (RR, 1.54; 95% CI, 1.40-1.68). Regarding histology, CADe resulted in a higher sessile serrated lesion per colonoscopy (RR, 1.52; 95% CI, 1.14-2.02), whereas a nonsignificant trend for advanced ADR was found (RR, 1.35; 95% CI, .74-2.47; PÂ = .33; I<sup>2</sup>Â = 69%). Level of evidence for RCTs was graded as moderate. According to available evidence, the incorporation of artificial intelligence as aid for detection of colorectal neoplasia results in a significant increase in the detection of colorectal neoplasia, and such effect is independent from main adenoma characteristics.","Hassan, Spadaccini, Iannone, Maselli, Jovani, Chandrasekar, Antonelli, Yu, Areia, Dinis-Ribeiro, Bhandari, Sharma, Rex, RÃƒÂ¶sch, Wallace, Repici","Hassan, Spadaccini, Iannone, Maselli, Jovani, Chandrasekar, Antonelli, Yu, Areia, Dinis-Ribeiro, Bhandari, Sharma, Rex, RÃ¶sch, Wallace, Repici",https://doi.org/10.1016/j.gie.2020.06.059,https://doi.org/10.1016/j.gie.2020.06.059,2021-08-03
16594.0,pubmed,pubmed,[Machine learning-based method for interpreting the guidelines of the diagnosis and treatment of COVID-19],[Machine learning-based method for interpreting the guidelines of the diagnosis and treatment of COVID-19],"The outbreak of pneumonia caused by novel coronavirus (COVID-19) at the end of 2019 was a major public health emergency in human history. In a short period of time, Chinese medical workers have experienced the gradual understanding, evidence accumulation and clinical practice of the unknown virus. So far, National Health Commission of the People's Republic of China has issued seven trial versions of the &quot;Guidelines for the Diagnosis and Treatment of COVID-19&quot;. However, it is difficult for clinicians and laymen to quickly and accurately distinguish the similarities and differences among the different versions and locate the key points of the new version. This paper reports a computer-aided intelligent analysis method based on machine learning, which can automatically analyze the similarities and differences of different treatment plans, present the focus of the new version to doctors, reduce the difficulty in interpreting the &quot;diagnosis and treatment plan&quot; for the professional, and help the general public better understand the professional knowledge of medicine. Experimental results show that this method can achieve the topic prediction and matching of the new version of the program text through unsupervised learning of the previous versions of the program topic with an accuracy of 100%. It enables the computer interpretation of &quot;diagnosis and treatment plan&quot; automatically and intelligently. 2019 Ã¥Â¹Â´Ã¥Âºâ€¢Ã¦Å¡Â´Ã¥Ââ€˜Ã§Å¡â€žÃ¦â€“Â°Ã¥Å¾â€¹Ã¥â€ Â Ã§Å Â¶Ã§â€”â€¦Ã¦Â¯â€™Ã¨â€šÂºÃ§â€šÅ½Ã¯Â¼Ë†COVID-19Ã¯Â¼â€°Ã§â€“Â«Ã¦Æ’â€¦Ã¦ËœÂ¯Ã¤ÂºÂºÃ§Â±Â»Ã¥ÂÂ²Ã¤Â¸Å Ã¤Â¸â‚¬Ã¦Â¬Â¡Ã©â€¡ÂÃ¥Â¤Â§Ã§ÂªÂÃ¥Ââ€˜Ã¥â€¦Â¬Ã¥â€¦Â±Ã¥ÂÂ«Ã§â€Å¸Ã¤Âºâ€¹Ã¤Â»Â¶Ã£â‚¬â€šÃ¤Â¸Â­Ã¥â€ºÂ½Ã¥Å’Â»Ã¥Â­Â¦Ã¥Â·Â¥Ã¤Â½Å“Ã¨â‚¬â€¦Ã¥Å“Â¨Ã§Å¸Â­Ã¦â€”Â¶Ã©â€”Â´Ã¥â€ â€¦Ã¯Â¼Å’Ã§Â»ÂÃ¥Å½â€ Ã¤Âºâ€ Ã¥Â¯Â¹Ã¨Â¯Â¥Ã¦Å“ÂªÃ§Å¸Â¥Ã§â€”â€¦Ã¦Â¯â€™Ã§Å¡â€žÃ©â‚¬ÂÃ¦Â­Â¥Ã¨Â®Â¤Ã¨Â¯â€ Ã£â‚¬ÂÃ¨Â¯ÂÃ¦ÂÂ®Ã§Â§Â¯Ã§Â´Â¯Ã¥â€™Å’Ã¤Â¸Â´Ã¥ÂºÅ Ã¥Â®Å¾Ã¨Â·ÂµÃ£â‚¬â€šÃ¦Ë†ÂªÃ¨â€¡Â³Ã§â€ºÂ®Ã¥â€°ÂÃ¯Â¼Å’Ã¤Â¸Â­Ã¥â€ºÂ½Ã¥â€ºÂ½Ã¥Â®Â¶Ã¥ÂÂ«Ã§â€Å¸Ã¥ÂÂ¥Ã¥ÂºÂ·Ã¥Â§â€Ã¥â€˜ËœÃ¤Â¼Å¡Ã¥Å“Â¨Ã¦â€¢Â°Ã¥ÂÂÃ¥Â¤Â©Ã¥â€ â€¦Ã¥Â¯â€ Ã©â€ºâ€ Ã¥Ââ€˜Ã¥Â¸Æ’Ã¤Âºâ€ Ã¤Â¸Æ’Ã¤Â¸ÂªÃ§â€°Ë†Ã¦Å“Â¬Ã§Å¡â€žÃ£â‚¬Å Ã¦â€“Â°Ã¥Å¾â€¹Ã¥â€ Â Ã§Å Â¶Ã§â€”â€¦Ã¦Â¯â€™Ã¦â€žÅ¸Ã¦Å¸â€œÃ§Å¡â€žÃ¨â€šÂºÃ§â€šÅ½Ã¨Â¯Å Ã§â€“â€”Ã¦â€“Â¹Ã¦Â¡Ë†Ã£â‚¬â€¹Ã¯Â¼Ë†Ã§Â®â‚¬Ã§Â§Â°Ã£â‚¬Å Ã¨Â¯Å Ã§â€“â€”Ã¦â€“Â¹Ã¦Â¡Ë†Ã£â‚¬â€¹Ã¯Â¼â€°Ã£â‚¬â€šÃ§â€žÂ¶Ã¨â‚¬Å’Ã¯Â¼Å’Ã¥Â¿Â«Ã©â‚¬Å¸Ã¥â€¡â€ Ã§Â¡Â®Ã¥Å“Â°Ã¦Â¯â€Ã¨Â¾Æ’Ã¥Ââ€žÃ§â€°Ë†Ã¦Å“Â¬Ã§Å¡â€žÃ¥Â¼â€šÃ¥ÂÅ’Ã¥â€™Å’Ã¦Å½Å’Ã¦ÂÂ¡Ã¦â€“Â°Ã§â€°Ë†Ã¦Å“Â¬Ã§Å¡â€žÃ©â€¡ÂÃ§â€šÂ¹Ã¥Â¯Â¹Ã¤Â¸Â´Ã¥ÂºÅ Ã¥Å’Â»Ã¦Å Â¤Ã¤ÂºÂºÃ¥â€˜ËœÃ¥â€™Å’Ã©ÂÅ¾Ã¤Â¸â€œÃ¤Â¸Å¡Ã¤ÂºÂºÃ¥â€˜ËœÃ¦ÂÂ¥Ã¨Â¯Â´Ã¥Â­ËœÃ¥Å“Â¨Ã¤Â¸â‚¬Ã¥Â®Å¡Ã¥â€ºÂ°Ã©Å¡Â¾Ã£â‚¬â€šÃ¦Å“Â¬Ã¦â€“â€¡Ã¦ÂÂÃ¥â€¡ÂºÃ¤Â¸â‚¬Ã§Â§ÂÃ¥Å¸ÂºÃ¤ÂºÅ½Ã¦Å“ÂºÃ¥â„¢Â¨Ã¥Â­Â¦Ã¤Â¹Â Ã§Å¡â€žÃ¨Â®Â¡Ã§Â®â€”Ã¦Å“ÂºÃ¨Â¾â€¦Ã¥Å Â©Ã¦â„¢ÂºÃ¨Æ’Â½Ã¥Ë†â€ Ã¦Å¾ÂÃ¦â€“Â¹Ã¦Â³â€¢Ã¯Â¼Å’Ã¥Â¯Â¹Ã¦â€“â€¡Ã¦Å“Â¬Ã¤Â¸Â»Ã©Â¢ËœÃ¨Â¿â€ºÃ¨Â¡Å’Ã¦â€”Â Ã§â€ºâ€˜Ã§ÂÂ£Ã¥Â­Â¦Ã¤Â¹Â Ã¯Â¼Å’Ã¨â€¡ÂªÃ¥Å Â¨Ã¥Ë†â€ Ã¦Å¾ÂÃ¤Â¸ÂÃ¥ÂÅ’Ã§â€°Ë†Ã¦Å“Â¬Ã£â‚¬Å Ã¨Â¯Å Ã§â€“â€”Ã¦â€“Â¹Ã¦Â¡Ë†Ã£â‚¬â€¹Ã§Å¡â€žÃ¥Â¼â€šÃ¥ÂÅ’Ã¯Â¼Å’Ã¤Â¸Â»Ã¥Å Â¨Ã§Â»â„¢Ã¥Å’Â»Ã¦Å Â¤Ã¤ÂºÂºÃ¥â€˜ËœÃ¦Å½Â¨Ã©â‚¬ÂÃ¦â€“Â°Ã§â€°Ë†Ã¦Å“Â¬Ã§Å¡â€žÃ¥â€¦Â³Ã¦Â³Â¨Ã©â€¡ÂÃ§â€šÂ¹Ã¯Â¼Å’Ã©â„¢ÂÃ¤Â½Å½Ã£â‚¬Å Ã¨Â¯Å Ã§â€“â€”Ã¦â€“Â¹Ã¦Â¡Ë†Ã£â‚¬â€¹Ã¨Â§Â£Ã¨Â¯Â»Ã§Å¡â€žÃ¤Â¸â€œÃ¤Â¸Å¡Ã©Å¡Â¾Ã¥ÂºÂ¦Ã¯Â¼Å’Ã¦ÂÂÃ©Â«ËœÃ©ÂÅ¾Ã¤Â¸â€œÃ¤Â¸Å¡Ã¤ÂºÂºÃ¥â€˜ËœÃ¥Â¯Â¹Ã¨Â¯Å Ã§â€“â€”Ã¦â€“Â¹Ã¦Â¡Ë†Ã§Å¡â€žÃ¨Â®Â¤Ã¨Â¯â€ Ã¦Â°Â´Ã¥Â¹Â³Ã£â‚¬â€šÃ¥Â®Å¾Ã©ÂªÅ’Ã¨Â¯ÂÃ¦ËœÅ½Ã¯Â¼Å’Ã¤Â¸Å½Ã¤ÂºÂºÃ¥Â·Â¥Ã¨Â§Â£Ã¨Â¯Â»Ã¦â€“Â¹Ã¥Â¼ÂÃ§â€ºÂ¸Ã¦Â¯â€Ã¨Â¾Æ’Ã¯Â¼Å’Ã¦Å“Â¬Ã¦â€“â€¡Ã¦â€“Â¹Ã¦Â³â€¢Ã¨Æ’Â½Ã¨â€¡ÂªÃ¥Å Â¨Ã¨Â®Â¡Ã§Â®â€”Ã¦â€“â€¡Ã¦Å“Â¬Ã¤Â¸Â»Ã©Â¢ËœÃ¯Â¼Å’Ã¥Â®Å¾Ã§Å½Â°Ã¤Â¸Â»Ã©Â¢ËœÃ§Å¡â€žÃ§Â²Â¾Ã¥â€¡â€ Ã¥Å’Â¹Ã©â€¦ÂÃ¯Â¼Å’Ã¥â€¡â€ Ã§Â¡Â®Ã§Å½â€¡Ã¨Â¾Â¾ 100%Ã¯Â¼Å’Ã¥Â¹Â¶Ã¥ÂÂ¯Ã¨â€¡ÂªÃ¥Å Â¨Ã§â€Å¸Ã¦Ë†ÂÃ¥â€¦Â³Ã©â€Â®Ã¨Â¯ÂÃ¥â€™Å’Ã¨Â¯Â­Ã¥ÂÂ¥Ã§ÂºÂ§Ã¥Ë†Â«Ã§Å¡â€žÃ¨Â§Â£Ã¨Â¯Â»Ã¦Å Â¥Ã¥â€˜Å Ã¯Â¼Å’Ã¥Â®Å¾Ã§Å½Â°Ã£â‚¬Å Ã¨Â¯Å Ã§â€“â€”Ã¦â€“Â¹Ã¦Â¡Ë†Ã£â‚¬â€¹Ã§Å¡â€žÃ¨Â®Â¡Ã§Â®â€”Ã¦Å“ÂºÃ¨â€¡ÂªÃ¥Å Â¨Ã¦â„¢ÂºÃ¨Æ’Â½Ã¨Â§Â£Ã¨Â¯Â»Ã£â‚¬â€š.","The outbreak of pneumonia caused by novel coronavirus (COVID-19) at the end of 2019 was a major public health emergency in human history. In a short period of time, Chinese medical workers have experienced the gradual understanding, evidence accumulation and clinical practice of the unknown virus. So far, National Health Commission of the People's Republic of China has issued seven trial versions of the ""Guidelines for the Diagnosis and Treatment of COVID-19"". However, it is difficult for clinicians and laymen to quickly and accurately distinguish the similarities and differences among the different versions and locate the key points of the new version. This paper reports a computer-aided intelligent analysis method based on machine learning, which can automatically analyze the similarities and differences of different treatment plans, present the focus of the new version to doctors, reduce the difficulty in interpreting the ""diagnosis and treatment plan"" for the professional, and help the general public better understand the professional knowledge of medicine. Experimental results show that this method can achieve the topic prediction and matching of the new version of the program text through unsupervised learning of the previous versions of the program topic with an accuracy of 100%. It enables the computer interpretation of ""diagnosis and treatment plan"" automatically and intelligently. 2019 å¹´åº•æš´å‘çš„æ–°åž‹å† çŠ¶ç—…æ¯’è‚ºç‚Žï¼ˆCOVID-19ï¼‰ç–«æƒ…æ˜¯äººç±»å²ä¸Šä¸€æ¬¡é‡å¤§çªå‘å…¬å…±å«ç”Ÿäº‹ä»¶ã€‚ä¸­å›½åŒ»å­¦å·¥ä½œè€…åœ¨çŸ­æ—¶é—´å†…ï¼Œç»åŽ†äº†å¯¹è¯¥æœªçŸ¥ç—…æ¯’çš„é€æ­¥è®¤è¯†ã€è¯æ®ç§¯ç´¯å’Œä¸´åºŠå®žè·µã€‚æˆªè‡³ç›®å‰ï¼Œä¸­å›½å›½å®¶å«ç”Ÿå¥åº·å§”å‘˜ä¼šåœ¨æ•°åå¤©å†…å¯†é›†å‘å¸ƒäº†ä¸ƒä¸ªç‰ˆæœ¬çš„ã€Šæ–°åž‹å† çŠ¶ç—…æ¯’æ„ŸæŸ“çš„è‚ºç‚Žè¯Šç–—æ–¹æ¡ˆã€‹ï¼ˆç®€ç§°ã€Šè¯Šç–—æ–¹æ¡ˆã€‹ï¼‰ã€‚ç„¶è€Œï¼Œå¿«é€Ÿå‡†ç¡®åœ°æ¯”è¾ƒå„ç‰ˆæœ¬çš„å¼‚åŒå’ŒæŽŒæ¡æ–°ç‰ˆæœ¬çš„é‡ç‚¹å¯¹ä¸´åºŠåŒ»æŠ¤äººå‘˜å’Œéžä¸“ä¸šäººå‘˜æ¥è¯´å­˜åœ¨ä¸€å®šå›°éš¾ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºŽæœºå™¨å­¦ä¹ çš„è®¡ç®—æœºè¾…åŠ©æ™ºèƒ½åˆ†æžæ–¹æ³•ï¼Œå¯¹æ–‡æœ¬ä¸»é¢˜è¿›è¡Œæ— ç›‘ç£å­¦ä¹ ï¼Œè‡ªåŠ¨åˆ†æžä¸åŒç‰ˆæœ¬ã€Šè¯Šç–—æ–¹æ¡ˆã€‹çš„å¼‚åŒï¼Œä¸»åŠ¨ç»™åŒ»æŠ¤äººå‘˜æŽ¨é€æ–°ç‰ˆæœ¬çš„å…³æ³¨é‡ç‚¹ï¼Œé™ä½Žã€Šè¯Šç–—æ–¹æ¡ˆã€‹è§£è¯»çš„ä¸“ä¸šéš¾åº¦ï¼Œæé«˜éžä¸“ä¸šäººå‘˜å¯¹è¯Šç–—æ–¹æ¡ˆçš„è®¤è¯†æ°´å¹³ã€‚å®žéªŒè¯æ˜Žï¼Œä¸Žäººå·¥è§£è¯»æ–¹å¼ç›¸æ¯”è¾ƒï¼Œæœ¬æ–‡æ–¹æ³•èƒ½è‡ªåŠ¨è®¡ç®—æ–‡æœ¬ä¸»é¢˜ï¼Œå®žçŽ°ä¸»é¢˜çš„ç²¾å‡†åŒ¹é…ï¼Œå‡†ç¡®çŽ‡è¾¾ 100%ï¼Œå¹¶å¯è‡ªåŠ¨ç”Ÿæˆå…³é”®è¯å’Œè¯­å¥çº§åˆ«çš„è§£è¯»æŠ¥å‘Šï¼Œå®žçŽ°ã€Šè¯Šç–—æ–¹æ¡ˆã€‹çš„è®¡ç®—æœºè‡ªåŠ¨æ™ºèƒ½è§£è¯»ã€‚.","Pu, Chen, Liu, Wen, Zhneng, Li","Pu, Chen, Liu, Wen, Zhneng, Li",https://doi.org/10.7507/1001-5515.202003045,https://doi.org/10.7507/1001-5515.202003045,2021-08-03
16603.0,pubmed,pubmed,Building a PubMed knowledge graph,Building a PubMed knowledge graph,"PubMed<sup>Ã‚Â®</sup> is an essential resource for the medical domain, but useful concepts are either difficult to extract or are ambiguous, which has significantly hindered knowledge discovery. To address this issue, we constructed a PubMed knowledge graph (PKG) by extracting bio-entities from 29 million PubMed abstracts, disambiguating author names, integrating funding data through the National Institutes of Health (NIH) ExPORTER, collectingÃ‚Â affiliation history and educational background of authors from ORCID<sup>Ã‚Â®</sup>, and identifyingÃ‚Â fine-grained affiliation data from MapAffil. Through theÃ‚Â integration of these credible multi-source data, we could create connections among the bio-entities, authors, articles, affiliations, and funding. Data validation revealed that the BioBERT deep learning method of bio-entity extraction significantly outperformed the state-of-the-art models based on the F1 score (by 0.51%), with the author name disambiguation (AND) achieving an F1 score of 98.09%. PKG can trigger broader innovations, not only enabling us to measure scholarly impact, knowledge usage, and knowledge transfer, but also assisting us in profiling authors and organizations based on their connections with bio-entities.","PubMed<sup>Â®</sup> is an essential resource for the medical domain, but useful concepts are either difficult to extract or are ambiguous, which has significantly hindered knowledge discovery. To address this issue, we constructed a PubMed knowledge graph (PKG) by extracting bio-entities from 29 million PubMed abstracts, disambiguating author names, integrating funding data through the National Institutes of Health (NIH) ExPORTER, collectingÂ affiliation history and educational background of authors from ORCID<sup>Â®</sup>, and identifyingÂ fine-grained affiliation data from MapAffil. Through theÂ integration of these credible multi-source data, we could create connections among the bio-entities, authors, articles, affiliations, and funding. Data validation revealed that the BioBERT deep learning method of bio-entity extraction significantly outperformed the state-of-the-art models based on the F1 score (by 0.51%), with the author name disambiguation (AND) achieving an F1 score of 98.09%. PKG can trigger broader innovations, not only enabling us to measure scholarly impact, knowledge usage, and knowledge transfer, but also assisting us in profiling authors and organizations based on their connections with bio-entities.","Xu, Kim, Song, Jeong, Kim, Kang, Rousseau, Li, Xu, Torvik, Bu, Chen, Ebeid, Li, Ding","Xu, Kim, Song, Jeong, Kim, Kang, Rousseau, Li, Xu, Torvik, Bu, Chen, Ebeid, Li, Ding",https://doi.org/10.1038/s41597-020-0543-2,https://doi.org/10.1038/s41597-020-0543-2,2021-08-03
16607.0,pubmed,pubmed,The 2019 Genitourinary Pathology Society (GUPS) White Paper on Contemporary Grading of Prostate Cancer,The 2019 Genitourinary Pathology Society (GUPS) White Paper on Contemporary Grading of Prostate Cancer,"Controversies and uncertainty persist in prostate cancer grading. To update grading recommendations. Critical review of the literature along with pathology and clinician surveys. Percent Gleason pattern 4 (%GP4) is as follows: (1) report %GP4 in needle biopsy with Grade Groups (GrGp) 2 and 3, and in needle biopsy on other parts (jars) of lower grade in cases with at least 1 part showing Gleason score (GS) 4 + 4 = 8; and (2) report %GP4: less than 5% or less than 10% and 10% increments thereafter. Tertiary grade patterns are as follows: (1) replace &quot;tertiary grade pattern&quot; in radical prostatectomy (RP) with &quot;minor tertiary pattern 5 (TP5),&quot; and only use in RP with GrGp 2 or 3 with less than 5% Gleason pattern 5; and (2) minor TP5 is noted along with the GS, with the GrGp based on the GS. Global score and magnetic resonance imaging (MRI)-targeted biopsies are as follows: (1) when multiple undesignated cores are taken from a single MRI-targeted lesion, an overall grade for that lesion is given as if all the involved cores were one long core; and (2) if providing a global score, when different scores are found in the standard and the MRI-targeted biopsy, give a single global score (factoring both the systematic standard and the MRI-targeted positive cores). Grade Groups are as follows: (1) Grade Groups (GrGp) is the terminology adopted by major world organizations; and (2) retain GS 3 + 5 = 8 in GrGp 4. Cribriform carcinoma is as follows: (1) report the presence or absence of cribriform glands in biopsy and RP with Gleason pattern 4 carcinoma. Intraductal carcinoma (IDC-P) is as follows: (1) report IDC-P in biopsy and RP; (2) use criteria based on dense cribriform glands (&gt;50% of the gland is composed of epithelium relative to luminal spaces) and/or solid nests and/or marked pleomorphism/necrosis; (3) it is not necessary to perform basal cell immunostains on biopsy and RP to identify IDC-P if the results would not change the overall (highest) GS/GrGp part per case; (4) do not include IDC-P in determining the final GS/GrGp on biopsy and/or RP; and (5) &quot;atypical intraductal proliferation (AIP)&quot; is preferred for an intraductal proliferation of prostatic secretory cells which shows a greater degree of architectural complexity and/or cytological atypia than typical high-grade prostatic intraepithelial neoplasia, yet falling short of the strict diagnostic threshold for IDC-P. Molecular testing is as follows: (1) Ki67 is not ready for routine clinical use; (2) additional studies of active surveillance cohorts are needed to establish the utility of PTEN in this setting; and (3) dedicated studies of RNA-based assays in active surveillance populations are needed to substantiate the utility of these expensive tests in this setting. Artificial intelligence and novel grading schema are as follows: (1) incorporating reactive stromal grade, percent GP4, minor tertiary GP5, and cribriform/intraductal carcinoma are not ready for adoption in current practice.","Controversies and uncertainty persist in prostate cancer grading. To update grading recommendations. Critical review of the literature along with pathology and clinician surveys. Percent Gleason pattern 4 (%GP4) is as follows: (1) report %GP4 in needle biopsy with Grade Groups (GrGp) 2 and 3, and in needle biopsy on other parts (jars) of lower grade in cases with at least 1 part showing Gleason score (GS) 4 + 4 = 8; and (2) report %GP4: less than 5% or less than 10% and 10% increments thereafter. Tertiary grade patterns are as follows: (1) replace ""tertiary grade pattern"" in radical prostatectomy (RP) with ""minor tertiary pattern 5 (TP5),"" and only use in RP with GrGp 2 or 3 with less than 5% Gleason pattern 5; and (2) minor TP5 is noted along with the GS, with the GrGp based on the GS. Global score and magnetic resonance imaging (MRI)-targeted biopsies are as follows: (1) when multiple undesignated cores are taken from a single MRI-targeted lesion, an overall grade for that lesion is given as if all the involved cores were one long core; and (2) if providing a global score, when different scores are found in the standard and the MRI-targeted biopsy, give a single global score (factoring both the systematic standard and the MRI-targeted positive cores). Grade Groups are as follows: (1) Grade Groups (GrGp) is the terminology adopted by major world organizations; and (2) retain GS 3 + 5 = 8 in GrGp 4. Cribriform carcinoma is as follows: (1) report the presence or absence of cribriform glands in biopsy and RP with Gleason pattern 4 carcinoma. Intraductal carcinoma (IDC-P) is as follows: (1) report IDC-P in biopsy and RP; (2) use criteria based on dense cribriform glands (&gt;50% of the gland is composed of epithelium relative to luminal spaces) and/or solid nests and/or marked pleomorphism/necrosis; (3) it is not necessary to perform basal cell immunostains on biopsy and RP to identify IDC-P if the results would not change the overall (highest) GS/GrGp part per case; (4) do not include IDC-P in determining the final GS/GrGp on biopsy and/or RP; and (5) ""atypical intraductal proliferation (AIP)"" is preferred for an intraductal proliferation of prostatic secretory cells which shows a greater degree of architectural complexity and/or cytological atypia than typical high-grade prostatic intraepithelial neoplasia, yet falling short of the strict diagnostic threshold for IDC-P. Molecular testing is as follows: (1) Ki67 is not ready for routine clinical use; (2) additional studies of active surveillance cohorts are needed to establish the utility of PTEN in this setting; and (3) dedicated studies of RNA-based assays in active surveillance populations are needed to substantiate the utility of these expensive tests in this setting. Artificial intelligence and novel grading schema are as follows: (1) incorporating reactive stromal grade, percent GP4, minor tertiary GP5, and cribriform/intraductal carcinoma are not ready for adoption in current practice.","Epstein, Amin, Fine, Algaba, Aron, Baydar, Beltran, Brimo, Cheville, Colecchia, Comperat, da Cunha, Delprado, DeMarzo, Giannico, Gordetsky, Guo, Hansel, Hirsch, Huang, Humphrey, Jimenez, Khani, Kong, Kryvenko, Kunju, Lal, Latour, Lotan, Maclean, Magi-Galluzzi, Mehra, Menon, Miyamoto, Montironi, Netto, Nguyen, Osunkoya, Parwani, Robinson, Rubin, Shah, So, Takahashi, Tavora, Tretiakova, True, Wobker, Yang, Zhou, Zynger, Trpkov","Epstein, Amin, Fine, Algaba, Aron, Baydar, Beltran, Brimo, Cheville, Colecchia, Comperat, da Cunha, Delprado, DeMarzo, Giannico, Gordetsky, Guo, Hansel, Hirsch, Huang, Humphrey, Jimenez, Khani, Kong, Kryvenko, Kunju, Lal, Latour, Lotan, Maclean, Magi-Galluzzi, Mehra, Menon, Miyamoto, Montironi, Netto, Nguyen, Osunkoya, Parwani, Robinson, Rubin, Shah, So, Takahashi, Tavora, Tretiakova, True, Wobker, Yang, Zhou, Zynger, Trpkov",https://doi.org/10.5858/arpa.2020-0015-RA,https://doi.org/10.5858/arpa.2020-0015-RA,2021-08-03
16617.0,pubmed,pubmed,An Electronic Health Record Text Mining Tool to Collect Real-World Drug Treatment Outcomes: A Validation Study in Patients With Metastatic Renal Cell Carcinoma,An Electronic Health Record Text Mining Tool to Collect Real-World Drug Treatment Outcomes: A Validation Study in Patients With Metastatic Renal Cell Carcinoma,"Real-world evidence can close the inferential gap between marketing authorization studies and clinical practice. However, the current standard for real-world data extraction from electronic health records (EHRs) for treatment evaluation is manual review (MR), which is time-consuming and laborious. Clinical Data Collector (CDC) is a novel natural language processing and text mining software tool for both structured and unstructured EHR data and only shows relevant EHR sections improving efficiency. We investigated CDC as a real-world data (RWD) collection method, through application of CDC queries for patient inclusion and information extraction on a cohort of patients with metastatic renal cell carcinoma (RCC) receiving systemic drug treatment. Baseline patient characteristics, disease characteristics, and treatment outcomes were extracted and these were compared with MR for validation. One hundred patients receiving 175 treatments were included using CDC, which corresponded to 99% with MR. Calculated median overall survival was 21.7Ã‚Â months (95% confidence interval (CI) 18.7-24.8) vs. 21.7Ã‚Â months (95% CI 18.6-24.8) and progression-free survival 8.9Ã‚Â months (95% CI 5.4-12.4) vs. 7.6Ã‚Â months (95% CI 5.7-9.4) for CDC vs. MR, respectively. Highest F1-score was found for cancer-related variables (88.1-100), followed by comorbidities (71.5-90.4) and adverse drug events (53.3-74.5), with most diverse scores on international metastatic RCC database criteria (51.4-100). Mean data collection time was 12Ã‚Â minutes (CDC) vs. 86Ã‚Â minutes (MR). In conclusion, CDC is a promising tool for retrieving RWD from EHRs because the correct patient population can be identified as well as relevant outcome data, such as overall survival and progression-free survival.","Real-world evidence can close the inferential gap between marketing authorization studies and clinical practice. However, the current standard for real-world data extraction from electronic health records (EHRs) for treatment evaluation is manual review (MR), which is time-consuming and laborious. Clinical Data Collector (CDC) is a novel natural language processing and text mining software tool for both structured and unstructured EHR data and only shows relevant EHR sections improving efficiency. We investigated CDC as a real-world data (RWD) collection method, through application of CDC queries for patient inclusion and information extraction on a cohort of patients with metastatic renal cell carcinoma (RCC) receiving systemic drug treatment. Baseline patient characteristics, disease characteristics, and treatment outcomes were extracted and these were compared with MR for validation. One hundred patients receiving 175 treatments were included using CDC, which corresponded to 99% with MR. Calculated median overall survival was 21.7Â months (95% confidence interval (CI) 18.7-24.8) vs. 21.7Â months (95% CI 18.6-24.8) and progression-free survival 8.9Â months (95% CI 5.4-12.4) vs. 7.6Â months (95% CI 5.7-9.4) for CDC vs. MR, respectively. Highest F1-score was found for cancer-related variables (88.1-100), followed by comorbidities (71.5-90.4) and adverse drug events (53.3-74.5), with most diverse scores on international metastatic RCC database criteria (51.4-100). Mean data collection time was 12Â minutes (CDC) vs. 86Â minutes (MR). In conclusion, CDC is a promising tool for retrieving RWD from EHRs because the correct patient population can be identified as well as relevant outcome data, such as overall survival and progression-free survival.","van Laar, Gombert-Handoko, Guchelaar, Zwaveling","van Laar, Gombert-Handoko, Guchelaar, Zwaveling",https://doi.org/10.1002/cpt.1966,https://doi.org/10.1002/cpt.1966,2021-08-03
16618.0,pubmed,pubmed,Task-based characterization of a deep learning image reconstruction and comparison with filtered back-projection and a partial model-based iterative reconstruction in abdominal CT: A phantom study,Task-based characterization of a deep learning image reconstruction and comparison with filtered back-projection and a partial model-based iterative reconstruction in abdominal CT: A phantom study,"We aimed to thoroughly characterize image quality of a novel deep learning image reconstruction (DLIR), and investigate its potential for dose reduction in abdominal CT in comparison with filtered back-projection (FBP) and a partial model-based iterative reconstruction (ASiR-V). We scanned a phantom at three dose levels: regular (7Ã‚Â mGy), low (3Ã‚Â mGy) and ultra-low (1Ã‚Â mGy). Images were reconstructed using DLIR (low, medium and high levels) and ASiR-V (0%Ã‚Â =Ã‚Â FBP, 50% and 100%). Noise and contrast-dependent spatial resolution were characterized by computing noise power spectra and target transfer functions, respectively. Detectability indexes of simulated acute appendicitis or colonic diverticulitis (low contrast), and calcium-containing urinary stones (high contrast) (|ÃŽâ€HU|Ã‚Â =Ã‚Â 50 and 500, respectively) were calculated using the nonprewhitening with eye filter model observer. At all dose levels, increasing DLIR and ASiR-V levels both markedly decreased noise magnitude compared with FBP, with DLIR low and medium maintaining noise texture overall. For both low- and high-contrast spatial resolution, DLIR not only maintained, but even slightly enhanced spatial resolution in comparison with FBP across all dose levels. Conversely, increasing ASiR-V impaired low-contrast spatial resolution compared with FBP. Overall, DLIR outperformed ASiR-V in all simulated clinical scenarios. For both low- and high-contrast diagnostic tasks, increasing DLIR substantially enhanced detectability at any dose and contrast levels for any simulated lesion size. Unlike ASiR-V, DLIR substantially reduces noise while maintaining noise texture and slightly enhancing spatial resolution overall. DLIR outperforms ASiR-V by enabling higher detectability of both low- and high-contrast simulated abdominal lesions across all investigated dose levels.","We aimed to thoroughly characterize image quality of a novel deep learning image reconstruction (DLIR), and investigate its potential for dose reduction in abdominal CT in comparison with filtered back-projection (FBP) and a partial model-based iterative reconstruction (ASiR-V). We scanned a phantom at three dose levels: regular (7Â mGy), low (3Â mGy) and ultra-low (1Â mGy). Images were reconstructed using DLIR (low, medium and high levels) and ASiR-V (0%Â =Â FBP, 50% and 100%). Noise and contrast-dependent spatial resolution were characterized by computing noise power spectra and target transfer functions, respectively. Detectability indexes of simulated acute appendicitis or colonic diverticulitis (low contrast), and calcium-containing urinary stones (high contrast) (|Î”HU|Â =Â 50 and 500, respectively) were calculated using the nonprewhitening with eye filter model observer. At all dose levels, increasing DLIR and ASiR-V levels both markedly decreased noise magnitude compared with FBP, with DLIR low and medium maintaining noise texture overall. For both low- and high-contrast spatial resolution, DLIR not only maintained, but even slightly enhanced spatial resolution in comparison with FBP across all dose levels. Conversely, increasing ASiR-V impaired low-contrast spatial resolution compared with FBP. Overall, DLIR outperformed ASiR-V in all simulated clinical scenarios. For both low- and high-contrast diagnostic tasks, increasing DLIR substantially enhanced detectability at any dose and contrast levels for any simulated lesion size. Unlike ASiR-V, DLIR substantially reduces noise while maintaining noise texture and slightly enhancing spatial resolution overall. DLIR outperforms ASiR-V by enabling higher detectability of both low- and high-contrast simulated abdominal lesions across all investigated dose levels.","Racine, Becce, Viry, Monnin, Thomsen, Verdun, Rotzinger","Racine, Becce, Viry, Monnin, Thomsen, Verdun, Rotzinger",https://doi.org/10.1016/j.ejmp.2020.06.004,https://doi.org/10.1016/j.ejmp.2020.06.004,2021-08-03
16646.0,pubmed,pubmed,FasTag: Automatic text classification of unstructured medical narratives,FasTag: Automatic text classification of unstructured medical narratives,"Unstructured clinical narratives are continuously being recorded as part of delivery of care in electronic health records, and dedicated tagging staff spend considerable effort manually assigning clinical codes for billing purposes. Despite these efforts, however, label availability and accuracy are both suboptimal. In this retrospective study, we aimed to automate the assignment of top-level International Classification of Diseases version 9 (ICD-9) codes to clinical records from human and veterinary data stores using minimal manual labor and feature curation. Automating top-level annotations could in turn enable rapid cohort identification, especially in a veterinary setting. To this end, we trained long short-term memory (LSTM) recurrent neural networks (RNNs) on 52,722 human and 89,591 veterinary records. We investigated the accuracy of both separate-domain and combined-domain models and probed model portability. We established relevant baseline classification performances by training Decision Trees (DT) and Random Forests (RF). We also investigated whether transforming the data using MetaMap Lite, a clinical natural language processing tool, affected classification performance. We showed that the LSTM-RNNs accurately classify veterinary and human text narratives into top-level categories with an average weighted macro F1 score of 0.74 and 0.68 respectively. In the &quot;neoplasia&quot; category, the model trained on veterinary data had a high validation accuracy in veterinary data and moderate accuracy in human data, with F1 scores of 0.91 and 0.70 respectively. Our LSTM method scored slightly higher than that of the DT and RF models. The use of LSTM-RNN models represents a scalable structure that could prove useful in cohort identification for comparative oncology studies. Digitization of human and veterinary health information will continue to be a reality, particularly in the form of unstructured narratives. Our approach is a step forward for these two domains to learn from and inform one another.","Unstructured clinical narratives are continuously being recorded as part of delivery of care in electronic health records, and dedicated tagging staff spend considerable effort manually assigning clinical codes for billing purposes. Despite these efforts, however, label availability and accuracy are both suboptimal. In this retrospective study, we aimed to automate the assignment of top-level International Classification of Diseases version 9 (ICD-9) codes to clinical records from human and veterinary data stores using minimal manual labor and feature curation. Automating top-level annotations could in turn enable rapid cohort identification, especially in a veterinary setting. To this end, we trained long short-term memory (LSTM) recurrent neural networks (RNNs) on 52,722 human and 89,591 veterinary records. We investigated the accuracy of both separate-domain and combined-domain models and probed model portability. We established relevant baseline classification performances by training Decision Trees (DT) and Random Forests (RF). We also investigated whether transforming the data using MetaMap Lite, a clinical natural language processing tool, affected classification performance. We showed that the LSTM-RNNs accurately classify veterinary and human text narratives into top-level categories with an average weighted macro F1 score of 0.74 and 0.68 respectively. In the ""neoplasia"" category, the model trained on veterinary data had a high validation accuracy in veterinary data and moderate accuracy in human data, with F1 scores of 0.91 and 0.70 respectively. Our LSTM method scored slightly higher than that of the DT and RF models. The use of LSTM-RNN models represents a scalable structure that could prove useful in cohort identification for comparative oncology studies. Digitization of human and veterinary health information will continue to be a reality, particularly in the form of unstructured narratives. Our approach is a step forward for these two domains to learn from and inform one another.","Venkataraman, Pineda, Bear Don't Walk Iv, Zehnder, Ayyar, Page, Bustamante, Rivas","Venkataraman, Pineda, Bear Don't Walk Iv, Zehnder, Ayyar, Page, Bustamante, Rivas",https://doi.org/10.1371/journal.pone.0234647,https://doi.org/10.1371/journal.pone.0234647,2021-08-03
16647.0,pubmed,pubmed,Efficacy of bone marrow stem cells combined with core decompression in the treatment of osteonecrosis of the femoral head: A PRISMA-compliant meta-analysis,Efficacy of bone marrow stem cells combined with core decompression in the treatment of osteonecrosis of the femoral head: A PRISMA-compliant meta-analysis,"This study used the meta-analytic approach to assess the safety and treatment efficacy of bone marrow stem cells (BMSCs) with core decompression (CD) for osteonecrosis of the femoral head (ONFH) based on randomized controlled trials (RCTs). Electronic database of PubMed, Embase, Google Scholar, China National Knowledge Infrastructure (CNKI), and Wanfang database was searched up to December 26, 2019 for relevant RCTs about combined utilization of BMSCs and CD versus CD alone for ONFH. Gray literature sources were also searched. We conducted a meta-analysis following the guidelines of the Cochrane Reviewer's Handbook. Two independent reviewers performed the data extraction and assessed study quality. Our outcomes included the Harris hip scores (HHS) at 12 months, HHS at 24 months, necrotic area of femoral head, conversion to total hip arthroplasty (THA), visual analog pain scale at final follow-up, and adverse effects. The meta-analysis was performed with Stata 12.0. A total of 15 published studies with 688 patients fulfilled the requirements of inclusion criteria. Across all populations, participants in combined utilization of BMSCs group showed a statistical significance with higher HHS at 12 months (standard mean difference [SMD] 0.53, 95% confidence interval [CI] 0.29-0.77) and 24 months (SMD 0.57, 95% CI 0.36-0.77). Similarly, participants in combined utilization of BMSCs group had more advantages in reducing necrotic area of femoral head (SMD -1.05, 95% CI -1.73 to -0.38) and the rate of conversion to THA (risk ratio [RR]Ã¢â‚¬Å =Ã¢â‚¬Å 0.53, 95% CI 0.38-0.74, PÃ¢â‚¬Å =Ã¢â‚¬Å .000). No significant differences were identified regarding postoperative adverse effects postoperatively (RRÃ¢â‚¬Å =Ã¢â‚¬Å 1.03, 95% CI 0.64-1.67, PÃ¢â‚¬Å =Ã¢â‚¬Å .893). Compared with CD treated alone in the treatment of ONFH, combined utilization of CD and autologous BMSCs implantation has a better pain relief and clinical outcomes and can delay the collapse of the femoral head more effectively.","This study used the meta-analytic approach to assess the safety and treatment efficacy of bone marrow stem cells (BMSCs) with core decompression (CD) for osteonecrosis of the femoral head (ONFH) based on randomized controlled trials (RCTs). Electronic database of PubMed, Embase, Google Scholar, China National Knowledge Infrastructure (CNKI), and Wanfang database was searched up to December 26, 2019 for relevant RCTs about combined utilization of BMSCs and CD versus CD alone for ONFH. Gray literature sources were also searched. We conducted a meta-analysis following the guidelines of the Cochrane Reviewer's Handbook. Two independent reviewers performed the data extraction and assessed study quality. Our outcomes included the Harris hip scores (HHS) at 12 months, HHS at 24 months, necrotic area of femoral head, conversion to total hip arthroplasty (THA), visual analog pain scale at final follow-up, and adverse effects. The meta-analysis was performed with Stata 12.0. A total of 15 published studies with 688 patients fulfilled the requirements of inclusion criteria. Across all populations, participants in combined utilization of BMSCs group showed a statistical significance with higher HHS at 12 months (standard mean difference [SMD] 0.53, 95% confidence interval [CI] 0.29-0.77) and 24 months (SMD 0.57, 95% CI 0.36-0.77). Similarly, participants in combined utilization of BMSCs group had more advantages in reducing necrotic area of femoral head (SMD -1.05, 95% CI -1.73 to -0.38) and the rate of conversion to THA (risk ratio [RR]â€Š=â€Š0.53, 95% CI 0.38-0.74, Pâ€Š=â€Š.000). No significant differences were identified regarding postoperative adverse effects postoperatively (RRâ€Š=â€Š1.03, 95% CI 0.64-1.67, Pâ€Š=â€Š.893). Compared with CD treated alone in the treatment of ONFH, combined utilization of CD and autologous BMSCs implantation has a better pain relief and clinical outcomes and can delay the collapse of the femoral head more effectively.","Wang, Hu, Chen, Tao, Bu, Zhang, Zhang","Wang, Hu, Chen, Tao, Bu, Zhang, Zhang",https://doi.org/10.1097/MD.0000000000020509,https://doi.org/10.1097/MD.0000000000020509,2021-08-03
16648.0,pubmed,pubmed,Artificial Intelligence and Big Data in Diabetes Care: A Position Statement of the Italian Association of Medical Diabetologists,Artificial Intelligence and Big Data in Diabetes Care: A Position Statement of the Italian Association of Medical Diabetologists,"Since the last decade, most of our daily activities have become digital. Digital health takes into account the ever-increasing synergy between advanced medical technologies, innovation, and digital communication. Thanks to machine learning, we are not limited anymore to a descriptive analysis of the data, as we can obtain greater value by identifying and predicting patterns resulting from inductive reasoning. Machine learning software programs that disclose the reasoning behind a prediction allow for &quot;what-if&quot; models by which it is possible to understand if and how, by changing certain factors, one may improve the outcomes, thereby identifying the optimal behavior. Currently, diabetes care is facing several challenges: the decreasing number of diabetologists, the increasing number of patients, the reduced time allowed for medical visits, the growing complexity of the disease both from the standpoints of clinical and patient care, the difficulty of achieving the relevant clinical targets, the growing burden of disease management for both the health care professional and the patient, and the health care accessibility and sustainability. In this context, new digital technologies and the use of artificial intelligence are certainly a great opportunity. Herein, we report the results of a careful analysis of the current literature and represent the vision of the Italian Association of Medical Diabetologists (AMD) on this controversial topic that, if well used, may be the key for a great scientific innovation. AMD believes that the use of artificial intelligence will enable the conversion of data (descriptive) into knowledge of the factors that &quot;affect&quot; the behavior and correlations (predictive), thereby identifying the key aspects that may establish an improvement of the expected results (prescriptive). Artificial intelligence can therefore become a tool of great technical support to help diabetologists become fully responsible of the individual patient, thereby assuring customized and precise medicine. This, in turn, will allow for comprehensive therapies to be built in accordance with the evidence criteria that should always be the ground for any therapeutic choice.","Since the last decade, most of our daily activities have become digital. Digital health takes into account the ever-increasing synergy between advanced medical technologies, innovation, and digital communication. Thanks to machine learning, we are not limited anymore to a descriptive analysis of the data, as we can obtain greater value by identifying and predicting patterns resulting from inductive reasoning. Machine learning software programs that disclose the reasoning behind a prediction allow for ""what-if"" models by which it is possible to understand if and how, by changing certain factors, one may improve the outcomes, thereby identifying the optimal behavior. Currently, diabetes care is facing several challenges: the decreasing number of diabetologists, the increasing number of patients, the reduced time allowed for medical visits, the growing complexity of the disease both from the standpoints of clinical and patient care, the difficulty of achieving the relevant clinical targets, the growing burden of disease management for both the health care professional and the patient, and the health care accessibility and sustainability. In this context, new digital technologies and the use of artificial intelligence are certainly a great opportunity. Herein, we report the results of a careful analysis of the current literature and represent the vision of the Italian Association of Medical Diabetologists (AMD) on this controversial topic that, if well used, may be the key for a great scientific innovation. AMD believes that the use of artificial intelligence will enable the conversion of data (descriptive) into knowledge of the factors that ""affect"" the behavior and correlations (predictive), thereby identifying the key aspects that may establish an improvement of the expected results (prescriptive). Artificial intelligence can therefore become a tool of great technical support to help diabetologists become fully responsible of the individual patient, thereby assuring customized and precise medicine. This, in turn, will allow for comprehensive therapies to be built in accordance with the evidence criteria that should always be the ground for any therapeutic choice.","Musacchio, Giancaterini, Guaita, Ozzello, Pellegrini, Ponzani, Russo, Zilich, de Micheli","Musacchio, Giancaterini, Guaita, Ozzello, Pellegrini, Ponzani, Russo, Zilich, de Micheli",https://doi.org/10.2196/16922,https://doi.org/10.2196/16922,2021-08-03
16650.0,pubmed,pubmed,Dissecting the Fornix in Basic Memory Processes and Neuropsychiatric Disease: A Review,Dissecting the Fornix in Basic Memory Processes and Neuropsychiatric Disease: A Review,"<b> <i>Background:</i> </b> The fornix is the primary axonal tract of the hippocampus, connecting it to modulatory subcortical structures. This review reveals that fornix damage causes cognitive deficits that closely mirror those resulting from hippocampal lesions. <b> <i>Methods:</i> </b> We reviewed the literature on the fornix, spanning non-human animal lesion research, clinical case studies of human patients with fornix damage, as well as diffusion-weighted imaging (DWI) work that evaluates fornix microstructure in vivo. <b> <i>Results:</i> </b> The fornix is essential for memory formation because it serves as the conduit for theta rhythms and acetylcholine, as well as providing mnemonic representations to deep brain structures that guide motivated behavior, such as when and where to eat. In rodents and non-human primates, fornix lesions lead to deficits in conditioning, reversal learning, and navigation. In humans, damage to the fornix manifests as anterograde amnesia. DWI research reveals that the fornix plays a key role in mild cognitive impairment and Alzheimer's Disease, and can potentially predict conversion from the former to the latter. Emerging DWI findings link perturbations in this structure to schizophrenia, mood disorders, and eating disorders. Cutting-edge research has investigated how deep brain stimulation of the fornix can potentially attenuate memory loss, control epileptic seizures, and even improve mood. <b> <i>Conclusions:</i> </b> The fornix is essential to a fully functioning memory system and is implicated in nearly all neurological functions that rely on the hippocampus. Future research needs to use optimized DWI methods to study the fornix in vivo, which we discuss, given the difficult nature of fornix reconstruction. Impact Statement The fornix is a white matter tract that connects the hippocampus to several subcortical brain regions and is pivotal for episodic memory functioning. Functionally, the fornix transmits essential neurotransmitters, as well as theta rhythms, to the hippocampus. In addition, it is the conduit by which memories guide decisions. The fornix is biomedically important because lesions to this tract result in irreversible anterograde amnesia. Research using in vivo imaging methods has linked fornix pathology to cognitive aging, mild cognitive impairment, psychosis, epilepsy, and, importantly, Alzheimer's Disease.","<b><i>Background:</i></b> The fornix is the primary axonal tract of the hippocampus, connecting it to modulatory subcortical structures. This review reveals that fornix damage causes cognitive deficits that closely mirror those resulting from hippocampal lesions. <b><i>Methods:</i></b> We reviewed the literature on the fornix, spanning non-human animal lesion research, clinical case studies of human patients with fornix damage, as well as diffusion-weighted imaging (DWI) work that evaluates fornix microstructure in vivo. <b><i>Results:</i></b> The fornix is essential for memory formation because it serves as the conduit for theta rhythms and acetylcholine, as well as providing mnemonic representations to deep brain structures that guide motivated behavior, such as when and where to eat. In rodents and non-human primates, fornix lesions lead to deficits in conditioning, reversal learning, and navigation. In humans, damage to the fornix manifests as anterograde amnesia. DWI research reveals that the fornix plays a key role in mild cognitive impairment and Alzheimer's Disease, and can potentially predict conversion from the former to the latter. Emerging DWI findings link perturbations in this structure to schizophrenia, mood disorders, and eating disorders. Cutting-edge research has investigated how deep brain stimulation of the fornix can potentially attenuate memory loss, control epileptic seizures, and even improve mood. <b><i>Conclusions:</i></b> The fornix is essential to a fully functioning memory system and is implicated in nearly all neurological functions that rely on the hippocampus. Future research needs to use optimized DWI methods to study the fornix in vivo, which we discuss, given the difficult nature of fornix reconstruction. Impact Statement The fornix is a white matter tract that connects the hippocampus to several subcortical brain regions and is pivotal for episodic memory functioning. Functionally, the fornix transmits essential neurotransmitters, as well as theta rhythms, to the hippocampus. In addition, it is the conduit by which memories guide decisions. The fornix is biomedically important because lesions to this tract result in irreversible anterograde amnesia. Research using in vivo imaging methods has linked fornix pathology to cognitive aging, mild cognitive impairment, psychosis, epilepsy, and, importantly, Alzheimer's Disease.","Benear, Ngo, Olson","Benear, Ngo, Olson",https://doi.org/10.1089/brain.2020.0749,https://doi.org/10.1089/brain.2020.0749,2021-08-03
16659.0,pubmed,pubmed,Accuracy of artificial intelligence-assisted detection of upper GI lesions: a systematic review and meta-analysis,Accuracy of artificial intelligence-assisted detection of upper GI lesions: a systematic review and meta-analysis,"Artificial intelligence (AI)-assisted detection is increasingly used in upper endoscopy. We performed a meta-analysis to determine the diagnostic accuracy of AI on detection of gastric and esophageal neoplastic lesions and Helicobacter pylori (HP) status. We searched Embase, PubMed, Medline, Web of Science, and Cochrane databases for studies on AI detection of gastric or esophageal neoplastic lesions and HP status. After assessing study quality using the Quality Assessment of Diagnostic Accuracy Studies tool, a bivariate meta-analysis following a random-effects model was used to summarize the data and plot hierarchical summary receiver-operating characteristic curves. The diagnostic accuracy was determined by the area under the hierarchical summary receiver-operating characteristic curve (AUC). Twenty-three studies including 969,318 images were included. The AUC of AI detection of neoplastic lesions in the stomach, Barrett's esophagus, and squamous esophagus and HP status were .96 (95% confidence interval [CI], .94-.99), .96 (95% CI, .93-.99), .88 (95% CI, .82-.96), and .92 (95% CI, .88-.97), respectively. AI using narrow-band imaging was superior to white-light imaging on detection of neoplastic lesions in squamous esophagus (.92 vs .83, PÃ‚Â &lt; .001). The performance of AI was superior to endoscopists in the detection of neoplastic lesions in the stomach (AUC, .98 vs .87; PÃ‚Â &lt; .001), Barrett's esophagus (AUC, .96 vs .82; PÃ‚Â &lt; .001), and HP status (AUC, .90 vs .82; PÃ‚Â &lt; .001). AI is accurate in the detection of upper GI neoplastic lesions and HP infection status. However, most studies were based on retrospective reviews of selected images, which requires further validation in prospective trials.","Artificial intelligence (AI)-assisted detection is increasingly used in upper endoscopy. We performed a meta-analysis to determine the diagnostic accuracy of AI on detection of gastric and esophageal neoplastic lesions and Helicobacter pylori (HP) status. We searched Embase, PubMed, Medline, Web of Science, and Cochrane databases for studies on AI detection of gastric or esophageal neoplastic lesions and HP status. After assessing study quality using the Quality Assessment of Diagnostic Accuracy Studies tool, a bivariate meta-analysis following a random-effects model was used to summarize the data and plot hierarchical summary receiver-operating characteristic curves. The diagnostic accuracy was determined by the area under the hierarchical summary receiver-operating characteristic curve (AUC). Twenty-three studies including 969,318 images were included. The AUC of AI detection of neoplastic lesions in the stomach, Barrett's esophagus, and squamous esophagus and HP status were .96 (95% confidence interval [CI], .94-.99), .96 (95% CI, .93-.99), .88 (95% CI, .82-.96), and .92 (95% CI, .88-.97), respectively. AI using narrow-band imaging was superior to white-light imaging on detection of neoplastic lesions in squamous esophagus (.92 vs .83, PÂ &lt; .001). The performance of AI was superior to endoscopists in the detection of neoplastic lesions in the stomach (AUC, .98 vs .87; PÂ &lt; .001), Barrett's esophagus (AUC, .96 vs .82; PÂ &lt; .001), and HP status (AUC, .90 vs .82; PÂ &lt; .001). AI is accurate in the detection of upper GI neoplastic lesions and HP infection status. However, most studies were based on retrospective reviews of selected images, which requires further validation in prospective trials.","Lui, Tsui, Leung","Lui, Tsui, Leung",https://doi.org/10.1016/j.gie.2020.06.034,https://doi.org/10.1016/j.gie.2020.06.034,2021-08-03
16666.0,pubmed,pubmed,Artificial intelligence for polyp detection during colonoscopy: a systematic review and meta-analysis,Artificial intelligence for polyp detection during colonoscopy: a systematic review and meta-analysis,"Ã¢â‚¬â€šArtificial intelligence (AI)-based polyp detection systems are used during colonoscopy with the aim of increasing lesion detection and improving colonoscopy quality. Ã¢â‚¬â€šWe performed a systematic review and meta-analysis of prospective trials to determine the value of AI-based polyp detection systems for detection of polyps and colorectal cancer. We performed systematic searches in MEDLINE, EMBASE, and Cochrane CENTRAL. Independent reviewers screened studies and assessed eligibility, certainty of evidence, and risk of bias. We compared colonoscopy with and without AI by calculating relative and absolute risks and mean differences for detection of polyps, adenomas, and colorectal cancer. Ã¢â‚¬â€šFive randomized trials were eligible for analysis. Colonoscopy with AI increased adenoma detection rates (ADRs) and polyp detection rates (PDRs) compared to colonoscopy without AI (values given with 95Ã¢â‚¬Å %CI). ADR with AI was 29.6Ã¢â‚¬Å % (22.2Ã¢â‚¬Å %Ã¢â‚¬Å -Ã¢â‚¬Å 37.0Ã¢â‚¬Å %) versus 19.3Ã¢â‚¬Å % (12.7Ã¢â‚¬Å %Ã¢â‚¬Å -Ã¢â‚¬Å 25.9Ã¢â‚¬Å %) without AI; relative risk (RR] 1.52 (1.31Ã¢â‚¬Å -Ã¢â‚¬Å 1.77), with high certainty. PDR was 45.4Ã¢â‚¬Å % (41.1Ã¢â‚¬Å %Ã¢â‚¬Å -Ã¢â‚¬Å 49.8Ã¢â‚¬Å %) with AI versus 30.6Ã¢â‚¬Å % (26.5Ã¢â‚¬Å %Ã¢â‚¬Å -Ã¢â‚¬Å 34.6Ã¢â‚¬Å %) without AI; RR 1.48 (1.37Ã¢â‚¬Å -Ã¢â‚¬Å 1.60), with high certainty. There was no difference in detection of advanced adenomas (mean advanced adenomas per colonoscopy 0.03 for each group, high certainty). Mean adenomas detected per colonoscopy was higher for small adenomas (Ã¢â€°Â¤Ã¢â‚¬Å 5Ã¢â‚¬Å mm) for AI versus non-AI (mean difference 0.15 [0.12Ã¢â‚¬Å -Ã¢â‚¬Å 0.18]), but not for larger adenomas (&gt;Ã¢â‚¬Å 5Ã¢â‚¬Å -Ã¢â‚¬Å Ã¢â€°Â¤Ã¢â‚¬Å 10Ã¢â‚¬Å mm, mean difference 0.03 [0.01Ã¢â‚¬Å -Ã¢â‚¬Å 0.05];Ã¢â‚¬Å &gt;Ã¢â‚¬Å 10Ã¢â‚¬Å mm, mean difference 0.01 [0.00Ã¢â‚¬Å -Ã¢â‚¬Å 0.02]; high certainty). Data on cancer are unavailable. Ã¢â‚¬â€šAI-based polyp detection systems during colonoscopy increase detection of small nonadvanced adenomas and polyps, but not of advanced adenomas.","â€‚Artificial intelligence (AI)-based polyp detection systems are used during colonoscopy with the aim of increasing lesion detection and improving colonoscopy quality. â€‚We performed a systematic review and meta-analysis of prospective trials to determine the value of AI-based polyp detection systems for detection of polyps and colorectal cancer. We performed systematic searches in MEDLINE, EMBASE, and Cochrane CENTRAL. Independent reviewers screened studies and assessed eligibility, certainty of evidence, and risk of bias. We compared colonoscopy with and without AI by calculating relative and absolute risks and mean differences for detection of polyps, adenomas, and colorectal cancer. â€‚Five randomized trials were eligible for analysis. Colonoscopy with AI increased adenoma detection rates (ADRs) and polyp detection rates (PDRs) compared to colonoscopy without AI (values given with 95â€Š%CI). ADR with AI was 29.6â€Š% (22.2â€Š%â€Š-â€Š37.0â€Š%) versus 19.3â€Š% (12.7â€Š%â€Š-â€Š25.9â€Š%) without AI; relative risk (RR] 1.52 (1.31â€Š-â€Š1.77), with high certainty. PDR was 45.4â€Š% (41.1â€Š%â€Š-â€Š49.8â€Š%) with AI versus 30.6â€Š% (26.5â€Š%â€Š-â€Š34.6â€Š%) without AI; RR 1.48 (1.37â€Š-â€Š1.60), with high certainty. There was no difference in detection of advanced adenomas (mean advanced adenomas per colonoscopy 0.03 for each group, high certainty). Mean adenomas detected per colonoscopy was higher for small adenomas (â‰¤â€Š5â€Šmm) for AI versus non-AI (mean difference 0.15 [0.12â€Š-â€Š0.18]), but not for larger adenomas (&gt;â€Š5â€Š-â€Šâ‰¤â€Š10â€Šmm, mean difference 0.03 [0.01â€Š-â€Š0.05];â€Š&gt;â€Š10â€Šmm, mean difference 0.01 [0.00â€Š-â€Š0.02]; high certainty). Data on cancer are unavailable. â€‚AI-based polyp detection systems during colonoscopy increase detection of small nonadvanced adenomas and polyps, but not of advanced adenomas.","Barua, Vinsard, Jodal, LÃƒÂ¸berg, Kalager, Holme, Misawa, Bretthauer, Mori","Barua, Vinsard, Jodal, LÃ¸berg, Kalager, Holme, Misawa, Bretthauer, Mori",https://doi.org/10.1055/a-1201-7165,https://doi.org/10.1055/a-1201-7165,2021-08-03
16667.0,pubmed,pubmed,A preliminary study to quantitatively evaluate the development of maturation degree for fetal lung based on transfer learning deep model from ultrasound images,A preliminary study to quantitatively evaluate the development of maturation degree for fetal lung based on transfer learning deep model from ultrasound images,"The evaluation of fetal lung maturity is critical for clinical practice since the lung immaturity is an important cause of neonatal morbidity and mortality. For the evaluation of the development of fetal lung maturation degree, our study established a deep model from ultrasound images of four-cardiac-chamber view plane. A two-stage transfer learning approach is proposed for the purpose of the study. A specific U-net structure is designed for the applied deep model. In the first stage, the model is to first learn the recognition of fetal lung region in the ultrasound images. It is hypothesized in our study that the development of fetal lung maturation degree is generally proportional to the gestational age. Then, in the second stage, the pretrained deep model is trained to accurately estimate the gestational age from the fetal lung region of ultrasound images. Totally 332 patients were included in our study, while the first 206 patients were used for training and the subsequent 126 patients were used for the independent testing. The testing results of the established deep model have the imprecision as 1.56Ã¢â‚¬â€°Ã‚Â±Ã¢â‚¬â€°2.17Ã‚Â weeks on the gestational age estimation. Its correlation coefficient with the ground truth of gestational age achieves 0.7624 (95% CI 0.6779 to 0.8270, P valueÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°0.00001). The hypothesis that the development of fetal lung maturation degree can be represented by the texture information from ultrasound images has been preliminarily validated. The fetal lung maturation degree can be considered as being represented by the deep model's output denoted by the estimated gestational age.","The evaluation of fetal lung maturity is critical for clinical practice since the lung immaturity is an important cause of neonatal morbidity and mortality. For the evaluation of the development of fetal lung maturation degree, our study established a deep model from ultrasound images of four-cardiac-chamber view plane. A two-stage transfer learning approach is proposed for the purpose of the study. A specific U-net structure is designed for the applied deep model. In the first stage, the model is to first learn the recognition of fetal lung region in the ultrasound images. It is hypothesized in our study that the development of fetal lung maturation degree is generally proportional to the gestational age. Then, in the second stage, the pretrained deep model is trained to accurately estimate the gestational age from the fetal lung region of ultrasound images. Totally 332 patients were included in our study, while the first 206 patients were used for training and the subsequent 126 patients were used for the independent testing. The testing results of the established deep model have the imprecision as 1.56â€‰Â±â€‰2.17Â weeks on the gestational age estimation. Its correlation coefficient with the ground truth of gestational age achieves 0.7624 (95% CI 0.6779 to 0.8270, P valueâ€‰&lt;â€‰0.00001). The hypothesis that the development of fetal lung maturation degree can be represented by the texture information from ultrasound images has been preliminarily validated. The fetal lung maturation degree can be considered as being represented by the deep model's output denoted by the estimated gestational age.","Chen, Chen, Deng, Wang, He, Lv, Yu","Chen, Chen, Deng, Wang, He, Lv, Yu",https://doi.org/10.1007/s11548-020-02211-1,https://doi.org/10.1007/s11548-020-02211-1,2021-08-03
16669.0,pubmed,pubmed,"Measuring the effectiveness of an automated text messaging active surveillance system for COVID-19 in the south of Ireland, March to April 2020","Measuring the effectiveness of an automated text messaging active surveillance system for COVID-19 in the south of Ireland, March to April 2020","We report the effectiveness of automated text messaging for active surveillance of asymptomatic close contacts of coronavirus disease (COVID-19) cases in the Cork/Kerry region of Ireland. In the first 7 weeks of the COVID-19 outbreak, 1,336 close contacts received 12,421 automated texts. Overall, 120 contacts (9.0%) reported symptoms which required referral for testing and 35 (2.6%) tested positive for COVID-19. Non-response was high (nÃ¢â‚¬â€°=Ã¢â‚¬â€°2,121; 17.1%) and this required substantial clinical and administrative resources for follow-up.","We report the effectiveness of automated text messaging for active surveillance of asymptomatic close contacts of coronavirus disease (COVID-19) cases in the Cork/Kerry region of Ireland. In the first 7 weeks of the COVID-19 outbreak, 1,336 close contacts received 12,421 automated texts. Overall, 120 contacts (9.0%) reported symptoms which required referral for testing and 35 (2.6%) tested positive for COVID-19. Non-response was high (nâ€‰=â€‰2,121; 17.1%) and this required substantial clinical and administrative resources for follow-up.","Barrett, Bambury, Kelly, Condon, Crompton, Sheahan","Barrett, Bambury, Kelly, Condon, Crompton, Sheahan",https://doi.org/10.2807/1560-7917.ES.2020.25.23.2000972,https://doi.org/10.2807/1560-7917.ES.2020.25.23.2000972,2021-08-03
16671.0,pubmed,pubmed,Personal protective equipment (PPE) during the COVID-19 pandemic for oculofacial plastic and orbital surgery,Personal protective equipment (PPE) during the COVID-19 pandemic for oculofacial plastic and orbital surgery,"To review the current literature on Coronavirus Disease 2019 (COVID-19) virology and transmission; to present a decision tree for risk stratifying oculofacial plastic and orbital surgeries; and to generate personal protective equipment (PPE) recommendations by risk category. A comprehensive literature review on COVID-19 was conducted. A two-stage modified Delphi technique involving 18 oculofacial plastic and orbital surgeons across Canada was used to determine consensus risk-stratification criteria and PPE recommendations for surgeries performed in the North American context. COVID-19 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). We summarize COVID-19 virology and transmission, as well as practice considerations for oculofacial plastic and orbital surgeons. Although SARS-CoV-2 is known to be transmitted predominantly by droplet mechanisms, some studies suggest that transmission is possible through aerosols. Among common procedures performed by oculofacial and plastic surgeons, some are likely to be considered aerosol-generating. Risk of transmission increases when manipulating structures known to harbor high viral loads. We present an algorithm for risk-stratification based on the nature of surgery and the anatomical sites involved and offer recommendations for PPE. Although universal droplet precautions are now recommended in most healthcare settings, some clinical situations require more stringent infection control measures. By highlighting high-risk scenarios specific to oculofacial plastic and orbital surgery, as well as PPE recommendations, we hope to enhance the safety of continued care during the COVID-19 pandemic.","<b>Purpose</b>: To review the current literature on Coronavirus Disease 2019 (COVID-19) virology and transmission; to present a decision tree for risk stratifying oculofacial plastic and orbital surgeries; and to generate personal protective equipment (PPE) recommendations by risk category.<b>Methods</b>: A comprehensive literature review on COVID-19 was conducted. A two-stage modified Delphi technique involving 18 oculofacial plastic and orbital surgeons across Canada was used to determine consensus risk-stratification criteria and PPE recommendations for surgeries performed in the North American context.<b>Results</b>: COVID-19 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). We summarize COVID-19 virology and transmission, as well as practice considerations for oculofacial plastic and orbital surgeons. Although SARS-CoV-2 is known to be transmitted predominantly by droplet mechanisms, some studies suggest that transmission is possible through aerosols. Among common procedures performed by oculofacial and plastic surgeons, some are likely to be considered aerosol-generating. Risk of transmission increases when manipulating structures known to harbor high viral loads. We present an algorithm for risk-stratification based on the nature of surgery and the anatomical sites involved and offer recommendations for PPE.<b>Conclusions</b>: Although universal droplet precautions are now recommended in most healthcare settings, some clinical situations require more stringent infection control measures. By highlighting high-risk scenarios specific to oculofacial plastic and orbital surgery, as well as PPE recommendations, we hope to enhance the safety of continued care during the COVID-19 pandemic.","Daigle, Leung, Yin, Kalin-Hajdu, Nijhawan","Daigle, Leung, Yin, Kalin-Hajdu, Nijhawan",https://doi.org/10.1080/01676830.2020.1781200,https://doi.org/10.1080/01676830.2020.1781200,2021-08-03
16673.0,pubmed,pubmed,Empirical assessment of bias in machine learning diagnostic test accuracy studies,Empirical assessment of bias in machine learning diagnostic test accuracy studies,"Machine learning (ML) diagnostic tools have significant potential to improve health care. However, methodological pitfalls may affect diagnostic test accuracy studies used to appraise such tools. We aimed to evaluate the prevalence and reporting of design characteristics within the literature. Further, we sought to empirically assess whether design features may be associated with different estimates of diagnostic accuracy. We systematically retrieved 2 Ãƒâ€” 2 tables (nÃ¢â‚¬â€°=Ã¢â‚¬â€°281) describing the performance of ML diagnostic tools, derived from 114 publications in 38 meta-analyses, from PubMed. Data extracted included test performance, sample sizes, and design features. A mixed-effects metaregression was run to quantify the association between design features and diagnostic accuracy. Participant ethnicity and blinding in test interpretation was unreported in 90% and 60% of studies, respectively. Reporting was occasionally lacking for rudimentary characteristics such as study design (28% unreported). Internal validation without appropriate safeguards was used in 44% of studies. Several design features were associated with larger estimates of accuracy, including having unreported (relative diagnostic odds ratio [RDOR], 2.11; 95% confidence interval [CI], 1.43-3.1) or case-control study designs (RDOR, 1.27; 95% CI, 0.97-1.66), and recruiting participants for the index test (RDOR, 1.67; 95% CI, 1.08-2.59). Significant underreporting of experimental details was present. Study design features may affect estimates of diagnostic performance in the ML diagnostic test accuracy literature. The present study identifies pitfalls that threaten the validity, generalizability, and clinical value of ML diagnostic tools and provides recommendations for improvement.","Machine learning (ML) diagnostic tools have significant potential to improve health care. However, methodological pitfalls may affect diagnostic test accuracy studies used to appraise such tools. We aimed to evaluate the prevalence and reporting of design characteristics within the literature. Further, we sought to empirically assess whether design features may be associated with different estimates of diagnostic accuracy. We systematically retrieved 2 Ã— 2 tables (nâ€‰=â€‰281) describing the performance of ML diagnostic tools, derived from 114 publications in 38 meta-analyses, from PubMed. Data extracted included test performance, sample sizes, and design features. A mixed-effects metaregression was run to quantify the association between design features and diagnostic accuracy. Participant ethnicity and blinding in test interpretation was unreported in 90% and 60% of studies, respectively. Reporting was occasionally lacking for rudimentary characteristics such as study design (28% unreported). Internal validation without appropriate safeguards was used in 44% of studies. Several design features were associated with larger estimates of accuracy, including having unreported (relative diagnostic odds ratio [RDOR], 2.11; 95% confidence interval [CI], 1.43-3.1) or case-control study designs (RDOR, 1.27; 95% CI, 0.97-1.66), and recruiting participants for the index test (RDOR, 1.67; 95% CI, 1.08-2.59). Significant underreporting of experimental details was present. Study design features may affect estimates of diagnostic performance in the ML diagnostic test accuracy literature. The present study identifies pitfalls that threaten the validity, generalizability, and clinical value of ML diagnostic tools and provides recommendations for improvement.","Crowley, Tan, Ioannidis","Crowley, Tan, Ioannidis",https://doi.org/10.1093/jamia/ocaa075,https://doi.org/10.1093/jamia/ocaa075,2021-08-03
16674.0,pubmed,pubmed,Lung Nodule Sizes Are Encoded When Scaling CT Image for CNN's,Lung Nodule Sizes Are Encoded When Scaling CT Image for CNN's,"Noninvasive diagnosis of lung cancer in early stages is one task where radiomics helps. Clinical practice shows that the size of a nodule has high predictive power for malignancy. In the literature, convolutional neural networks (CNNs) have become widely used in medical image analysis. We study the ability of a CNN to capture nodule size in computed tomography images after images are resized for CNN input. For our experiments, we used the National Lung Screening Trial data set. Nodules were labeled into 2 categories (small/large) based on the original size of a nodule. After all extracted patches were re-sampled into 100-by-100-pixel images, a CNN was able to successfully classify test nodules into small- and large-size groups with high accuracy. To show the generality of our discovery, we repeated size classification experiments using Common Objects in Context (COCO) data set. From the data set, we selected 3 categories of images, namely, bears, cats, and dogs. For all 3 categories a 5- Ãƒâ€” 2-fold cross-validation was performed to put them into small and large classes. The average area under receiver operating curve is 0.954, 0.952, and 0.979 for the bear, cat, and dog categories, respectively. Thus, camera image rescaling also enables a CNN to discover the size of an object. The source code for experiments with the COCO data set is publicly available in Github (https://github.com/VisionAI-USF/COCO_Size_Decoding/).","Noninvasive diagnosis of lung cancer in early stages is one task where radiomics helps. Clinical practice shows that the size of a nodule has high predictive power for malignancy. In the literature, convolutional neural networks (CNNs) have become widely used in medical image analysis. We study the ability of a CNN to capture nodule size in computed tomography images after images are resized for CNN input. For our experiments, we used the National Lung Screening Trial data set. Nodules were labeled into 2 categories (small/large) based on the original size of a nodule. After all extracted patches were re-sampled into 100-by-100-pixel images, a CNN was able to successfully classify test nodules into small- and large-size groups with high accuracy. To show the generality of our discovery, we repeated size classification experiments using Common Objects in Context (COCO) data set. From the data set, we selected 3 categories of images, namely, bears, cats, and dogs. For all 3 categories a 5- Ã— 2-fold cross-validation was performed to put them into small and large classes. The average area under receiver operating curve is 0.954, 0.952, and 0.979 for the bear, cat, and dog categories, respectively. Thus, camera image rescaling also enables a CNN to discover the size of an object. The source code for experiments with the COCO data set is publicly available in Github (https://github.com/VisionAI-USF/COCO_Size_Decoding/).","Cherezov, Paul, Fetisov, Gillies, Schabath, Goldgof, Hall","Cherezov, Paul, Fetisov, Gillies, Schabath, Goldgof, Hall",https://doi.org/10.18383/j.tom.2019.00024,https://doi.org/10.18383/j.tom.2019.00024,2021-08-03
16679.0,pubmed,pubmed,Artificial intelligence and radiomics in pediatric molecular imaging,Artificial intelligence and radiomics in pediatric molecular imaging,"In the past decade, a new approach for quantitative analysis of medical images and prognostic modelling has emerged. Defined as the extraction and analysis of a large number of quantitative parameters from medical images, radiomics is an evolving field in precision medicine with the ultimate goal of the discovery of new imaging biomarkers for disease. Radiomics has already shown promising results in extracting diagnostic, prognostic, and molecular information latent in medical images. After acquisition of the medical images as part of the standard of care, a region of interest is defined often via a manual or semi-automatic approach. An algorithm then extracts and computes quantitative radiomics parameters from the region of interest. Whereas radiomics captures quantitative values of shape and texture based on predefined mathematical terms, neural networks have recently been used to directly learn and identify predictive features from medical images. Thereby, neural networks largely forego the need for so called &quot;hand-engineered&quot; features, which appears to result in significantly improved performance and reliability. Opportunities for radiomics and neural networks in pediatric nuclear medicine/radiology/molecular imaging are broad and can be thought of in three categories: automating well-defined administrative or clinical tasks, augmenting broader administrative or clinical tasks, and unlocking new methods of generating value. Specific applications include intelligent order sets, automated protocoling, improved image acquisition, computer aided triage and detection of abnormalities, next generation voice dictation systems, biomarker development, and therapy planning.","In the past decade, a new approach for quantitative analysis of medical images and prognostic modelling has emerged. Defined as the extraction and analysis of a large number of quantitative parameters from medical images, radiomics is an evolving field in precision medicine with the ultimate goal of the discovery of new imaging biomarkers for disease. Radiomics has already shown promising results in extracting diagnostic, prognostic, and molecular information latent in medical images. After acquisition of the medical images as part of the standard of care, a region of interest is defined often via a manual or semi-automatic approach. An algorithm then extracts and computes quantitative radiomics parameters from the region of interest. Whereas radiomics captures quantitative values of shape and texture based on predefined mathematical terms, neural networks have recently been used to directly learn and identify predictive features from medical images. Thereby, neural networks largely forego the need for so called ""hand-engineered"" features, which appears to result in significantly improved performance and reliability. Opportunities for radiomics and neural networks in pediatric nuclear medicine/radiology/molecular imaging are broad and can be thought of in three categories: automating well-defined administrative or clinical tasks, augmenting broader administrative or clinical tasks, and unlocking new methods of generating value. Specific applications include intelligent order sets, automated protocoling, improved image acquisition, computer aided triage and detection of abnormalities, next generation voice dictation systems, biomarker development, and therapy planning.","Wagner, Bilbily, Beheshti, Shammas, Vali","Wagner, Bilbily, Beheshti, Shammas, Vali",https://doi.org/10.1016/j.ymeth.2020.06.008,https://doi.org/10.1016/j.ymeth.2020.06.008,2021-08-03
16681.0,pubmed,pubmed,Use of a Machine Learning Program to Correctly Triage Incoming Text Messaging Replies From a Cardiovascular Text-Based Secondary Prevention Program: Feasibility Study,Use of a Machine Learning Program to Correctly Triage Incoming Text Messaging Replies From a Cardiovascular Text-Based Secondary Prevention Program: Feasibility Study,"SMS text messaging programs are increasingly being used for secondary prevention, and have been shown to be effective in a number of health conditions including cardiovascular disease. SMS text messaging programs have the potential to increase the reach of an intervention, at a reduced cost, to larger numbers of people who may not access traditional programs. However, patients regularly reply to the SMS text messages, leading to additional staffing requirements to monitor and moderate the patients' SMS text messaging replies. This additional staff requirement directly impacts the cost-effectiveness and scalability of SMS text messaging interventions. This study aimed to test the feasibility and accuracy of developing a machine learning (ML) program to triage SMS text messaging replies (ie, identify which SMS text messaging replies require a health professional review). SMS text messaging replies received from 2 clinical trials were manually coded (1) into &quot;Is staff review required?&quot; (binary response of yes/no); and then (2) into 12 general categories. Five ML models (NaÃƒÂ¯ve Bayes, OneVsRest, Random Forest Decision Trees, Gradient Boosted Trees, and Multilayer Perceptron) and an ensemble model were tested. For each model run, data were randomly allocated into training set (2183/3118, 70.01%) and test set (935/3118, 29.98%). Accuracy for the yes/no classification was calculated using area under the receiver operating characteristics curve (AUC), false positives, and false negatives. Accuracy for classification into 12 categories was compared using multiclass classification evaluators. A manual review of 3118 SMS text messaging replies showed that 22.00% (686/3118) required staff review. For determining need for staff review, the Multilayer Perceptron model had highest accuracy (AUC 0.86; 4.85% false negatives; and 4.63% false positives); with addition of heuristics (specified keywords) fewer false negatives were identified (3.19%), with small increase in false positives (7.66%) and AUC 0.79. Application of this model would result in 26.7% of SMS text messaging replies requiring review (true + false positives). The ensemble model produced the lowest false negatives (1.43%) at the expense of higher false positives (16.19%). OneVsRest was the most accurate (72.3%) for the 12-category classification. The ML program has high sensitivity for identifying the SMS text messaging replies requiring staff input; however, future research is required to validate the models against larger data sets. Incorporation of an ML program to review SMS text messaging replies could significantly reduce staff workload, as staff would not have to review all incoming SMS text messages. This could lead to substantial improvements in cost-effectiveness, scalability, and capacity of SMS text messaging-based interventions.","SMS text messaging programs are increasingly being used for secondary prevention, and have been shown to be effective in a number of health conditions including cardiovascular disease. SMS text messaging programs have the potential to increase the reach of an intervention, at a reduced cost, to larger numbers of people who may not access traditional programs. However, patients regularly reply to the SMS text messages, leading to additional staffing requirements to monitor and moderate the patients' SMS text messaging replies. This additional staff requirement directly impacts the cost-effectiveness and scalability of SMS text messaging interventions. This study aimed to test the feasibility and accuracy of developing a machine learning (ML) program to triage SMS text messaging replies (ie, identify which SMS text messaging replies require a health professional review). SMS text messaging replies received from 2 clinical trials were manually coded (1) into ""Is staff review required?"" (binary response of yes/no); and then (2) into 12 general categories. Five ML models (NaÃ¯ve Bayes, OneVsRest, Random Forest Decision Trees, Gradient Boosted Trees, and Multilayer Perceptron) and an ensemble model were tested. For each model run, data were randomly allocated into training set (2183/3118, 70.01%) and test set (935/3118, 29.98%). Accuracy for the yes/no classification was calculated using area under the receiver operating characteristics curve (AUC), false positives, and false negatives. Accuracy for classification into 12 categories was compared using multiclass classification evaluators. A manual review of 3118 SMS text messaging replies showed that 22.00% (686/3118) required staff review. For determining need for staff review, the Multilayer Perceptron model had highest accuracy (AUC 0.86; 4.85% false negatives; and 4.63% false positives); with addition of heuristics (specified keywords) fewer false negatives were identified (3.19%), with small increase in false positives (7.66%) and AUC 0.79. Application of this model would result in 26.7% of SMS text messaging replies requiring review (true + false positives). The ensemble model produced the lowest false negatives (1.43%) at the expense of higher false positives (16.19%). OneVsRest was the most accurate (72.3%) for the 12-category classification. The ML program has high sensitivity for identifying the SMS text messaging replies requiring staff input; however, future research is required to validate the models against larger data sets. Incorporation of an ML program to review SMS text messaging replies could significantly reduce staff workload, as staff would not have to review all incoming SMS text messages. This could lead to substantial improvements in cost-effectiveness, scalability, and capacity of SMS text messaging-based interventions.","Lowres, Duckworth, Redfern, Thiagalingam, Chow","Lowres, Duckworth, Redfern, Thiagalingam, Chow",https://doi.org/10.2196/19200,https://doi.org/10.2196/19200,2021-08-03
16692.0,pubmed,pubmed,Generating high-quality data abstractions from scanned clinical records: text-mining-assisted extraction of endometrial carcinoma pathology features as proof of principle,Generating high-quality data abstractions from scanned clinical records: text-mining-assisted extraction of endometrial carcinoma pathology features as proof of principle,"Medical research studies often rely on the manual collection of data from scanned typewritten clinical records, which can be laborious, time consuming and error prone because of the need to review individual clinical records. We aimed to use text mining to assist with the extraction of clinical features from complex text-based scanned pathology records for medical research studies. Text mining performance was measured by extracting and annotating three distinct pathological features from scanned photocopies of endometrial carcinoma clinical pathology reports, and comparing results to manually abstracted terms. Inclusion and exclusion keyword trigger terms to capture leiomyomas, endometriosis and adenomyosis were provided based on expert knowledge. Terms were expanded with character variations based on common optical character recognition (OCR) error patterns as well as negation phrases found in sample reports. The approach was evaluated on an unseen test set of 1293 scanned pathology reports originating from laboratories across Australia. Scanned typewritten pathology reports for women aged 18-79 years with newly diagnosed endometrial cancer (2005-2007) in Australia. High concordance with final abstracted codes was observed for identifying the presence of three pathology features (94%-98% F-measure). The approach was more consistent and reliable than manual abstractions, identifying 3%-14%Ã¢â‚¬â€°additional feature instances. Keyword trigger-based automation with OCR error correction and negation handling proved not only to be rapid and convenient, but also providing consistent and reliable data abstractions from scanned clinical records. In conjunction with manual review, it can assist in the generation of high-quality data abstractions for medical research studies.","Medical research studies often rely on the manual collection of data from scanned typewritten clinical records, which can be laborious, time consuming and error prone because of the need to review individual clinical records. We aimed to use text mining to assist with the extraction of clinical features from complex text-based scanned pathology records for medical research studies. Text mining performance was measured by extracting and annotating three distinct pathological features from scanned photocopies of endometrial carcinoma clinical pathology reports, and comparing results to manually abstracted terms. Inclusion and exclusion keyword trigger terms to capture leiomyomas, endometriosis and adenomyosis were provided based on expert knowledge. Terms were expanded with character variations based on common optical character recognition (OCR) error patterns as well as negation phrases found in sample reports. The approach was evaluated on an unseen test set of 1293 scanned pathology reports originating from laboratories across Australia. Scanned typewritten pathology reports for women aged 18-79 years with newly diagnosed endometrial cancer (2005-2007) in Australia. High concordance with final abstracted codes was observed for identifying the presence of three pathology features (94%-98% F-measure). The approach was more consistent and reliable than manual abstractions, identifying 3%-14%â€‰additional feature instances. Keyword trigger-based automation with OCR error correction and negation handling proved not only to be rapid and convenient, but also providing consistent and reliable data abstractions from scanned clinical records. In conjunction with manual review, it can assist in the generation of high-quality data abstractions for medical research studies.","Nguyen, O'Dwyer, Vu, Webb, Johnatty, Spurdle","Nguyen, O'Dwyer, Vu, Webb, Johnatty, Spurdle",https://doi.org/10.1136/bmjopen-2020-037740,https://doi.org/10.1136/bmjopen-2020-037740,2021-08-03
16693.0,pubmed,pubmed,Mindfulness Ased Stress Reduction Interventions for Cancer Related Fatigue: A Meta-Analysis and Systematic Review,Mindfulness Ased Stress Reduction Interventions for Cancer Related Fatigue: A Meta-Analysis and Systematic Review,"This meta-analysis aims to systematically evaluate the evidence for mindfulness-based stress reduction (MBSR) in cancer related fatigue (CRF). In October 2018, PubMed, Embase, Cochrane Library, Clinical Trials, China Biomedical Literature Database (CBM), China National Knowledge Infrastructure (CNKI) and China Science Periodical Database (CSPD) were searched for randomized controlled trials on MBSR in CRF patients. Literature screening and data extraction were conducted by two reviewers. Methodological quality evaluation was assessed by the Cochrane risk of bias tool. Revman 5.3.0 performs data analysis. The trial sequential analysis software estimated the required information size for each outcome indicator. There have been 5 studies included in this research for meta-analysis, 356 cases in the experimental group and 344 cases in the control group. The meta-analysis result indicates that: MBSR can reduce the cancer-related fatigue score of cancer patients, SMDÃ‚Â =Ã‚Â -0.51,95%CI [-0.81-0.20], PÃ‚Â =Ã‚Â 0.001, and the difference is statistically significant. The trial sequential analysis indicates that: The RIS required for the indicator to reach the level of significance test should be 1768. The sample size (700 cases) included in the study has not reached the RIS, but it has crossed the traditional threshold and the TSA threshold, indicating that the results tend to be stable. The grading results are shown as low-quality evidence. This research has used evidence-based medicine to evaluate whether MBSR can alleviate CRF in cancer patients and provide evidence for the comprehensive intervention program for patients with cancer-related fatigue.","This meta-analysis aims to systematically evaluate the evidence for mindfulness-based stress reduction (MBSR) in cancer related fatigue (CRF). In October 2018, PubMed, Embase, Cochrane Library, Clinical Trials, China Biomedical Literature Database (CBM), China National Knowledge Infrastructure (CNKI) and China Science Periodical Database (CSPD) were searched for randomized controlled trials on MBSR in CRF patients. Literature screening and data extraction were conducted by two reviewers. Methodological quality evaluation was assessed by the Cochrane risk of bias tool. Revman 5.3.0 performs data analysis. The trial sequential analysis software estimated the required information size for each outcome indicator. There have been 5 studies included in this research for meta-analysis, 356 cases in the experimental group and 344 cases in the control group. The meta-analysis result indicates that: MBSR can reduce the cancer-related fatigue score of cancer patients, SMDÂ =Â -0.51,95%CI [-0.81-0.20], PÂ =Â 0.001, and the difference is statistically significant. The trial sequential analysis indicates that: The RIS required for the indicator to reach the level of significance test should be 1768. The sample size (700 cases) included in the study has not reached the RIS, but it has crossed the traditional threshold and the TSA threshold, indicating that the results tend to be stable. The grading results are shown as low-quality evidence. This research has used evidence-based medicine to evaluate whether MBSR can alleviate CRF in cancer patients and provide evidence for the comprehensive intervention program for patients with cancer-related fatigue.","He, Hou, Qi, Zhang, Wang, Qian","He, Hou, Qi, Zhang, Wang, Qian",https://doi.org/10.1016/j.jnma.2020.04.006,https://doi.org/10.1016/j.jnma.2020.04.006,2021-08-03
16695.0,pubmed,pubmed,Prostate MRI radiomics: A systematic review and radiomic quality score assessment,Prostate MRI radiomics: A systematic review and radiomic quality score assessment,"Radiomics have the potential to further increase the value of MRI in prostate cancer management. However, implementation in clinical practice is still far and concerns have been raised regarding the methodological quality of radiomic studies. Therefore, we aimed to systematically review the literature to assess the quality of prostate MRI radiomic studies using the radiomics quality score (RQS). Multiple medical literature archives (PubMed, Web of Science and EMBASE) were searched to retrieve original investigations focused on prostate MRI radiomic approaches up to the end of June 2019. Three researchers independently assessed each paper using the RQS. Data from the most experienced researcher were used for descriptive analysis. Inter-rater reproducibility was assessed using the intraclass correlation coefficient (ICC) on the total RQS score. 73 studies were included in the analysis. Overall, the average RQS total score was 7.93Ã¢â‚¬â€°Ã‚Â±Ã¢â‚¬â€°5.13 on a maximum of 36 points, with a final average percentage of 23 Ã‚Â± 13%. Among the most critical items, the lack of feature robustness testing strategies and external validation datasets. The ICC resulted poor to moderate, with an average value of 0.57 and 95% Confidence Intervals between 0.44 and 0.69. Current studies on prostate MRI radiomics still lack the quality required to allow their introduction in clinical practice.","Radiomics have the potential to further increase the value of MRI in prostate cancer management. However, implementation in clinical practice is still far and concerns have been raised regarding the methodological quality of radiomic studies. Therefore, we aimed to systematically review the literature to assess the quality of prostate MRI radiomic studies using the radiomics quality score (RQS). Multiple medical literature archives (PubMed, Web of Science and EMBASE) were searched to retrieve original investigations focused on prostate MRI radiomic approaches up to the end of June 2019. Three researchers independently assessed each paper using the RQS. Data from the most experienced researcher were used for descriptive analysis. Inter-rater reproducibility was assessed using the intraclass correlation coefficient (ICC) on the total RQS score. 73 studies were included in the analysis. Overall, the average RQS total score was 7.93â€‰Â±â€‰5.13 on a maximum of 36 points, with a final average percentage of 23 Â± 13%. Among the most critical items, the lack of feature robustness testing strategies and external validation datasets. The ICC resulted poor to moderate, with an average value of 0.57 and 95% Confidence Intervals between 0.44 and 0.69. Current studies on prostate MRI radiomics still lack the quality required to allow their introduction in clinical practice.","Stanzione, Gambardella, Cuocolo, Ponsiglione, Romeo, Imbriaco","Stanzione, Gambardella, Cuocolo, Ponsiglione, Romeo, Imbriaco",https://doi.org/10.1016/j.ejrad.2020.109095,https://doi.org/10.1016/j.ejrad.2020.109095,2021-08-03
16699.0,pubmed,pubmed,The Effects of Cognitive Training on Brain Network Activity and Connectivity in Aging and Neurodegenerative Diseases: a Systematic Review,The Effects of Cognitive Training on Brain Network Activity and Connectivity in Aging and Neurodegenerative Diseases: a Systematic Review,"Cognitive training (CT) is an increasingly popular, non-pharmacological intervention for improving cognitive functioning in neurodegenerative diseases and healthy aging. Although meta-analyses support the efficacy of CT in improving cognitive functioning, the neural mechanisms underlying the effects of CT are still unclear. We performed a systematic review of literature in the PubMed, Embase and PsycINFO databases on controlled CT trials (NÃ‚Â &gt;Ã¢â‚¬â€°20) in aging and neurodegenerative diseases with pre- and post-training functional MRI outcomes up to November 23rd 2018 (PROSPERO registration number CRD42019103662). Twenty articles were eligible for our systematic review. We distinguished between multi-domain and single-domain CT. CT induced both increases and decreases in task-related functional activation, possibly indicative of an inverted U-shaped curve association between regional brain activity and task performance. Functional connectivity within 'cognitive' brain networks was consistently reported to increase after CT while a minority of studies additionally reported increased segregation of frontoparietal and default mode brain networks. Although we acknowledge the large heterogeneity in type of CT, imaging methodology, in-scanner task paradigm and analysis methods between studies, we propose a working model of the effects of CT on brain activity and connectivity in the context of current knowledge on compensatory mechanisms that are associated with aging and neurodegenerative diseases.","Cognitive training (CT) is an increasingly popular, non-pharmacological intervention for improving cognitive functioning in neurodegenerative diseases and healthy aging. Although meta-analyses support the efficacy of CT in improving cognitive functioning, the neural mechanisms underlying the effects of CT are still unclear. We performed a systematic review of literature in the PubMed, Embase and PsycINFO databases on controlled CT trials (NÂ &gt;â€‰20) in aging and neurodegenerative diseases with pre- and post-training functional MRI outcomes up to November 23rd 2018 (PROSPERO registration number CRD42019103662). Twenty articles were eligible for our systematic review. We distinguished between multi-domain and single-domain CT. CT induced both increases and decreases in task-related functional activation, possibly indicative of an inverted U-shaped curve association between regional brain activity and task performance. Functional connectivity within 'cognitive' brain networks was consistently reported to increase after CT while a minority of studies additionally reported increased segregation of frontoparietal and default mode brain networks. Although we acknowledge the large heterogeneity in type of CT, imaging methodology, in-scanner task paradigm and analysis methods between studies, we propose a working model of the effects of CT on brain activity and connectivity in the context of current knowledge on compensatory mechanisms that are associated with aging and neurodegenerative diseases.","van Balkom, van den Heuvel, Berendse, van der Werf, Vriend","van Balkom, van den Heuvel, Berendse, van der Werf, Vriend",https://doi.org/10.1007/s11065-020-09440-w,https://doi.org/10.1007/s11065-020-09440-w,2021-08-03
16705.0,pubmed,pubmed,The use of machine learning in rare diseases: a scoping review,The use of machine learning in rare diseases: a scoping review,"Emerging machine learning technologies are beginning to transform medicine and healthcare and could also improve the diagnosis and treatment of rare diseases. Currently, there are no systematic reviews that investigate, from a general perspective, how machine learning is used in a rare disease context. This scoping review aims to address this gap and explores the use of machine learning in rare diseases, investigating, for example, in which rare diseases machine learning is applied, which types of algorithms and input data are used or which medical applications (e.g., diagnosis, prognosis or treatment) are studied. Using a complex search string including generic search terms and 381 individual disease names, studies from the past 10 years (2010-2019) that applied machine learning in a rare disease context were identified on PubMed. To systematically map the research activity, eligible studies were categorized along different dimensions (e.g., rare disease group, type of algorithm, input data), and the number of studies within these categories was analyzed. Two hundred eleven studies from 32 countries investigating 74 different rare diseases were identified. Diseases with a higher prevalence appeared more often in the studies than diseases with a lower prevalence. Moreover, some rare disease groups were investigated more frequently than to be expected (e.g., rare neurologic diseases and rare systemic or rheumatologic diseases), others less frequently (e.g., rare inborn errors of metabolism and rare skin diseases). Ensemble methods (36.0%), support vector machines (32.2%) and artificial neural networks (31.8%) were the algorithms most commonly applied in the studies. Only a small proportion of studies evaluated their algorithms on an external data set (11.8%) or against a human expert (2.4%). As input data, images (32.2%), demographic data (27.0%) and &quot;omics&quot; data (26.5%) were used most frequently. Most studies used machine learning for diagnosis (40.8%) or prognosis (38.4%) whereas studies aiming to improve treatment were relatively scarce (4.7%). Patient numbers in the studies were small, typically ranging from 20 to 99 (35.5%). Our review provides an overview of the use of machine learning in rare diseases. Mapping the current research activity, it can guide future work and help to facilitate the successful application of machine learning in rare diseases.","Emerging machine learning technologies are beginning to transform medicine and healthcare and could also improve the diagnosis and treatment of rare diseases. Currently, there are no systematic reviews that investigate, from a general perspective, how machine learning is used in a rare disease context. This scoping review aims to address this gap and explores the use of machine learning in rare diseases, investigating, for example, in which rare diseases machine learning is applied, which types of algorithms and input data are used or which medical applications (e.g., diagnosis, prognosis or treatment) are studied. Using a complex search string including generic search terms and 381 individual disease names, studies from the past 10 years (2010-2019) that applied machine learning in a rare disease context were identified on PubMed. To systematically map the research activity, eligible studies were categorized along different dimensions (e.g., rare disease group, type of algorithm, input data), and the number of studies within these categories was analyzed. Two hundred eleven studies from 32 countries investigating 74 different rare diseases were identified. Diseases with a higher prevalence appeared more often in the studies than diseases with a lower prevalence. Moreover, some rare disease groups were investigated more frequently than to be expected (e.g., rare neurologic diseases and rare systemic or rheumatologic diseases), others less frequently (e.g., rare inborn errors of metabolism and rare skin diseases). Ensemble methods (36.0%), support vector machines (32.2%) and artificial neural networks (31.8%) were the algorithms most commonly applied in the studies. Only a small proportion of studies evaluated their algorithms on an external data set (11.8%) or against a human expert (2.4%). As input data, images (32.2%), demographic data (27.0%) and ""omics"" data (26.5%) were used most frequently. Most studies used machine learning for diagnosis (40.8%) or prognosis (38.4%) whereas studies aiming to improve treatment were relatively scarce (4.7%). Patient numbers in the studies were small, typically ranging from 20 to 99 (35.5%). Our review provides an overview of the use of machine learning in rare diseases. Mapping the current research activity, it can guide future work and help to facilitate the successful application of machine learning in rare diseases.","Schaefer, Lehne, Schepers, Prasser, Thun","Schaefer, Lehne, Schepers, Prasser, Thun",https://doi.org/10.1186/s13023-020-01424-6,https://doi.org/10.1186/s13023-020-01424-6,2021-08-03
16715.0,pubmed,pubmed,Investigating exploration for deep reinforcement learning of concentric tube robot control,Investigating exploration for deep reinforcement learning of concentric tube robot control,"Concentric tube robots are composed of multiple concentric, pre-curved, super-elastic, telescopic tubes that are compliant and have a small diameter suitable for interventions that must be minimally invasive like fetal surgery. Combinations of rotation and extension of the tubes can alter the robot's shape but the inverse kinematics are complex to model due to the challenge of incorporating friction and other tube interactions or manufacturing imperfections. We propose a model-free reinforcement learning approach to form the inverse kinematics solution and directly obtain a control policy. Three exploration strategies are shown for deep deterministic policy gradient with hindsight experience replay for concentric tube robots in simulation environments. The aim is to overcome the joint to Cartesian sampling bias and be scalable with the number of robotic tubes. To compare strategies, evaluation of the trained policy network to selected Cartesian goals and associated errors are analyzed. The learned control policy is demonstrated with trajectory following tasks. Separation of extension and rotation joints for Gaussian exploration is required to overcome Cartesian sampling bias. Parameter noise and Ornstein-Uhlenbeck were found to be optimal strategies with less than 1 mm error in all simulation environments. Various trajectories can be followed with the optimal exploration strategy learned policy at high joint extension values. Our inverse kinematics solver in evaluation has 0.44Ã‚Â mm extension and [Formula: see text] rotation error. We demonstrate the feasibility of effective model-free control for concentric tube robots. Directly using the control policy, arbitrary trajectories can be followed and this is an important step towards overcoming the challenge of concentric tube robot control for clinical use in minimally invasive interventions.","Concentric tube robots are composed of multiple concentric, pre-curved, super-elastic, telescopic tubes that are compliant and have a small diameter suitable for interventions that must be minimally invasive like fetal surgery. Combinations of rotation and extension of the tubes can alter the robot's shape but the inverse kinematics are complex to model due to the challenge of incorporating friction and other tube interactions or manufacturing imperfections. We propose a model-free reinforcement learning approach to form the inverse kinematics solution and directly obtain a control policy. Three exploration strategies are shown for deep deterministic policy gradient with hindsight experience replay for concentric tube robots in simulation environments. The aim is to overcome the joint to Cartesian sampling bias and be scalable with the number of robotic tubes. To compare strategies, evaluation of the trained policy network to selected Cartesian goals and associated errors are analyzed. The learned control policy is demonstrated with trajectory following tasks. Separation of extension and rotation joints for Gaussian exploration is required to overcome Cartesian sampling bias. Parameter noise and Ornstein-Uhlenbeck were found to be optimal strategies with less than 1 mm error in all simulation environments. Various trajectories can be followed with the optimal exploration strategy learned policy at high joint extension values. Our inverse kinematics solver in evaluation has 0.44Â mm extension and [Formula: see text] rotation error. We demonstrate the feasibility of effective model-free control for concentric tube robots. Directly using the control policy, arbitrary trajectories can be followed and this is an important step towards overcoming the challenge of concentric tube robot control for clinical use in minimally invasive interventions.","Iyengar, Dwyer, Stoyanov","Iyengar, Dwyer, Stoyanov",https://doi.org/10.1007/s11548-020-02194-z,https://doi.org/10.1007/s11548-020-02194-z,2021-08-03
16718.0,pubmed,pubmed,360-degree Delphi: addressing sociotechnical challenges of healthcare IT,360-degree Delphi: addressing sociotechnical challenges of healthcare IT,"IT systems in the healthcare field can have a marked sociotechnical impact: they modify communication habits, alter clinical processes and may have serious ethical implications. The introduction of such systems involves very different groups of stakeholders because of the inherent multi-professionalism in medicine and the role of patients and their relatives that are often underrepresented. Each group contributes distinct perspectives and particular needs, which create specific requirements for IT systems and may strongly influence their acceptance and success. In the past, needs analysis, challenges and requirements for medical IT systems have often been addressed using consensus techniques such as the Delphi technique. Facing the heterogeneous spectrum of stakeholders there is a need to develop these techniques further to control the (strong) influence of the composition of the expert panel on the outcome and to deal systematically with potentially incompatible needs of stakeholder groups. This approach uses the strong advantages a Delphi study has, identifies the disadvantages of traditional Delphi techniques and aims to introduce and evaluate a modified approach called 360-Degree Delphi. Key aspects of 360-Degree Delphi are tested by applying the approach to the needs and requirements analysis of a system for managing patients' advance directives and living wills. 360-Degree Delphi (short 360Ã‚Â°D), as a modified Delphi process, is specified as a structured workflow with the optional use of stakeholder groups. The approach redefines the composition of the expert panel by setting up groups of different stakeholders. Consensus is created within individual stakeholder groups, but is also communicated between groups, while the iterative structure of the Delphi process remains unchanged. We hypothesize that (1) 360-Degree Delphi yields complementary statements from different stakeholders, which would be lost in classical Delphi; while (2) the variation of statements within individual stakeholder groups is lower than within the total collective. A user study is performed that addresses five stakeholder groups (patients, relatives, medical doctors, nurses and software developers) on the topic of living will communication in an emergency context. Qualitative open questions are used in a Delphi round 0. Answer texts are coded by independent raters who carry out systematic bottom-up qualitative text analysis. Inter-rater reliability is calculated and the resulting codes are used to test the hypotheses. Qualitative results are transferred into quantitative questions and then surveyed in round 1. The study took place in Germany. About 25% of the invited experts (stakeholders) agreed to take part in the Delphi round 0 (three patients, two relatives, three medical doctors, two qualified nurses and three developers), forming a structured panel of the five stakeholder groups. Two raters created a bottom-up coding, and 238 thematic codes were identified by the qualitative text analysis. The inter-rater reliability showed that 44.95% of the codes were semantically similar and coded for the same parts of the raw textual replies. Based on a consented coding list, a quantitative online-questionnaire was developed and send to different stakeholder groups. With respect to the hypotheses, Delphi round 0 had the following results: (1) doctors had a completely different focus from all the other stakeholder groups on possible channels of communications with the patient; (2) the dispersion of codes within individual stakeholder groups and within the total collective - visualized by box plots - was approximately 28% higher in the total collective than in the sub-collectives, but without a marked effect size. With respect to the hypotheses, Delphi round 1 had the following results: different stakeholder groups had highly diverging opinions with respect to central questions on IT-development. For example, when asked to rate the importance of access control against high availability of data (likert scale, 1 meaning restrictive data access, 6 easy access to all data), patients (mean 4.862, Stdev +/-Ã¢â‚¬â€°1.866) and caregivers (mean 5.667, Stdev: +/-Ã¢â‚¬â€°0.816) highly favored data availability, while relatives would restrict data access (mean 2.778, stdev +/-Ã¢â‚¬â€°1.093). In comparison, the total group would not be representative of either of these individual stakeholder needs (mean 4.344, stdev +/-Ã¢â‚¬â€°1.870). 360-Degree Delphi is feasible and allows different stakeholder groups within an expert panel to reach agreement individually. Thus, it generates a more detailed consensus which pays more tribute to individual stakeholders needs. This has the potential to improve the time to consensus as well as to produce a more representative and precise needs and requirements analysis. However, the method may create new challenges for the IT development process, which will have to deal with complementary or even contradictory statements from different stakeholder groups.","IT systems in the healthcare field can have a marked sociotechnical impact: they modify communication habits, alter clinical processes and may have serious ethical implications. The introduction of such systems involves very different groups of stakeholders because of the inherent multi-professionalism in medicine and the role of patients and their relatives that are often underrepresented. Each group contributes distinct perspectives and particular needs, which create specific requirements for IT systems and may strongly influence their acceptance and success. In the past, needs analysis, challenges and requirements for medical IT systems have often been addressed using consensus techniques such as the Delphi technique. Facing the heterogeneous spectrum of stakeholders there is a need to develop these techniques further to control the (strong) influence of the composition of the expert panel on the outcome and to deal systematically with potentially incompatible needs of stakeholder groups. This approach uses the strong advantages a Delphi study has, identifies the disadvantages of traditional Delphi techniques and aims to introduce and evaluate a modified approach called 360-Degree Delphi. Key aspects of 360-Degree Delphi are tested by applying the approach to the needs and requirements analysis of a system for managing patients' advance directives and living wills. 360-Degree Delphi (short 360Â°D), as a modified Delphi process, is specified as a structured workflow with the optional use of stakeholder groups. The approach redefines the composition of the expert panel by setting up groups of different stakeholders. Consensus is created within individual stakeholder groups, but is also communicated between groups, while the iterative structure of the Delphi process remains unchanged. We hypothesize that (1) 360-Degree Delphi yields complementary statements from different stakeholders, which would be lost in classical Delphi; while (2) the variation of statements within individual stakeholder groups is lower than within the total collective. A user study is performed that addresses five stakeholder groups (patients, relatives, medical doctors, nurses and software developers) on the topic of living will communication in an emergency context. Qualitative open questions are used in a Delphi round 0. Answer texts are coded by independent raters who carry out systematic bottom-up qualitative text analysis. Inter-rater reliability is calculated and the resulting codes are used to test the hypotheses. Qualitative results are transferred into quantitative questions and then surveyed in round 1. The study took place in Germany. About 25% of the invited experts (stakeholders) agreed to take part in the Delphi round 0 (three patients, two relatives, three medical doctors, two qualified nurses and three developers), forming a structured panel of the five stakeholder groups. Two raters created a bottom-up coding, and 238 thematic codes were identified by the qualitative text analysis. The inter-rater reliability showed that 44.95% of the codes were semantically similar and coded for the same parts of the raw textual replies. Based on a consented coding list, a quantitative online-questionnaire was developed and send to different stakeholder groups. With respect to the hypotheses, Delphi round 0 had the following results: (1) doctors had a completely different focus from all the other stakeholder groups on possible channels of communications with the patient; (2) the dispersion of codes within individual stakeholder groups and within the total collective - visualized by box plots - was approximately 28% higher in the total collective than in the sub-collectives, but without a marked effect size. With respect to the hypotheses, Delphi round 1 had the following results: different stakeholder groups had highly diverging opinions with respect to central questions on IT-development. For example, when asked to rate the importance of access control against high availability of data (likert scale, 1 meaning restrictive data access, 6 easy access to all data), patients (mean 4.862, Stdev +/-â€‰1.866) and caregivers (mean 5.667, Stdev: +/-â€‰0.816) highly favored data availability, while relatives would restrict data access (mean 2.778, stdev +/-â€‰1.093). In comparison, the total group would not be representative of either of these individual stakeholder needs (mean 4.344, stdev +/-â€‰1.870). 360-Degree Delphi is feasible and allows different stakeholder groups within an expert panel to reach agreement individually. Thus, it generates a more detailed consensus which pays more tribute to individual stakeholders needs. This has the potential to improve the time to consensus as well as to produce a more representative and precise needs and requirements analysis. However, the method may create new challenges for the IT development process, which will have to deal with complementary or even contradictory statements from different stakeholder groups.","WaldmÃƒÂ¼ller, Spreckelsen, Rudat, Krumm, Rolke, Jonas","WaldmÃ¼ller, Spreckelsen, Rudat, Krumm, Rolke, Jonas",https://doi.org/10.1186/s12911-020-1071-x,https://doi.org/10.1186/s12911-020-1071-x,2021-08-03
16719.0,pubmed,pubmed,Can network pharmacology identify the anti-virus and anti- inflammatory activities of Shuanghuanglian oral liquid used in Chinese medicine for respiratory tract infection?,Can network pharmacology identify the anti-virus and anti- inflammatory activities of Shuanghuanglian oral liquid used in Chinese medicine for respiratory tract infection?,"Shuanghuanglian (SHL) oral liquid is a well-known traditional Chinese medicine preparation administered for respiratory tract infections in China. However, the underlying pharmacological mechanisms remain unclear. The present study aims to determine the potential pharmacological mechanisms of SHL oral liquid based on network pharmacology. Network pharmacology-based strategy including collection and analysis of putative compounds and target genes, network construction, Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway, and Gene Ontology (GO) enrichment, identification of key compounds and target genes, and molecule docking was performed in this study. A total of 82 bioactive compounds and 226 putative target genes of SHL oral liquid were collected. Of note, 28 hub target genes including 4 major hub target genes: estrogen receptor 1 (ESR1), nuclear receptor coactivator 2 (NCOA2), nuclear receptor coactivator 1 (NCOA1), androgen receptor (AR) and 5 key compounds (quercetin, luteolin, baicalein, kaempferol and wogonin) were identified based on network analysis. The hub target genes mainly enriched in pathways including PI3K-Akt signaling pathway, human cytomegalovirus infection, and human papillomavirus infection, which could be the underlying pharmacological mechanisms of SHL oral liquid for treating diseases. Moreover, the key compounds had great molecule docking binding affinity with the major hub target genes. Using network pharmacology analysis, SHL oral liquid was found to contain anti-virus, anti-inflammatory, and &quot;multi-compounds and multi-targets&quot; with therapeutic actions. These findings may provide a valuable direction for further clinical application and research.","Shuanghuanglian (SHL) oral liquid is a well-known traditional Chinese medicine preparation administered for respiratory tract infections in China. However, the underlying pharmacological mechanisms remain unclear. The present study aims to determine the potential pharmacological mechanisms of SHL oral liquid based on network pharmacology. Network pharmacology-based strategy including collection and analysis of putative compounds and target genes, network construction, Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway, and Gene Ontology (GO) enrichment, identification of key compounds and target genes, and molecule docking was performed in this study. A total of 82 bioactive compounds and 226 putative target genes of SHL oral liquid were collected. Of note, 28 hub target genes including 4 major hub target genes: estrogen receptor 1 (ESR1), nuclear receptor coactivator 2 (NCOA2), nuclear receptor coactivator 1 (NCOA1), androgen receptor (AR) and 5 key compounds (quercetin, luteolin, baicalein, kaempferol and wogonin) were identified based on network analysis. The hub target genes mainly enriched in pathways including PI3K-Akt signaling pathway, human cytomegalovirus infection, and human papillomavirus infection, which could be the underlying pharmacological mechanisms of SHL oral liquid for treating diseases. Moreover, the key compounds had great molecule docking binding affinity with the major hub target genes. Using network pharmacology analysis, SHL oral liquid was found to contain anti-virus, anti-inflammatory, and ""multi-compounds and multi-targets"" with therapeutic actions. These findings may provide a valuable direction for further clinical application and research.","Zhuang, Wen, Zhang, Zhang, Zhong, Chen, Luo","Zhuang, Wen, Zhang, Zhang, Zhong, Chen, Luo",https://doi.org/10.1016/j.eujim.2020.101139,https://doi.org/10.1016/j.eujim.2020.101139,2021-08-03
16724.0,pubmed,pubmed,Brain Network Disruption in Whiplash,Brain Network Disruption in Whiplash,"Whiplash-associated disorders frequently develop following motor vehicle collisions and often involve a range of cognitive and affective symptoms, though the neural correlates of the disorder are largely unknown. In this study, a sample of participants with chronic whiplash injuries were scanned by using resting-state fMRI to assess brain network changes associated with long-term outcome metrics. Resting-state fMRI was collected for 23 participants and used to calculate network modularity, a quantitative measure of the functional segregation of brain region communities. This was analyzed for associations with whiplash-associated disorder outcome metrics, including scales of neck disability, traumatic distress, depression, and pain. In addition to these clinical scales, cervical muscle fat infiltration was quantified by using Dixon fat-water imaging, which has shown promise as a biomarker for assessing disorder severity and predicting recovery in chronic whiplash. An association was found between brain network structure and muscle fat infiltration, wherein lower network modularity was associated with larger amounts of cervical muscle fat infiltration after controlling for age, sex, body mass index, and scan motion (<i>t</i>Ã¢â‚¬â€°=<i>Ã¢â‚¬â€°</i>-4.02, partial <i>R</i><sup>2</sup><i>Ã¢â‚¬â€°</i>=<i>Ã¢â‚¬â€°</i>0.49, <i>PÃ¢â‚¬â€°</i>&lt;Ã¢â‚¬â€°.001). This work contributes to the existing whiplash literature by examining a sample of participants with whiplash-associated disorder by using resting-state fMRI. Less modular brain networks were found to be associated with greater amounts of cervical muscle fat infiltration suggesting a connection between disorder severity and neurologic changes, and a potential role for neuroimaging in understanding the pathophysiology of chronic whiplash-associated disorders.","Whiplash-associated disorders frequently develop following motor vehicle collisions and often involve a range of cognitive and affective symptoms, though the neural correlates of the disorder are largely unknown. In this study, a sample of participants with chronic whiplash injuries were scanned by using resting-state fMRI to assess brain network changes associated with long-term outcome metrics. Resting-state fMRI was collected for 23 participants and used to calculate network modularity, a quantitative measure of the functional segregation of brain region communities. This was analyzed for associations with whiplash-associated disorder outcome metrics, including scales of neck disability, traumatic distress, depression, and pain. In addition to these clinical scales, cervical muscle fat infiltration was quantified by using Dixon fat-water imaging, which has shown promise as a biomarker for assessing disorder severity and predicting recovery in chronic whiplash. An association was found between brain network structure and muscle fat infiltration, wherein lower network modularity was associated with larger amounts of cervical muscle fat infiltration after controlling for age, sex, body mass index, and scan motion (<i>t</i>â€‰=<i>â€‰</i>-4.02, partial <i>R</i> <sup>2</sup> <i>â€‰</i>=<i>â€‰</i>0.49, <i>Pâ€‰</i>&lt;â€‰.001). This work contributes to the existing whiplash literature by examining a sample of participants with whiplash-associated disorder by using resting-state fMRI. Less modular brain networks were found to be associated with greater amounts of cervical muscle fat infiltration suggesting a connection between disorder severity and neurologic changes, and a potential role for neuroimaging in understanding the pathophysiology of chronic whiplash-associated disorders.","Higgins, Elliott, Parrish","Higgins, Elliott, Parrish",https://doi.org/10.3174/ajnr.A6569,https://doi.org/10.3174/ajnr.A6569,2021-08-03
16730.0,pubmed,pubmed,A systematic review of economic evaluations of treatment regimens in multiple myeloma,A systematic review of economic evaluations of treatment regimens in multiple myeloma,"The expansion of advanced expensive therapeutic innovations for Multiple Myeloma (MM) led to increased disclosure of economic evaluations. The present analysis systematically reviewed and appraised the reporting quality of economic evaluations in MM. A comprehensive literature search in Ovid, MEDLINE(R), PubMed, and Cochrane libraries was conducted for studies published in the past decade. Two independent authors performed study selection and data extraction in a standardized form. Study methodological quality assessment was performed using 10-item Drummond's tool. Of potentially eligible 1150 retrieved studies, 17Ã‚Â met eligibility criteria. Six evaluations (35%) were in newly diagnosed MM and 11 (65%) in relapse refractory (RR) MM. Nine studies (53%) embraced the payer's perspective, five (29%) adopted health care system, one (6%) societal and two did not report. Six (35%) employed partitioned survival model, 4(24%) discrete event simulation, 4(24%) Markov model and 2(12%) used decision tree model. The methodological quality has improved significantly; 16 (94%) studies comprehended a well-defined question by affirming the analysis perspective and examined both costs and outcomes while 13 (71%) provided a comprehensive description of competing alternatives. The addition of novel drugs to the treatment armamentarium of MM is considerably cost-effective. The evaluations became more frequent, methodological quality has improved in the last decade.","The expansion of advanced expensive therapeutic innovations for Multiple Myeloma (MM) led to increased disclosure of economic evaluations. The present analysis systematically reviewed and appraised the reporting quality of economic evaluations in MM. A comprehensive literature search in Ovid, MEDLINE(R), PubMed, and Cochrane libraries was conducted for studies published in the past decade. Two independent authors performed study selection and data extraction in a standardized form. Study methodological quality assessment was performed using 10-item Drummond's tool. Of potentially eligible 1150 retrieved studies, 17Â met eligibility criteria. Six evaluations (35%) were in newly diagnosed MM and 11 (65%) in relapse refractory (RR) MM. Nine studies (53%) embraced the payer's perspective, five (29%) adopted health care system, one (6%) societal and two did not report. Six (35%) employed partitioned survival model, 4(24%) discrete event simulation, 4(24%) Markov model and 2(12%) used decision tree model. The methodological quality has improved significantly; 16 (94%) studies comprehended a well-defined question by affirming the analysis perspective and examined both costs and outcomes while 13 (71%) provided a comprehensive description of competing alternatives. The addition of novel drugs to the treatment armamentarium of MM is considerably cost-effective. The evaluations became more frequent, methodological quality has improved in the last decade.","Asrar, Lad, Prinja, Bansal","Asrar, Lad, Prinja, Bansal",https://doi.org/10.1080/14737167.2020.1779064,https://doi.org/10.1080/14737167.2020.1779064,2021-08-03
16734.0,pubmed,pubmed,The semi-automation of title and abstract screening: a retrospective exploration of ways to leverage Abstrackr's relevance predictions in systematic and rapid reviews,The semi-automation of title and abstract screening: a retrospective exploration of ways to leverage Abstrackr's relevance predictions in systematic and rapid reviews,"We investigated the feasibility of using a machine learning tool's relevance predictions to expedite title and abstract screening. We subjected 11 systematic reviews and six rapid reviews to four retrospective screening simulations (automated and semi-automated approaches to single-reviewer and dual independent screening) in Abstrackr, a freely-available machine learning software. We calculated the proportion missed, workload savings, and time savings compared to single-reviewer and dual independent screening by human reviewers. We performed cited reference searches to determine if missed studies would be identified via reference list scanning. For systematic reviews, the semi-automated, dual independent screening approach provided the best balance of time savings (median (range) 20 (3-82) hours) and reliability (median (range) proportion missed records, 1 (0-14)%). The cited references search identified 59% (nÃ¢â‚¬â€°=Ã¢â‚¬â€°10/17) of the records missed. For the rapid reviews, the fully and semi-automated approaches saved time (median (range) 9 (2-18) hours and 3 (1-10) hours, respectively), but less so than for the systematic reviews. The median (range) proportion missed records for both approaches was 6 (0-22)%. Using Abstrackr to assist one of two reviewers in systematic reviews saves time with little risk of missing relevant records. Many missed records would be identified via other means.","We investigated the feasibility of using a machine learning tool's relevance predictions to expedite title and abstract screening. We subjected 11 systematic reviews and six rapid reviews to four retrospective screening simulations (automated and semi-automated approaches to single-reviewer and dual independent screening) in Abstrackr, a freely-available machine learning software. We calculated the proportion missed, workload savings, and time savings compared to single-reviewer and dual independent screening by human reviewers. We performed cited reference searches to determine if missed studies would be identified via reference list scanning. For systematic reviews, the semi-automated, dual independent screening approach provided the best balance of time savings (median (range) 20 (3-82) hours) and reliability (median (range) proportion missed records, 1 (0-14)%). The cited references search identified 59% (nâ€‰=â€‰10/17) of the records missed. For the rapid reviews, the fully and semi-automated approaches saved time (median (range) 9 (2-18) hours and 3 (1-10) hours, respectively), but less so than for the systematic reviews. The median (range) proportion missed records for both approaches was 6 (0-22)%. Using Abstrackr to assist one of two reviewers in systematic reviews saves time with little risk of missing relevant records. Many missed records would be identified via other means.","Gates, Gates, Sebastianski, Guitard, Elliott, Hartling","Gates, Gates, Sebastianski, Guitard, Elliott, Hartling",https://doi.org/10.1186/s12874-020-01031-w,https://doi.org/10.1186/s12874-020-01031-w,2021-08-03
16735.0,pubmed,pubmed,Neuro4Neuro: A neural network approach for neural tract segmentation using large-scale population-based diffusion imaging,Neuro4Neuro: A neural network approach for neural tract segmentation using large-scale population-based diffusion imaging,"Subtle changes in white matter (WM) microstructure have been associated with normal aging and neurodegeneration. To study these associations in more detail, it is highly important that the WM tracts can be accurately and reproducibly characterized from brain diffusion MRI. In addition, to enable analysis of WM tracts in large datasets and in clinical practice it is essential to have methodology that is fast and easy to apply. This work therefore presents a new approach for WM tract segmentation: Neuro4Neuro, that is capable of direct extraction of WM tracts from diffusion tensor images using convolutional neural network (CNN). This 3D end-to-end method is trained to segment 25 WM tracts in aging individuals from a large population-based study (NÃ‚Â Ã¢â‚¬â€¹=Ã‚Â Ã¢â‚¬â€¹9752, 1.5T MRI). The proposed method showed good segmentation performance and high reproducibility, i.e., a high spatial agreement (Cohen's kappa, ÃŽÂº=0.72-0.83) and a low scan-rescan error in tract-specific diffusion measures (e.g., fractional anisotropy: ÃŽÂµ=1%-5%). The reproducibility of the proposed method was higher than that of a tractography-based segmentation algorithm, while being orders of magnitude faster (0.5s to segment one tract). In addition, we showed that the method successfully generalizes to diffusion scans from an external dementia dataset (NÃ‚Â Ã¢â‚¬â€¹=Ã‚Â Ã¢â‚¬â€¹58, 3T MRI). In two proof-of-principle experiments, we associated WM microstructure obtained using the proposed method with age in a normal elderly population, and with disease subtypes in a dementia cohort. In concordance with the literature, results showed a widespread reduction of microstructural organization with aging and substantial group-wise microstructure differences between dementia subtypes. In conclusion, we presented a highly reproducible and fast method for WM tract segmentation that has the potential of being used in large-scale studies and clinical practice.","Subtle changes in white matter (WM) microstructure have been associated with normal aging and neurodegeneration. To study these associations in more detail, it is highly important that the WM tracts can be accurately and reproducibly characterized from brain diffusion MRI. In addition, to enable analysis of WM tracts in large datasets and in clinical practice it is essential to have methodology that is fast and easy to apply. This work therefore presents a new approach for WM tract segmentation: Neuro4Neuro, that is capable of direct extraction of WM tracts from diffusion tensor images using convolutional neural network (CNN). This 3D end-to-end method is trained to segment 25 WM tracts in aging individuals from a large population-based study (NÂ â€‹=Â â€‹9752, 1.5T MRI). The proposed method showed good segmentation performance and high reproducibility, i.e., a high spatial agreement (Cohen's kappa, Îº=0.72-0.83) and a low scan-rescan error in tract-specific diffusion measures (e.g., fractional anisotropy: Îµ=1%-5%). The reproducibility of the proposed method was higher than that of a tractography-based segmentation algorithm, while being orders of magnitude faster (0.5s to segment one tract). In addition, we showed that the method successfully generalizes to diffusion scans from an external dementia dataset (NÂ â€‹=Â â€‹58, 3T MRI). In two proof-of-principle experiments, we associated WM microstructure obtained using the proposed method with age in a normal elderly population, and with disease subtypes in a dementia cohort. In concordance with the literature, results showed a widespread reduction of microstructural organization with aging and substantial group-wise microstructure differences between dementia subtypes. In conclusion, we presented a highly reproducible and fast method for WM tract segmentation that has the potential of being used in large-scale studies and clinical practice.","Li, de Groot, Steketee, Meijboom, Smits, Vernooij, Ikram, Liu, Niessen, Bron","Li, de Groot, Steketee, Meijboom, Smits, Vernooij, Ikram, Liu, Niessen, Bron",https://doi.org/10.1016/j.neuroimage.2020.116993,https://doi.org/10.1016/j.neuroimage.2020.116993,2021-08-03
16738.0,pubmed,pubmed,External validation of radiomics-based predictive models in low-dose CT screening for early lung cancer diagnosis,External validation of radiomics-based predictive models in low-dose CT screening for early lung cancer diagnosis,"Low-dose CT screening allows early lung cancer detection, but is affected by frequent false positive results, inter/intra observer variation and uncertain diagnoses of lung nodules. Radiomics-based models have recently been introduced to overcome these issues, but limitations in demonstrating their generalizability on independent datasets are slowing their introduction to clinic. The aim of this study is to evaluate two radiomics-based models to classify malignant pulmonary nodules in low-dose CT screening, and to externally validate them on an independent cohort. The effect of a radiomics features harmonization technique is also investigated to evaluate its impact on the classification of lung nodules from a multicenter data. Pulmonary nodules from two independent cohorts were considered in this study; the first cohort (110 subjects, 113 nodules) was used to train prediction models, and the second cohort (72 nodules) to externally validate them. Literature-based radiomics features were extracted and, after feature selection, used as predictive variables in models for malignancy identification. An in-house prediction model based on artificial neural network (ANN) was implemented and evaluated, along with an alternative model from the literature, based on a support vector machine (SVM) classifier coupled with a least absolute shrinkage and selection operator (LASSO). External validation was performed on the second cohort to evaluate models' generalization ability. Additionally, the impact of the Combat harmonization method was investigated to compensate for multicenter datasets variabilities. A new training of the models based on harmonized features was performed on the first cohort, then tested separately on the harmonized and non-harmonized features of the second cohort. Preliminary results showed a good accuracy of the investigated models in distinguishing benign from malignant pulmonary nodules with both sets of radiomics features (i.e., non-harmonized and harmonized). The performance of the models, quantified in terms of Area Under the Curve (AUC), wasÃ‚Â &gt;Ã‚Â 0.89 in the training set andÃ‚Â &gt;Ã‚Â 0.82 in the external validation set for all the investigated scenarios, outperforming the clinical standard (AUC of 0.76). Slightly higher performance was observed for the SVM-LASSO model than the ANN in the external dataset, although they did not result significantly different. For both harmonized and non-harmonized features, no statistical difference was found between Receiver operating characteristic (ROC) curves related to training and test set for both models. Although no significant improvements were observed when applying the Combat harmonization method, both in-house and literature-based models were able to classify lung nodules with good generalization to an independent dataset, thus showing their potential as tools for clinical decision-making in lung cancer screening.","Low-dose CT screening allows early lung cancer detection, but is affected by frequent false positive results, inter/intra observer variation and uncertain diagnoses of lung nodules. Radiomics-based models have recently been introduced to overcome these issues, but limitations in demonstrating their generalizability on independent datasets are slowing their introduction to clinic. The aim of this study is to evaluate two radiomics-based models to classify malignant pulmonary nodules in low-dose CT screening, and to externally validate them on an independent cohort. The effect of a radiomics features harmonization technique is also investigated to evaluate its impact on the classification of lung nodules from a multicenter data. Pulmonary nodules from two independent cohorts were considered in this study; the first cohort (110 subjects, 113 nodules) was used to train prediction models, and the second cohort (72 nodules) to externally validate them. Literature-based radiomics features were extracted and, after feature selection, used as predictive variables in models for malignancy identification. An in-house prediction model based on artificial neural network (ANN) was implemented and evaluated, along with an alternative model from the literature, based on a support vector machine (SVM) classifier coupled with a least absolute shrinkage and selection operator (LASSO). External validation was performed on the second cohort to evaluate models' generalization ability. Additionally, the impact of the Combat harmonization method was investigated to compensate for multicenter datasets variabilities. A new training of the models based on harmonized features was performed on the first cohort, then tested separately on the harmonized and non-harmonized features of the second cohort. Preliminary results showed a good accuracy of the investigated models in distinguishing benign from malignant pulmonary nodules with both sets of radiomics features (i.e., non-harmonized and harmonized). The performance of the models, quantified in terms of Area Under the Curve (AUC), wasÂ &gt;Â 0.89 in the training set andÂ &gt;Â 0.82 in the external validation set for all the investigated scenarios, outperforming the clinical standard (AUC of 0.76). Slightly higher performance was observed for the SVM-LASSO model than the ANN in the external dataset, although they did not result significantly different. For both harmonized and non-harmonized features, no statistical difference was found between Receiver operating characteristic (ROC) curves related to training and test set for both models. Although no significant improvements were observed when applying the Combat harmonization method, both in-house and literature-based models were able to classify lung nodules with good generalization to an independent dataset, thus showing their potential as tools for clinical decision-making in lung cancer screening.","Garau, Paganelli, Summers, Choi, Alam, Lu, Fanciullo, Bellomi, Baroni, Rampinelli","Garau, Paganelli, Summers, Choi, Alam, Lu, Fanciullo, Bellomi, Baroni, Rampinelli",https://doi.org/10.1002/mp.14308,https://doi.org/10.1002/mp.14308,2021-08-03
16739.0,pubmed,pubmed,A Bibliometric Analysis of Cirrhosis Nursing Research on Web of Science,A Bibliometric Analysis of Cirrhosis Nursing Research on Web of Science,"The aims and objectives of this study were to (1) analyze the bibliometric characteristics of articles on Web of Science from 1986 to the present using literature mining and information visualization technologies developed by CiteSpace software, (2) reflect the current situation in this field as far as possible, and (3) provide evidence for improving research on nursing and clinical liver cirrhosis in Mainland China. No bibliometric analysis exists on Web of Science regarding cirrhosis nursing research. The status of current research, including hotspots and trends, has been assessed in this study through a bibliometric analysis. Literature related to cirrhosis and nursing was identified via Web of Science. Data were then analyzed using CiteSpace software. From 1986 to 2018, a total of 179 articles were published in 109 journals by 830 researchers in 36 countries/regions. The terms &quot;cirrhosis,&quot; &quot;management,&quot; and &quot;quality of life&quot; emerged most frequently and indicated the hotspots in liver cirrhosis nursing literature. Among all countries/regions, the United States contributed the most research overall; Asia also played an important role in the field of liver cirrhosis nursing research. The journal Gastroenterology Nursing published the greatest number of articles. Liver cirrhosis nursing research has attracted increasing amounts of attention around the world, although it remains less robust than other fields. Cirrhosis nursing research is still in its infancy in Mainland China, and there is an urgent need for additional support from government or research institutions to improve this research focus and promote international acceptance of the research outcomes.","The aims and objectives of this study were to (1) analyze the bibliometric characteristics of articles on Web of Science from 1986 to the present using literature mining and information visualization technologies developed by CiteSpace software, (2) reflect the current situation in this field as far as possible, and (3) provide evidence for improving research on nursing and clinical liver cirrhosis in Mainland China. No bibliometric analysis exists on Web of Science regarding cirrhosis nursing research. The status of current research, including hotspots and trends, has been assessed in this study through a bibliometric analysis. Literature related to cirrhosis and nursing was identified via Web of Science. Data were then analyzed using CiteSpace software. From 1986 to 2018, a total of 179 articles were published in 109 journals by 830 researchers in 36 countries/regions. The terms ""cirrhosis,"" ""management,"" and ""quality of life"" emerged most frequently and indicated the hotspots in liver cirrhosis nursing literature. Among all countries/regions, the United States contributed the most research overall; Asia also played an important role in the field of liver cirrhosis nursing research. The journal Gastroenterology Nursing published the greatest number of articles. Liver cirrhosis nursing research has attracted increasing amounts of attention around the world, although it remains less robust than other fields. Cirrhosis nursing research is still in its infancy in Mainland China, and there is an urgent need for additional support from government or research institutions to improve this research focus and promote international acceptance of the research outcomes.","Guo, Lu, Tian","Guo, Lu, Tian",https://doi.org/10.1097/SGA.0000000000000457,https://doi.org/10.1097/SGA.0000000000000457,2021-08-03
16742.0,pubmed,pubmed,Artificial intelligence (AI) in urology-Current use and future directions: An iTRUE study,Artificial intelligence (AI) in urology-Current use and future directions: An iTRUE study,"Artificial intelligence (AI) is used in various urological conditions such as urolithiasis, pediatric urology, urogynecology, benign prostate hyperplasia (BPH), renal transplant, and uro-oncology. The various models of AI and its application in urology subspecialties are reviewed and discussed. Search strategy was adapted to identify and review the literature pertaining to the application of AI in urology using the keywords &quot;urology,&quot; &quot;artificial intelligence,&quot; &quot;machine learning,&quot; &quot;deep learning,&quot; &quot;artificial neural networks,&quot; &quot;computer vision,&quot; and &quot;natural language processing&quot; were included and categorized. Review articles, editorial comments, and non-urologic studies were excluded. The article reviewed 47 articles that reported characteristics and implementation of AI in urological cancer. In all cases with benign conditions, artificial intelligence was used to predict outcomes of the surgical procedure. In urolithiasis, it was used to predict stone composition, whereas in pediatric urology and BPH, it was applied to predict the severity of condition. In cases with malignant conditions, it was applied to predict the treatment response, survival, prognosis, and recurrence on the basis of the genomic and biomarker studies. These results were also found to be statistically better than routine approaches. Application of radiomics in classification and nuclear grading of renal masses, cystoscopic diagnosis of bladder cancers, predicting Gleason score, and magnetic resonance imaging with computer-assisted diagnosis for prostate cancers are few applications of AI that have been studied extensively. In the near future, we will see a shift in the clinical paradigm as AI applications will find their place in the guidelines and revolutionize the decision-making process.","Artificial intelligence (AI) is used in various urological conditions such as urolithiasis, pediatric urology, urogynecology, benign prostate hyperplasia (BPH), renal transplant, and uro-oncology. The various models of AI and its application in urology subspecialties are reviewed and discussed. Search strategy was adapted to identify and review the literature pertaining to the application of AI in urology using the keywords ""urology,"" ""artificial intelligence,"" ""machine learning,"" ""deep learning,"" ""artificial neural networks,"" ""computer vision,"" and ""natural language processing"" were included and categorized. Review articles, editorial comments, and non-urologic studies were excluded. The article reviewed 47 articles that reported characteristics and implementation of AI in urological cancer. In all cases with benign conditions, artificial intelligence was used to predict outcomes of the surgical procedure. In urolithiasis, it was used to predict stone composition, whereas in pediatric urology and BPH, it was applied to predict the severity of condition. In cases with malignant conditions, it was applied to predict the treatment response, survival, prognosis, and recurrence on the basis of the genomic and biomarker studies. These results were also found to be statistically better than routine approaches. Application of radiomics in classification and nuclear grading of renal masses, cystoscopic diagnosis of bladder cancers, predicting Gleason score, and magnetic resonance imaging with computer-assisted diagnosis for prostate cancers are few applications of AI that have been studied extensively. In the near future, we will see a shift in the clinical paradigm as AI applications will find their place in the guidelines and revolutionize the decision-making process.","Shah, Naik, Somani, Hameed","Shah, Naik, Somani, Hameed",https://doi.org/10.5152/tud.2020.20117,https://doi.org/10.5152/tud.2020.20117,2021-08-03
16759.0,pubmed,pubmed,Interpretable multimodal deep learning for real-time pan-tissue pan-disease pathology search on social media,Interpretable multimodal deep learning for real-time pan-tissue pan-disease pathology search on social media,"Pathologists are responsible for rapidly providing a diagnosis on critical health issues. Challenging cases benefit from additional opinions of pathologist colleagues. In addition to on-site colleagues, there is an active worldwide community of pathologists on social media for complementary opinions. Such access to pathologists worldwide has the capacity to improve diagnostic accuracy and generate broader consensus on next steps in patient care. From Twitter we curate 13,626 images from 6,351 tweets from 25 pathologists from 13 countries. We supplement the Twitter data with 113,161 images from 1,074,484 PubMed articles. We develop machine learning and deep learning models to (i) accurately identify histopathology stains, (ii) discriminate between tissues, and (iii) differentiate disease states. Area Under Receiver Operating Characteristic (AUROC) is 0.805-0.996 for these tasks. We repurpose the disease classifier to search for similar disease states given an image and clinical covariates. We report precision@kÃ¢â‚¬â€°=Ã¢â‚¬â€°1Ã¢â‚¬â€°=Ã¢â‚¬â€°0.7618Ã¢â‚¬â€°Ã‚Â±Ã¢â‚¬â€°0.0018 (chance 0.397Ã¢â‚¬â€°Ã‚Â±Ã¢â‚¬â€°0.004, meanÃ¢â‚¬â€°Ã‚Â±stdevÃ¢â‚¬â€°). The classifiers find that texture and tissue are important clinico-visual features of disease. Deep features trained only on natural images (e.g., cats and dogs) substantially improved search performance, while pathology-specific deep features and cell nuclei features further improved search to a lesser extent. We implement a social media bot (@pathobot on Twitter) to use the trained classifiers to aid pathologists in obtaining real-time feedback on challenging cases. If a social media post containing pathology text and images mentions the bot, the bot generates quantitative predictions of disease state (normal/artifact/infection/injury/nontumor, preneoplastic/benign/low-grade-malignant-potential, or malignant) and lists similar cases across social media and PubMed. Our project has become a globally distributed expert system that facilitates pathological diagnosis and brings expertise to underserved regions or hospitals with less expertise in a particular disease. This is the first pan-tissue pan-disease (i.e., from infection to malignancy) method for prediction and search on social media, and the first pathology study prospectively tested in public on social media. We will share data through http://pathobotology.org . We expect our project to cultivate a more connected world of physicians and improve patient care worldwide.","Pathologists are responsible for rapidly providing a diagnosis on critical health issues. Challenging cases benefit from additional opinions of pathologist colleagues. In addition to on-site colleagues, there is an active worldwide community of pathologists on social media for complementary opinions. Such access to pathologists worldwide has the capacity to improve diagnostic accuracy and generate broader consensus on next steps in patient care. From Twitter we curate 13,626 images from 6,351 tweets from 25 pathologists from 13 countries. We supplement the Twitter data with 113,161 images from 1,074,484 PubMed articles. We develop machine learning and deep learning models to (i) accurately identify histopathology stains, (ii) discriminate between tissues, and (iii) differentiate disease states. Area Under Receiver Operating Characteristic (AUROC) is 0.805-0.996 for these tasks. We repurpose the disease classifier to search for similar disease states given an image and clinical covariates. We report precision@kâ€‰=â€‰1â€‰=â€‰0.7618â€‰Â±â€‰0.0018 (chance 0.397â€‰Â±â€‰0.004, meanâ€‰Â±stdevâ€‰). The classifiers find that texture and tissue are important clinico-visual features of disease. Deep features trained only on natural images (e.g., cats and dogs) substantially improved search performance, while pathology-specific deep features and cell nuclei features further improved search to a lesser extent. We implement a social media bot (@pathobot on Twitter) to use the trained classifiers to aid pathologists in obtaining real-time feedback on challenging cases. If a social media post containing pathology text and images mentions the bot, the bot generates quantitative predictions of disease state (normal/artifact/infection/injury/nontumor, preneoplastic/benign/low-grade-malignant-potential, or malignant) and lists similar cases across social media and PubMed. Our project has become a globally distributed expert system that facilitates pathological diagnosis and brings expertise to underserved regions or hospitals with less expertise in a particular disease. This is the first pan-tissue pan-disease (i.e., from infection to malignancy) method for prediction and search on social media, and the first pathology study prospectively tested in public on social media. We will share data through http://pathobotology.org . We expect our project to cultivate a more connected world of physicians and improve patient care worldwide.","Schaumberg, Juarez-Nicanor, Choudhury, PastriÃƒÂ¡n, Pritt, Prieto Pozuelo, Sotillo SÃƒÂ¡nchez, Ho, Zahra, Sener, Yip, Xu, Annavarapu, Morini, Jones, Rosado-Orozco, Mukhopadhyay, Miguel, Yang, Rosen, Ali, Folaranmi, Gardner, Rusu, Stayerman, Gross, Suleiman, Sirintrapun, Aly, Fuchs","Schaumberg, Juarez-Nicanor, Choudhury, PastriÃ¡n, Pritt, Prieto Pozuelo, Sotillo SÃ¡nchez, Ho, Zahra, Sener, Yip, Xu, Annavarapu, Morini, Jones, Rosado-Orozco, Mukhopadhyay, Miguel, Yang, Rosen, Ali, Folaranmi, Gardner, Rusu, Stayerman, Gross, Suleiman, Sirintrapun, Aly, Fuchs",https://doi.org/10.1038/s41379-020-0540-1,https://doi.org/10.1038/s41379-020-0540-1,2021-08-03
16768.0,pubmed,pubmed,Predicting Phenotypic Polymyxin Resistance in Klebsiella pneumoniae through Machine Learning Analysis of Genomic Data,Predicting Phenotypic Polymyxin Resistance in Klebsiella pneumoniae through Machine Learning Analysis of Genomic Data,"Polymyxins are used as treatments of last resort for Gram-negative bacterial infections. Their increased use has led to concerns about emerging polymyxin resistance (PR). Phenotypic polymyxin susceptibility testing is resource intensive and difficult to perform accurately. The complex polygenic nature of PR and our incomplete understanding of its genetic basis make it difficult to predict PR using detection of resistance determinants. We therefore applied machine learning (ML) to whole-genome sequencing data from &gt;600 <i>Klebsiella pneumoniae</i> clonal group 258 (CG258) genomes to predict phenotypic PR. Using a reference-based representation of genomic data with ML outperformed a rule-based approach that detected variants in known PR genes (area under receiver-operator curve [AUROC], 0.894 versus 0.791, <i>PÃ¢â‚¬â€°=Ã¢â‚¬â€°</i>0.006). We noted modest increases in performance by using a bacterial genome-wide association study to filter relevant genomic features and by integrating clinical data in the form of prior polymyxin exposure. Conversely, reference-free representation of genomic data as k-mers was associated with decreased performance (AUROC, 0.692 versus 0.894, <i>PÃ¢â‚¬â€°=Ã¢â‚¬â€°</i>0.015). When ML models were interpreted to extract genomic features, six of seven known PR genes were correctly identified by models without prior programming and several genes involved in stress responses and maintenance of the cell membrane were identified as potential novel determinants of PR. These findings are a proof of concept that whole-genome sequencing data can accurately predict PR in <i>K. pneumoniae</i> CG258 and may be applicable to other forms of complex antimicrobial resistance.<b>IMPORTANCE</b> Polymyxins are last-resort antibiotics used to treat highly resistant Gram-negative bacteria. There are increasing reports of polymyxin resistance emerging, raising concerns of a postantibiotic era. Polymyxin resistance is therefore a significant public health threat, but current phenotypic methods for detection are difficult and time-consuming to perform. There have been increasing efforts to use whole-genome sequencing for detection of antibiotic resistance, but this has been difficult to apply to polymyxin resistance because of its complex polygenic nature. The significance of our research is that we successfully applied machine learning methods to predict polymyxin resistance in <i>Klebsiella pneumoniae</i> clonal group 258, a common health care-associated and multidrug-resistant pathogen. Our findings highlight that machine learning can be successfully applied even in complex forms of antibiotic resistance and represent a significant contribution to the literature that could be used to predict resistance in other bacteria and to other antibiotics.","Polymyxins are used as treatments of last resort for Gram-negative bacterial infections. Their increased use has led to concerns about emerging polymyxin resistance (PR). Phenotypic polymyxin susceptibility testing is resource intensive and difficult to perform accurately. The complex polygenic nature of PR and our incomplete understanding of its genetic basis make it difficult to predict PR using detection of resistance determinants. We therefore applied machine learning (ML) to whole-genome sequencing data from &gt;600 <i>Klebsiella pneumoniae</i> clonal group 258 (CG258) genomes to predict phenotypic PR. Using a reference-based representation of genomic data with ML outperformed a rule-based approach that detected variants in known PR genes (area under receiver-operator curve [AUROC], 0.894 versus 0.791, <i>Pâ€‰=â€‰</i>0.006). We noted modest increases in performance by using a bacterial genome-wide association study to filter relevant genomic features and by integrating clinical data in the form of prior polymyxin exposure. Conversely, reference-free representation of genomic data as k-mers was associated with decreased performance (AUROC, 0.692 versus 0.894, <i>Pâ€‰=â€‰</i>0.015). When ML models were interpreted to extract genomic features, six of seven known PR genes were correctly identified by models without prior programming and several genes involved in stress responses and maintenance of the cell membrane were identified as potential novel determinants of PR. These findings are a proof of concept that whole-genome sequencing data can accurately predict PR in <i>K. pneumoniae</i> CG258 and may be applicable to other forms of complex antimicrobial resistance.<b>IMPORTANCE</b> Polymyxins are last-resort antibiotics used to treat highly resistant Gram-negative bacteria. There are increasing reports of polymyxin resistance emerging, raising concerns of a postantibiotic era. Polymyxin resistance is therefore a significant public health threat, but current phenotypic methods for detection are difficult and time-consuming to perform. There have been increasing efforts to use whole-genome sequencing for detection of antibiotic resistance, but this has been difficult to apply to polymyxin resistance because of its complex polygenic nature. The significance of our research is that we successfully applied machine learning methods to predict polymyxin resistance in <i>Klebsiella pneumoniae</i> clonal group 258, a common health care-associated and multidrug-resistant pathogen. Our findings highlight that machine learning can be successfully applied even in complex forms of antibiotic resistance and represent a significant contribution to the literature that could be used to predict resistance in other bacteria and to other antibiotics.","Macesic, Bear Don't Walk, Pe'er, Tatonetti, Peleg, Uhlemann","Macesic, Bear Don't Walk, Pe'er, Tatonetti, Peleg, Uhlemann",https://doi.org/10.1128/mSystems.00656-19,https://doi.org/10.1128/mSystems.00656-19,2021-08-03
16769.0,pubmed,pubmed,Should free-text data in electronic medical records be shared for research? A citizens' jury study in the UK,Should free-text data in electronic medical records be shared for research? A citizens' jury study in the UK,"Use of routinely collected patient data for research and service planning is an explicit policy of the UK National Health Service and UK government. Much clinical information is recorded in free-text letters, reports and notes. These text data are generally lost to research, due to the increased privacy risk compared with structured data. We conducted a citizens' jury which asked members of the public whether their medical free-text data should be shared for research for public benefit, to inform an ethical policy. Eighteen citizens took part over 3Ã¢â‚¬â€°days. Jurors heard a range of expert presentations as well as arguments for and against sharing free text, and then questioned presenters and deliberated together. They answered a questionnaire on whether and how free text should be shared for research, gave reasons for and against sharing and suggestions for alleviating their concerns. Jurors were in favour of sharing medical data and agreed this would benefit health research, but were more cautious about sharing free-text than structured data. They preferred processing of free text where a computer extracted information at scale. Their concerns were lack of transparency in uses of data, and privacy risks. They suggested keeping patients informed about uses of their data, and giving clear pathways to opt out of data sharing. Informed citizens suggested a transparent culture of research for the public benefit, and continuous improvement of technology to protect patient privacy, to mitigate their concerns regarding privacy risks of using patient text data.","Use of routinely collected patient data for research and service planning is an explicit policy of the UK National Health Service and UK government. Much clinical information is recorded in free-text letters, reports and notes. These text data are generally lost to research, due to the increased privacy risk compared with structured data. We conducted a citizens' jury which asked members of the public whether their medical free-text data should be shared for research for public benefit, to inform an ethical policy. Eighteen citizens took part over 3â€‰days. Jurors heard a range of expert presentations as well as arguments for and against sharing free text, and then questioned presenters and deliberated together. They answered a questionnaire on whether and how free text should be shared for research, gave reasons for and against sharing and suggestions for alleviating their concerns. Jurors were in favour of sharing medical data and agreed this would benefit health research, but were more cautious about sharing free-text than structured data. They preferred processing of free text where a computer extracted information at scale. Their concerns were lack of transparency in uses of data, and privacy risks. They suggested keeping patients informed about uses of their data, and giving clear pathways to opt out of data sharing. Informed citizens suggested a transparent culture of research for the public benefit, and continuous improvement of technology to protect patient privacy, to mitigate their concerns regarding privacy risks of using patient text data.","Ford, Oswald, Hassan, Bozentko, Nenadic, Cassell","Ford, Oswald, Hassan, Bozentko, Nenadic, Cassell",https://doi.org/10.1136/medethics-2019-105472,https://doi.org/10.1136/medethics-2019-105472,2021-08-03
16772.0,pubmed,pubmed,Taking stock of what is known about faculty development in competency-based medical education: A scoping review paper,Taking stock of what is known about faculty development in competency-based medical education: A scoping review paper,"<b>Purpose:</b> The primary objective was to inventory what is currently known about faculty development (FD) for competency-based medical educations (CBME) and identify gaps in the literature.<b>Methods:</b> A scoping review methodology was employed. Inclusion criteria for article selection were established with two reviewers completing a full-text analysis. Quality checks were included, along with iterative consultation on data collection and consensus decision making via a grounded theory approach.<b>Results:</b> The review identified 19 articles published between 2009 and 2018. Most articles (<i>N</i>Ã¢â‚¬â€°=Ã¢â‚¬â€°15) offered suggestions as to what should happen with FD in CBME, but few (<i>N</i>Ã¢â‚¬â€°=Ã¢â‚¬â€°4) adopted an experimental design. Six main themes were identified with three main features of FD noted across themes: (1) The importance of direct and timely feedback to faculty members on their teaching and assessment skills. (2) The role of establishing shared mental models for CBME curricula. (3) That FD is thought of longitudinally, not as a one-time bolus.<b>Conclusion:</b> This work illustrates that there is limited, high quality research in FD for CBME. Future FD activities should consider employing a longitudinal and multi-modal program format that includes feedback for the faculty participants on their teaching and assessments skills, including the development of faculty coaching skills.","<b>Purpose:</b> The primary objective was to inventory what is currently known about faculty development (FD) for competency-based medical educations (CBME) and identify gaps in the literature.<b>Methods:</b> A scoping review methodology was employed. Inclusion criteria for article selection were established with two reviewers completing a full-text analysis. Quality checks were included, along with iterative consultation on data collection and consensus decision making via a grounded theory approach.<b>Results:</b> The review identified 19 articles published between 2009 and 2018. Most articles (<i>N</i>â€‰=â€‰15) offered suggestions as to what should happen with FD in CBME, but few (<i>N</i>â€‰=â€‰4) adopted an experimental design. Six main themes were identified with three main features of FD noted across themes: (1) The importance of direct and timely feedback to faculty members on their teaching and assessment skills. (2) The role of establishing shared mental models for CBME curricula. (3) That FD is thought of longitudinally, not as a one-time bolus.<b>Conclusion:</b> This work illustrates that there is limited, high quality research in FD for CBME. Future FD activities should consider employing a longitudinal and multi-modal program format that includes feedback for the faculty participants on their teaching and assessments skills, including the development of faculty coaching skills.","Sirianni, Glover Takahashi, Myers","Sirianni, Glover Takahashi, Myers",https://doi.org/10.1080/0142159X.2020.1763285,https://doi.org/10.1080/0142159X.2020.1763285,2021-08-03
16778.0,pubmed,pubmed,Interventions improving health professionals' practice for addressing patients' weight management behaviours: systematic review of reviews,Interventions improving health professionals' practice for addressing patients' weight management behaviours: systematic review of reviews,"Health professionals require education and training to implement obesity management guidelines and ultimately impact on the health outcomes experienced by their patients. Therefore, a systematic review of systematic reviews that evaluated interventions designed to change the practice of health professionals when addressing diet and physical activity with their patients was conducted. MEDLINE Complete; Cochrane database of systematic reviews; PsycINFO; CINAHL Complete; Global Health; Embase; INFORMIT: Health Subset; Health System Evidence and RX for change were searched in March 2019, with no date or language limits. Identified references underwent screening, full-text analyses and data extraction in duplicate. The search identified 15Ã‚Â 230 references. Five systematic reviews that provided a narrative syntheses of a combined 38 studies were included. Health professional participants generally reported being satisfied with the training interventions. Heterogeneity between and within included reviews, non-controlled designs of individual studies and low quality of evidence at an individual study level and review level made it difficult to draw firm conclusions regarding what interventions are most effective in changing health professionals' knowledge, skills, self-efficacy, attitudes and practice. However, similar gaps in the literature were identified across included reviews. Key areas that could be addressed in future interventions including organization and system-level barriers to providing advice, health professionals' attitudes and motivation and weight stigma have been highlighted. Health professionals and patients could be more involved in the planning and development of interventions that work towards improving diet and physical activity advice and support provided in healthcare.","Health professionals require education and training to implement obesity management guidelines and ultimately impact on the health outcomes experienced by their patients. Therefore, a systematic review of systematic reviews that evaluated interventions designed to change the practice of health professionals when addressing diet and physical activity with their patients was conducted. MEDLINE Complete; Cochrane database of systematic reviews; PsycINFO; CINAHL Complete; Global Health; Embase; INFORMIT: Health Subset; Health System Evidence and RX for change were searched in March 2019, with no date or language limits. Identified references underwent screening, full-text analyses and data extraction in duplicate. The search identified 15Â 230 references. Five systematic reviews that provided a narrative syntheses of a combined 38 studies were included. Health professional participants generally reported being satisfied with the training interventions. Heterogeneity between and within included reviews, non-controlled designs of individual studies and low quality of evidence at an individual study level and review level made it difficult to draw firm conclusions regarding what interventions are most effective in changing health professionals' knowledge, skills, self-efficacy, attitudes and practice. However, similar gaps in the literature were identified across included reviews. Key areas that could be addressed in future interventions including organization and system-level barriers to providing advice, health professionals' attitudes and motivation and weight stigma have been highlighted. Health professionals and patients could be more involved in the planning and development of interventions that work towards improving diet and physical activity advice and support provided in healthcare.","Yazdizadeh, Walker, Skouteris, Olander, Hill","Yazdizadeh, Walker, Skouteris, Olander, Hill",https://doi.org/10.1093/heapro/daaa039,https://doi.org/10.1093/heapro/daaa039,2021-08-03
16779.0,pubmed,pubmed,Challenges of Developing a Natural Language Processing Method With Electronic Health Records to Identify Persons With Chronic Mobility Disability,Challenges of Developing a Natural Language Processing Method With Electronic Health Records to Identify Persons With Chronic Mobility Disability,"To assess the utility of applying natural language processing (NLP) to electronic health records (EHRs) to identify individuals with chronic mobility disability. We used EHRs from the Research Patient Data Repository, which contains EHRs from a large Massachusetts health care delivery system. This analysis was part of a larger study assessing the effects of disability on diagnosis of colorectal cancer. We applied NLP text extraction software to longitudinal EHRs of colorectal cancer patients to identify persons who use a wheelchair (our indicator of mobility disability for this analysis). We manually reviewed the clinical notes identified by NLP using directed content analysis to identify true cases using wheelchairs, duration or chronicity of use, and documentation quality. EHRs from large health care delivery system PARTICIPANTS: Patients (N=14,877) 21-75 years old who were newly diagnosed with colorectal cancer between 2005 andÃ‚Â 2017. Not applicable. Confirmation of patients' chronic wheelchair use in NLP-flagged notes; quality of disability documentation. We identified 14,877 patients with colorectal cancer with 303,182 associated clinical notes. NLP screening identified 1482 (0.5%) notes that contained 1+ wheelchair-associated keyword. These notes were associated with 420 patients (2.8% of colorectal cancer population). Of the 1482 notes, 286 (19.3%, representing 105 patients, 0.7% of the total) contained documentation of reason for wheelchair use and duration. Directed content analysis identified 3 themes concerning disability documentation: (1) wheelchair keywords used in specific EHR contexts; (2) reason for wheelchair not clearly stated; and (3) duration of wheelchair use not consistently documented. NLP offers an option to screen for patients with chronic mobility disability in much less time than required by manual chart review. Nonetheless, manual chart review must confirm that flagged patients have chronic mobility disability (are not false positives). Notes, however, often have inadequate disability documentation.","To assess the utility of applying natural language processing (NLP) to electronic health records (EHRs) to identify individuals with chronic mobility disability. We used EHRs from the Research Patient Data Repository, which contains EHRs from a large Massachusetts health care delivery system. This analysis was part of a larger study assessing the effects of disability on diagnosis of colorectal cancer. We applied NLP text extraction software to longitudinal EHRs of colorectal cancer patients to identify persons who use a wheelchair (our indicator of mobility disability for this analysis). We manually reviewed the clinical notes identified by NLP using directed content analysis to identify true cases using wheelchairs, duration or chronicity of use, and documentation quality. EHRs from large health care delivery system PARTICIPANTS: Patients (N=14,877) 21-75 years old who were newly diagnosed with colorectal cancer between 2005 andÂ 2017. Not applicable. Confirmation of patients' chronic wheelchair use in NLP-flagged notes; quality of disability documentation. We identified 14,877 patients with colorectal cancer with 303,182 associated clinical notes. NLP screening identified 1482 (0.5%) notes that contained 1+ wheelchair-associated keyword. These notes were associated with 420 patients (2.8% of colorectal cancer population). Of the 1482 notes, 286 (19.3%, representing 105 patients, 0.7% of the total) contained documentation of reason for wheelchair use and duration. Directed content analysis identified 3 themes concerning disability documentation: (1) wheelchair keywords used in specific EHR contexts; (2) reason for wheelchair not clearly stated; and (3) duration of wheelchair use not consistently documented. NLP offers an option to screen for patients with chronic mobility disability in much less time than required by manual chart review. Nonetheless, manual chart review must confirm that flagged patients have chronic mobility disability (are not false positives). Notes, however, often have inadequate disability documentation.","Agaronnik, Lindvall, El-Jawahri, He, Iezzoni","Agaronnik, Lindvall, El-Jawahri, He, Iezzoni",https://doi.org/10.1016/j.apmr.2020.04.024,https://doi.org/10.1016/j.apmr.2020.04.024,2021-08-03
16781.0,pubmed,pubmed,Using artificial intelligence algorithms to identify existing knowledge within the back pain literature,Using artificial intelligence algorithms to identify existing knowledge within the back pain literature,"Artificial intelligence algorithms can now identify hidden data patterns within the scientific literature. In 2019, these algorithms identified a thermoelectric material within the pre-2009 chemistry literature; years before its discovery in 2012. This approach inspired us to apply this algorithm to the back pain literature as the cause of back pain remains unknown in 90% of cases. We created a subset of all PubMed abstracts containing &quot;back&quot; and &quot;pain&quot; and then trained the Word2vec algorithm to predict word proximity. We then identified word pairings having high vector proximities between three spinal domains: anatomy, pathology and treatment. We plotted both between-domain and within-domain proximities then used the highest proximity pairs as ground truths in analogy testing to identify known associations (e.g., Canal is to Stenosis as Multifidus is to ?) RESULTS: We found Ã‚Â 50,038 abstracts resulting in 27,984 unique wordsÃ‚Â and 108,252 instances of &quot;backÃ‚Â pain&quot;. Ground truth pairings ranged in proximity from 0.86 to 0.70. Plotting revealed unique proximity representations between the three spine domains. From analogy testing, we identified 13 known word associations (pars_interarticularis is to stress_reaction as nerve_root is to compression). Artificial intelligence algorithms can successfully extract complex concepts from back pain literature. While use of AI algorithms to discover potentially unknown word associations requires future validation, our results provide investigators with a novel tool to generate new hypotheses regarding the origins of LBP and other spine related topics. To encourage use of these tools, we have created a free web-based app for investigator-driven queries.","Artificial intelligence algorithms can now identify hidden data patterns within the scientific literature. In 2019, these algorithms identified a thermoelectric material within the pre-2009 chemistry literature; years before its discovery in 2012. This approach inspired us to apply this algorithm to the back pain literature as the cause of back pain remains unknown in 90% of cases. We created a subset of all PubMed abstracts containing ""back"" and ""pain"" and then trained the Word2vec algorithm to predict word proximity. We then identified word pairings having high vector proximities between three spinal domains: anatomy, pathology and treatment. We plotted both between-domain and within-domain proximities then used the highest proximity pairs as ground truths in analogy testing to identify known associations (e.g., Canal is to Stenosis as Multifidus is to ?) RESULTS: We found Â 50,038 abstracts resulting in 27,984 unique wordsÂ and 108,252 instances of ""backÂ pain"". Ground truth pairings ranged in proximity from 0.86 to 0.70. Plotting revealed unique proximity representations between the three spine domains. From analogy testing, we identified 13 known word associations (pars_interarticularis is to stress_reaction as nerve_root is to compression). Artificial intelligence algorithms can successfully extract complex concepts from back pain literature. While use of AI algorithms to discover potentially unknown word associations requires future validation, our results provide investigators with a novel tool to generate new hypotheses regarding the origins of LBP and other spine related topics. To encourage use of these tools, we have created a free web-based app for investigator-driven queries.","Kawchuk, Guan, Keen, Hauer, Kondrak","Kawchuk, Guan, Keen, Hauer, Kondrak",https://doi.org/10.1007/s00586-020-06447-y,https://doi.org/10.1007/s00586-020-06447-y,2021-08-03
16782.0,pubmed,pubmed,The cost-effectiveness of using results-based financing to reduce maternal and perinatal mortality in Malawi,The cost-effectiveness of using results-based financing to reduce maternal and perinatal mortality in Malawi,"Results-based financing (RBF) is being promoted to increase coverage and quality of maternal and perinatal healthcare in sub-Saharan Africa (SSA) countries. Evidence on the cost-effectiveness of RBF is limited. We assessed the cost-effectiveness within the context of an RBF intervention, including performance-based financing and conditional cash transfers, in rural Malawi. We used a decision tree model to estimate expected costs and effects of RBF compared with status quo care during single pregnancy episodes. RBF effects on maternal case fatality rates were modelled based on data from a maternal and perinatal programme evaluation in Zambia and Uganda. We obtained complementary epidemiological information from the published literature. Service utilisation rates for normal and complicated deliveries and associated costs of care were based on the RBF intervention in Malawi. Costs were estimated from a societal perspective. We estimated incremental cost-effectiveness ratios per disability adjusted life year (DALY) averted, death averted and life-year gained (LYG) and conducted sensitivity analyses to how robust results were to variations in key model parameters. Relative to status quo, RBF implied incremental costs of US$1122, US$26Ã¢â‚¬â€°220 and US$987 per additional DALY averted, death averted and LYG, respectively. The share of non-RBF facilities that provide quality care, life expectancy of mothers at time of delivery and the share of births in non-RBF facilities strongly influenced cost-effectiveness values. At a willingness to pay of US$1485 (3 times Malawi gross domestic product per capita) per DALY averted, RBF has a 77% probability of being cost-effective. At high thresholds of wiliness-to-pay, RBF is a cost-effective intervention to improve quality of maternal and perinatal healthcare and outcomes, compared with the non-RBF based approach. More RBF cost-effectiveness analyses are needed in the SSA region to complement the few published studies and narrow the uncertainties surrounding cost-effectiveness estimates.","Results-based financing (RBF) is being promoted to increase coverage and quality of maternal and perinatal healthcare in sub-Saharan Africa (SSA) countries. Evidence on the cost-effectiveness of RBF is limited. We assessed the cost-effectiveness within the context of an RBF intervention, including performance-based financing and conditional cash transfers, in rural Malawi. We used a decision tree model to estimate expected costs and effects of RBF compared with status quo care during single pregnancy episodes. RBF effects on maternal case fatality rates were modelled based on data from a maternal and perinatal programme evaluation in Zambia and Uganda. We obtained complementary epidemiological information from the published literature. Service utilisation rates for normal and complicated deliveries and associated costs of care were based on the RBF intervention in Malawi. Costs were estimated from a societal perspective. We estimated incremental cost-effectiveness ratios per disability adjusted life year (DALY) averted, death averted and life-year gained (LYG) and conducted sensitivity analyses to how robust results were to variations in key model parameters. Relative to status quo, RBF implied incremental costs of US$1122, US$26â€‰220 and US$987 per additional DALY averted, death averted and LYG, respectively. The share of non-RBF facilities that provide quality care, life expectancy of mothers at time of delivery and the share of births in non-RBF facilities strongly influenced cost-effectiveness values. At a willingness to pay of US$1485 (3 times Malawi gross domestic product per capita) per DALY averted, RBF has a 77% probability of being cost-effective. At high thresholds of wiliness-to-pay, RBF is a cost-effective intervention to improve quality of maternal and perinatal healthcare and outcomes, compared with the non-RBF based approach. More RBF cost-effectiveness analyses are needed in the SSA region to complement the few published studies and narrow the uncertainties surrounding cost-effectiveness estimates.","Chinkhumba, De Allegri, Brenner, Muula, Robberstad","Chinkhumba, De Allegri, Brenner, Muula, Robberstad",https://doi.org/10.1136/bmjgh-2019-002260,https://doi.org/10.1136/bmjgh-2019-002260,2021-08-03
16788.0,pubmed,pubmed,"An interim internal Threshold of Toxicologic Concern (iTTC) for chemicals in consumer products, with support from an automated assessment of ToxCastÃ¢â€žÂ¢ dose response data","An interim internal Threshold of Toxicologic Concern (iTTC) for chemicals in consumer products, with support from an automated assessment of ToxCastâ„¢ dose response data","Additional non-animal methods are urgently needed to meet regulatory and animal welfare goals. TTC is a broadly used risk assessment tool. TTC based on external dose has limited utility for multi-route exposure and some types of structure activity relationship assessments. An internal TTC (iTTC), where thresholds are based on blood concentration, would extend the applicability of TTC. While work is on-going to develop robust iTTC thresholds, we propose an interim conservative iTTC. Specifically, an interim iTTC of 1Ã‚Â ÃŽÂ¼M, supported by the published experience of the pharmaceutical industry, a literature review of non-drug chemical/receptor interactions, and analysis of ToxCastÃ¢â€žÂ¢ data. ToxCastÃ¢â€žÂ¢ data were used to explore activity versus the 1Ã‚Â ÃŽÂ¼M interim iTTC and recommendations for the analysis and interpretation of HTS data. Test concentration-based points of departure were classified to identify quality of fit to the Hill Model. We identified, for exclusion from the approach, estrogen receptor and androgen receptor targets as potent chemical/receptor interactions potentially associated with low dose exposure to non-pharmaceutical active ingredients in addition to the original TTC exclusions. With these exclusions, we conclude that a 1Ã‚Â ÃŽÂ¼M plasma concentration is unlikely to be associated with significant biological effects from chemicals not intentionally designed for biological activity.","Additional non-animal methods are urgently needed to meet regulatory and animal welfare goals. TTC is a broadly used risk assessment tool. TTC based on external dose has limited utility for multi-route exposure and some types of structure activity relationship assessments. An internal TTC (iTTC), where thresholds are based on blood concentration, would extend the applicability of TTC. While work is on-going to develop robust iTTC thresholds, we propose an interim conservative iTTC. Specifically, an interim iTTC of 1Â Î¼M, supported by the published experience of the pharmaceutical industry, a literature review of non-drug chemical/receptor interactions, and analysis of ToxCastâ„¢ data. ToxCastâ„¢ data were used to explore activity versus the 1Â Î¼M interim iTTC and recommendations for the analysis and interpretation of HTS data. Test concentration-based points of departure were classified to identify quality of fit to the Hill Model. We identified, for exclusion from the approach, estrogen receptor and androgen receptor targets as potent chemical/receptor interactions potentially associated with low dose exposure to non-pharmaceutical active ingredients in addition to the original TTC exclusions. With these exclusions, we conclude that a 1Â Î¼M plasma concentration is unlikely to be associated with significant biological effects from chemicals not intentionally designed for biological activity.","Blackburn, Carr, Rose, Selman","Blackburn, Carr, Rose, Selman",https://doi.org/10.1016/j.yrtph.2020.104656,https://doi.org/10.1016/j.yrtph.2020.104656,2021-08-03
16792.0,pubmed,pubmed,A genome-wide association study of interhemispheric theta EEG coherence: implications for neural connectivity and alcohol use behavior,A genome-wide association study of interhemispheric theta EEG coherence: implications for neural connectivity and alcohol use behavior,"Aberrant connectivity of large-scale brain networks has been observed among individuals with alcohol use disorders (AUDs) as well as in those at risk, suggesting deficits in neural communication between brain regions in the liability to develop AUD. Electroencephalographical (EEG) coherence, which measures the degree of synchrony between brain regions, may be a useful measure of connectivity patterns in neural networks for studying the genetics of AUD. In 8810 individuals (6644 of European and 2166 of African ancestry) from the Collaborative Study on the Genetics of Alcoholism (COGA), we performed a Multi-Trait Analyses of genome-wide association studies (MTAG) on parietal resting-state theta (3-7Ã¢â‚¬â€°Hz) EEG coherence, which previously have been associated with AUD. We also examined developmental effects of GWAS findings on trajectories of neural connectivity in a longitudinal subsample of 2316 adolescent/young adult offspring from COGA families (ages 12-30) and examined the functional and clinical significance of GWAS variants. Six correlated single nucleotide polymorphisms located in a brain-expressed lincRNA (ENSG00000266213) on chromosome 18q23 were associated with posterior interhemispheric low theta EEG coherence (3-5Ã¢â‚¬â€°Hz). These same variants were also associated with alcohol use behavior and posterior corpus callosum volume, both in a subset of COGA and in the UK Biobank. Analyses in the subsample of COGA offspring indicated that the association of rs12954372 with low theta EEG coherence occurred only in females, most prominently between ages 25 and 30 (pÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°2Ã¢â‚¬â€°Ãƒâ€”Ã¢â‚¬â€°10<sup>-9</sup>). Converging data provide support for the role of genetic variants on chromosome 18q23 in regulating neural connectivity and alcohol use behavior, potentially via dysregulated myelination. While findings were less robust, genome-wide associations were also observed with rs151174000 and parieto-frontal low theta coherence, rs14429078 and parieto-occipital interhemispheric high theta coherence, and rs116445911 with centro-parietal low theta coherence. These novel genetic findings highlight the utility of the endophenotype approach in enhancing our understanding of mechanisms underlying addiction susceptibility.","Aberrant connectivity of large-scale brain networks has been observed among individuals with alcohol use disorders (AUDs) as well as in those at risk, suggesting deficits in neural communication between brain regions in the liability to develop AUD. Electroencephalographical (EEG) coherence, which measures the degree of synchrony between brain regions, may be a useful measure of connectivity patterns in neural networks for studying the genetics of AUD. In 8810 individuals (6644 of European and 2166 of African ancestry) from the Collaborative Study on the Genetics of Alcoholism (COGA), we performed a Multi-Trait Analyses of genome-wide association studies (MTAG) on parietal resting-state theta (3-7â€‰Hz) EEG coherence, which previously have been associated with AUD. We also examined developmental effects of GWAS findings on trajectories of neural connectivity in a longitudinal subsample of 2316 adolescent/young adult offspring from COGA families (ages 12-30) and examined the functional and clinical significance of GWAS variants. Six correlated single nucleotide polymorphisms located in a brain-expressed lincRNA (ENSG00000266213) on chromosome 18q23 were associated with posterior interhemispheric low theta EEG coherence (3-5â€‰Hz). These same variants were also associated with alcohol use behavior and posterior corpus callosum volume, both in a subset of COGA and in the UK Biobank. Analyses in the subsample of COGA offspring indicated that the association of rs12954372 with low theta EEG coherence occurred only in females, most prominently between ages 25 and 30 (pâ€‰&lt;â€‰2â€‰Ã—â€‰10<sup>-9</sup>). Converging data provide support for the role of genetic variants on chromosome 18q23 in regulating neural connectivity and alcohol use behavior, potentially via dysregulated myelination. While findings were less robust, genome-wide associations were also observed with rs151174000 and parieto-frontal low theta coherence, rs14429078 and parieto-occipital interhemispheric high theta coherence, and rs116445911 with centro-parietal low theta coherence. These novel genetic findings highlight the utility of the endophenotype approach in enhancing our understanding of mechanisms underlying addiction susceptibility.","Meyers, Zhang, Chorlian, Pandey, Kamarajan, Wang, Wetherill, Lai, Chao, Chan, Kinreich, Kapoor, Bertelsen, McClintick, Bauer, Hesselbrock, Kuperman, Kramer, Salvatore, Dick, Agrawal, Foroud, Edenberg, Goate, Porjesz","Meyers, Zhang, Chorlian, Pandey, Kamarajan, Wang, Wetherill, Lai, Chao, Chan, Kinreich, Kapoor, Bertelsen, McClintick, Bauer, Hesselbrock, Kuperman, Kramer, Salvatore, Dick, Agrawal, Foroud, Edenberg, Goate, Porjesz",https://doi.org/10.1038/s41380-020-0777-6,https://doi.org/10.1038/s41380-020-0777-6,2021-08-03
16795.0,pubmed,pubmed,Identification of potentially undiagnosed patients with nontuberculous mycobacterial lung disease using machine learning applied to primary care data in the UK,Identification of potentially undiagnosed patients with nontuberculous mycobacterial lung disease using machine learning applied to primary care data in the UK,"Nontuberculous mycobacterial lung disease (NTMLD) is a rare lung disease often missed due to a low index of suspicion and unspecific clinical presentation. This retrospective study was designed to characterise the prediagnosis features of NTMLD patients in primary care and to assess the feasibility of using machine learning to identify undiagnosed NTMLD patients.IQVIA Medical Research Data (incorporating THIN, a Cegedim Database), a UK electronic medical records primary care database was used. NTMLD patients were identified between 2003 and 2017 by diagnosis in primary or secondary care or record of NTMLD treatment regimen. Risk factors and treatments were extracted in the prediagnosis period, guided by literature and expert clinical opinion. The control population was enriched to have at least one of these features.741 NTMLD and 112Ã¢â‚¬Å 784 control patients were selected. Annual prevalence rates of NTMLD from 2006 to 2016 increased from 2.7 to 5.1 per 100Ã¢â‚¬Å 000. The most common pre-existing diagnoses and treatments for NTMLD patients were COPD and asthma and penicillin, macrolides and inhaled corticosteroids. Compared to random testing, machine learning improved detection of patients with NTMLD by almost a thousand-fold with AUC of 0.94. The total prevalence of diagnosed and undiagnosed cases of NTMLD in 2016 was estimated to range between 9 and 16 per 100Ã¢â‚¬Å 000.This study supports the feasibility of machine learning applied to primary care data to screen for undiagnosed NTMLD patients, with results indicating that there may be a substantial number of undiagnosed cases of NTMLD in the UK.","Nontuberculous mycobacterial lung disease (NTMLD) is a rare lung disease often missed due to a low index of suspicion and unspecific clinical presentation. This retrospective study was designed to characterise the prediagnosis features of NTMLD patients in primary care and to assess the feasibility of using machine learning to identify undiagnosed NTMLD patients.IQVIA Medical Research Data (incorporating THIN, a Cegedim Database), a UK electronic medical records primary care database was used. NTMLD patients were identified between 2003 and 2017 by diagnosis in primary or secondary care or record of NTMLD treatment regimen. Risk factors and treatments were extracted in the prediagnosis period, guided by literature and expert clinical opinion. The control population was enriched to have at least one of these features.741 NTMLD and 112â€Š784 control patients were selected. Annual prevalence rates of NTMLD from 2006 to 2016 increased from 2.7 to 5.1 per 100â€Š000. The most common pre-existing diagnoses and treatments for NTMLD patients were COPD and asthma and penicillin, macrolides and inhaled corticosteroids. Compared to random testing, machine learning improved detection of patients with NTMLD by almost a thousand-fold with AUC of 0.94. The total prevalence of diagnosed and undiagnosed cases of NTMLD in 2016 was estimated to range between 9 and 16 per 100â€Š000.This study supports the feasibility of machine learning applied to primary care data to screen for undiagnosed NTMLD patients, with results indicating that there may be a substantial number of undiagnosed cases of NTMLD in the UK.","Doyle, van der Laan, Obradovic, McMahon, Daniels, Pitcher, Loebinger","Doyle, van der Laan, Obradovic, McMahon, Daniels, Pitcher, Loebinger",https://doi.org/10.1183/13993003.00045-2020,https://doi.org/10.1183/13993003.00045-2020,2021-08-03
16799.0,pubmed,pubmed,Longitudinal assessment of carotid plaque texture in three-dimensional ultrasound images based on semi-supervised graph-based dimensionality reduction and feature selection,Longitudinal assessment of carotid plaque texture in three-dimensional ultrasound images based on semi-supervised graph-based dimensionality reduction and feature selection,"With continuous development of therapeutic options for atherosclerosis, image-based biomarkers sensitive to the effect of new interventions are required to be developed for cost-effective clinical evaluation. Although 3D ultrasound measurement of total plaque volume (TPV) showed the efficacy of high-dose statin, more sensitive biomarkers are needed to establish the efficacy of dietary supplements expected to confer a smaller beneficial effect. This study involved 171 subjects who participated in a one-year placebo-controlled trial evaluating the effect of pomegranate. A framework involving a feature selection technique known as discriminative feature selection (DFS) and a semi-supervised graph-based regression (SSGBR) technique was proposed for sensitive detection of plaque textural changes over the trial. 376 textual features of plaques were extracted from 3D ultrasound images acquired at baseline and a follow-up session. A scalar biomarker for each subject were generated by SSGBR based on prominent textural features selected by DFS. The ability of this biomarker for discriminating pomegranate from placebo subjects was quantified by the p-values obtained in Mann-Whitney U test. The discriminative power of SSGBR was compared with global and local dimensionality reduction techniques, including linear discriminant analysis (LDA), maximum margin criterion (MMC) and Laplacian Eigenmap (LE). Only SSGBR (p=4.12Ãƒâ€”10<sup>-6</sup>) and normalized LE (p=0.002) detected a difference between the two groups at the 5% significance level. As compared with ÃŽâ€TPV, SSGBR reduced the sample size required to establish a significant difference by a factor of 60. The application of this framework will substantially reduce the cost incurred in clinical trials.","With continuous development of therapeutic options for atherosclerosis, image-based biomarkers sensitive to the effect of new interventions are required to be developed for cost-effective clinical evaluation. Although 3D ultrasound measurement of total plaque volume (TPV) showed the efficacy of high-dose statin, more sensitive biomarkers are needed to establish the efficacy of dietary supplements expected to confer a smaller beneficial effect. This study involved 171 subjects who participated in a one-year placebo-controlled trial evaluating the effect of pomegranate. A framework involving a feature selection technique known as discriminative feature selection (DFS) and a semi-supervised graph-based regression (SSGBR) technique was proposed for sensitive detection of plaque textural changes over the trial. 376 textual features of plaques were extracted from 3D ultrasound images acquired at baseline and a follow-up session. A scalar biomarker for each subject were generated by SSGBR based on prominent textural features selected by DFS. The ability of this biomarker for discriminating pomegranate from placebo subjects was quantified by the p-values obtained in Mann-Whitney U test. The discriminative power of SSGBR was compared with global and local dimensionality reduction techniques, including linear discriminant analysis (LDA), maximum margin criterion (MMC) and Laplacian Eigenmap (LE). Only SSGBR (p=4.12Ã—10<sup>-6</sup>) and normalized LE (p=0.002) detected a difference between the two groups at the 5% significance level. As compared with Î”TPV, SSGBR reduced the sample size required to establish a significant difference by a factor of 60. The application of this framework will substantially reduce the cost incurred in clinical trials.","Lin, Cui, Chen, van Engelen, de Bruijne, Azarpazhooh, Sohrevardi, Spence, Chiu","Lin, Cui, Chen, van Engelen, de Bruijne, Azarpazhooh, Sohrevardi, Spence, Chiu",https://doi.org/10.1016/j.compbiomed.2019.103586,https://doi.org/10.1016/j.compbiomed.2019.103586,2021-08-03
16807.0,pubmed,pubmed,A dynamic reaction picklist for improving allergy reaction documentation in the electronic health record,A dynamic reaction picklist for improving allergy reaction documentation in the electronic health record,"Incomplete and static reaction picklists in the allergy module led to free-text and missing entries that inhibit the clinical decision support intended to prevent adverse drug reactions. We developed a novel, data-driven, &quot;dynamic&quot; reaction picklist to improve allergy documentation in the electronic health record (EHR). We split 3 decades of allergy entries in the EHR of a large Massachusetts healthcare system into development and validation datasets. We consolidated duplicate allergens and those with the same ingredients or allergen groups. We created a reaction value set via expert review of a previously developed value set and then applied natural language processing to reconcile reactions from structured and free-text entries. Three association rule-mining measures were used to develop a comprehensive reaction picklist dynamically ranked by allergen. The dynamic picklist was assessed using recall at top k suggested reactions, comparing performance to the static picklist. The modified reaction value set contained 490 reaction concepts. Among 4Ã‚Â 234Ã‚Â 327 allergy entries collected, 7463 unique consolidated allergens and 469 unique reactions were identified. Of the 3 dynamic reaction picklists developed, the 1 with the optimal ranking achieved recalls of 0.632, 0.763, and 0.822 at the top 5, 10, and 15, respectively, significantly outperforming the static reaction picklist ranked by reaction frequency. The dynamic reaction picklist developed using EHR data and a statistical measure was superior to the static picklist and suggested proper reactions for allergy documentation. Further studies might evaluate the usability and impact on allergy documentation in the EHR.","Incomplete and static reaction picklists in the allergy module led to free-text and missing entries that inhibit the clinical decision support intended to prevent adverse drug reactions. We developed a novel, data-driven, ""dynamic"" reaction picklist to improve allergy documentation in the electronic health record (EHR). We split 3 decades of allergy entries in the EHR of a large Massachusetts healthcare system into development and validation datasets. We consolidated duplicate allergens and those with the same ingredients or allergen groups. We created a reaction value set via expert review of a previously developed value set and then applied natural language processing to reconcile reactions from structured and free-text entries. Three association rule-mining measures were used to develop a comprehensive reaction picklist dynamically ranked by allergen. The dynamic picklist was assessed using recall at top k suggested reactions, comparing performance to the static picklist. The modified reaction value set contained 490 reaction concepts. Among 4Â 234Â 327 allergy entries collected, 7463 unique consolidated allergens and 469 unique reactions were identified. Of the 3 dynamic reaction picklists developed, the 1 with the optimal ranking achieved recalls of 0.632, 0.763, and 0.822 at the top 5, 10, and 15, respectively, significantly outperforming the static reaction picklist ranked by reaction frequency. The dynamic reaction picklist developed using EHR data and a statistical measure was superior to the static picklist and suggested proper reactions for allergy documentation. Further studies might evaluate the usability and impact on allergy documentation in the EHR.","Wang, Blackley, Blumenthal, Yerneni, Goss, Lo, Shah, Ortega, Korach, Seger, Zhou","Wang, Blackley, Blumenthal, Yerneni, Goss, Lo, Shah, Ortega, Korach, Seger, Zhou",https://doi.org/10.1093/jamia/ocaa042,https://doi.org/10.1093/jamia/ocaa042,2021-08-03
16808.0,pubmed,pubmed,Explainable artificial intelligence models using real-world electronic healthÃ‚Â record data: a systematic scoping review,Explainable artificial intelligence models using real-world electronic healthÂ record data: a systematic scoping review,"To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions. We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges. Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (NÃ¢â‚¬â€°=Ã¢â‚¬â€°13), intrinsically interpretable models (NÃ¢â‚¬â€°=Ã¢â‚¬â€°9), data dimensionality reduction (NÃ¢â‚¬â€°=Ã¢â‚¬â€°8), attention mechanism (NÃ¢â‚¬â€°=Ã¢â‚¬â€°7), and feature interaction and importance (NÃ¢â‚¬â€°=Ã¢â‚¬â€°5). XAI evaluation is an open issue that requiresÃ‚Â a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view. Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.","To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions. We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges. Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (Nâ€‰=â€‰13), intrinsically interpretable models (Nâ€‰=â€‰9), data dimensionality reduction (Nâ€‰=â€‰8), attention mechanism (Nâ€‰=â€‰7), and feature interaction and importance (Nâ€‰=â€‰5). XAI evaluation is an open issue that requiresÂ a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view. Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.","Payrovnaziri, Chen, Rengifo-Moreno, Miller, Bian, Chen, Liu, He","Payrovnaziri, Chen, Rengifo-Moreno, Miller, Bian, Chen, Liu, He",https://doi.org/10.1093/jamia/ocaa053,https://doi.org/10.1093/jamia/ocaa053,2021-08-03
16811.0,pubmed,pubmed,Neural dynamics of sentiment processing during naturalistic sentence reading,Neural dynamics of sentiment processing during naturalistic sentence reading,"When we read, our eyes move through the text in a series of fixations and high-velocity saccades to extract visual information. This process allows the brain to obtain meaning, e.g., about sentiment, or the emotional valence, expressed in the written text. How exactly the brain extracts the sentiment of single words during naturalistic reading is largely unknown. This is due to the challenges of naturalistic imaging, which has previously led researchers to employ highly controlled, timed word-by-word presentations of custom reading materials that lack ecological validity. Here, we aimed to assess the electrical neural correlates of word sentiment processing during naturalistic reading of English sentences. We used a publicly available dataset of simultaneous electroencephalography (EEG), eye-tracking recordings, and word-level semantic annotations from 7129 words in 400 sentences (Zurich Cognitive Language Processing Corpus; Hollenstein etÃ‚Â al., 2018). We computed fixation-related potentials (FRPs), which are evoked electrical responses time-locked to the onset of fixations. A general linear mixed model analysis of FRPs cleaned from visual- and motor-evoked activity showed a topographical difference between the positive and negative sentiment condition in the 224-304Ã‚Â Ã¢â‚¬â€¹ms interval after fixation onset in left-central and right-posterior electrode clusters. An additional analysis that included word-, phrase-, and sentence-level sentiment predictors showed the same FRP differences for the word-level sentiment, but no additional FRP differences for phrase- and sentence-level sentiment. Furthermore, decoding analysis that classified word sentiment (positive or negative) from sentiment-matched 40-trial average FRPs showed a 0.60 average accuracy (95% confidence interval: [0.58, 0.61]). Control analyses ruled out that these results were based on differences in eye movements or linguistic features other than word sentiment. Our results extend previous research by showing that the emotional valence of lexico-semantic stimuli evoke a fast electrical neural response upon word fixation during naturalistic reading. These results provide an important step to identify the neural processes of lexico-semantic processing in ecologically valid conditions and can serve to improve computer algorithms for natural language processing.","When we read, our eyes move through the text in a series of fixations and high-velocity saccades to extract visual information. This process allows the brain to obtain meaning, e.g., about sentiment, or the emotional valence, expressed in the written text. How exactly the brain extracts the sentiment of single words during naturalistic reading is largely unknown. This is due to the challenges of naturalistic imaging, which has previously led researchers to employ highly controlled, timed word-by-word presentations of custom reading materials that lack ecological validity. Here, we aimed to assess the electrical neural correlates of word sentiment processing during naturalistic reading of English sentences. We used a publicly available dataset of simultaneous electroencephalography (EEG), eye-tracking recordings, and word-level semantic annotations from 7129 words in 400 sentences (Zurich Cognitive Language Processing Corpus; Hollenstein etÂ al., 2018). We computed fixation-related potentials (FRPs), which are evoked electrical responses time-locked to the onset of fixations. A general linear mixed model analysis of FRPs cleaned from visual- and motor-evoked activity showed a topographical difference between the positive and negative sentiment condition in the 224-304Â â€‹ms interval after fixation onset in left-central and right-posterior electrode clusters. An additional analysis that included word-, phrase-, and sentence-level sentiment predictors showed the same FRP differences for the word-level sentiment, but no additional FRP differences for phrase- and sentence-level sentiment. Furthermore, decoding analysis that classified word sentiment (positive or negative) from sentiment-matched 40-trial average FRPs showed a 0.60 average accuracy (95% confidence interval: [0.58, 0.61]). Control analyses ruled out that these results were based on differences in eye movements or linguistic features other than word sentiment. Our results extend previous research by showing that the emotional valence of lexico-semantic stimuli evoke a fast electrical neural response upon word fixation during naturalistic reading. These results provide an important step to identify the neural processes of lexico-semantic processing in ecologically valid conditions and can serve to improve computer algorithms for natural language processing.","Pfeiffer, Hollenstein, Zhang, Langer","Pfeiffer, Hollenstein, Zhang, Langer",https://doi.org/10.1016/j.neuroimage.2020.116934,https://doi.org/10.1016/j.neuroimage.2020.116934,2021-08-03
16821.0,pubmed,pubmed,Automatic Triage of 12-Lead ECGs Using Deep Convolutional Neural Networks,Automatic Triage of 12-Lead ECGs Using Deep Convolutional Neural Networks,"BACKGROUND The correct interpretation of the ECG is pivotal for the accurate diagnosis of many cardiac abnormalities, and conventional computerized interpretation has not been able to reach physician-level accuracy in detecting (acute) cardiac abnormalities. This study aims to develop and validate a deep neural network for comprehensive automated ECG triage in daily practice. METHODS AND RESULTS We developed a 37-layer convolutional residual deep neural network on a data set of free-text physician-annotated 12-lead ECGs. The deep neural network was trained on a data set with 336.835 recordings from 142.040 patients and validated on an independent validation data set (n=984), annotated by a panel of 5 cardiologists electrophysiologists. The 12-lead ECGs were acquired in all noncardiology departments of the University Medical Center Utrecht. The algorithm learned to classify these ECGs into the following 4 triage categories: normal, abnormal not acute, subacute, and acute. Discriminative performance is presented with overall and category-specific concordance statistics, polytomous discrimination indexes, sensitivities, specificities, and positive and negative predictive values. The patients in the validation data set had a mean age of 60.4Ã‚Â years and 54.3% were men. The deep neural network showed excellent overall discrimination with an overall concordance statistic of 0.93 (95% CI, 0.92-0.95) and a polytomous discriminatory index of 0.83 (95% CI, 0.79-0.87). CONCLUSIONS This study demonstrates that an end-to-end deep neural network can be accurately trained on unstructured free-text physician annotations and used to consistently triage 12-lead ECGs. When further fine-tuned with other clinical outcomes and externally validated in clinical practice, the demonstrated deep learning-based ECG interpretation can potentially improve time to treatment and decrease healthcare burden.","BACKGROUND The correct interpretation of the ECG is pivotal for the accurate diagnosis of many cardiac abnormalities, and conventional computerized interpretation has not been able to reach physician-level accuracy in detecting (acute) cardiac abnormalities. This study aims to develop and validate a deep neural network for comprehensive automated ECG triage in daily practice. METHODS AND RESULTS We developed a 37-layer convolutional residual deep neural network on a data set of free-text physician-annotated 12-lead ECGs. The deep neural network was trained on a data set with 336.835 recordings from 142.040 patients and validated on an independent validation data set (n=984), annotated by a panel of 5 cardiologists electrophysiologists. The 12-lead ECGs were acquired in all noncardiology departments of the University Medical Center Utrecht. The algorithm learned to classify these ECGs into the following 4 triage categories: normal, abnormal not acute, subacute, and acute. Discriminative performance is presented with overall and category-specific concordance statistics, polytomous discrimination indexes, sensitivities, specificities, and positive and negative predictive values. The patients in the validation data set had a mean age of 60.4Â years and 54.3% were men. The deep neural network showed excellent overall discrimination with an overall concordance statistic of 0.93 (95% CI, 0.92-0.95) and a polytomous discriminatory index of 0.83 (95% CI, 0.79-0.87). CONCLUSIONS This study demonstrates that an end-to-end deep neural network can be accurately trained on unstructured free-text physician annotations and used to consistently triage 12-lead ECGs. When further fine-tuned with other clinical outcomes and externally validated in clinical practice, the demonstrated deep learning-based ECG interpretation can potentially improve time to treatment and decrease healthcare burden.","van de Leur, Blom, Gavves, Hof, van der Heijden, Clappers, Doevendans, Hassink, van Es","van de Leur, Blom, Gavves, Hof, van der Heijden, Clappers, Doevendans, Hassink, van Es",https://doi.org/10.1161/JAHA.119.015138,https://doi.org/10.1161/JAHA.119.015138,2021-08-03
16823.0,pubmed,pubmed,Current methods for development of rapid reviews about diagnostic tests: an international survey,Current methods for development of rapid reviews about diagnostic tests: an international survey,"Rapid reviews (RRs) have emerged as an efficient alternative to time-consuming systematic reviews-they can help meet the demand for accelerated evidence synthesis to inform decision-making in healthcare. The synthesis of diagnostic evidence has important methodological challenges. Here, we performed an international survey to identify the current practice of producing RRs for diagnostic tests. We developed and administered an online survey inviting institutions that perform RRs of diagnostic tests from all over the world. All participants (NÃ¢â‚¬â€°=Ã¢â‚¬â€°25) reported the implementation of one or more methods to define the scope of the RR; however, only one strategy (defining a structured question) was used by Ã¢â€°Â¥90% of participants. All participants used at least one methodological shortcut including the use of a previous review as a starting point (92%) and the use of limits on the search (96%). Parallelization and automation of review tasks were not extensively used (48 and 20%, respectively). Our survey indicates a greater use of shortcuts and limits for conducting diagnostic test RRs versus the results of a recent scoping review analyzing published RRs. Several shortcuts are used without knowing how their implementation affects the results of the evidence synthesis in the setting of diagnostic test reviews. Thus, a structured evaluation of the challenges and implications of the adoption of these RR methods is warranted.","Rapid reviews (RRs) have emerged as an efficient alternative to time-consuming systematic reviews-they can help meet the demand for accelerated evidence synthesis to inform decision-making in healthcare. The synthesis of diagnostic evidence has important methodological challenges. Here, we performed an international survey to identify the current practice of producing RRs for diagnostic tests. We developed and administered an online survey inviting institutions that perform RRs of diagnostic tests from all over the world. All participants (Nâ€‰=â€‰25) reported the implementation of one or more methods to define the scope of the RR; however, only one strategy (defining a structured question) was used by â‰¥90% of participants. All participants used at least one methodological shortcut including the use of a previous review as a starting point (92%) and the use of limits on the search (96%). Parallelization and automation of review tasks were not extensively used (48 and 20%, respectively). Our survey indicates a greater use of shortcuts and limits for conducting diagnostic test RRs versus the results of a recent scoping review analyzing published RRs. Several shortcuts are used without knowing how their implementation affects the results of the evidence synthesis in the setting of diagnostic test reviews. Thus, a structured evaluation of the challenges and implications of the adoption of these RR methods is warranted.","Arevalo-Rodriguez, Steingart, Tricco, Nussbaumer-Streit, Kaunelis, Alonso-Coello, Baxter, Bossuyt, Emparanza, Zamora","Arevalo-Rodriguez, Steingart, Tricco, Nussbaumer-Streit, Kaunelis, Alonso-Coello, Baxter, Bossuyt, Emparanza, Zamora",https://doi.org/10.1186/s12874-020-01004-z,https://doi.org/10.1186/s12874-020-01004-z,2021-08-03
16833.0,pubmed,pubmed,Applications of radiomics and machine learning for radiotherapy of malignant brain tumors,Applications of radiomics and machine learning for radiotherapy of malignant brain tumors,"Magnetic resonance imaging (MRI) and amino acid positron-emission tomography (PET) of the brain contain aÃ‚Â vast amount of structural and functional information that can be analyzed by machine learning algorithms and radiomics for the use of radiotherapy in patients with malignant brain tumors. This study is based on comprehensive literature research on machine learning and radiomics analyses in neuroimaging and their potential application for radiotherapy in patients with malignant glioma or brain metastases. Feature-based radiomics and deep learning-based machine learning methods can be used to improve brain tumor diagnostics and automate various steps of radiotherapy planning. In glioma patients, important applications are the determination of WHO grade and molecular markers for integrated diagnosis in patients not eligible for biopsy or resection, automatic image segmentation for target volume planning, prediction of the location of tumor recurrence, and differentiation of pseudoprogression from actual tumor progression. In patients with brain metastases, radiomics is applied for additional detection of smaller brain metastases, accurate segmentation of multiple larger metastases, prediction of local response after radiosurgery, and differentiation of radiation injury from local brain metastasis relapse. Importantly, high diagnostic accuracies of 80-90% can be achieved by most approaches, despite aÃ‚Â large variety in terms of applied imaging techniques and computational methods. Clinical application of automated image analyses based on radiomics and artificial intelligence has aÃ‚Â great potential for improving radiotherapy in patients with malignant brain tumors. However, aÃ‚Â common problem associated with these techniques is the large variability and the lack of standardization of the methods applied.","Magnetic resonance imaging (MRI) and amino acid positron-emission tomography (PET) of the brain contain aÂ vast amount of structural and functional information that can be analyzed by machine learning algorithms and radiomics for the use of radiotherapy in patients with malignant brain tumors. This study is based on comprehensive literature research on machine learning and radiomics analyses in neuroimaging and their potential application for radiotherapy in patients with malignant glioma or brain metastases. Feature-based radiomics and deep learning-based machine learning methods can be used to improve brain tumor diagnostics and automate various steps of radiotherapy planning. In glioma patients, important applications are the determination of WHO grade and molecular markers for integrated diagnosis in patients not eligible for biopsy or resection, automatic image segmentation for target volume planning, prediction of the location of tumor recurrence, and differentiation of pseudoprogression from actual tumor progression. In patients with brain metastases, radiomics is applied for additional detection of smaller brain metastases, accurate segmentation of multiple larger metastases, prediction of local response after radiosurgery, and differentiation of radiation injury from local brain metastasis relapse. Importantly, high diagnostic accuracies of 80-90% can be achieved by most approaches, despite aÂ large variety in terms of applied imaging techniques and computational methods. Clinical application of automated image analyses based on radiomics and artificial intelligence has aÂ great potential for improving radiotherapy in patients with malignant brain tumors. However, aÂ common problem associated with these techniques is the large variability and the lack of standardization of the methods applied.","Kocher, Ruge, Galldiks, Lohmann","Kocher, Ruge, Galldiks, Lohmann",https://doi.org/10.1007/s00066-020-01626-8,https://doi.org/10.1007/s00066-020-01626-8,2021-08-03
16843.0,pubmed,pubmed,Does the Setting of External Ventricular Drain Placement Affect Morbidity? A Systematic Literature Review Comparing Intensive Care Unit versus Operating Room Procedures,Does the Setting of External Ventricular Drain Placement Affect Morbidity? A Systematic Literature Review Comparing Intensive Care Unit versus Operating Room Procedures,"External ventricular drain (EVD) placement can be performed at the bedside in the neurosurgical intensive care unit (ICU) or in the operating room (OR). Systematic review and meta-analysis may permit stronger recommendations to improve accuracy and complication rates. Systematic review of PubMed was performed (inception-December 12, 2019) following PRISMA guidelines. Our search yielded 356 articles, of which 37 studies underwent full-text analysis. Nine studies met inclusion criteria. Studies were segregated into OR only (nÃ‚Â = 3; 1011 patients), ICU only (nÃ‚Â = 3; 325 patients), and ORÃ‚Â + ICU (nÃ‚Â = 3; 613 patients) cohorts. Studies were in addition divided by outcome measures, including catheter placement accuracy (ICU, 4 studies, nÃ‚Â = 280 [68.29%] vs. OR, 2 studies, nÃ‚Â = 198 [84.25%]); iatrogenic hemorrhagic complications (ICU, 4 studies, nÃ‚Â = 112 [18.16%] vs. OR, 2 studies, nÃ‚Â = 35 [17.50%]); and ventriculostomy-related infection rates (ICU, 4 studies, nÃ‚Â = 48 [7.28%] vs. OR: 5 studies, nÃ‚Â = 92 [8.06%]). There are likely specific patient populations who would benefit from EVD placement in the ICU versus OR setting. The literature comparing efficacy and morbidity between EVDs placed in the ICU and OR settings is overall inconclusive in both sample size and congruence of methodology. Agreement in outcome metrics and data reporting on this topic is necessary to synthesize high-quality evidence to form practice-changing recommendations for this debated topic.","External ventricular drain (EVD) placement can be performed at the bedside in the neurosurgical intensive care unit (ICU) or in the operating room (OR). Systematic review and meta-analysis may permit stronger recommendations to improve accuracy and complication rates. Systematic review of PubMed was performed (inception-December 12, 2019) following PRISMA guidelines. Our search yielded 356 articles, of which 37 studies underwent full-text analysis. Nine studies met inclusion criteria. Studies were segregated into OR only (nÂ = 3; 1011 patients), ICU only (nÂ = 3; 325 patients), and ORÂ + ICU (nÂ = 3; 613 patients) cohorts. Studies were in addition divided by outcome measures, including catheter placement accuracy (ICU, 4 studies, nÂ = 280 [68.29%] vs. OR, 2 studies, nÂ = 198 [84.25%]); iatrogenic hemorrhagic complications (ICU, 4 studies, nÂ = 112 [18.16%] vs. OR, 2 studies, nÂ = 35 [17.50%]); and ventriculostomy-related infection rates (ICU, 4 studies, nÂ = 48 [7.28%] vs. OR: 5 studies, nÂ = 92 [8.06%]). There are likely specific patient populations who would benefit from EVD placement in the ICU versus OR setting. The literature comparing efficacy and morbidity between EVDs placed in the ICU and OR settings is overall inconclusive in both sample size and congruence of methodology. Agreement in outcome metrics and data reporting on this topic is necessary to synthesize high-quality evidence to form practice-changing recommendations for this debated topic.","Dawod, Henkel, Karim, Caras, Qaqish, Mugge, Medhkour","Dawod, Henkel, Karim, Caras, Qaqish, Mugge, Medhkour",https://doi.org/10.1016/j.wneu.2020.04.215,https://doi.org/10.1016/j.wneu.2020.04.215,2021-08-03
16846.0,pubmed,pubmed,Using word embeddings to improve the privacy of clinical notes,Using word embeddings to improve the privacy of clinical notes,"In this work, we introduce a privacy technique for anonymizing clinical notes that guarantees all private health information is secured (including sensitive data, such as family history, that are not adequately covered by current techniques). We employ a new &quot;random replacement&quot; paradigm (replacing each token in clinical notes with neighboring word vectors from the embedding space) to achieve 100% recall on the removal of sensitive information, unachievable with current &quot;search-and-secure&quot; paradigms. We demonstrate the utility of this paradigm on multiple corpora in a diverse set of classification tasks. We empirically evaluate the effect of our anonymization technique both on upstream and downstream natural language processing tasks to show that our perturbations, while increasing security (ie, achieving 100% recall on any dataset), do not greatly impact the results of end-to-end machine learning approaches. As long as current approaches utilize precision and recall to evaluate deidentification algorithms, there will remain a risk of overlooking sensitive information. Inspired by differential privacy, we sought to make it statistically infeasible to recreate the original data, although at the cost of readability. We hope that the work will serve as a catalyst to further research into alternative deidentification methods that can address current weaknesses. Our proposed technique can secure clinical texts at a low cost and extremely high recall with a readability trade-off while remaining useful for natural language processing classification tasks. We hope that our work can be used by risk-averse data holders to release clinical texts to researchers.","In this work, we introduce a privacy technique for anonymizing clinical notes that guarantees all private health information is secured (including sensitive data, such as family history, that are not adequately covered by current techniques). We employ a new ""random replacement"" paradigm (replacing each token in clinical notes with neighboring word vectors from the embedding space) to achieve 100% recall on the removal of sensitive information, unachievable with current ""search-and-secure"" paradigms. We demonstrate the utility of this paradigm on multiple corpora in a diverse set of classification tasks. We empirically evaluate the effect of our anonymization technique both on upstream and downstream natural language processing tasks to show that our perturbations, while increasing security (ie, achieving 100% recall on any dataset), do not greatly impact the results of end-to-end machine learning approaches. As long as current approaches utilize precision and recall to evaluate deidentification algorithms, there will remain a risk of overlooking sensitive information. Inspired by differential privacy, we sought to make it statistically infeasible to recreate the original data, although at the cost of readability. We hope that the work will serve as a catalyst to further research into alternative deidentification methods that can address current weaknesses. Our proposed technique can secure clinical texts at a low cost and extremely high recall with a readability trade-off while remaining useful for natural language processing classification tasks. We hope that our work can be used by risk-averse data holders to release clinical texts to researchers.","Abdalla, Abdalla, Rudzicz, Hirst","Abdalla, Abdalla, Rudzicz, Hirst",https://doi.org/10.1093/jamia/ocaa038,https://doi.org/10.1093/jamia/ocaa038,2021-08-03
16848.0,pubmed,pubmed,Estimating Time to Progression of Chronic Obstructive Pulmonary Disease with Tolerance,Estimating Time to Progression of Chronic Obstructive Pulmonary Disease With Tolerance,"We defined tolerance range as the distance of observing similar disease conditions or functional status from the upper to the lower boundaries of a specified time interval. A tolerance range was identified for linear regression and support vector machines to optimize the improvement rate (defined as IR) on accuracy in predicting mortality risk in patients with chronic obstructive pulmonary disease using clinical notes. The corpus includes pulmonary, cardiology, and radiology reports of 15,500 patients who died between 2011 and 2017. Their performance was compared against a long short-term memory recurrent neural network. The results demonstrate an overall improvement by those basic machine learning approaches after considering an optimal tolerance range: the average IR of linear regression was 90.1% and the maximum IR of support vector machines was 66.2%. There was a similitude between the time segments produced by our tolerance algorithms and those produced by the long short-term memory.","We defined tolerance range as the distance of observing similar disease conditions or functional status from the upper to the lower boundaries of a specified time interval. A tolerance range was identified for linear regression and support vector machines to optimize the improvement rate (defined as IR) on accuracy in predicting mortality risk in patients with chronic obstructive pulmonary disease using clinical notes. The corpus includes pulmonary, cardiology, and radiology reports of 15,500 patients who died between 2011 and 2017. Their performance was compared against a long short-term memory recurrent neural network. The results demonstrate an overall improvement by those basic machine learning approaches after considering an optimal tolerance range: the average IR of linear regression was 90.1% and the maximum IR of support vector machines was 66.2%. There was a similitude between the time segments produced by our tolerance algorithms and those produced by the long short-term memory.","Tang, Plasek, Shi, Wan, Zhang, Kang, Wang, Dulgarian, Xiong, Ma, Bates, Zhou","Tang, Plasek, Shi, Wan, Zhang, Kang, Wang, Dulgarian, Xiong, Ma, Bates, Zhou",https://doi.org/10.1109/JBHI.2020.2992259,https://doi.org/10.1109/JBHI.2020.2992259,2021-08-03
16851.0,pubmed,pubmed,"Digital conversations about suicide among teenagers and adults with epilepsy: A big-data, machine learning analysis","Digital conversations about suicide among teenagers and adults with epilepsy: A big-data, machine learning analysis","Digital media conversations can provide important insight into the concerns and struggles of people with epilepsy (PWE) outside of formal clinical settings and help generate useful information for treatment planning. Our study aimed to explore the big data from open-source digital conversations among PWE with regard to suicidality, specifically comparing teenagers and adults, using machine learning technology. Advanced machine-learning empowered methodology was used to mine and structure open-source digital conversations of self-identifying teenagers and adults who endorsed suffering from epilepsy and engaged in conversation about suicide. The search was limited to 12Ã‚Â months and included only conversations originating from US internet protocol (IP) addresses. Natural language processing and text analytics were employed to develop a thematic analysis. A total of 222Ã‚Â 000 unique conversations about epilepsy, including 9000 (4%) related to suicide, were posted during the study period. The suicide-related conversations were posted by 7.8% of teenagers and 3.2% of adults in the study. Several critical differences were noted between teenagers and adults. A higher percentage of teenagers are: fearful of &quot;the unknown&quot; due to seizures (63% vs 12% adults), concerned about social consequences of seizures (30% vs 21%), and seek emotional support (29% vs 19%). In contrast, a significantly higher percentage of adults show a defeatist (&quot;given up&quot;) attitude compared to teenagers (42% vs 4%). There were important differences in the author's determined sentiments behind the conversations among teenagers and adults. In this first of its kind big data analysis of nearly a quarter-million digital conversations about epilepsy using machine learning, we found that teenagers engage in an online conversation about suicide more often than adults. There are some key differences in the attitudes and concerns, which may have implications for the treatment of younger patients with epilepsy.","Digital media conversations can provide important insight into the concerns and struggles of people with epilepsy (PWE) outside of formal clinical settings and help generate useful information for treatment planning. Our study aimed to explore the big data from open-source digital conversations among PWE with regard to suicidality, specifically comparing teenagers and adults, using machine learning technology. Advanced machine-learning empowered methodology was used to mine and structure open-source digital conversations of self-identifying teenagers and adults who endorsed suffering from epilepsy and engaged in conversation about suicide. The search was limited to 12Â months and included only conversations originating from US internet protocol (IP) addresses. Natural language processing and text analytics were employed to develop a thematic analysis. A total of 222Â 000 unique conversations about epilepsy, including 9000 (4%) related to suicide, were posted during the study period. The suicide-related conversations were posted by 7.8% of teenagers and 3.2% of adults in the study. Several critical differences were noted between teenagers and adults. A higher percentage of teenagers are: fearful of ""the unknown"" due to seizures (63% vs 12% adults), concerned about social consequences of seizures (30% vs 21%), and seek emotional support (29% vs 19%). In contrast, a significantly higher percentage of adults show a defeatist (""given up"") attitude compared to teenagers (42% vs 4%). There were important differences in the author's determined sentiments behind the conversations among teenagers and adults. In this first of its kind big data analysis of nearly a quarter-million digital conversations about epilepsy using machine learning, we found that teenagers engage in an online conversation about suicide more often than adults. There are some key differences in the attitudes and concerns, which may have implications for the treatment of younger patients with epilepsy.","Falcone, Dagar, Castilla-Puentes, Anand, Brethenoux, Valleta, Furey, Timmons-Mitchell, Pestana-Knight","Falcone, Dagar, Castilla-Puentes, Anand, Brethenoux, Valleta, Furey, Timmons-Mitchell, Pestana-Knight",https://doi.org/10.1111/epi.16507,https://doi.org/10.1111/epi.16507,2021-08-03
16858.0,pubmed,pubmed,Delivering Benefits at Speed Through Real-World Repurposing of Off-Patent Drugs: The COVID-19 Pandemic as a Case in Point,Delivering Benefits at Speed Through Real-World Repurposing of Off-Patent Drugs: The COVID-19 Pandemic as a Case in Point,"Real-world drug repurposing-the immediate &quot;off-label&quot; prescribing of drugs to address urgent clinical needs-is a widely overlooked opportunity. Off-label prescribing (ie, for a nonapproved indication) is legal in most countries and tends to shift the burden of liability and cost to physicians and patients, respectively. Nevertheless, health crises may mean that real-world repurposing is the only realistic source for solutions. Optimal real-world repurposing requires a track record of safety, affordability, and access for drug candidates. Although thousands of such drugs are already available, there is no central repository of off-label uses to facilitate immediate identification and selection of potentially useful interventions during public health crises. Using the current coronavirus disease (COVID-19) pandemic as an example, we provide a glimpse of the extensive literature that supports the rationale behind six generic drugs, in four classes, all of which are affordable, supported by decades of safety data, and targeted toward the underlying pathophysiology that makes COVID-19 so deadly. This paper briefly summarizes why cimetidine or famotidine, dipyridamole, fenofibrate or bezafibrate, and sildenafil citrate are worth considering for patients with COVID-19. Clinical trials to assess efficacy are already underway for famotidine, dipyridamole, and sildenafil, and further trials of all these agents will be important in due course. These examples also reveal the unlimited opportunity to future-proof our health care systems by proactively mining, synthesizing, cataloging, and evaluating the off-label treatment opportunities of thousands of safe, well-established, and affordable generic drugs.","Real-world drug repurposing-the immediate ""off-label"" prescribing of drugs to address urgent clinical needs-is a widely overlooked opportunity. Off-label prescribing (ie, for a nonapproved indication) is legal in most countries and tends to shift the burden of liability and cost to physicians and patients, respectively. Nevertheless, health crises may mean that real-world repurposing is the only realistic source for solutions. Optimal real-world repurposing requires a track record of safety, affordability, and access for drug candidates. Although thousands of such drugs are already available, there is no central repository of off-label uses to facilitate immediate identification and selection of potentially useful interventions during public health crises. Using the current coronavirus disease (COVID-19) pandemic as an example, we provide a glimpse of the extensive literature that supports the rationale behind six generic drugs, in four classes, all of which are affordable, supported by decades of safety data, and targeted toward the underlying pathophysiology that makes COVID-19 so deadly. This paper briefly summarizes why cimetidine or famotidine, dipyridamole, fenofibrate or bezafibrate, and sildenafil citrate are worth considering for patients with COVID-19. Clinical trials to assess efficacy are already underway for famotidine, dipyridamole, and sildenafil, and further trials of all these agents will be important in due course. These examples also reveal the unlimited opportunity to future-proof our health care systems by proactively mining, synthesizing, cataloging, and evaluating the off-label treatment opportunities of thousands of safe, well-established, and affordable generic drugs.","Rogosnitzky, Berkowitz, Jadad","Rogosnitzky, Berkowitz, Jadad",https://doi.org/10.2196/19199,https://doi.org/10.2196/19199,2021-08-03
16871.0,pubmed,pubmed,"A Systematic Review of Methodological Variation in Healthcare Provider Perspective Tuberculosis Costing Papers Conducted in Low- and Middle-Income Settings, Using An Intervention-Standardised Unit Cost Typology","A Systematic Review of Methodological Variation in Healthcare Provider Perspective Tuberculosis Costing Papers Conducted in Low- and Middle-Income Settings, Using An Intervention-Standardised Unit Cost Typology","There is a need for easily accessible tuberculosis unit cost data, as well as an understanding of the variability of methods used and reporting standards of that data. The aim of this systematic review was to descriptively review papers reporting tuberculosis unit costs from a healthcareÃ‚Â provider perspective looking at methodological variation; to assess quality using a study quality rating system and machine learning to investigate the indicators of reporting quality; and to identify the data gaps to inform standardised tuberculosis unit cost collection and consistent principles for reporting going forward. We searched grey and published literature in five sources and eight databases, respectively, using search terms linked to cost, tuberculosis and tuberculosis health services including tuberculosis treatment and prevention. For inclusion, the papers needed to contain empirical unit cost estimates for tuberculosis interventions from low- and middle-income countries, with reference years between 1990 and 2018. A total of 21,691 papers were found and screened in a phased manner. Data were extracted from the eligible papers into a detailed Microsoft Excel tool, extensively cleaned and analysed with R software (R Project, Vienna, Austria) using the user interface of RStudio. A study quality rating was applied to the reviewed papers based on the inclusion or omission of a selection of variables and their relative importance. Following this, machine learning using a recursive partitioning method was utilised to construct a classification tree to assess the reporting quality. This systematic review included 103 provider perspective papers with 627 unit costs (costs not presented here) for tuberculosis interventions among a total of 140 variables. The interventions covered were active, passive and intensified case finding; tuberculosis treatment; above-service costs; and tuberculosis prevention. Passive case finding is the detection of tuberculosis cases where individuals self-identify at health facilities; active case finding is detection of cases of those not in health facilities, such as through outreach; and intensified case finding is detection of cases in high-risk populations. There was heterogeneity in some of the reported methods used such cost allocation, amortisation and the use of top-down, bottom-up or mixed approaches to the costing. Uncertainty checking through sensitivity analysis was only reported on by half of the papers (54%), while purposive and convenience sampling was reported by 72% of papers. Machine learning indicated that reporting on 'Intervention' (in particular), 'Urbanicity' and 'Site Sampling', were the most likely indicators of quality of reporting. The largest data gap identified was for tuberculosis vaccination cost data, the Bacillus Calmette-GuÃƒÂ©rin (BCG) vaccine in particular. There is a gap in available unit costs for 12 of 30 high tuberculosis burden countries, as well as for the interventions of above-service costs, tuberculosis prevention, and active and intensified case finding. Variability in the methods and reporting used makes comparison difficult and makes it hard for decision makers to know which unit costs they can trust. The study quality rating system used in this review as well as the classification tree enable focus on specific reporting aspects that should improve variability and increase confidence in unit costs. Researchers should endeavour to be explicit and transparent in how they cost interventions following the principles as laid out in the Global Health Cost Consortium's Reference Case for Estimating the Costs of Global Health Services and Interventions, which in turn will lead to repeatability, comparability and enhanced learning from others.","There is a need for easily accessible tuberculosis unit cost data, as well as an understanding of the variability of methods used and reporting standards of that data. The aim of this systematic review was to descriptively review papers reporting tuberculosis unit costs from a healthcareÂ provider perspective looking at methodological variation; to assess quality using a study quality rating system and machine learning to investigate the indicators of reporting quality; and to identify the data gaps to inform standardised tuberculosis unit cost collection and consistent principles for reporting going forward. We searched grey and published literature in five sources and eight databases, respectively, using search terms linked to cost, tuberculosis and tuberculosis health services including tuberculosis treatment and prevention. For inclusion, the papers needed to contain empirical unit cost estimates for tuberculosis interventions from low- and middle-income countries, with reference years between 1990 and 2018. A total of 21,691 papers were found and screened in a phased manner. Data were extracted from the eligible papers into a detailed Microsoft Excel tool, extensively cleaned and analysed with R software (R Project, Vienna, Austria) using the user interface of RStudio. A study quality rating was applied to the reviewed papers based on the inclusion or omission of a selection of variables and their relative importance. Following this, machine learning using a recursive partitioning method was utilised to construct a classification tree to assess the reporting quality. This systematic review included 103 provider perspective papers with 627 unit costs (costs not presented here) for tuberculosis interventions among a total of 140 variables. The interventions covered were active, passive and intensified case finding; tuberculosis treatment; above-service costs; and tuberculosis prevention. Passive case finding is the detection of tuberculosis cases where individuals self-identify at health facilities; active case finding is detection of cases of those not in health facilities, such as through outreach; and intensified case finding is detection of cases in high-risk populations. There was heterogeneity in some of the reported methods used such cost allocation, amortisation and the use of top-down, bottom-up or mixed approaches to the costing. Uncertainty checking through sensitivity analysis was only reported on by half of the papers (54%), while purposive and convenience sampling was reported by 72% of papers. Machine learning indicated that reporting on 'Intervention' (in particular), 'Urbanicity' and 'Site Sampling', were the most likely indicators of quality of reporting. The largest data gap identified was for tuberculosis vaccination cost data, the Bacillus Calmette-GuÃ©rin (BCG) vaccine in particular. There is a gap in available unit costs for 12 of 30 high tuberculosis burden countries, as well as for the interventions of above-service costs, tuberculosis prevention, and active and intensified case finding. Variability in the methods and reporting used makes comparison difficult and makes it hard for decision makers to know which unit costs they can trust. The study quality rating system used in this review as well as the classification tree enable focus on specific reporting aspects that should improve variability and increase confidence in unit costs. Researchers should endeavour to be explicit and transparent in how they cost interventions following the principles as laid out in the Global Health Cost Consortium's Reference Case for Estimating the Costs of Global Health Services and Interventions, which in turn will lead to repeatability, comparability and enhanced learning from others.","Cunnama, Gomez, Siapka, Herzel, Hill, Kairu, Levin, Okello, DeCormier Plosky, Garcia Baena, Sweeney, Vassall, Sinanovic","Cunnama, Gomez, Siapka, Herzel, Hill, Kairu, Levin, Okello, DeCormier Plosky, Garcia Baena, Sweeney, Vassall, Sinanovic",https://doi.org/10.1007/s40273-020-00910-w,https://doi.org/10.1007/s40273-020-00910-w,2021-08-03
16872.0,pubmed,pubmed,Assessment and application of the coherent point drift algorithm to augmented reality surgical navigation for laparoscopic partial nephrectomy,Assessment and application of the coherent point drift algorithm to augmented reality surgical navigation for laparoscopic partial nephrectomy,"The surface-based registration approach to laparoscopic augmented reality (AR) has clear advantages. Nonrigid point-set registration paves the way for surface-based registration. Among current non-rigid point set registration methods, the coherent point drift (CPD) algorithm is rarely used because of two challenges: (1) volumetric deformation is difficult to predict, and (2) registration from intraoperative visible tissue surface to whole anatomical preoperative model is a &quot;part-to-whole&quot; registration that CPD cannot be applied directly to. We preliminarily applied CPD on surgical navigation for laparoscopic partial nephrectomy (LPN). However, it introduces normalization errors and lacks navigation robustness. This paper presents important advances for more effectively applying CPD to LPN surgical navigation while attempting to quantitatively evaluate the accuracy of CPD-based surgical navigation. First, an optimized volumetric deformation (Op-VD) algorithm is proposed to achieve accurate prediction of volume deformation. Then, a projection-based partial selection method is presented to conveniently and robustly apply the CPD to LPN surgical navigation. Finally, kidneys with different deformations in vitro, phantom and in vivo experiments are performed to evaluate the accuracy and effectiveness of our approach. The average root-mean-square error of volume deformation was refined to 0.84 mm. The mean target registration error (TRE) of the surface and inside markers in the in vitro experiments decreased to 1.51 mm and 1.29 mm, respectively. The robustness and precision of CPD-based navigation were validated in phantom and in vivo experiments, and the mean navigation TRE of the phantom experiments was found to be [Formula: see text] mm. Accurate volumetric deformation and robust navigation results can be achieved in AR navigation of LPN by using surface-based registration with CPD. Evaluation results demonstrate the effectiveness of our proposed methods while showing the clinical application potential of CPD. This work has important guiding significance for the application of the CPD in laparoscopic AR.","The surface-based registration approach to laparoscopic augmented reality (AR) has clear advantages. Nonrigid point-set registration paves the way for surface-based registration. Among current non-rigid point set registration methods, the coherent point drift (CPD) algorithm is rarely used because of two challenges: (1) volumetric deformation is difficult to predict, and (2) registration from intraoperative visible tissue surface to whole anatomical preoperative model is a ""part-to-whole"" registration that CPD cannot be applied directly to. We preliminarily applied CPD on surgical navigation for laparoscopic partial nephrectomy (LPN). However, it introduces normalization errors and lacks navigation robustness. This paper presents important advances for more effectively applying CPD to LPN surgical navigation while attempting to quantitatively evaluate the accuracy of CPD-based surgical navigation. First, an optimized volumetric deformation (Op-VD) algorithm is proposed to achieve accurate prediction of volume deformation. Then, a projection-based partial selection method is presented to conveniently and robustly apply the CPD to LPN surgical navigation. Finally, kidneys with different deformations in vitro, phantom and in vivo experiments are performed to evaluate the accuracy and effectiveness of our approach. The average root-mean-square error of volume deformation was refined to 0.84 mm. The mean target registration error (TRE) of the surface and inside markers in the in vitro experiments decreased to 1.51 mm and 1.29 mm, respectively. The robustness and precision of CPD-based navigation were validated in phantom and in vivo experiments, and the mean navigation TRE of the phantom experiments was found to be [Formula: see text] mm. Accurate volumetric deformation and robust navigation results can be achieved in AR navigation of LPN by using surface-based registration with CPD. Evaluation results demonstrate the effectiveness of our proposed methods while showing the clinical application potential of CPD. This work has important guiding significance for the application of the CPD in laparoscopic AR.","Zhang, Wang, Zhang, Zhang, Wang","Zhang, Wang, Zhang, Zhang, Wang",https://doi.org/10.1007/s11548-020-02163-6,https://doi.org/10.1007/s11548-020-02163-6,2021-08-03
16880.0,pubmed,pubmed,Publicly available machine learning models for identifying opioid misuse from the clinical notes of hospitalized patients,Publicly available machine learning models for identifying opioid misuse from the clinical notes of hospitalized patients,"Automated de-identification methods for removing protected health information (PHI) from the source notes of the electronic health record (EHR) rely on building systems to recognize mentions of PHI in text, but they remain inadequate at ensuring perfect PHI removal. As an alternative to relying on de-identification systems, we propose the following solutions: (1) Mapping the corpus of documents to standardized medical vocabulary (concept unique identifier [CUI] codes mapped from the Unified Medical Language System) thus eliminating PHI as inputs to a machine learning model; and (2) training character-based machine learning models that obviate the need for a dictionary containing input words/n-grams. We aim to test the performance of models with and without PHI in a use-case for an opioid misuse classifier. An observational cohort sampled from adult hospital inpatient encounters at a health system between 2007 and 2017. A case-control stratified sampling (nÃ¢â‚¬â€°=Ã¢â‚¬â€°1000) was performed to build an annotated dataset for a reference standard of cases and non-cases of opioid misuse. Models for training and testing included CUI codes, character-based, and n-gram features. Models applied were machine learning with neural network and logistic regression as well as expert consensus with a rule-based model for opioid misuse. The area under the receiver operating characteristic curves (AUROC) were compared between models for discrimination. The Hosmer-Lemeshow test and visual plots measured model fit and calibration. Machine learning models with CUI codes performed similarly to n-gram models with PHI. The top performing models with AUROCs &gt;Ã¢â‚¬â€°0.90 included CUI codes as inputs to a convolutional neural network, max pooling network, and logistic regression model. The top calibrated models with the best model fit were the CUI-based convolutional neural network and max pooling network. The top weighted CUI codes in logistic regression has the related terms 'Heroin' and 'Victim of abuse'. We demonstrate good test characteristics for an opioid misuse computable phenotype that is void of any PHI and performs similarly to models that use PHI. Herein we share a PHI-free, trained opioid misuse classifier for other researchers and health systems to use and benchmark to overcome privacy and security concerns.","Automated de-identification methods for removing protected health information (PHI) from the source notes of the electronic health record (EHR) rely on building systems to recognize mentions of PHI in text, but they remain inadequate at ensuring perfect PHI removal. As an alternative to relying on de-identification systems, we propose the following solutions: (1) Mapping the corpus of documents to standardized medical vocabulary (concept unique identifier [CUI] codes mapped from the Unified Medical Language System) thus eliminating PHI as inputs to a machine learning model; and (2) training character-based machine learning models that obviate the need for a dictionary containing input words/n-grams. We aim to test the performance of models with and without PHI in a use-case for an opioid misuse classifier. An observational cohort sampled from adult hospital inpatient encounters at a health system between 2007 and 2017. A case-control stratified sampling (nâ€‰=â€‰1000) was performed to build an annotated dataset for a reference standard of cases and non-cases of opioid misuse. Models for training and testing included CUI codes, character-based, and n-gram features. Models applied were machine learning with neural network and logistic regression as well as expert consensus with a rule-based model for opioid misuse. The area under the receiver operating characteristic curves (AUROC) were compared between models for discrimination. The Hosmer-Lemeshow test and visual plots measured model fit and calibration. Machine learning models with CUI codes performed similarly to n-gram models with PHI. The top performing models with AUROCs &gt;â€‰0.90 included CUI codes as inputs to a convolutional neural network, max pooling network, and logistic regression model. The top calibrated models with the best model fit were the CUI-based convolutional neural network and max pooling network. The top weighted CUI codes in logistic regression has the related terms 'Heroin' and 'Victim of abuse'. We demonstrate good test characteristics for an opioid misuse computable phenotype that is void of any PHI and performs similarly to models that use PHI. Herein we share a PHI-free, trained opioid misuse classifier for other researchers and health systems to use and benchmark to overcome privacy and security concerns.","Sharma, Dligach, Swope, Salisbury-Afshar, Karnik, Joyce, Afshar","Sharma, Dligach, Swope, Salisbury-Afshar, Karnik, Joyce, Afshar",https://doi.org/10.1186/s12911-020-1099-y,https://doi.org/10.1186/s12911-020-1099-y,2021-08-03
16884.0,pubmed,pubmed,A Systematic Review of Health Dialog Systems,A Systematic Review of Health Dialog Systems,"Ã¢â‚¬Æ’Health dialog systems have seen increased adoption by patients, hospitals, and universities due to the confluence of advancements in machine learning and the ubiquity of high-performance hardware that supports real-time speech recognition, high-fidelity text-to-speech, and semantic understanding of natural language. Ã¢â‚¬Æ’This review seeks to enumerate opportunities to apply dialog systems toward the improvement of health outcomes while identifying both gaps in the current literature that may impede their implementation and recommendations that may improve their success in medical practice. Ã¢â‚¬Æ’A search over PubMed and the ACM Digital Library was conducted on September 12, 2017 to collect all articles related to dialog systems within the domain of health care. These results were screened for eligibility with the main criteria being a peer-reviewed study of a system that includes both a natural language interface and either end-user testing or practical implementation. Ã¢â‚¬Æ’Forty-six studies met the inclusion criteria including 24 quasi-experimental studies, 16 randomized control trials, 2 case-control studies, 2 prospective cohort studies, 1 system description, and 1 human-computer conversation analysis. These studies evaluated dialog systems in five application domains: medical education (<i>n</i>Ã¢â‚¬â€°=Ã¢â‚¬â€°20), clinical processes (<i>n</i>Ã¢â‚¬â€°=Ã¢â‚¬â€°14), mental health (<i>n</i>Ã¢â‚¬â€°=Ã¢â‚¬â€°5), personal health agents (<i>n</i>Ã¢â‚¬â€°=Ã¢â‚¬â€°5), and patient education (<i>n</i>Ã¢â‚¬â€°=Ã¢â‚¬â€°2). Ã¢â‚¬Æ’We found that dialog systems have been widely applied to health care; however, most studies are not reproducible making direct comparison between systems and independent confirmation of findings difficult. Widespread adoption will also require the adoption of standard evaluation and reporting methods for health dialog systems to demonstrate clinical significance.","â€ƒHealth dialog systems have seen increased adoption by patients, hospitals, and universities due to the confluence of advancements in machine learning and the ubiquity of high-performance hardware that supports real-time speech recognition, high-fidelity text-to-speech, and semantic understanding of natural language. â€ƒThis review seeks to enumerate opportunities to apply dialog systems toward the improvement of health outcomes while identifying both gaps in the current literature that may impede their implementation and recommendations that may improve their success in medical practice. â€ƒA search over PubMed and the ACM Digital Library was conducted on September 12, 2017 to collect all articles related to dialog systems within the domain of health care. These results were screened for eligibility with the main criteria being a peer-reviewed study of a system that includes both a natural language interface and either end-user testing or practical implementation. â€ƒForty-six studies met the inclusion criteria including 24 quasi-experimental studies, 16 randomized control trials, 2 case-control studies, 2 prospective cohort studies, 1 system description, and 1 human-computer conversation analysis. These studies evaluated dialog systems in five application domains: medical education (<i>n</i>â€‰=â€‰20), clinical processes (<i>n</i>â€‰=â€‰14), mental health (<i>n</i>â€‰=â€‰5), personal health agents (<i>n</i>â€‰=â€‰5), and patient education (<i>n</i>â€‰=â€‰2). â€ƒWe found that dialog systems have been widely applied to health care; however, most studies are not reproducible making direct comparison between systems and independent confirmation of findings difficult. Widespread adoption will also require the adoption of standard evaluation and reporting methods for health dialog systems to demonstrate clinical significance.","Kearns, Chi, Choi, Lin, Thompson, Demiris","Kearns, Chi, Choi, Lin, Thompson, Demiris",https://doi.org/10.1055/s-0040-1708807,https://doi.org/10.1055/s-0040-1708807,2021-08-03
16894.0,pubmed,pubmed,Precise proximal femur fracture classification for interactive training and surgical planning,Precise proximal femur fracture classification for interactive training and surgical planning,"Demonstrate the feasibility of a fully automatic computer-aided diagnosis (CAD) tool, based on deep learning, that localizes and classifies proximal femur fractures on X-ray images according to the AO classification. The proposed framework aims to improve patient treatment planning and provide support for the training of trauma surgeon residents. A database of 1347 clinical radiographic studies was collected. Radiologists and trauma surgeons annotated all fractures with bounding boxes and provided a classification according to the AO standard. In all experiments, the dataset was split patient-wise in three with the ratio 70%:10%:20% to build the training, validation and test sets, respectively. ResNet-50 and AlexNet architectures were implemented as deep learning classification and localization models, respectively. Accuracy, precision, recall and [Formula: see text]-score were reported as classification metrics. Retrieval of similar cases was evaluated in terms of precision and recall. The proposed CAD tool for the classification of radiographs into types &quot;A,&quot; &quot;B&quot; and &quot;not-fractured&quot; reaches a [Formula: see text]-score of 87% and AUC of 0.95. When classifying fractures versus not-fractured cases it improves up to 94% and 0.98. Prior localization of the fracture results in an improvement with respect to full-image classification. In total, 100% of the predicted centers of the region of interest are contained in the manually provided bounding boxes. The system retrieves on average 9 relevant images (from the same class) out of 10 cases. Our CAD scheme localizes, detects and further classifies proximal femur fractures achieving results comparable to expert-level and state-of-the-art performance. Our auxiliary localization model was highly accurate predicting the region of interest in the radiograph. We further investigated several strategies of verification for its adoption into the daily clinical routine. A sensitivity analysis of the size of the ROI and image retrieval as a clinical use case were presented.","Demonstrate the feasibility of a fully automatic computer-aided diagnosis (CAD) tool, based on deep learning, that localizes and classifies proximal femur fractures on X-ray images according to the AO classification. The proposed framework aims to improve patient treatment planning and provide support for the training of trauma surgeon residents. A database of 1347 clinical radiographic studies was collected. Radiologists and trauma surgeons annotated all fractures with bounding boxes and provided a classification according to the AO standard. In all experiments, the dataset was split patient-wise in three with the ratio 70%:10%:20% to build the training, validation and test sets, respectively. ResNet-50 and AlexNet architectures were implemented as deep learning classification and localization models, respectively. Accuracy, precision, recall and [Formula: see text]-score were reported as classification metrics. Retrieval of similar cases was evaluated in terms of precision and recall. The proposed CAD tool for the classification of radiographs into types ""A,"" ""B"" and ""not-fractured"" reaches a [Formula: see text]-score of 87% and AUC of 0.95. When classifying fractures versus not-fractured cases it improves up to 94% and 0.98. Prior localization of the fracture results in an improvement with respect to full-image classification. In total, 100% of the predicted centers of the region of interest are contained in the manually provided bounding boxes. The system retrieves on average 9 relevant images (from the same class) out of 10 cases. Our CAD scheme localizes, detects and further classifies proximal femur fractures achieving results comparable to expert-level and state-of-the-art performance. Our auxiliary localization model was highly accurate predicting the region of interest in the radiograph. We further investigated several strategies of verification for its adoption into the daily clinical routine. A sensitivity analysis of the size of the ROI and image retrieval as a clinical use case were presented.","JimÃƒÂ©nez-SÃƒÂ¡nchez, Kazi, Albarqouni, Kirchhoff, Biberthaler, Navab, Kirchhoff, Mateus","JimÃ©nez-SÃ¡nchez, Kazi, Albarqouni, Kirchhoff, Biberthaler, Navab, Kirchhoff, Mateus",https://doi.org/10.1007/s11548-020-02150-x,https://doi.org/10.1007/s11548-020-02150-x,2021-08-03
16898.0,pubmed,pubmed,Is there an Increased Risk for Unfavorable Obstetric Outcomes in Women with Endometriosis? An Evaluation of Evidences,Is there an Increased Risk for Unfavorable Obstetric Outcomes in Women with Endometriosis? An Evaluation of Evidences,"Ã¢â‚¬Æ’The present study is a systematic review of the literature to assess whether the presence of endometriosis determines or contributes to adverse obstetric outcomes. Ã¢â‚¬Æ’The present work was carried out at the Hospital Israelita Albert Einstein, SÃƒÂ£o Paulo, state of SÃƒÂ£o Paulo, Brazil, in accordance to the PRISMA methodology for systematic reviews. A review of the literature was performed using PubMed, Web of Science and Scopus databases. The keywords used were: <i>pregnancy</i><i>outcome</i>, <i>pregnancy</i><i>complications</i>, <i>obstetrical</i><i>complications</i>, <i>obstetrics</i>, <i>obstetric</i><i>outcomes</i> and <i>endometriosis</i>. The survey was further completed by a manually executed review of cross-referenced articles, which was last performed on November 30, 2018. Ã¢â‚¬Æ’The survey disclosed a total of 2,468 articles, published from May 1946 to October 2017. A total of 18 studies were selected to be further classified according to their quality and relevance. Ã¢â‚¬Æ’The Newcastle-Ottawa Quality Assessment Scale was used for classification. Five studies of greater impact and superior evidence quality and 13 studies of moderate evidence quality were selected. We analyzed the studies for the characteristics of their patients plus how endometriosis was diagnosed and their respective obstetric outcomes taking into account their statistical relevance. Ã¢â‚¬Æ’Analyses of the higher impact and better quality studies have shown high incidence of preterm birth and placenta previa in patients with endometriosis. Ã¢â‚¬Æ’Placenta previa and preterm birth are the most statistically significant outcomes related to endometriosis, as indicated by our systematic review. The present information is useful to alert obstetricians and patients about possible unfavorable obstetric outcomes. Ã¢â‚¬Æ’Realizar uma revisÃƒÂ£o sistemÃƒÂ¡tica e crÃƒÂ­tica da literatura de modo a avaliar se a presenÃƒÂ§a de endometriose determina desfechos obstÃƒÂ©tricos adversos na gestaÃƒÂ§ÃƒÂ£o. Ã¢â‚¬Æ’O presente estudo foi realizado no Hospital Israelita Albert Einstein, SÃƒÂ£o Paulo, SP, Brasil, de acordo com a metodologia PRISMA para revisÃƒÂµes sistemÃƒÂ¡ticas. As bases de dados usadas para a revisÃƒÂ£o de literatura foram Pubmed, Web of Science e Scopus. As palavras-chave usadas foram: <i>pregnancy</i><i>outcome</i>, <i>pregnancy</i><i>complications</i>, <i>obstetrical</i><i>complications</i>, <i>obstetrics</i>, <i>obstetric</i><i>outcomes</i> e <i>endometriosis</i>. Uma revisÃƒÂ£o manual de artigos com referÃƒÂªncias cruzadas completou a pesquisa, que foi realizada pela ÃƒÂºltima vez em 30 de novembro de 2018. SELEÃƒÂ§ÃƒÂ£O DOS ESTUDOS: Ã¢â‚¬Æ’A pesquisa contou com o total de 2.468 artigos, publicados de maio de 1946 a outubro de 2017. Foram selecionados 18 estudos com base em sua relevÃƒÂ¢ncia. Ã¢â‚¬Æ’A metodologia NewcastleÃ¢â‚¬â€œOttawa Quality Assessment Scale foi usada para selecionar 5 estudos cuja evidÃƒÂªncia era de melhor qualidade e 13 estudos de moderada qualidade de evidÃƒÂªncia. As caracterÃƒÂ­sticas das populaÃƒÂ§ÃƒÂµes dos estudos foram analisadas, assim como a doenÃƒÂ§a endometriose foi diagnosticada e os respectivos desfechos obstÃƒÂ©tricos nas pacientes observando-se a relevÃƒÂ¢ncia estatÃƒÂ­stica dos estudos. SÃƒÂ­NTESE DOS DADOS: Ã¢â‚¬Æ’A anÃƒÂ¡lise dos estudos de maior impacto e de melhor qualidade de evidÃƒÂªncia mostram que placenta prÃƒÂ©via e ocorrÃƒÂªncia de nascimentos prÃƒÂ©-termo sÃƒÂ£o os desfechos obstÃƒÂ©tricos desfavorÃƒÂ¡veis de maior incidÃƒÂªncia em pacientes com endometriose. CONCLUSÃƒÂ£O: Ã¢â‚¬Æ’Placenta prÃƒÂ©via e nascimentos prÃƒÂ©-termo sÃƒÂ£o os desfechos obstÃƒÂ©tricos com maior significÃƒÂ¢ncia estatÃƒÂ­stica relacionados Ãƒ endometriose. Esta informaÃƒÂ§ÃƒÂ£o ÃƒÂ© ÃƒÂºtil para alertar obstetras e pacientes com endometriose para possÃƒÂ­veis desfechos obstÃƒÂ©tricos desfavorÃƒÂ¡veis.","â€ƒThe present study is a systematic review of the literature to assess whether the presence of endometriosis determines or contributes to adverse obstetric outcomes. â€ƒThe present work was carried out at the Hospital Israelita Albert Einstein, SÃ£o Paulo, state of SÃ£o Paulo, Brazil, in accordance to the PRISMA methodology for systematic reviews. A review of the literature was performed using PubMed, Web of Science and Scopus databases. The keywords used were: <i>pregnancy</i> <i>outcome</i>, <i>pregnancy</i> <i>complications</i>, <i>obstetrical</i> <i>complications</i>, <i>obstetrics</i>, <i>obstetric</i> <i>outcomes</i> and <i>endometriosis</i>. The survey was further completed by a manually executed review of cross-referenced articles, which was last performed on November 30, 2018. â€ƒThe survey disclosed a total of 2,468 articles, published from May 1946 to October 2017. A total of 18 studies were selected to be further classified according to their quality and relevance. â€ƒThe Newcastle-Ottawa Quality Assessment Scale was used for classification. Five studies of greater impact and superior evidence quality and 13 studies of moderate evidence quality were selected. We analyzed the studies for the characteristics of their patients plus how endometriosis was diagnosed and their respective obstetric outcomes taking into account their statistical relevance. â€ƒAnalyses of the higher impact and better quality studies have shown high incidence of preterm birth and placenta previa in patients with endometriosis. â€ƒPlacenta previa and preterm birth are the most statistically significant outcomes related to endometriosis, as indicated by our systematic review. The present information is useful to alert obstetricians and patients about possible unfavorable obstetric outcomes. â€ƒRealizar uma revisÃ£o sistemÃ¡tica e crÃ­tica da literatura de modo a avaliar se a presenÃ§a de endometriose determina desfechos obstÃ©tricos adversos na gestaÃ§Ã£o. â€ƒO presente estudo foi realizado no Hospital Israelita Albert Einstein, SÃ£o Paulo, SP, Brasil, de acordo com a metodologia PRISMA para revisÃµes sistemÃ¡ticas. As bases de dados usadas para a revisÃ£o de literatura foram Pubmed, Web of Science e Scopus. As palavras-chave usadas foram: <i>pregnancy</i> <i>outcome</i>, <i>pregnancy</i> <i>complications</i>, <i>obstetrical</i> <i>complications</i>, <i>obstetrics</i>, <i>obstetric</i> <i>outcomes</i> e <i>endometriosis</i>. Uma revisÃ£o manual de artigos com referÃªncias cruzadas completou a pesquisa, que foi realizada pela Ãºltima vez em 30 de novembro de 2018. SELEÃ§Ã£O DOS ESTUDOS: â€ƒA pesquisa contou com o total de 2.468 artigos, publicados de maio de 1946 a outubro de 2017. Foram selecionados 18 estudos com base em sua relevÃ¢ncia. â€ƒA metodologia Newcastleâ€“Ottawa Quality Assessment Scale foi usada para selecionar 5 estudos cuja evidÃªncia era de melhor qualidade e 13 estudos de moderada qualidade de evidÃªncia. As caracterÃ­sticas das populaÃ§Ãµes dos estudos foram analisadas, assim como a doenÃ§a endometriose foi diagnosticada e os respectivos desfechos obstÃ©tricos nas pacientes observando-se a relevÃ¢ncia estatÃ­stica dos estudos. SÃ­NTESE DOS DADOS: â€ƒA anÃ¡lise dos estudos de maior impacto e de melhor qualidade de evidÃªncia mostram que placenta prÃ©via e ocorrÃªncia de nascimentos prÃ©-termo sÃ£o os desfechos obstÃ©tricos desfavorÃ¡veis de maior incidÃªncia em pacientes com endometriose. CONCLUSÃ£O: â€ƒPlacenta prÃ©via e nascimentos prÃ©-termo sÃ£o os desfechos obstÃ©tricos com maior significÃ¢ncia estatÃ­stica relacionados Ã endometriose. Esta informaÃ§Ã£o Ã© Ãºtil para alertar obstetras e pacientes com endometriose para possÃ­veis desfechos obstÃ©tricos desfavorÃ¡veis.","Annicchino, Malvezzi, Piccinato, Podgaec","Annicchino, Malvezzi, Piccinato, Podgaec",https://doi.org/10.1055/s-0040-1708885,https://doi.org/10.1055/s-0040-1708885,2021-08-03
16901.0,pubmed,pubmed,A meta-analysis and systematic review of venous thromboembolism prophylaxis in patients undergoing vascular surgery procedures,A meta-analysis and systematic review of venous thromboembolism prophylaxis in patients undergoing vascular surgery procedures,"Perioperative venous thromboembolism (VTE) is generally considered preventable. Whereas the non-vascular surgery literature is rich in providing data about the impact of VTE prophylaxis on VTE outcomes, vascular surgery data are relatively sparse on this topic. This study sought to evaluate the evidence for VTE prophylaxis specifically for the population of vascular surgery patients. A systematic search was conducted in MEDLINE, Cochrane, and Embase databases in December 2018. Included were studies reporting primary and secondary outcomes for common vascular surgery procedures (open aortic operation, endovascular aneurysm repair [EVAR], peripheral artery bypass, amputation, venous reflux operation). A meta-analysis was performed comparing the patients who did not receive VTE prophylaxis and had VTE complications with patients who developed VTE despite receiving prophylaxis. From 3757 uniquely identified articles, 42 publications met the criteria for inclusion in this review (1 for the category of all vascular operations, 5 for open aortic reconstructions, 2 for EVAR, 1 for open aortic surgery or EVAR, 3 for abdominal or bypass surgery, 2 for peripheral bypass surgery, 2 for amputations, 1 for vascular trauma, and 25 for surgical treatment of superficial venous disease). Five studies met the criteria for inclusion in the meta-analysis. The results demonstrated slightly lower relative risk for development of VTE among patients receiving VTE prophylaxis (relative risk, 0.70; 95% confidence interval, 0.26-1.87). After open aortic reconstruction, the risk of VTE is 13% to 18% and is not reduced by VTE prophylaxis. For EVAR patients, the risk of VTE without prophylaxis is 6%. For patients undergoing peripheral bypass surgery and not receiving therapeutic or prophylactic anticoagulation, the risk of VTE isÃ‚Â &lt;2%. For patients undergoing amputations, VTE prophylaxis reduces the risk of VTE. For patients undergoing surgical treatment of superficial venous disease, there is an abundance of literature exploring the utility of VTE prophylaxis, but the evidence is conflicting; some studies demonstrated a benefit, whereas others showed no reduction of VTE with prophylaxis. Overall, there is a paucity of literature that addresses the effectiveness of VTE prophylaxis specifically in the population of vascular surgery patients. Our meta-analysis of the literature does not demonstrate a statistically significant benefit of VTE prophylaxis among the vascular surgery patients evaluated; however, it does suggest a low incidence of VTE among patients who receive VTE prophylaxis. Clinicians should identify the patients at high risk for development of postoperative VTE as the risk-benefit ratio may favor VTE prophylaxis in a selected group of patients. Clinicians should use their judgment and established VTE risk prediction models to assess VTE risk for patients. Vascular surgeons should consider reporting VTE incidence as a secondary outcome in publications.","Perioperative venous thromboembolism (VTE) is generally considered preventable. Whereas the non-vascular surgery literature is rich in providing data about the impact of VTE prophylaxis on VTE outcomes, vascular surgery data are relatively sparse on this topic. This study sought to evaluate the evidence for VTE prophylaxis specifically for the population of vascular surgery patients. A systematic search was conducted in MEDLINE, Cochrane, and Embase databases in December 2018. Included were studies reporting primary and secondary outcomes for common vascular surgery procedures (open aortic operation, endovascular aneurysm repair [EVAR], peripheral artery bypass, amputation, venous reflux operation). A meta-analysis was performed comparing the patients who did not receive VTE prophylaxis and had VTE complications with patients who developed VTE despite receiving prophylaxis. From 3757 uniquely identified articles, 42 publications met the criteria for inclusion in this review (1 for the category of all vascular operations, 5 for open aortic reconstructions, 2 for EVAR, 1 for open aortic surgery or EVAR, 3 for abdominal or bypass surgery, 2 for peripheral bypass surgery, 2 for amputations, 1 for vascular trauma, and 25 for surgical treatment of superficial venous disease). Five studies met the criteria for inclusion in the meta-analysis. The results demonstrated slightly lower relative risk for development of VTE among patients receiving VTE prophylaxis (relative risk, 0.70; 95% confidence interval, 0.26-1.87). After open aortic reconstruction, the risk of VTE is 13% to 18% and is not reduced by VTE prophylaxis. For EVAR patients, the risk of VTE without prophylaxis is 6%. For patients undergoing peripheral bypass surgery and not receiving therapeutic or prophylactic anticoagulation, the risk of VTE isÂ &lt;2%. For patients undergoing amputations, VTE prophylaxis reduces the risk of VTE. For patients undergoing surgical treatment of superficial venous disease, there is an abundance of literature exploring the utility of VTE prophylaxis, but the evidence is conflicting; some studies demonstrated a benefit, whereas others showed no reduction of VTE with prophylaxis. Overall, there is a paucity of literature that addresses the effectiveness of VTE prophylaxis specifically in the population of vascular surgery patients. Our meta-analysis of the literature does not demonstrate a statistically significant benefit of VTE prophylaxis among the vascular surgery patients evaluated; however, it does suggest a low incidence of VTE among patients who receive VTE prophylaxis. Clinicians should identify the patients at high risk for development of postoperative VTE as the risk-benefit ratio may favor VTE prophylaxis in a selected group of patients. Clinicians should use their judgment and established VTE risk prediction models to assess VTE risk for patients. Vascular surgeons should consider reporting VTE incidence as a secondary outcome in publications.","Toth, Flohr, Schubart, Knehans, Castello, Aziz","Toth, Flohr, Schubart, Knehans, Castello, Aziz",https://doi.org/10.1016/j.jvsv.2020.03.017,https://doi.org/10.1016/j.jvsv.2020.03.017,2021-08-03
16903.0,pubmed,pubmed,Clinician-recalled quoted speech in electronic health records and risk of suicide attempt: a case-crossover study,Clinician-recalled quoted speech in electronic health records and risk of suicide attempt: a case-crossover study,"Clinician narrative style in electronic health records (EHR) has rarely been investigated. Clinicians sometimes record brief quotations from patients, possibly more frequently when higher risk is perceived. We investigated whether the frequency of quoted phrases in an EHR was higher in time periods closer to a suicide attempt. A case-crossover study was conducted in a large mental health records database. A natural language processing tool was developed using regular expression matching to identify text occurring within quotation marks in the EHR. Electronic records from a large mental healthcare provider serving a geographic catchment of 1.3Ã¢â‚¬â€°million residents in South London were linked with hospitalisation data. 1503 individuals were identified as having a hospitalised suicide attempt from 1 April 2006 to 31 March 2017 with at least one document in both the case period (1-30 days prior to admission) and the control period (61-90 days prior to admission). The number of quoted phrases in the control as compared with the case period. Both attended (OR 1.05, 95%Ã¢â‚¬â€°CI 1.02 to 1.08) and non-attended (OR 1.15, 95%Ã¢â‚¬â€°CI 1.04 to 1.26) clinical appointments were independently higher in the case compared with control period, while there was no difference in mental healthcare hospitalisation (OR 0.99, 95%Ã¢â‚¬â€°CI 0.98 to 1.01). In addition, there was no difference in the levels of quoted text between the comparison time periods (OR 1.09, 95%Ã¢â‚¬â€°CI 0.91 to 1.30). This study successfully developed an algorithm to identify quoted speech in text fields from routine mental healthcare records. Contrary to the hypothesis, no association between this exposure and proximity to a suicide attempt was found; however, further evaluation is warranted on the way in which clinician-perceived risk might be feasibly characterised from clinical text.","Clinician narrative style in electronic health records (EHR) has rarely been investigated. Clinicians sometimes record brief quotations from patients, possibly more frequently when higher risk is perceived. We investigated whether the frequency of quoted phrases in an EHR was higher in time periods closer to a suicide attempt. A case-crossover study was conducted in a large mental health records database. A natural language processing tool was developed using regular expression matching to identify text occurring within quotation marks in the EHR. Electronic records from a large mental healthcare provider serving a geographic catchment of 1.3â€‰million residents in South London were linked with hospitalisation data. 1503 individuals were identified as having a hospitalised suicide attempt from 1 April 2006 to 31 March 2017 with at least one document in both the case period (1-30 days prior to admission) and the control period (61-90 days prior to admission). The number of quoted phrases in the control as compared with the case period. Both attended (OR 1.05, 95%â€‰CI 1.02 to 1.08) and non-attended (OR 1.15, 95%â€‰CI 1.04 to 1.26) clinical appointments were independently higher in the case compared with control period, while there was no difference in mental healthcare hospitalisation (OR 0.99, 95%â€‰CI 0.98 to 1.01). In addition, there was no difference in the levels of quoted text between the comparison time periods (OR 1.09, 95%â€‰CI 0.91 to 1.30). This study successfully developed an algorithm to identify quoted speech in text fields from routine mental healthcare records. Contrary to the hypothesis, no association between this exposure and proximity to a suicide attempt was found; however, further evaluation is warranted on the way in which clinician-perceived risk might be feasibly characterised from clinical text.","Jayasinghe, Bittar, Dutta, Stewart","Jayasinghe, Bittar, Dutta, Stewart",https://doi.org/10.1136/bmjopen-2019-036186,https://doi.org/10.1136/bmjopen-2019-036186,2021-08-03
16908.0,pubmed,pubmed,Artificial Intelligence in radiotherapy: state of the art and future directions,Artificial Intelligence in radiotherapy: state of the art and future directions,"Recent advances in computing capability allowed the development of sophisticated predictive models to assess complex relationships within observational data, described as Artificial Intelligence. Medicine is one of the several fields of application and Radiation oncology could benefit from these approaches, particularly in patients' medical records, imaging, baseline pathology, planning or instrumental data. Artificial Intelligence systems could simplify many steps of the complex workflow of radiotherapy such as segmentation, planning or delivery. However, Artificial Intelligence could be considered as a &quot;black box&quot; in which human operator may only understand input and output predictions and its application to the clinical practice remains a challenge. The low transparency of the overall system is questionable from manifold points of view (ethical included). Given the complexity of this issue, we collected the basic definitions to help the clinician to understand current literature, and overviewed experiences regarding implementation of AI within radiotherapy clinical workflow, aiming to describe this field from the clinician perspective.","Recent advances in computing capability allowed the development of sophisticated predictive models to assess complex relationships within observational data, described as Artificial Intelligence. Medicine is one of the several fields of application and Radiation oncology could benefit from these approaches, particularly in patients' medical records, imaging, baseline pathology, planning or instrumental data. Artificial Intelligence systems could simplify many steps of the complex workflow of radiotherapy such as segmentation, planning or delivery. However, Artificial Intelligence could be considered as a ""black box"" in which human operator may only understand input and output predictions and its application to the clinical practice remains a challenge. The low transparency of the overall system is questionable from manifold points of view (ethical included). Given the complexity of this issue, we collected the basic definitions to help the clinician to understand current literature, and overviewed experiences regarding implementation of AI within radiotherapy clinical workflow, aiming to describe this field from the clinician perspective.","Francolini, Desideri, Stocchi, Salvestrini, Ciccone, Garlatti, Loi, Livi","Francolini, Desideri, Stocchi, Salvestrini, Ciccone, Garlatti, Loi, Livi",https://doi.org/10.1007/s12032-020-01374-w,https://doi.org/10.1007/s12032-020-01374-w,2021-08-03
16911.0,pubmed,pubmed,A Hybrid Reporting Platform for Extended RadLex Coding Combining Structured Reporting Templates and Natural Language Processing,A Hybrid Reporting Platform for Extended RadLex Coding Combining Structured Reporting Templates and Natural Language Processing,"Structured reporting is a favorable and sustainable form of reporting in radiology. Among its advantages are better presentation, clearer nomenclature, and higher quality. By using MRRT-compliant templates, the content of the categorized items (e.g., select fields) can be automatically stored in a database, which allows further research and quality analytics based on established ontologies like RadLexÃ‚Â® linked to the items. Additionally, it is relevant to provide free-text input for descriptions of findings and impressions in complex imaging studies or for the information included with the clinical referral. So far, however, this unstructured content cannot be categorized. We developed a solution to analyze and code these free-text parts of the templates in our MRRT-compliant reporting platform, using natural language processing (NLP) with RadLexÃ‚Â® terms in addition to the already categorized items. The established hybrid reporting concept is working successfully. The NLP tool provides RadLexÃ‚Â® codes with modifiers (affirmed, speculated, negated). Radiologists can confirm or reject codes provided by NLP before finalizing the structured report. Furthermore, users can suggest RadLexÃ‚Â® codes from free text that is not correctly coded with NLP or can suggest to change the modifier. Analyzing free-text fields took 1.23Ã‚Â s on average. Hybrid reporting enables coding of free-text information in our MRRT-compliant templates and thus increases the amount of categorized data that can be stored in the database. This enhances the possibilities for further analyses, such as correlating clinical information with radiological findings or storing high-quality structured information for machine-learning approaches.","Structured reporting is a favorable and sustainable form of reporting in radiology. Among its advantages are better presentation, clearer nomenclature, and higher quality. By using MRRT-compliant templates, the content of the categorized items (e.g., select fields) can be automatically stored in a database, which allows further research and quality analytics based on established ontologies like RadLexÂ® linked to the items. Additionally, it is relevant to provide free-text input for descriptions of findings and impressions in complex imaging studies or for the information included with the clinical referral. So far, however, this unstructured content cannot be categorized. We developed a solution to analyze and code these free-text parts of the templates in our MRRT-compliant reporting platform, using natural language processing (NLP) with RadLexÂ® terms in addition to the already categorized items. The established hybrid reporting concept is working successfully. The NLP tool provides RadLexÂ® codes with modifiers (affirmed, speculated, negated). Radiologists can confirm or reject codes provided by NLP before finalizing the structured report. Furthermore, users can suggest RadLexÂ® codes from free text that is not correctly coded with NLP or can suggest to change the modifier. Analyzing free-text fields took 1.23Â s on average. Hybrid reporting enables coding of free-text information in our MRRT-compliant templates and thus increases the amount of categorized data that can be stored in the database. This enhances the possibilities for further analyses, such as correlating clinical information with radiological findings or storing high-quality structured information for machine-learning approaches.","Jungmann, Arnhold, KÃƒÂ¤mpgen, Jorg, DÃƒÂ¼ber, Mildenberger, Kloeckner","Jungmann, Arnhold, KÃ¤mpgen, Jorg, DÃ¼ber, Mildenberger, Kloeckner",https://doi.org/10.1007/s10278-020-00342-0,https://doi.org/10.1007/s10278-020-00342-0,2021-08-03
16915.0,pubmed,pubmed,Translating research findings into clinical practice: a systematic and critical review of neuroimaging-based clinical tools for brain disorders,Translating research findings into clinical practice: a systematic and critical review of neuroimaging-based clinical tools for brain disorders,"A pivotal aim of psychiatric and neurological research is to promote the translation of the findings into clinical practice to improve diagnostic and prognostic assessment of individual patients. Structural neuroimaging holds much promise, with neuroanatomical measures accounting for up to 40% of the variance in clinical outcome. Building on these findings, a number of imaging-based clinical tools have been developed to make diagnostic and prognostic inferences about individual patients from their structural Magnetic Resonance Imaging scans. This systematic review describes and compares the technical characteristics of the available tools, with the aim to assess their translational potential into real-world clinical settings. The results reveal that a total of eight tools. All of these were specifically developed for neurological disorders, and as such are not suitable for application to psychiatric disorders. Furthermore, most of the tools were trained and validated in a single dataset, which can result in poor generalizability, or using a small number of individuals, which can cause overoptimistic results. In addition, all of the tools rely on two strategies to detect brain abnormalities in single individuals, one based on univariate comparison, and the other based on multivariate machine-learning algorithms. We discuss current barriers to the adoption of these tools in clinical practice and propose a checklist of pivotal characteristics that should be included in an &quot;ideal&quot; neuroimaging-based clinical tool for brain disorders.","A pivotal aim of psychiatric and neurological research is to promote the translation of the findings into clinical practice to improve diagnostic and prognostic assessment of individual patients. Structural neuroimaging holds much promise, with neuroanatomical measures accounting for up to 40% of the variance in clinical outcome. Building on these findings, a number of imaging-based clinical tools have been developed to make diagnostic and prognostic inferences about individual patients from their structural Magnetic Resonance Imaging scans. This systematic review describes and compares the technical characteristics of the available tools, with the aim to assess their translational potential into real-world clinical settings. The results reveal that a total of eight tools. All of these were specifically developed for neurological disorders, and as such are not suitable for application to psychiatric disorders. Furthermore, most of the tools were trained and validated in a single dataset, which can result in poor generalizability, or using a small number of individuals, which can cause overoptimistic results. In addition, all of the tools rely on two strategies to detect brain abnormalities in single individuals, one based on univariate comparison, and the other based on multivariate machine-learning algorithms. We discuss current barriers to the adoption of these tools in clinical practice and propose a checklist of pivotal characteristics that should be included in an ""ideal"" neuroimaging-based clinical tool for brain disorders.","Scarpazza, Ha, Baecker, Garcia-Dias, Pinaya, Vieira, Mechelli","Scarpazza, Ha, Baecker, Garcia-Dias, Pinaya, Vieira, Mechelli",https://doi.org/10.1038/s41398-020-0798-6,https://doi.org/10.1038/s41398-020-0798-6,2021-08-03
16916.0,pubmed,pubmed,Magnetic resonance imaging features of tumor and lymph node to predict clinical outcome in node-positive cervical cancer: a retrospective analysis,Magnetic resonance imaging features of tumor and lymph node to predict clinical outcome in node-positive cervical cancer: a retrospective analysis,"Current chemoradiation regimens for locally advanced cervical cancer are fairly uniform despite a profound diversity of treatment response and recurrence patterns. The wide range of treatment responses and prognoses to standardized concurrent chemoradiation highlights the need for a reliable tool to predict treatment outcomes. We investigated pretreatment magnetic resonance (MR) imaging features of primary tumor and involved lymph node for predicting clinical outcome in cervical cancer patients. We included 93 node-positive cervical cancer patients treated with definitive chemoradiotherapy at our institution between 2006 and 2017. The median follow-up period was 38Ã¢â‚¬â€°months (range, 5-128). Primary tumor and involved lymph node were manually segmented on axial gadolinium-enhanced T1-weighted images as well as T2-weighted images and saved as 3-dimensional regions of interest (ROI). After the segmentation, imaging features related to histogram, shape, and texture were extracted from each ROI. Using these features, random survival forest (RSF) models were built to predict local control (LC), regional control (RC), distant metastasis-free survival (DMFS), and overall survival (OS) in the training dataset (nÃ¢â‚¬â€°=Ã¢â‚¬â€°62). The generated models were then tested in the validation dataset (nÃ¢â‚¬â€°=Ã¢â‚¬â€°31). For predicting LC, models generated from primary tumor imaging features showed better predictive performance (C-index, 0.72) than those from lymph node features (C-index, 0.62). In contrast, models from lymph nodes showed superior performance for predicting RC, DMFS, and OS compared to models of the primary tumor. According to the 3-year time-dependent receiver operating characteristic analysis of LC, RC, DMFS, and OS prediction, the respective area under the curve values for the predicted risk of the models generated from the training dataset were 0.634, 0.796, 0.733, and 0.749 in the validation dataset. Our results suggest that tumor and lymph node imaging features may play complementary roles for predicting clinical outcomes in node-positive cervical cancer.","Current chemoradiation regimens for locally advanced cervical cancer are fairly uniform despite a profound diversity of treatment response and recurrence patterns. The wide range of treatment responses and prognoses to standardized concurrent chemoradiation highlights the need for a reliable tool to predict treatment outcomes. We investigated pretreatment magnetic resonance (MR) imaging features of primary tumor and involved lymph node for predicting clinical outcome in cervical cancer patients. We included 93 node-positive cervical cancer patients treated with definitive chemoradiotherapy at our institution between 2006 and 2017. The median follow-up period was 38â€‰months (range, 5-128). Primary tumor and involved lymph node were manually segmented on axial gadolinium-enhanced T1-weighted images as well as T2-weighted images and saved as 3-dimensional regions of interest (ROI). After the segmentation, imaging features related to histogram, shape, and texture were extracted from each ROI. Using these features, random survival forest (RSF) models were built to predict local control (LC), regional control (RC), distant metastasis-free survival (DMFS), and overall survival (OS) in the training dataset (nâ€‰=â€‰62). The generated models were then tested in the validation dataset (nâ€‰=â€‰31). For predicting LC, models generated from primary tumor imaging features showed better predictive performance (C-index, 0.72) than those from lymph node features (C-index, 0.62). In contrast, models from lymph nodes showed superior performance for predicting RC, DMFS, and OS compared to models of the primary tumor. According to the 3-year time-dependent receiver operating characteristic analysis of LC, RC, DMFS, and OS prediction, the respective area under the curve values for the predicted risk of the models generated from the training dataset were 0.634, 0.796, 0.733, and 0.749 in the validation dataset. Our results suggest that tumor and lymph node imaging features may play complementary roles for predicting clinical outcomes in node-positive cervical cancer.","Park, Hahm, Bae, Chong, Jeong, Na, Jeong, Kim","Park, Hahm, Bae, Chong, Jeong, Na, Jeong, Kim",https://doi.org/10.1186/s13014-020-01502-w,https://doi.org/10.1186/s13014-020-01502-w,2021-08-03
16944.0,pubmed,pubmed,Online Assessment of Applied Anatomy Knowledge: The Effect of Images on Medical Students' Performance,Online Assessment of Applied Anatomy Knowledge: The Effect of Images on Medical Students' Performance,"Anatomical examinations have been designed to assess topographical and/or applied knowledge of anatomy with or without the inclusion of visual resources such as cadaveric specimens or images, radiological images, and/or clinical photographs. Multimedia learning theories have advanced the understanding of how words and images are processed during learning. However, the evidence of the impact of including anatomical and radiological images within written assessments is sparse. This study investigates the impact of including images within clinically oriented single-best-answer questions on students' scores in a tailored online tool. Second-year medical students (nÃ‚Â =Ã‚Â 174) from six schools in the United Kingdom participated voluntarily in the examination, and 55 students provided free-text comments which were thematically analyzed. All questions were categorized as to whether their stimulus format was purely textual or included an associated image. The type (anatomical and radiological image) and deep structure of images (question referring to a bone or soft tissue on the image) were taken into consideration. Students scored significantly better on questions with images compared to questions without images (PÃ‚Â &lt;Ã‚Â 0.001), and on questions referring to bones than to soft tissue (PÃ‚Â &lt;Ã‚Â 0.001), but no difference was found in their performance on anatomical and radiological image questions. The coding highlighted areas of &quot;test applicability&quot; and &quot;challenges faced by the students.&quot; In conclusion, images are critical in medical practice for investigating a patient's anatomy, and this study sets out a way to understand the effects of images on students' performance and their views in commonly employed written assessments.","Anatomical examinations have been designed to assess topographical and/or applied knowledge of anatomy with or without the inclusion of visual resources such as cadaveric specimens or images, radiological images, and/or clinical photographs. Multimedia learning theories have advanced the understanding of how words and images are processed during learning. However, the evidence of the impact of including anatomical and radiological images within written assessments is sparse. This study investigates the impact of including images within clinically oriented single-best-answer questions on students' scores in a tailored online tool. Second-year medical students (nÂ =Â 174) from six schools in the United Kingdom participated voluntarily in the examination, and 55 students provided free-text comments which were thematically analyzed. All questions were categorized as to whether their stimulus format was purely textual or included an associated image. The type (anatomical and radiological image) and deep structure of images (question referring to a bone or soft tissue on the image) were taken into consideration. Students scored significantly better on questions with images compared to questions without images (PÂ &lt;Â 0.001), and on questions referring to bones than to soft tissue (PÂ &lt;Â 0.001), but no difference was found in their performance on anatomical and radiological image questions. The coding highlighted areas of ""test applicability"" and ""challenges faced by the students."" In conclusion, images are critical in medical practice for investigating a patient's anatomy, and this study sets out a way to understand the effects of images on students' performance and their views in commonly employed written assessments.","Sagoo, Vorstenbosch, Bazira, Ellis, Kambouri, Owen","Sagoo, Vorstenbosch, Bazira, Ellis, Kambouri, Owen",https://doi.org/10.1002/ase.1965,https://doi.org/10.1002/ase.1965,2021-08-03
16949.0,pubmed,pubmed,Artificial intelligence-assisted prediction of preeclampsia: Development and external validation of a nationwide health insurance dataset of the BPJS Kesehatan in Indonesia,Artificial intelligence-assisted prediction of preeclampsia: Development and external validation of a nationwide health insurance dataset of the BPJS Kesehatan in Indonesia,"We developed and validated an artificial intelligence (AI)-assisted prediction of preeclampsia applied to a nationwide health insurance dataset in Indonesia. The BPJS Kesehatan dataset have been preprocessed using a nested case-control design into preeclampsia/eclampsia (nÃ‚Â =Ã‚Â 3318) and normotensive pregnant women (nÃ‚Â =Ã‚Â 19,883) from all women with one pregnancy. The dataset provided 95 features consisting of demographic variables and medical histories started from 24 months to event and ended by delivery as the event. Six algorithms were compared by area under the receiver operating characteristics curve (AUROC) with a subgroup analysis by time to the event. We compared our model to similar prediction models from systematically reviewed studies. In addition, we conducted a text mining analysis based on natural language processing techniques to interpret our modeling results. The best model consisted of 17 predictors extracted by a random forest algorithm. NineÃ¢Ë†Â¼12 months to the event was the period that had the best AUROC in external validation by either geographical (0.88, 95% confidence interval (CI) 0.88-0.89) or temporal split (0.86, 95% CI 0.85-0.86). We compared this model to prediction models in seven studies from 869 records in PUBMED, EMBASE, and SCOPUS. This model outperformed the previous models in terms of the precision, sensitivity, and specificity in all validation sets. Our low-cost model improved preliminary prediction to decide pregnant women that will be predicted by the models with high specificity and advanced predictors. This work was supported by grant no. MOST108-2221-E-038-018 from the Ministry of Science and Technology of Taiwan.","We developed and validated an artificial intelligence (AI)-assisted prediction of preeclampsia applied to a nationwide health insurance dataset in Indonesia. The BPJS Kesehatan dataset have been preprocessed using a nested case-control design into preeclampsia/eclampsia (nÂ =Â 3318) and normotensive pregnant women (nÂ =Â 19,883) from all women with one pregnancy. The dataset provided 95 features consisting of demographic variables and medical histories started from 24 months to event and ended by delivery as the event. Six algorithms were compared by area under the receiver operating characteristics curve (AUROC) with a subgroup analysis by time to the event. We compared our model to similar prediction models from systematically reviewed studies. In addition, we conducted a text mining analysis based on natural language processing techniques to interpret our modeling results. The best model consisted of 17 predictors extracted by a random forest algorithm. Nineâˆ¼12 months to the event was the period that had the best AUROC in external validation by either geographical (0.88, 95% confidence interval (CI) 0.88-0.89) or temporal split (0.86, 95% CI 0.85-0.86). We compared this model to prediction models in seven studies from 869 records in PUBMED, EMBASE, and SCOPUS. This model outperformed the previous models in terms of the precision, sensitivity, and specificity in all validation sets. Our low-cost model improved preliminary prediction to decide pregnant women that will be predicted by the models with high specificity and advanced predictors. This work was supported by grant no. MOST108-2221-E-038-018 from the Ministry of Science and Technology of Taiwan.","Sufriyana, Wu, Su","Sufriyana, Wu, Su",https://doi.org/10.1016/j.ebiom.2020.102710,https://doi.org/10.1016/j.ebiom.2020.102710,2021-08-03
16952.0,pubmed,pubmed,Interdisciplinarity: An essential requirement for translation of radiomics research into clinical practice -a systematic review focused on thoracic oncology,Interdisciplinarity: An essential requirement for translation of radiomics research into clinical practice -a systematic review focused on thoracic oncology,"Recently, evidence has accumulated that demonstrates the potential for future applications of radiomics in many clinical settings, including thoracic oncology. Methodological reasons for the immaturity of image mining (radiomics and artificial intelligence-based) studies have been identified. However, data on the influence of the composition of the research team on the quality of investigations in radiomics are lacking. This review aims to evaluate the interdisciplinarity within studies on radiomics in thoracic oncology in order to assess its influence on the quality of research (QUADAS-2 score) in the image mining field. We considered for inclusion radiomics investigations with objectives relating to clinical practice in thoracic oncology. Subsequently, we interviewed the corresponding authors. The field of expertise and/or educational degree was then used to assess interdisciplinarity. Subsequently, all studies were evaluated applying the QUADAS-2 score and assigned to a research phase from 0 to IV. Overall, 27 studies were included. The study quality according to the QUADAS-2 score was low (score Ã¢â€°Â¤5) in 8, moderate (=6) in 12, and high (Ã¢â€°Â¥7) in 7 papers. An interdisciplinary team (at least 3 different expertise categories) was involved in half of the papers without any type of validation and in all papers with independent validation. Clinicians were not involved in phase 0 studies while they contributed to all papers classified as phase I and to 4/5 papers classified as phase II with independent validation. The composition of the research team influences the quality of investigations in radiomics. Also, growth in interdisciplinarity appears to reflect research development from the early phase to a more mature, clinically oriented stage of investigation.","Recently, evidence has accumulated that demonstrates the potential for future applications of radiomics in many clinical settings, including thoracic oncology. Methodological reasons for the immaturity of image mining (radiomics and artificial intelligence-based) studies have been identified. However, data on the influence of the composition of the research team on the quality of investigations in radiomics are lacking. This review aims to evaluate the interdisciplinarity within studies on radiomics in thoracic oncology in order to assess its influence on the quality of research (QUADAS-2 score) in the image mining field. We considered for inclusion radiomics investigations with objectives relating to clinical practice in thoracic oncology. Subsequently, we interviewed the corresponding authors. The field of expertise and/or educational degree was then used to assess interdisciplinarity. Subsequently, all studies were evaluated applying the QUADAS-2 score and assigned to a research phase from 0 to IV. Overall, 27 studies were included. The study quality according to the QUADAS-2 score was low (score â‰¤5) in 8, moderate (=6) in 12, and high (â‰¥7) in 7 papers. An interdisciplinary team (at least 3 different expertise categories) was involved in half of the papers without any type of validation and in all papers with independent validation. Clinicians were not involved in phase 0 studies while they contributed to all papers classified as phase I and to 4/5 papers classified as phase II with independent validation. The composition of the research team influences the quality of investigations in radiomics. Also, growth in interdisciplinarity appears to reflect research development from the early phase to a more mature, clinically oriented stage of investigation.","Sollini, Gelardi, Matassa, Delgado Bolton, Chiti, Kirienko","Sollini, Gelardi, Matassa, Delgado Bolton, Chiti, Kirienko",https://doi.org/10.1016/j.remn.2019.10.003,https://doi.org/10.1016/j.remn.2019.10.003,2021-08-03
16962.0,pubmed,pubmed,Diagnostic accuracy of texture analysis and machine learning for quantification of liver fibrosis in MRI: correlation with MR elastography and histopathology,Diagnostic accuracy of texture analysis and machine learning for quantification of liver fibrosis in MRI: correlation with MR elastography and histopathology,"To compare the diagnostic accuracy of texture analysis (TA)-derived parameters combined with machine learning (ML) of non-contrast-enhanced T1w and T2w fat-saturated (fs) images with MR elastography (MRE) for liver fibrosis quantification. In this IRB-approved prospective study, liver MRIs of participants with suspected chronic liver disease who underwent liver biopsy between August 2015 and May 2018 were analyzed. Two readers blinded to clinical and histopathological findings performed TA. The participants were categorized into no or low-stage (0-2) and high-stage (3-4) fibrosis groups. Confusion matrices were calculated using a support vector machine combined with principal component analysis. The diagnostic accuracy of ML-based TA of liver fibrosis and MRE was assessed by area under the receiver operating characteristic curves (AUC). Histopathology served as reference standard. A total of 62 consecutive participants (40 men; mean age Ã‚Â± standard deviation, 48Ã¢â‚¬â€°Ã‚Â±Ã¢â‚¬â€°13Ã‚Â years) were included. The accuracy of TA and ML on T1w was 85.7% (95% confidence interval [CI] 63.7-97.0) and 61.9% (95% CI 38.4-81.9) on T2w fs for classification of liver fibrosis into low-stage and high-stage fibrosis. The AUC for TA on T1w was similar to MRE (0.82 [95% CI 0.59-0.95] vs. 0.92 [95% CI 0.71-0.99], pÃ¢â‚¬â€°=Ã¢â‚¬â€°0.41), while the AUC for T2w fs was significantly lower compared to MRE (0.57 [95% CI 0.34-0.78] vs. 0.92 [95% CI 0.71-0.99], pÃ¢â‚¬â€°=Ã¢â‚¬â€°0.008). Our results suggest that liver fibrosis can be quantified with TA-derived parameters of T1w when combined with a ML algorithm with similar accuracy compared to MRE. Ã¢â‚¬Â¢ Liver fibrosis can be categorized into low-stage fibrosis (0-2) and high-stage fibrosis (3-4) using texture analysis-derived parameters of T1-weighted images with a machine learning approach. Ã¢â‚¬Â¢ For the differentiation of low-stage fibrosis and high-stage fibrosis, the diagnostic accuracy of texture analysis on T1-weighted images combined with a machine learning algorithm is similar compared to MR elastography.","To compare the diagnostic accuracy of texture analysis (TA)-derived parameters combined with machine learning (ML) of non-contrast-enhanced T1w and T2w fat-saturated (fs) images with MR elastography (MRE) for liver fibrosis quantification. In this IRB-approved prospective study, liver MRIs of participants with suspected chronic liver disease who underwent liver biopsy between August 2015 and May 2018 were analyzed. Two readers blinded to clinical and histopathological findings performed TA. The participants were categorized into no or low-stage (0-2) and high-stage (3-4) fibrosis groups. Confusion matrices were calculated using a support vector machine combined with principal component analysis. The diagnostic accuracy of ML-based TA of liver fibrosis and MRE was assessed by area under the receiver operating characteristic curves (AUC). Histopathology served as reference standard. A total of 62 consecutive participants (40 men; mean age Â± standard deviation, 48â€‰Â±â€‰13Â years) were included. The accuracy of TA and ML on T1w was 85.7% (95% confidence interval [CI] 63.7-97.0) and 61.9% (95% CI 38.4-81.9) on T2w fs for classification of liver fibrosis into low-stage and high-stage fibrosis. The AUC for TA on T1w was similar to MRE (0.82 [95% CI 0.59-0.95] vs. 0.92 [95% CI 0.71-0.99], pâ€‰=â€‰0.41), while the AUC for T2w fs was significantly lower compared to MRE (0.57 [95% CI 0.34-0.78] vs. 0.92 [95% CI 0.71-0.99], pâ€‰=â€‰0.008). Our results suggest that liver fibrosis can be quantified with TA-derived parameters of T1w when combined with a ML algorithm with similar accuracy compared to MRE. â€¢ Liver fibrosis can be categorized into low-stage fibrosis (0-2) and high-stage fibrosis (3-4) using texture analysis-derived parameters of T1-weighted images with a machine learning approach. â€¢ For the differentiation of low-stage fibrosis and high-stage fibrosis, the diagnostic accuracy of texture analysis on T1-weighted images combined with a machine learning algorithm is similar compared to MR elastography.","Schawkat, Ciritsis, von Ulmenstein, Honcharova-Biletska, JÃƒÂ¼ngst, Weber, Gubler, Mertens, Reiner","Schawkat, Ciritsis, von Ulmenstein, Honcharova-Biletska, JÃ¼ngst, Weber, Gubler, Mertens, Reiner",https://doi.org/10.1007/s00330-020-06831-8,https://doi.org/10.1007/s00330-020-06831-8,2021-08-03
16964.0,pubmed,pubmed,A low-cost texture-based pipeline for predicting myocardial tissue remodeling and fibrosis using cardiac ultrasound,A low-cost texture-based pipeline for predicting myocardial tissue remodeling and fibrosis using cardiac ultrasound,"Maturation of ultrasound myocardial tissue characterization may have far-reaching implications as a widely available alternative to cardiac magnetic resonance (CMR) for risk stratification in left ventricular (LV) remodeling. We extracted 328 texture-based features of myocardium from still ultrasound images. After we explored the phenotypes of myocardial textures using unsupervised similarity networks, global LV remodeling parameters were predicted using supervised machine learning models. Separately, we also developed supervised models for predicting the presence of myocardial fibrosis using another cohort who underwent cardiac magnetic resonance (CMR). For the prediction, patients were divided into a training and test set (80:20). Texture-based tissue feature extraction was feasible in 97% of total 534 patients. Interpatient similarity analysis delineated two patient groups based on the texture features: one group had more advanced LV remodeling parameters compared to the other group. Furthermore, this group was associated with a higher incidence of cardiac deaths (pÃ‚Â =Ã‚Â 0.001) and major adverse cardiac events (p &lt; 0.001). The supervised models predicted reduced LV ejection fraction (&lt;50%) and global longitudinal strain (&lt;16%) with area under the receiver-operator-characteristics curves (ROC AUC) of 0.83 and 0.87 in the hold-out test set, respectively. Furthermore, the presence of myocardial fibrosis was predicted from only ultrasound myocardial texture with an ROC AUC of 0.84 (sensitivity 86.4% and specificity 83.3%) in the test set. Ultrasound texture-based myocardial tissue characterization identified phenotypic features of LV remodeling from still ultrasound images. Further clinical validation may address critical barriers in the adoption of ultrasound techniques for myocardial tissue characterization. None.","Maturation of ultrasound myocardial tissue characterization may have far-reaching implications as a widely available alternative to cardiac magnetic resonance (CMR) for risk stratification in left ventricular (LV) remodeling. We extracted 328 texture-based features of myocardium from still ultrasound images. After we explored the phenotypes of myocardial textures using unsupervised similarity networks, global LV remodeling parameters were predicted using supervised machine learning models. Separately, we also developed supervised models for predicting the presence of myocardial fibrosis using another cohort who underwent cardiac magnetic resonance (CMR). For the prediction, patients were divided into a training and test set (80:20). Texture-based tissue feature extraction was feasible in 97% of total 534 patients. Interpatient similarity analysis delineated two patient groups based on the texture features: one group had more advanced LV remodeling parameters compared to the other group. Furthermore, this group was associated with a higher incidence of cardiac deaths (pÂ =Â 0.001) and major adverse cardiac events (p &lt; 0.001). The supervised models predicted reduced LV ejection fraction (&lt;50%) and global longitudinal strain (&lt;16%) with area under the receiver-operator-characteristics curves (ROC AUC) of 0.83 and 0.87 in the hold-out test set, respectively. Furthermore, the presence of myocardial fibrosis was predicted from only ultrasound myocardial texture with an ROC AUC of 0.84 (sensitivity 86.4% and specificity 83.3%) in the test set. Ultrasound texture-based myocardial tissue characterization identified phenotypic features of LV remodeling from still ultrasound images. Further clinical validation may address critical barriers in the adoption of ultrasound techniques for myocardial tissue characterization. None.","Kagiyama, Shrestha, Cho, Khalil, Singh, Challa, Casaclang-Verzosa, Sengupta","Kagiyama, Shrestha, Cho, Khalil, Singh, Challa, Casaclang-Verzosa, Sengupta",https://doi.org/10.1016/j.ebiom.2020.102726,https://doi.org/10.1016/j.ebiom.2020.102726,2021-08-03
16965.0,pubmed,pubmed,The impact of deep convolutional neural network-based artificial intelligence on colonoscopy outcomes: A systematic review with meta-analysis,The impact of deep convolutional neural network-based artificial intelligence on colonoscopy outcomes: A systematic review with meta-analysis,"The utility of artificial intelligence (AI) in colonoscopy has gained popularity in current times. Recent trials have evaluated the efficacy of deep convolutional neural network (DCNN)-based AI system in colonoscopy for improving adenoma detection rate (ADR) and polyp detection rate (PDR). We performed a systematic review and meta-analysis of the available studies to assess the impact of DCNN-based AI-assisted colonoscopy in improving the ADR and PDR. We queried the following database for this study: PubMed, Embase, Cochrane Library, Web of Sciences, and Computers and Applied Sciences. We only included randomized controlled trials that compared AI colonoscopy to standard colonoscopy (SC). Our outcomes included ADR and PDR. Risk ratios (RR) with 95% confidence interval (CI) were calculated using random effects model and DerSimonian-Laird approach for each outcome. A total of three studies with 2815 patients (1415 in SC group and 1400 in AI group) were included. AI colonoscopy resulted in significantly improved ADR (32.9% vs 20.8%, RR: 1.58, 95% CI 1.39-1.80, PÃ‚Â =Ã‚Â &lt;Ã‚Â 0.001) and PDR (43.0% vs 27.8%, RR: 1.55, 95% CI 1.39-1.72, PÃ‚Â =Ã‚Â &lt;Ã‚Â 0.001) compared with SC. Given the results and limitations, the utility of AI colonoscopy holds promise and should be evaluated in more randomized controlled trials across different population, especially in patients solely undergoing colonoscopy for screening purpose as improved ADR will ultimately help in reducing incident colorectal cancer.","The utility of artificial intelligence (AI) in colonoscopy has gained popularity in current times. Recent trials have evaluated the efficacy of deep convolutional neural network (DCNN)-based AI system in colonoscopy for improving adenoma detection rate (ADR) and polyp detection rate (PDR). We performed a systematic review and meta-analysis of the available studies to assess the impact of DCNN-based AI-assisted colonoscopy in improving the ADR and PDR. We queried the following database for this study: PubMed, Embase, Cochrane Library, Web of Sciences, and Computers and Applied Sciences. We only included randomized controlled trials that compared AI colonoscopy to standard colonoscopy (SC). Our outcomes included ADR and PDR. Risk ratios (RR) with 95% confidence interval (CI) were calculated using random effects model and DerSimonian-Laird approach for each outcome. A total of three studies with 2815 patients (1415 in SC group and 1400 in AI group) were included. AI colonoscopy resulted in significantly improved ADR (32.9% vs 20.8%, RR: 1.58, 95% CI 1.39-1.80, PÂ =Â &lt;Â 0.001) and PDR (43.0% vs 27.8%, RR: 1.55, 95% CI 1.39-1.72, PÂ =Â &lt;Â 0.001) compared with SC. Given the results and limitations, the utility of AI colonoscopy holds promise and should be evaluated in more randomized controlled trials across different population, especially in patients solely undergoing colonoscopy for screening purpose as improved ADR will ultimately help in reducing incident colorectal cancer.","Aziz, Fatima, Dong, Lee-Smith, Nawras","Aziz, Fatima, Dong, Lee-Smith, Nawras",https://doi.org/10.1111/jgh.15070,https://doi.org/10.1111/jgh.15070,2021-08-03
16971.0,pubmed,pubmed,Evidence synthesis relevant to COVID-19: a protocol for multiple systematic reviews and overviews of systematic reviews,Evidence synthesis relevant to COVID-19: a protocol for multiple systematic reviews and overviews of systematic reviews,"The evidence on COVID-19 is being produced at high speed, so it is challenging for decision-makers to keep up. It seems appropriate, then, to put into practice a novel approach able to provide the scientific community and other interested parties with quality evidence that is actionable, and rapidly and efficiently produced. We designed a protocol for multiple parallel systematic reviews and overviews of systematic reviews in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols (PRISMA-P). We will search for primary studies and systematic reviews that answer different questions related to COVID-19 using both a centralized repository (Epistemonikos database) and a manual search in MEDLINE/PubMed, EMBASE, and the Cochrane Central Register of Controlled Trials. We will also search for literature in several other sources. At least two researchers will independently undertake the selection of studies, data extraction, and assessment of the quality of the included studies. We will synthesize data for each question using meta-analysis, when possible, and we will prepare Summary of Findings tables according to the GRADE approach. All the evidence will be organized in an open platform (LÃ‚Â·OVE - Living OVerview of Evidence) that will be continuously updated using artificial intelligence and a broad network of experts. No ethics approval is considered necessary. The results of these articles will be widely disseminated via peer-reviewed publications, social networks, and traditional media, and will be sent to relevant international organizations discussing this topic.","The evidence on COVID-19 is being produced at high speed, so it is challenging for decision-makers to keep up. It seems appropriate, then, to put into practice a novel approach able to provide the scientific community and other interested parties with quality evidence that is actionable, and rapidly and efficiently produced. We designed a protocol for multiple parallel systematic reviews and overviews of systematic reviews in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Protocols (PRISMA-P). We will search for primary studies and systematic reviews that answer different questions related to COVID-19 using both a centralized repository (Epistemonikos database) and a manual search in MEDLINE/PubMed, EMBASE, and the Cochrane Central Register of Controlled Trials. We will also search for literature in several other sources. At least two researchers will independently undertake the selection of studies, data extraction, and assessment of the quality of the included studies. We will synthesize data for each question using meta-analysis, when possible, and we will prepare Summary of Findings tables according to the GRADE approach. All the evidence will be organized in an open platform (LÂ·OVE - Living OVerview of Evidence) that will be continuously updated using artificial intelligence and a broad network of experts. No ethics approval is considered necessary. The results of these articles will be widely disseminated via peer-reviewed publications, social networks, and traditional media, and will be sent to relevant international organizations discussing this topic.","Rada, Verdugo-Paiva, ÃƒÂvila, Morel-Marambio, Bravo-Jeria, Pesce, Madrid, Izcovich, Almendra-Pegueros, Alvares, Araneda, ÃƒÂvila, Baladia, Bohorquez-Blanco, Bravo-Jeria, Buhring-Bonacich, Carrasco-CarrÃƒÂ©, Carvajal-JuliÃƒÂ¡, Cuadrado, Ferrada, Flores, Franco, Garnham, Garroz, Gempeler-Rojas, Goez-Mogollon, Gonzalez-AlarcÃƒÂ³n, Izcovich, Madrid, MarquÃƒÂ©s, MartÃƒÂ­nez, Meza, Morel-Marambio, Neumann, Ojeda, OlguÃƒÂ­n, Ortiz-MuÃƒÂ±oz, Pesce, PeÃƒÂ±a, Pizarro, Poloni, Prieto, PÃƒÂ©rez, PÃƒÂ©rez-Bracchiglione, PÃƒÂ©rez-Gaxiola, Rada, Ragusa, Rojas, Santillan-Garcia, SepÃƒÂºlveda, Torres-LÃƒÂ³pez, Urrea, Vargas-Peirano, Verdejo, Verdugo-Paiva, Vergara, Villar","Rada, Verdugo-Paiva, Ãvila, Morel-Marambio, Bravo-Jeria, Pesce, Madrid, Izcovich, Almendra-Pegueros, Alvares, Araneda, Ãvila, Baladia, Bohorquez-Blanco, Bravo-Jeria, Buhring-Bonacich, Carrasco-CarrÃ©, Carvajal-JuliÃ¡, Cuadrado, Ferrada, Flores, Franco, Garnham, Garroz, Gempeler-Rojas, Goez-Mogollon, Gonzalez-AlarcÃ³n, Izcovich, Madrid, MarquÃ©s, MartÃ­nez, Meza, Morel-Marambio, Neumann, Ojeda, OlguÃ­n, Ortiz-MuÃ±oz, Pesce, PeÃ±a, Pizarro, Poloni, Prieto, PÃ©rez, PÃ©rez-Bracchiglione, PÃ©rez-Gaxiola, Rada, Ragusa, Rojas, Santillan-Garcia, SepÃºlveda, Torres-LÃ³pez, Urrea, Vargas-Peirano, Verdejo, Verdugo-Paiva, Vergara, Villar",https://doi.org/10.5867/medwave.2020.03.7867,https://doi.org/10.5867/medwave.2020.03.7867,2021-08-03
16973.0,pubmed,pubmed,Economic Evaluation of Chimeric Antigen Receptor T-Cell Therapy by Site of Care Among Patients With Relapsed or Refractory Large B-Cell Lymphoma,Economic Evaluation of Chimeric Antigen Receptor T-Cell Therapy by Site of Care Among Patients With Relapsed or Refractory Large B-Cell Lymphoma,"Chimeric antigen receptor (CAR) T-cell therapies are currently administered at a limited number of cancer centers and are primarily delivered in an inpatient setting. However, variations in total costs associated with these therapies remain unknown. To estimate the economic differences in the administration of CAR T-cell therapy by the site of care and the incidence of key adverse events. A decision-tree model was designed to capture clinical outcomes and associated costs during a predefined period (from lymphodepletion to 30 days after the receipt of CAR T-cell infusion) to account for the potential incidence of acute adverse events and to evaluate variations in total costs for the administration of CAR T-cell therapy by site of care. Cost estimates were from the health care practitioner perspective and were based on data obtained from the literature and publicly available databases, including the Healthcare Cost and Utilization Project National Inpatient Sample, the Medicare Hospital Outpatient Prospective Payment System, the Medicare physician fee schedule, the Centers for Medicare and Medicaid Services Healthcare Common Procedure Coding System, and the IBM Micromedex RED BOOK. The model evaluated an average adult patient with relapsed or refractory large B-cell lymphoma who received CAR T-cell therapy in an academic inpatient hospital or nonacademic specialty oncology network. The administration of CAR T-cell therapy. Total cost of the administration of CAR T-cell therapy by site of care. The costs associated with lymphodepletion, acquisition and infusion of CAR T cells, and management of acute adverse events were also examined. The estimated total cost of care associated with the administration of CAR T-cell therapy was $454Ã¢â‚¬Â¯611 (95% CI, $452Ã¢â‚¬Â¯466-$458Ã¢â‚¬Â¯267) in the academic hospital inpatient setting compared with $421Ã¢â‚¬Â¯624 (95% CI, $417Ã¢â‚¬Â¯204-$422Ã¢â‚¬Â¯325) in the nonacademic specialty oncology network setting, for a difference of $32Ã¢â‚¬Â¯987. After excluding the CAR T-cell acquisition cost, hospitalization and office visit costs were $53Ã¢â‚¬Â¯360 (65.3% of the total cost) in academic inpatient hospitals and $23Ã¢â‚¬Â¯526 (48.4% of the total cost) in nonacademic specialty oncology networks. The administration of CAR T-cell therapy in nonacademic specialty oncology networks was associated with a $29Ã¢â‚¬Â¯834 (55.9%) decrease in hospitalization and office visit costs and a $3154 (20.1%) decrease in procedure costs. The potential availability of CAR T-cell therapies that are associated with a lower incidence of adverse events and are suitable for outpatient administration may reduce the total costs of care by enabling the use of these therapies in nonacademic specialty oncology networks.","Chimeric antigen receptor (CAR) T-cell therapies are currently administered at a limited number of cancer centers and are primarily delivered in an inpatient setting. However, variations in total costs associated with these therapies remain unknown. To estimate the economic differences in the administration of CAR T-cell therapy by the site of care and the incidence of key adverse events. A decision-tree model was designed to capture clinical outcomes and associated costs during a predefined period (from lymphodepletion to 30 days after the receipt of CAR T-cell infusion) to account for the potential incidence of acute adverse events and to evaluate variations in total costs for the administration of CAR T-cell therapy by site of care. Cost estimates were from the health care practitioner perspective and were based on data obtained from the literature and publicly available databases, including the Healthcare Cost and Utilization Project National Inpatient Sample, the Medicare Hospital Outpatient Prospective Payment System, the Medicare physician fee schedule, the Centers for Medicare and Medicaid Services Healthcare Common Procedure Coding System, and the IBM Micromedex RED BOOK. The model evaluated an average adult patient with relapsed or refractory large B-cell lymphoma who received CAR T-cell therapy in an academic inpatient hospital or nonacademic specialty oncology network. The administration of CAR T-cell therapy. Total cost of the administration of CAR T-cell therapy by site of care. The costs associated with lymphodepletion, acquisition and infusion of CAR T cells, and management of acute adverse events were also examined. The estimated total cost of care associated with the administration of CAR T-cell therapy was $454â€¯611 (95% CI, $452â€¯466-$458â€¯267) in the academic hospital inpatient setting compared with $421â€¯624 (95% CI, $417â€¯204-$422â€¯325) in the nonacademic specialty oncology network setting, for a difference of $32â€¯987. After excluding the CAR T-cell acquisition cost, hospitalization and office visit costs were $53â€¯360 (65.3% of the total cost) in academic inpatient hospitals and $23â€¯526 (48.4% of the total cost) in nonacademic specialty oncology networks. The administration of CAR T-cell therapy in nonacademic specialty oncology networks was associated with a $29â€¯834 (55.9%) decrease in hospitalization and office visit costs and a $3154 (20.1%) decrease in procedure costs. The potential availability of CAR T-cell therapies that are associated with a lower incidence of adverse events and are suitable for outpatient administration may reduce the total costs of care by enabling the use of these therapies in nonacademic specialty oncology networks.","Lyman, Nguyen, Snyder, Gitlin, Chung","Lyman, Nguyen, Snyder, Gitlin, Chung",https://doi.org/10.1001/jamanetworkopen.2020.2072,https://doi.org/10.1001/jamanetworkopen.2020.2072,2021-08-03
16974.0,pubmed,pubmed,Association of Physical Education With Improvement of Health-Related Physical Fitness Outcomes and Fundamental Motor Skills Among Youths: A Systematic Review and Meta-analysis,Association of Physical Education With Improvement of Health-Related Physical Fitness Outcomes and Fundamental Motor Skills Among Youths: A Systematic Review and Meta-analysis,"Whether quality- or quantity-based physical education (PE) interventions are associated with improvement of health-related physical fitness outcomes and fundamental motor skills (FMSs) in children and adolescents is unknown. To examine the association of interventions aimed at optimizing PE in terms of quality (teaching strategies or fitness infusion) or quantity (lessons per week) with health-related physical fitness and FMSs in children and adolescents. For this systematic review and meta-analysis, studies were identified through a systematic search of Ovid MEDLINE, Embase, Cochrane Controlled Trials Registry, and SPORTDiscus databases (from inception to October 10, 2019) with the keywords physical education OR PE OR P.E. AND fitness AND motor ability OR skills. Manual examination of references in selected articles was also performed. Studies that assessed the association of quality- or quantity-based PE interventions with improvement in physical fitness and/or FMSs in youths (aged 3-18 years) were included. Data were processed according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guideline. Random-effects models were used to estimate the pooled effect size (Hedges g). Health-related physical fitness outcomes and FMSs. Fifty-six trials composed of 48Ã¢â‚¬Â¯185 youths (48% girls) were included in the meta-analysis. Quality-based PE interventions were associated with small increases in health-related physical fitness (cardiorespiratory fitness [Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°0.24; 95% CI, 0.16-0.32] and muscular strength [Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°0.19; 95% CI, 0.09-0.29]) and FMSs (Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°0.38; 95% CI, 0.27-0.49). Subgroup analyses found stronger associations for quality-based PE interventions on body mass index (Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.18; 95% CI, -0.26 to -0.09), body fat (Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°-0.28; 95% CI, -0.37 to -0.18), cardiorespiratory fitness (Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°0.31; 95% CI, 0.23-0.39), and muscular strength (Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°0.29; 95% CI, 0.18-0.39). Quantity-based PE interventions were associated with small increases in only cardiorespiratory fitness (Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°0.42; 95% CI, 0.30-0.55), muscular strength (Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°0.20; 95% CI, 0.08-0.31), and speed agility (Hedges gÃ¢â‚¬â€°=Ã¢â‚¬â€°0.29; 95% CI, 0.07-0.51). The findings suggest that quality-based PE interventions are associated with small increases in both student health-related physical fitness components and FMSs regardless of frequency or duration of PE lessons. Because PE aims to improve more than health, high levels of active learning time may need to be balanced with opportunities for instruction, feedback, and reflection.","Whether quality- or quantity-based physical education (PE) interventions are associated with improvement of health-related physical fitness outcomes and fundamental motor skills (FMSs) in children and adolescents is unknown. To examine the association of interventions aimed at optimizing PE in terms of quality (teaching strategies or fitness infusion) or quantity (lessons per week) with health-related physical fitness and FMSs in children and adolescents. For this systematic review and meta-analysis, studies were identified through a systematic search of Ovid MEDLINE, Embase, Cochrane Controlled Trials Registry, and SPORTDiscus databases (from inception to October 10, 2019) with the keywords physical education OR PE OR P.E. AND fitness AND motor ability OR skills. Manual examination of references in selected articles was also performed. Studies that assessed the association of quality- or quantity-based PE interventions with improvement in physical fitness and/or FMSs in youths (aged 3-18 years) were included. Data were processed according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guideline. Random-effects models were used to estimate the pooled effect size (Hedges g). Health-related physical fitness outcomes and FMSs. Fifty-six trials composed of 48â€¯185 youths (48% girls) were included in the meta-analysis. Quality-based PE interventions were associated with small increases in health-related physical fitness (cardiorespiratory fitness [Hedges gâ€‰=â€‰0.24; 95% CI, 0.16-0.32] and muscular strength [Hedges gâ€‰=â€‰0.19; 95% CI, 0.09-0.29]) and FMSs (Hedges gâ€‰=â€‰0.38; 95% CI, 0.27-0.49). Subgroup analyses found stronger associations for quality-based PE interventions on body mass index (Hedges gâ€‰=â€‰-0.18; 95% CI, -0.26 to -0.09), body fat (Hedges gâ€‰=â€‰-0.28; 95% CI, -0.37 to -0.18), cardiorespiratory fitness (Hedges gâ€‰=â€‰0.31; 95% CI, 0.23-0.39), and muscular strength (Hedges gâ€‰=â€‰0.29; 95% CI, 0.18-0.39). Quantity-based PE interventions were associated with small increases in only cardiorespiratory fitness (Hedges gâ€‰=â€‰0.42; 95% CI, 0.30-0.55), muscular strength (Hedges gâ€‰=â€‰0.20; 95% CI, 0.08-0.31), and speed agility (Hedges gâ€‰=â€‰0.29; 95% CI, 0.07-0.51). The findings suggest that quality-based PE interventions are associated with small increases in both student health-related physical fitness components and FMSs regardless of frequency or duration of PE lessons. Because PE aims to improve more than health, high levels of active learning time may need to be balanced with opportunities for instruction, feedback, and reflection.","GarcÃƒÂ­a-Hermoso, Alonso-MartÃƒÂ­nez, RamÃƒÂ­rez-VÃƒÂ©lez, PÃƒÂ©rez-Sousa, RamÃƒÂ­rez-Campillo, Izquierdo","GarcÃ­a-Hermoso, Alonso-MartÃ­nez, RamÃ­rez-VÃ©lez, PÃ©rez-Sousa, RamÃ­rez-Campillo, Izquierdo",https://doi.org/10.1001/jamapediatrics.2020.0223,https://doi.org/10.1001/jamapediatrics.2020.0223,2021-08-03
16976.0,pubmed,pubmed,Radiomics in gliomas: clinical implications of computational modeling and fractal-based analysis,Radiomics in gliomas: clinical implications of computational modeling and fractal-based analysis,"Radiomics is an emerging field that involves extraction and quantification of features from medical images. These data can be mined through computational analysis and models to identify predictive image biomarkers that characterize intra-tumoral dynamics throughout the course of treatment. This is particularly difficult in gliomas, where heterogeneity has been well established at a molecular level as well as visually in conventional imaging. Thus, acquiring clinically useful features remains difficult due to temporal variations in tumor dynamics. Identifying surrogate biomarkers through radiomics may provide a non-invasive means of characterizing biologic activities of gliomas. We present an extensive literature review of radiomics-based analysis, with a particular focus on computational modeling, machine learning, and fractal-based analysis in improving differential diagnosis and predicting clinical outcomes. Novel strategies in extracting quantitative features, segmentation methods, and their clinical applications are producingÃ‚Â promising results. Moreover, we provide a detailed summary of the morphometric parameters that have so far been proposed as a means of quantifying imaging characteristics of gliomas. Newly emerging radiomic techniques via machine learning and fractal-based analyses holds considerable potential for improving diagnostic and prognostic accuracy of gliomas. Key pointsÃ¢â‚¬Â¢ Radiomic features can be mined through computational analysis to produce quantitative imaging biomarkers that characterize intra-tumoral dynamics throughout the course of treatment.Ã¢â‚¬Â¢ Surrogate image biomarkers identified through radiomics could enable a non-invasive means of characterizing biologic activities of gliomas.Ã¢â‚¬Â¢ With novel analytic algorithms, quantification of morphological or sub-regional tumor features to predict survival outcomes is producing promising results.Ã¢â‚¬Â¢ Quantifying intra-tumoral heterogeneity may improve grading and molecular sub-classifications of gliomas.Ã¢â‚¬Â¢ Computational fractal-based analysis of gliomas allows geometrical evaluation of tumor irregularities and complexity, leading to novel techniques for tumor segmentation, grading, and therapeutic monitoring.","Radiomics is an emerging field that involves extraction and quantification of features from medical images. These data can be mined through computational analysis and models to identify predictive image biomarkers that characterize intra-tumoral dynamics throughout the course of treatment. This is particularly difficult in gliomas, where heterogeneity has been well established at a molecular level as well as visually in conventional imaging. Thus, acquiring clinically useful features remains difficult due to temporal variations in tumor dynamics. Identifying surrogate biomarkers through radiomics may provide a non-invasive means of characterizing biologic activities of gliomas. We present an extensive literature review of radiomics-based analysis, with a particular focus on computational modeling, machine learning, and fractal-based analysis in improving differential diagnosis and predicting clinical outcomes. Novel strategies in extracting quantitative features, segmentation methods, and their clinical applications are producingÂ promising results. Moreover, we provide a detailed summary of the morphometric parameters that have so far been proposed as a means of quantifying imaging characteristics of gliomas. Newly emerging radiomic techniques via machine learning and fractal-based analyses holds considerable potential for improving diagnostic and prognostic accuracy of gliomas. Key pointsâ€¢ Radiomic features can be mined through computational analysis to produce quantitative imaging biomarkers that characterize intra-tumoral dynamics throughout the course of treatment.â€¢ Surrogate image biomarkers identified through radiomics could enable a non-invasive means of characterizing biologic activities of gliomas.â€¢ With novel analytic algorithms, quantification of morphological or sub-regional tumor features to predict survival outcomes is producing promising results.â€¢ Quantifying intra-tumoral heterogeneity may improve grading and molecular sub-classifications of gliomas.â€¢ Computational fractal-based analysis of gliomas allows geometrical evaluation of tumor irregularities and complexity, leading to novel techniques for tumor segmentation, grading, and therapeutic monitoring.","Jang, Russo, Di Ieva","Jang, Russo, Di Ieva",https://doi.org/10.1007/s00234-020-02403-1,https://doi.org/10.1007/s00234-020-02403-1,2021-08-03
16977.0,pubmed,pubmed,Cost-effectiveness of introducing an MF59-adjuvanted trivalent influenza vaccine for older adults in Argentina,Cost-effectiveness of introducing an MF59-adjuvanted trivalent influenza vaccine for older adults in Argentina,"Influenza surveillance in Argentina reported influenza-like illness at a rate of 3500/100,000, a hospitalization rate of 15.5/100,000, and a death rate of 0.32/100,000 annually in adults aged over 65Ã¢â‚¬Â¯years. The high burden of disease may be due to a combination of immunosenescence and the suboptimal clinical effectiveness of conventional, non-adjuvanted influenza vaccines in this age group. There is a clinical need for more effective influenza vaccines in this population. This study evaluated the cost-effectiveness of an MF59Ã‚Â®-adjuvanted trivalent influenza vaccine (aTIV) in adults aged over 65Ã¢â‚¬Â¯years in Argentina compared with the non-adjuvanted trivalent influenza vaccine (TIV) used under the current national vaccination policy. A decision tree cost-effectiveness model was developed to estimate the cost-effectiveness of switching from TIV to aTIV in Argentinian older adults. The model compared cost and health benefits of vaccination in one influenza season from the payer perspective. The main predictions included survival, quality-adjusted survival, and costs. Model inputs were sourced from Argentina or internationally where local data was considered inaccurate. Vaccine efficacy assumptions were extracted from recently published, peer-reviewed scientific literature. Switching from TIV to aTIV would result in 170 deaths averted and 1310 incremental quality-adjusted life years (QALYs) gained. The incremental cost-effectiveness ratio per QALY was US $2660.59 from the payer perspective. In all sensitivity analyses, aTIV remained highly cost-effective. The probabilistic sensitivity analyses showed a 95% CI per QALY of US $113.74-7721.67. Introducing an adjuvanted influenza vaccine in Argentina is potentially beneficial and cost-effective relative to the currently-used TIV through the reduction of disease burden and utilization of healthcare resources.","Influenza surveillance in Argentina reported influenza-like illness at a rate of 3500/100,000, a hospitalization rate of 15.5/100,000, and a death rate of 0.32/100,000 annually in adults aged over 65â€¯years. The high burden of disease may be due to a combination of immunosenescence and the suboptimal clinical effectiveness of conventional, non-adjuvanted influenza vaccines in this age group. There is a clinical need for more effective influenza vaccines in this population. This study evaluated the cost-effectiveness of an MF59Â®-adjuvanted trivalent influenza vaccine (aTIV) in adults aged over 65â€¯years in Argentina compared with the non-adjuvanted trivalent influenza vaccine (TIV) used under the current national vaccination policy. A decision tree cost-effectiveness model was developed to estimate the cost-effectiveness of switching from TIV to aTIV in Argentinian older adults. The model compared cost and health benefits of vaccination in one influenza season from the payer perspective. The main predictions included survival, quality-adjusted survival, and costs. Model inputs were sourced from Argentina or internationally where local data was considered inaccurate. Vaccine efficacy assumptions were extracted from recently published, peer-reviewed scientific literature. Switching from TIV to aTIV would result in 170 deaths averted and 1310 incremental quality-adjusted life years (QALYs) gained. The incremental cost-effectiveness ratio per QALY was US $2660.59 from the payer perspective. In all sensitivity analyses, aTIV remained highly cost-effective. The probabilistic sensitivity analyses showed a 95% CI per QALY of US $113.74-7721.67. Introducing an adjuvanted influenza vaccine in Argentina is potentially beneficial and cost-effective relative to the currently-used TIV through the reduction of disease burden and utilization of healthcare resources.","Nguyen, Vizzotti, Uruena, Giglio, Magneres, Richmond","Nguyen, Vizzotti, Uruena, Giglio, Magneres, Richmond",https://doi.org/10.1016/j.vaccine.2020.02.081,https://doi.org/10.1016/j.vaccine.2020.02.081,2021-08-03
16981.0,pubmed,pubmed,Blended learning via distance in pre-registration nursing education: A scoping review,Blended learning via distance in pre-registration nursing education: A scoping review,"Prior to the Covid-19 global pandemic, we reviewed literature and identified comprehensive evidence of the efficacy of blended learning for pre-registration nursing students who learn across distances and/or via satellite campuses. Following a methodological framework, a scoping literature review was undertaken. We searched six databases (EBSCOHOST (CINHAL plus; Education research Complete; Australia/New Zealand Reference Centre); Google Scholar; EMBASE (Ovid) [ERIC (Ovid); Medline (Ovid)]; PubMed: ProQuest Education Journals &amp; ProQuest Nursing &amp; Allied Health Source) for the period 2005-December 2015. Critical appraisal for critiquing qualitative and quantitative studies was undertaken, as was a thematic analysis. Twenty-eight articles were included for review, which reported nursing research (nÃ‚Â =Ã‚Â 23) and student experiences of blended learning in higher education (nÃ‚Â =Ã‚Â 5). Four key themes were identified in the literature: active learning, technological barriers, support, and communication. The results suggest that when delivered purposefully, blended learning can positively influence and impact on the achievements of students, especially when utilised to manage and support distance education. Further research is needed about satellite campuses with student nurses, to assist with the development of future educational practice.","Prior to the Covid-19 global pandemic, we reviewed literature and identified comprehensive evidence of the efficacy of blended learning for pre-registration nursing students who learn across distances and/or via satellite campuses. Following a methodological framework, a scoping literature review was undertaken. We searched six databases (EBSCOHOST (CINHAL plus; Education research Complete; Australia/New Zealand Reference Centre); Google Scholar; EMBASE (Ovid) [ERIC (Ovid); Medline (Ovid)]; PubMed: ProQuest Education Journals &amp; ProQuest Nursing &amp; Allied Health Source) for the period 2005-December 2015. Critical appraisal for critiquing qualitative and quantitative studies was undertaken, as was a thematic analysis. Twenty-eight articles were included for review, which reported nursing research (nÂ =Â 23) and student experiences of blended learning in higher education (nÂ =Â 5). Four key themes were identified in the literature: active learning, technological barriers, support, and communication. The results suggest that when delivered purposefully, blended learning can positively influence and impact on the achievements of students, especially when utilised to manage and support distance education. Further research is needed about satellite campuses with student nurses, to assist with the development of future educational practice.","Jowsey, Foster, Cooper-Ioelu, Jacobs","Jowsey, Foster, Cooper-Ioelu, Jacobs",https://doi.org/10.1016/j.nepr.2020.102775,https://doi.org/10.1016/j.nepr.2020.102775,2021-08-03
16987.0,pubmed,pubmed,Real-time Burn Classification using Ultrasound Imaging,Real-time Burn Classification using Ultrasound Imaging,"This article presents a real-time approach for classification of burn depth based on B-mode ultrasound imaging. A grey-level co-occurrence matrix (GLCM) computed from the ultrasound images of the tissue is employed to construct the textural feature set and the classification is performed using nonlinear support vector machine and kernel Fisher discriminant analysis. A leave-one-out cross-validation is used for the independent assessment of the classifiers. The model is tested for pair-wise binary classification of four burn conditions in ex vivo porcine skin tissue: (i) 200Ã¢â‚¬â€°Ã‚Â°F for 10Ã¢â‚¬â€°s, (ii) 200Ã¢â‚¬â€°Ã‚Â°F for 30Ã¢â‚¬â€°s, (iii) 450Ã¢â‚¬â€°Ã‚Â°F for 10Ã¢â‚¬â€°s, and (iv) 450Ã¢â‚¬â€°Ã‚Â°F for 30Ã¢â‚¬â€°s. The average classification accuracy for pairwise separation is 99% with just over 30 samples in each burn group and the average multiclass classification accuracy is 93%. The results highlight that the ultrasound imaging-based burn classification approach in conjunction with the GLCM texture features provide an accurate assessment of altered tissue characteristics with relatively moderate sample sizes, which is often the case with experimental and clinical datasets. The proposed method is shown to have the potential to assist with the real-time clinical assessment of burn degrees, particularly for discriminating between superficial and deep second degree burns, which is challenging in clinical practice.","This article presents a real-time approach for classification of burn depth based on B-mode ultrasound imaging. A grey-level co-occurrence matrix (GLCM) computed from the ultrasound images of the tissue is employed to construct the textural feature set and the classification is performed using nonlinear support vector machine and kernel Fisher discriminant analysis. A leave-one-out cross-validation is used for the independent assessment of the classifiers. The model is tested for pair-wise binary classification of four burn conditions in ex vivo porcine skin tissue: (i) 200â€‰Â°F for 10â€‰s, (ii) 200â€‰Â°F for 30â€‰s, (iii) 450â€‰Â°F for 10â€‰s, and (iv) 450â€‰Â°F for 30â€‰s. The average classification accuracy for pairwise separation is 99% with just over 30 samples in each burn group and the average multiclass classification accuracy is 93%. The results highlight that the ultrasound imaging-based burn classification approach in conjunction with the GLCM texture features provide an accurate assessment of altered tissue characteristics with relatively moderate sample sizes, which is often the case with experimental and clinical datasets. The proposed method is shown to have the potential to assist with the real-time clinical assessment of burn degrees, particularly for discriminating between superficial and deep second degree burns, which is challenging in clinical practice.","Lee, Rahul, Ye, Chittajallu, Kruger, Boyko, Lukan, Enquobahrie, Norfleet, De","Lee, Rahul, Ye, Chittajallu, Kruger, Boyko, Lukan, Enquobahrie, Norfleet, De",https://doi.org/10.1038/s41598-020-62674-9,https://doi.org/10.1038/s41598-020-62674-9,2021-08-03
14989.0,,pubmed,Machine learning for screening prioritization in systematic reviews: comparative performance of Abstrackr and EPPI-Reviewer,Machine learning for screening prioritization in systematic reviews: comparative performance of Abstrackr and EPPI-Reviewer,"Background Improving the speed of systematic review (SR) development is key to supporting evidence-based medicine. Machine learning tools which semi-automate citation screening might improve efficiency. Few studies have assessed use of screening prioritization functionality or compared two tools head to head. In this project, we compared performance of two machine-learning tools for potential use in citation screening. Methods Using 9 evidence reports previously completed by the ECRI Institute Evidence-based Practice Center team, we compared performance of Abstrackr and EPPI-Reviewer, two off-the-shelf citations screening tools, for identifying relevant citations. Screening prioritization functionality was tested for 3 large reports and 6 small reports on a range of clinical topics. Large report topics were imaging for pancreatic cancer, indoor allergen reduction, and inguinal hernia repair. We trained Abstrackr and EPPI-Reviewer and screened all citations in 10% increments. In Task 1, we inputted whether an abstract was ordered for full-text screening; in Task 2, we inputted whether an abstract was included in the final report. For both tasks, screening continued until all studies ordered and included for the actual reports were identified. We assessed potential reductions in hypothetical screening burden (proportion of citations screened to identify all included studies) offered by each tool for all 9 reports. Results For the 3 large reports, both EPPI-Reviewer and Abstrackr performed well with potential reductions in screening burden of 4 to 49% (Abstrackr) and 9 to 60% (EPPI-Reviewer). Both tools had markedly poorer performance for 1 large report (inguinal hernia), possibly due to its heterogeneous key questions. Based on McNemar's test for paired proportions in the 3 large reports, EPPI-Reviewer outperformed Abstrackr for identifying articles ordered for full-text review, but Abstrackr performed better in 2 of 3 reports for identifying articles included in the final report. For small reports, both tools provided benefits but EPPI-Reviewer generally outperformed Abstrackr in both tasks, although these results were often not statistically significant. Conclusions Abstrackr and EPPI-Reviewer performed well, but prioritization accuracy varied greatly across reports. Our work suggests screening prioritization functionality is a promising modality offering efficiency gains without giving up human involvement in the screening process.","Improving the speed of systematic review (SR) development is key to supporting evidence-based medicine. Machine learning tools which semi-automate citation screening might improve efficiency. Few studies have assessed use of screening prioritization functionality or compared two tools head to head. In this project, we compared performance of two machine-learning tools for potential use in citation screening. Using 9 evidence reports previously completed by the ECRI Institute Evidence-based Practice Center team, we compared performance of Abstrackr and EPPI-Reviewer, two off-the-shelf citations screening tools, for identifying relevant citations. Screening prioritization functionality was tested for 3 large reports and 6 small reports on a range of clinical topics. Large report topics were imaging for pancreatic cancer, indoor allergen reduction, and inguinal hernia repair. We trained Abstrackr and EPPI-Reviewer and screened all citations in 10% increments. In Task 1, we inputted whether an abstract was ordered for full-text screening; in Task 2, we inputted whether an abstract was included in the final report. For both tasks, screening continued until all studies ordered and included for the actual reports were identified. We assessed potential reductions in hypothetical screening burden (proportion of citations screened to identify all included studies) offered by each tool for all 9 reports. For the 3 large reports, both EPPI-Reviewer and Abstrackr performed well with potential reductions in screening burden of 4 to 49% (Abstrackr) and 9 to 60% (EPPI-Reviewer). Both tools had markedly poorer performance for 1 large report (inguinal hernia), possibly due to its heterogeneous key questions. Based on McNemar's test for paired proportions in the 3 large reports, EPPI-Reviewer outperformed Abstrackr for identifying articles ordered for full-text review, but Abstrackr performed better in 2 of 3 reports for identifying articles included in the final report. For small reports, both tools provided benefits but EPPI-Reviewer generally outperformed Abstrackr in both tasks, although these results were often not statistically significant. Abstrackr and EPPI-Reviewer performed well, but prioritization accuracy varied greatly across reports. Our work suggests screening prioritization functionality is a promising modality offering efficiency gains without giving up human involvement in the screening process.","Tsou, A. Y. Treadwell, J. R. Erinoff, E. Schoelles, K.","Tsou, Treadwell, Erinoff, Schoelles",<Go to ISI>://WOS:000523737800006,https://doi.org/10.1186/s13643-020-01324-7,2021-08-03
16998.0,pubmed,pubmed,The Impacts of Orthognathic Surgery on the Facial Appearance and Age Perception of Patients Presenting Skeletal Class III Deformity: An Outcome Study Using the FACE-Q Report and Surgical Professional-Based Panel Assessment,The Impacts of Orthognathic Surgery on the Facial Appearance and Age Perception of Patients Presenting Skeletal Class III Deformity: An Outcome Study Using the FACE-Q Report and Surgical Professional-Based Panel Assessment,"A recent artificial intelligence-based investigation has shown the impacts of orthognathic surgery on the patient's facial appearance and apparent age. However, appearance and age perception as reported by patients and surgical professionals have not been addressed in the same cohort to date. FACE-Q facial appraisal (appearance and age) and quality-of-life scale scores obtained before and after orthognathic surgery, in addition to three-dimensional photographs of 70 patients with skeletal class III deformity, were collected for a comparative cross-sectional study. Seven blinded plastic surgeons rated all photographs for apparent facial aesthetic and age scales. The FACE-Q data from 57 matched normal individuals were adopted for the comparative analyses. The correlation between the FACE-Q and the professional-based scales was tested. Pre-orthognathic surgery versus post-orthognathic surgery comparisons showed significant differences (p &lt; 0.001) for all FACE-Q scales and panel assessments, with higher (FACE-Q scales and professional-based aesthetic parameters) and lower (FACE-Q patient-perceived age scale and professional-based age parameter) values for post-orthognathic surgery measurements. Patients had significantly (p &lt; 0.001) higher (patient-perceived age scale) and lower (facial appraisal and quality-of-life scales) FACE-Q values than normal individuals for pre-orthognathic surgery but not for post-orthognathic surgery measurements. The FACE-Q facial appearance overall scale had significant correlations (p &lt; 0.001) with the panel assessment for the parameters &quot;beautiful&quot; and &quot;attractive&quot; but not for the &quot;pleasant&quot; parameter. No significant correlations were observed for facial age scales. This study contributes to the orthognathic surgery literature by revealing that orthognathic surgery positively impacts the perception of apparent facial age and improves facial appearance and quality of life. Therapeutic, IV.","A recent artificial intelligence-based investigation has shown the impacts of orthognathic surgery on the patient's facial appearance and apparent age. However, appearance and age perception as reported by patients and surgical professionals have not been addressed in the same cohort to date. FACE-Q facial appraisal (appearance and age) and quality-of-life scale scores obtained before and after orthognathic surgery, in addition to three-dimensional photographs of 70 patients with skeletal class III deformity, were collected for a comparative cross-sectional study. Seven blinded plastic surgeons rated all photographs for apparent facial aesthetic and age scales. The FACE-Q data from 57 matched normal individuals were adopted for the comparative analyses. The correlation between the FACE-Q and the professional-based scales was tested. Pre-orthognathic surgery versus post-orthognathic surgery comparisons showed significant differences (p &lt; 0.001) for all FACE-Q scales and panel assessments, with higher (FACE-Q scales and professional-based aesthetic parameters) and lower (FACE-Q patient-perceived age scale and professional-based age parameter) values for post-orthognathic surgery measurements. Patients had significantly (p &lt; 0.001) higher (patient-perceived age scale) and lower (facial appraisal and quality-of-life scales) FACE-Q values than normal individuals for pre-orthognathic surgery but not for post-orthognathic surgery measurements. The FACE-Q facial appearance overall scale had significant correlations (p &lt; 0.001) with the panel assessment for the parameters ""beautiful"" and ""attractive"" but not for the ""pleasant"" parameter. No significant correlations were observed for facial age scales. This study contributes to the orthognathic surgery literature by revealing that orthognathic surgery positively impacts the perception of apparent facial age and improves facial appearance and quality of life. Therapeutic, IV.","Denadai, Chou, Su, Lin, Ho, Lo","Denadai, Chou, Su, Lin, Ho, Lo",https://doi.org/10.1097/PRS.0000000000006650,https://doi.org/10.1097/PRS.0000000000006650,2021-08-03
16999.0,pubmed,pubmed,Hold the Salt: History of Salt Restriction as a First-line Therapy for MeniÃƒÂ¨re's Disease,Hold the Salt: History of Salt Restriction as a First-line Therapy for MeniÃ¨re's Disease,"To determine the historical origins of the usage of the salt restriction diet as an intervention for MeniÃƒÂ¨re's disease (MD). Articles on MD and salt restriction were identified using Pubmed and Google scholar. Original manuscripts from 19th and 20th century as well as selected otological textbooks in English, German, and French were also reviewed. The oldest recommendation of salt restriction in the literature was by Dederding (1889-1955) in 1929. She and her mentor, Sydney Holger Mygind (1884-1970), believed MD was caused by dysfunctional water metabolism. In several published manuscripts, they proposed that a reduced salt and fluid diet was an effective treatment for MD. Their contemporaries supported their findings, most notably, Albert C. Furstenberg (1890-1969) who suggested salt restriction alone as treatment for MD. Furstenberg, in his initial study implementing salt restriction in 15 patients with MD and then in a larger study with 150 patients, was the first to produce results that supported salt restriction as therapy for MD. It was not until 1980, when LB Jongkees first published his criticism of this treatment, that salt restriction was questioned. Since then, numerous published articles have been critical of salt restriction therapy and skeptical of its initial adoption into clinical practice. Since Dederding's and Mygind's publications in 1929 and Furstenberg's trial in 1934, the salt restriction diet has remained a primary first-line treatment for MD. Since the 1950s, various publications have both supported and argued this treatment, and the evidence of its validity remains inconclusive.","To determine the historical origins of the usage of the salt restriction diet as an intervention for MeniÃ¨re's disease (MD). Articles on MD and salt restriction were identified using Pubmed and Google scholar. Original manuscripts from 19th and 20th century as well as selected otological textbooks in English, German, and French were also reviewed. The oldest recommendation of salt restriction in the literature was by Dederding (1889-1955) in 1929. She and her mentor, Sydney Holger Mygind (1884-1970), believed MD was caused by dysfunctional water metabolism. In several published manuscripts, they proposed that a reduced salt and fluid diet was an effective treatment for MD. Their contemporaries supported their findings, most notably, Albert C. Furstenberg (1890-1969) who suggested salt restriction alone as treatment for MD. Furstenberg, in his initial study implementing salt restriction in 15 patients with MD and then in a larger study with 150 patients, was the first to produce results that supported salt restriction as therapy for MD. It was not until 1980, when LB Jongkees first published his criticism of this treatment, that salt restriction was questioned. Since then, numerous published articles have been critical of salt restriction therapy and skeptical of its initial adoption into clinical practice. Since Dederding's and Mygind's publications in 1929 and Furstenberg's trial in 1934, the salt restriction diet has remained a primary first-line treatment for MD. Since the 1950s, various publications have both supported and argued this treatment, and the evidence of its validity remains inconclusive.","Shim, Strum, Mudry, Monfared","Shim, Strum, Mudry, Monfared",https://doi.org/10.1097/MAO.0000000000002635,https://doi.org/10.1097/MAO.0000000000002635,2021-08-03
17001.0,pubmed,pubmed,Technical and imaging factors influencing performance of deep learning systems for diabetic retinopathy,Technical and imaging factors influencing performance of deep learning systems for diabetic retinopathy,"Deep learning (DL) has been shown to be effective in developing diabetic retinopathy (DR) algorithms, possibly tackling financial and manpower challenges hindering implementation of DR screening. However, our systematic review of the literature reveals few studies studied the impact of different factors on these DL algorithms, that are important for clinical deployment in real-world settings. Using 455,491 retinal images, we evaluated two technical and three image-related factors in detection of referable DR. For technical factors, the performances of four DL models (VGGNet, ResNet, DenseNet, Ensemble) and two computational frameworks (Caffe, TensorFlow) were evaluated while for image-related factors, we evaluated image compression levels (reducing image size, 350, 300, 250, 200, 150Ã¢â‚¬â€°KB), number of fields (7-field, 2-field, 1-field) and media clarity (pseudophakic vs phakic). In detection of referable DR, four DL models showed comparable diagnostic performance (AUC 0.936-0.944). To develop the VGGNet model, two computational frameworks had similar AUC (0.936). The DL performance dropped when image size decreased below 250Ã¢â‚¬â€°KB (AUC 0.936, 0.900, <i>p</i>Ã¢â‚¬â€°&lt;Ã¢â‚¬â€°0.001). The DL performance performed better when there were increased number of fields (dataset 1: 2-field vs 1-field-AUC 0.936 vs 0.908, <i>p</i>Ã¢â‚¬â€°&lt;Ã¢â‚¬â€°0.001; dataset 2: 7-field vs 2-field vs 1-field, AUC 0.949 vs 0.911 vs 0.895). DL performed better in the pseudophakic than phakic eyes (AUC 0.918 vs 0.833, <i>p</i>Ã¢â‚¬â€°&lt;Ã¢â‚¬â€°0.001). Various image-related factors play more significant roles than technical factors in determining the diagnostic performance, suggesting the importance of having robust training and testing datasets for DL training and deployment in the real-world settings.","Deep learning (DL) has been shown to be effective in developing diabetic retinopathy (DR) algorithms, possibly tackling financial and manpower challenges hindering implementation of DR screening. However, our systematic review of the literature reveals few studies studied the impact of different factors on these DL algorithms, that are important for clinical deployment in real-world settings. Using 455,491 retinal images, we evaluated two technical and three image-related factors in detection of referable DR. For technical factors, the performances of four DL models (VGGNet, ResNet, DenseNet, Ensemble) and two computational frameworks (Caffe, TensorFlow) were evaluated while for image-related factors, we evaluated image compression levels (reducing image size, 350, 300, 250, 200, 150â€‰KB), number of fields (7-field, 2-field, 1-field) and media clarity (pseudophakic vs phakic). In detection of referable DR, four DL models showed comparable diagnostic performance (AUC 0.936-0.944). To develop the VGGNet model, two computational frameworks had similar AUC (0.936). The DL performance dropped when image size decreased below 250â€‰KB (AUC 0.936, 0.900, <i>p</i>â€‰&lt;â€‰0.001). The DL performance performed better when there were increased number of fields (dataset 1: 2-field vs 1-field-AUC 0.936 vs 0.908, <i>p</i>â€‰&lt;â€‰0.001; dataset 2: 7-field vs 2-field vs 1-field, AUC 0.949 vs 0.911 vs 0.895). DL performed better in the pseudophakic than phakic eyes (AUC 0.918 vs 0.833, <i>p</i>â€‰&lt;â€‰0.001). Various image-related factors play more significant roles than technical factors in determining the diagnostic performance, suggesting the importance of having robust training and testing datasets for DL training and deployment in the real-world settings.","Yip, Lim, Lim, Nguyen, Chong, Yu, Bellemo, Xie, Lee, Hamzah, Ho, Tan, Sabanayagam, Grzybowski, Tan, Hsu, Lee, Wong, Ting","Yip, Lim, Lim, Nguyen, Chong, Yu, Bellemo, Xie, Lee, Hamzah, Ho, Tan, Sabanayagam, Grzybowski, Tan, Hsu, Lee, Wong, Ting",https://doi.org/10.1038/s41746-020-0247-1,https://doi.org/10.1038/s41746-020-0247-1,2021-08-03
17003.0,pubmed,pubmed,Radiomic Features of Primary Rectal Cancers on Baseline T<sub>2</sub> -Weighted MRI Are Associated With Pathologic Complete Response to Neoadjuvant Chemoradiation: A Multisite Study,Radiomic Features of Primary Rectal Cancers on Baseline T<sub>2</sub> -Weighted MRI Are Associated With Pathologic Complete Response to Neoadjuvant Chemoradiation: A Multisite Study,"Twenty-five percent of rectal adenocarcinoma patients achieve pathologic complete response (pCR) to neoadjuvant chemoradiation and could avoid proctectomy. However, pretreatment clinical or imaging markers are lacking in predicting response to chemoradiation. Radiomic texture features from MRI have recently been associated with therapeutic response in other cancers. To construct a radiomics texture model based on pretreatment MRI for identifying patients who will achieve pCR to neoadjuvant chemoradiation in rectal cancer, including validation across multiple scanners and sites. Retrospective. In all, 104 rectal cancer patients staged with MRI prior to long-course chemoradiation followed by proctectomy; curated from three institutions. 1.5T-3.0T, axial higher resolution T<sub>2</sub> -weighted turbo spin echo sequence. Pathologic response was graded on postsurgical specimens. In total, 764 radiomic features were extracted from single-slice sections of rectal tumors on processed pretreatment T<sub>2</sub> -weighted MRI. Three feature selection schemes were compared for identifying radiomic texture descriptors associated with pCR via a discovery cohort (one site, N = 60, cross-validation). The top-selected radiomic texture features were used to train and validate a random forest classifier model for pretreatment identification of pCR (two external sites, N = 44). Model performance was evaluated via area under the curve (AUC), accuracy, sensitivity, and specificity. Laws kernel responses and gradient organization features were most associated with pCR (PÃ¢â‚¬â€°Ã¢â€°Â¤Ã¢â‚¬â€°0.01); as well as being commonly identified across all feature selection schemes. The radiomics model yielded a discovery AUC of 0.699Ã¢â‚¬â€°Ã‚Â±Ã¢â‚¬â€°0.076 and a hold-out validation AUC of 0.712 with 70.5% accuracy (70.0% sensitivity, 70.6% specificity) in identifying pCR. Radiomic texture features were resilient to variations in magnetic field strength as well as being consistent between two different expert annotations. Univariate analysis revealed no significant associations of baseline clinicopathologic or MRI findings with pCR (P = 0.07-0.96). Radiomic texture features from pretreatment MRIs may enable early identification of potential pCR to neoadjuvant chemoradiation, as well as generalize across sites. 3 TECHNICAL EFFICACY STAGE: 2.","Twenty-five percent of rectal adenocarcinoma patients achieve pathologic complete response (pCR) to neoadjuvant chemoradiation and could avoid proctectomy. However, pretreatment clinical or imaging markers are lacking in predicting response to chemoradiation. Radiomic texture features from MRI have recently been associated with therapeutic response in other cancers. To construct a radiomics texture model based on pretreatment MRI for identifying patients who will achieve pCR to neoadjuvant chemoradiation in rectal cancer, including validation across multiple scanners and sites. Retrospective. In all, 104 rectal cancer patients staged with MRI prior to long-course chemoradiation followed by proctectomy; curated from three institutions. 1.5T-3.0T, axial higher resolution T<sub>2</sub> -weighted turbo spin echo sequence. Pathologic response was graded on postsurgical specimens. In total, 764 radiomic features were extracted from single-slice sections of rectal tumors on processed pretreatment T<sub>2</sub> -weighted MRI. Three feature selection schemes were compared for identifying radiomic texture descriptors associated with pCR via a discovery cohort (one site, N = 60, cross-validation). The top-selected radiomic texture features were used to train and validate a random forest classifier model for pretreatment identification of pCR (two external sites, N = 44). Model performance was evaluated via area under the curve (AUC), accuracy, sensitivity, and specificity. Laws kernel responses and gradient organization features were most associated with pCR (Pâ€‰â‰¤â€‰0.01); as well as being commonly identified across all feature selection schemes. The radiomics model yielded a discovery AUC of 0.699â€‰Â±â€‰0.076 and a hold-out validation AUC of 0.712 with 70.5% accuracy (70.0% sensitivity, 70.6% specificity) in identifying pCR. Radiomic texture features were resilient to variations in magnetic field strength as well as being consistent between two different expert annotations. Univariate analysis revealed no significant associations of baseline clinicopathologic or MRI findings with pCR (P = 0.07-0.96). Radiomic texture features from pretreatment MRIs may enable early identification of potential pCR to neoadjuvant chemoradiation, as well as generalize across sites. 3 TECHNICAL EFFICACY STAGE: 2.","Antunes, Ofshteyn, Bera, Wang, Brady, Willis, Friedman, Marderstein, Kalady, Stein, Purysko, Paspulati, Gollamudi, Madabhushi, Viswanath","Antunes, Ofshteyn, Bera, Wang, Brady, Willis, Friedman, Marderstein, Kalady, Stein, Purysko, Paspulati, Gollamudi, Madabhushi, Viswanath",https://doi.org/10.1002/jmri.27140,https://doi.org/10.1002/jmri.27140,2021-08-03
17004.0,pubmed,pubmed,Dosimetric evaluation of synthetic CT generated with GANs for MRI-only proton therapy treatment planning of brain tumors,Dosimetric evaluation of synthetic CT generated with GANs for MRI-only proton therapy treatment planning of brain tumors,"The purpose of this study was to address the dosimetric accuracy of synthetic computed tomography (sCT) images of patients with brain tumor generated using a modified generative adversarial network (GAN) method, for their use in magnetic resonance imaging (MRI)-only treatment planning for proton therapy. Dose volume histogram (DVH) analysis was performed on CT and sCT images of patients with brain tumor for plans generated for intensity-modulated proton therapy (IMPT). All plans were robustly optimized using a commercially available treatment planning system (RayStation, from RaySearch Laboratories) and standard robust parameters reported in the literature. The IMPT plan was then used to compute the dose on CT and sCT images for dosimetric comparison, using RayStation analytical (pencil beam) dose algorithm. We used a second, independent Monte Carlo dose calculation engine to recompute the dose on both CT and sCT images to ensure a proper analysis of the dosimetric accuracy of the sCT images. The results extracted from RayStation showed excellent agreement for most DVH metrics computed on the CT and sCT for the nominal case, with a mean absolute difference below 0.5% (0.3Ã‚Â Gy) of the prescription dose for the clinical target volume (CTV) and below 2% (1.2Ã‚Â Gy) for the organs at risk (OARs) considered. This demonstrates a high dosimetric accuracy for the generated sCT images, especially in the target volume. The metrics obtained from the Monte Carlo doses mostly agreed with the values extracted from RayStation for the nominal and worst-case scenarios (mean difference below 3%). This work demonstrated the feasibility of using sCT generated with a GAN-based deep learning method for MRI-only treatment planning of patients with brain tumor in intensity-modulated proton therapy.","The purpose of this study was to address the dosimetric accuracy of synthetic computed tomography (sCT) images of patients with brain tumor generated using a modified generative adversarial network (GAN) method, for their use in magnetic resonance imaging (MRI)-only treatment planning for proton therapy. Dose volume histogram (DVH) analysis was performed on CT and sCT images of patients with brain tumor for plans generated for intensity-modulated proton therapy (IMPT). All plans were robustly optimized using a commercially available treatment planning system (RayStation, from RaySearch Laboratories) and standard robust parameters reported in the literature. The IMPT plan was then used to compute the dose on CT and sCT images for dosimetric comparison, using RayStation analytical (pencil beam) dose algorithm. We used a second, independent Monte Carlo dose calculation engine to recompute the dose on both CT and sCT images to ensure a proper analysis of the dosimetric accuracy of the sCT images. The results extracted from RayStation showed excellent agreement for most DVH metrics computed on the CT and sCT for the nominal case, with a mean absolute difference below 0.5% (0.3Â Gy) of the prescription dose for the clinical target volume (CTV) and below 2% (1.2Â Gy) for the organs at risk (OARs) considered. This demonstrates a high dosimetric accuracy for the generated sCT images, especially in the target volume. The metrics obtained from the Monte Carlo doses mostly agreed with the values extracted from RayStation for the nominal and worst-case scenarios (mean difference below 3%). This work demonstrated the feasibility of using sCT generated with a GAN-based deep learning method for MRI-only treatment planning of patients with brain tumor in intensity-modulated proton therapy.","Kazemifar, BarragÃƒÂ¡n Montero, Souris, Rivas, Timmerman, Park, Jiang, Geets, Sterpin, Owrangi","Kazemifar, BarragÃ¡n Montero, Souris, Rivas, Timmerman, Park, Jiang, Geets, Sterpin, Owrangi",https://doi.org/10.1002/acm2.12856,https://doi.org/10.1002/acm2.12856,2021-08-03
17006.0,pubmed,pubmed,Heart rate variability as a measure of mental stress in surgery: a systematic review,Heart rate variability as a measure of mental stress in surgery: a systematic review,"There is increasing interest in the use of heart rate variability (HRV) as an objective measurement of mental stress in the surgical setting. To identify areas of improvement, the aim of our study was to review current use of HRV measurements in the surgical setting, evaluate the different methods used for the analysis of HRV, and to assess whether HRV is being measured correctly. A systematic review was performed according to the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA). 17 studies regarding HRV as a measurement of mental stress in the surgical setting were included and analysed. 24% of the studies performed long-term measurements (24Ã‚Â h and longer) to assess the long-term effects of and recovery from mental stress. In 24% of the studies, artefact correction took place. HRV showed to be a good objective assessment method of stress induced in the workplace environment: it was able to pinpoint stressors during operations, determine which operating techniques induced most stress for surgeons, and indicate differences in stress levels between performing and assisting surgery. For future research, this review recommends using singular guidelines to standardize research, and performing artefact correction. This will improve further evaluation of the long-term effects of mental stress and its recovery.","There is increasing interest in the use of heart rate variability (HRV) as an objective measurement of mental stress in the surgical setting. To identify areas of improvement, the aim of our study was to review current use of HRV measurements in the surgical setting, evaluate the different methods used for the analysis of HRV, and to assess whether HRV is being measured correctly. A systematic review was performed according to the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA). 17 studies regarding HRV as a measurement of mental stress in the surgical setting were included and analysed. 24% of the studies performed long-term measurements (24Â h and longer) to assess the long-term effects of and recovery from mental stress. In 24% of the studies, artefact correction took place. HRV showed to be a good objective assessment method of stress induced in the workplace environment: it was able to pinpoint stressors during operations, determine which operating techniques induced most stress for surgeons, and indicate differences in stress levels between performing and assisting surgery. For future research, this review recommends using singular guidelines to standardize research, and performing artefact correction. This will improve further evaluation of the long-term effects of mental stress and its recovery.","The, Reijmerink, van der Laan, Cnossen","The, Reijmerink, van der Laan, Cnossen",https://doi.org/10.1007/s00420-020-01525-6,https://doi.org/10.1007/s00420-020-01525-6,2021-08-03
17011.0,pubmed,pubmed,Telemedicine for Age-Related Macular Degeneration,Telemedicine for Age-Related Macular Degeneration,"<b> <i>Background:</i> </b> <i>As the leading cause of vision loss in the United States, age-related macular degeneration (AMD) would seem to be amenable to interventions that increase access to screening and management services for patients. AMD poses several unique challenges for telemedicine, however. The disease lacks clinical consensus on the effectiveness and cost-effectiveness of screening the general population, and more complex imaging modalities may be required than for what has traditionally been used for diabetic retinopathy telehealth systems.</i> <b> <i>Methods:</i> </b> <i>The current literature was reviewed to find clinical trials and expert consensus documents on the state-of-the-art of telemedicine for AMD.</i> <b> <i>Results:</i> </b> <i>A range of feasibility studies have reported success with telemedicine strategies for AMD. Several investigators have reported experience with AMD screening and remote-monitoring systems as well as artificial intelligence applications.</i> <b> <i>Conclusions:</i> </b> <i>There are currently no large-scale telemedicine programs for either screening or managing AMD, but new approaches to screening and managing the condition may allow for expansion of high-quality convenient care for an increasing patient population.</i>","<b><i>Background:</i></b> <i>As the leading cause of vision loss in the United States, age-related macular degeneration (AMD) would seem to be amenable to interventions that increase access to screening and management services for patients. AMD poses several unique challenges for telemedicine, however. The disease lacks clinical consensus on the effectiveness and cost-effectiveness of screening the general population, and more complex imaging modalities may be required than for what has traditionally been used for diabetic retinopathy telehealth systems.</i> <b><i>Methods:</i></b> <i>The current literature was reviewed to find clinical trials and expert consensus documents on the state-of-the-art of telemedicine for AMD.</i> <b><i>Results:</i></b> <i>A range of feasibility studies have reported success with telemedicine strategies for AMD. Several investigators have reported experience with AMD screening and remote-monitoring systems as well as artificial intelligence applications.</i> <b><i>Conclusions:</i></b> <i>There are currently no large-scale telemedicine programs for either screening or managing AMD, but new approaches to screening and managing the condition may allow for expansion of high-quality convenient care for an increasing patient population.</i>","Brady, Garg","Brady, Garg",https://doi.org/10.1089/tmj.2020.0011,https://doi.org/10.1089/tmj.2020.0011,2021-08-03
11154.0,,pubmed,SWIFT-Active Screener: Accelerated document screening through active learning and integrated recall estimation,SWIFT-Active Screener: Accelerated document screening through active learning and integrated recall estimation,"Background: In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate articles is relevant to the research question under consideration. A typical review may require screening thousands or tens of thousands of articles in and can utilize hundreds of person-hours of labor. Methods: Here we introduce SWIFT-Active Screener, a web-based, collaborative systematic review software application, designed to reduce the overall screening burden required during this resource-intensive phase of the review process. To prioritize articles for review, SWIFT-Active Screener uses active learning, a type of machine learning that incorporates user feedback during screening. Meanwhile, a negative binomial model is employed to estimate the number of relevant articles remaining in the unscreened document list. Using a simulation involving 26 diverse systematic review datasets that were previously screened by reviewers, we evaluated both the document prioritization and recall estimation methods. Results: On average, 95% of the relevant articles were identified after screening only 40% of the total reference list. In the 5 document sets with 5,000 or more references, 95% recall was achieved after screening only 34% of the available references, on average. Furthermore, the recall estimator we have proposed provides a useful, conservative estimate of the percentage of relevant documents identified during the screening process. Conclusion: SWIFT-Active Screener can result in significant time savings compared to traditional screening and the savings are increased for larger project sizes. Moreover, the integration of explicit recall estimation during screening solves an important challenge faced by all machine learning systems for document screening: when to stop screening a prioritized reference list. The software is currently available in the form of a multi-user, collaborative, online web application.","In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate articles is relevant to the research question under consideration. A typical review may require screening thousands or tens of thousands of articles in and can utilize hundreds of person-hours of labor. Here we introduce SWIFT-Active Screener, a web-based, collaborative systematic review software application, designed to reduce the overall screening burden required during this resource-intensive phase of the review process. To prioritize articles for review, SWIFT-Active Screener uses active learning, a type of machine learning that incorporates user feedback during screening. Meanwhile, a negative binomial model is employed to estimate the number of relevant articles remaining in the unscreened document list. Using a simulation involving 26 diverse systematic review datasets that were previously screened by reviewers, we evaluated both the document prioritization and recall estimation methods. On average, 95% of the relevant articles were identified after screening only 40% of the total reference list. In the 5 document sets with 5,000 or more references, 95% recall was achieved after screening only 34% of the available references, on average. Furthermore, the recall estimator we have proposed provides a useful, conservative estimate of the percentage of relevant documents identified during the screening process. SWIFT-Active Screener can result in significant time savings compared to traditional screening and the savings are increased for larger project sizes. Moreover, the integration of explicit recall estimation during screening solves an important challenge faced by all machine learning systems for document screening: when to stop screening a prioritized reference list. The software is currently available in the form of a multi-user, collaborative, online web application.","Howard, B. E. Phillips, J. Tandon, A. Maharana, A. Elmore, R. Mav, D. Sedykh, A. Thayer, K. Merrick, B. A. Walker, V. Rooney, A. Shah, R. R.","Howard, Phillips, Tandon, Maharana, Elmore, Mav, Sedykh, Thayer, Merrick, Walker, Rooney, Shah",<Go to ISI>://WOS:000522749900066,https://doi.org/10.1016/j.envint.2020.105623,2021-08-03
17016.0,pubmed,pubmed,Changes in corneal biomechanics in patients with diabetes mellitus: a systematic review and meta-analysis,Changes in corneal biomechanics in patients with diabetes mellitus: a systematic review and meta-analysis,"To determine the changes in corneal biomechanical parameters in patients with diabetes mellitus (DM) in comparison with controls. Pertinent studies were identified by comprehensively search of PubMed, Embase, the Web of Science, the Cochrane Library, Scopus, the China National Knowledge Infrastructure and the Chinese biomedical disc (CBM) databases. Pooling analyses by random models using the D-L method were performed for corneal hysteresis (CH), the corneal resistance factor (CRF), corneal-compensated intraocular pressure (IOPcc) and Goldmann-correlated intraocular pressure (IOPg). A total of 15 studies were included in the final analysis, involving 1506 eyes in the diabetic group and 2190 eyes in the control group. The diabetic group had significantly higher CH, CRF, IOPg and IOPcc values than the control group. The pooled mean differences were 1.34Ã‚Â mmHg (95% confidence interval [CI] 0.60-2.08Ã‚Â mmHg, PÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°0.001) for IOPg and 0.85Ã‚Â mmHg (95% CI 0.18-1.51Ã‚Â mmHg, PÃ¢â‚¬â€°=Ã¢â‚¬â€°0.013) for IOPcc, 0.38Ã‚Â mmHg (95% CI 0.01-0.75, PÃ¢â‚¬â€°=Ã¢â‚¬â€°0.047) for CH and 0.63Ã‚Â mmHg (95% CI 0.27-0.98, PÃ¢â‚¬â€°=Ã¢â‚¬â€°0.001) for the CRF. Sensitivity analyses using the leave-one-out method showed a consistent significant difference between the groups (all PÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°0.001). Corneal biomechanics changed in the patients with DM. High CH, CRF, IOPcc and IOPg values may be associated factors for diabetes mellitus. Future studies are warranted to clarify the underlying mechanisms and explore the relationship between corneal biomechanics, glaucoma and diabetes mellitus. PROSPERO registration No CRD4201705465.","To determine the changes in corneal biomechanical parameters in patients with diabetes mellitus (DM) in comparison with controls. Pertinent studies were identified by comprehensively search of PubMed, Embase, the Web of Science, the Cochrane Library, Scopus, the China National Knowledge Infrastructure and the Chinese biomedical disc (CBM) databases. Pooling analyses by random models using the D-L method were performed for corneal hysteresis (CH), the corneal resistance factor (CRF), corneal-compensated intraocular pressure (IOPcc) and Goldmann-correlated intraocular pressure (IOPg). A total of 15 studies were included in the final analysis, involving 1506 eyes in the diabetic group and 2190 eyes in the control group. The diabetic group had significantly higher CH, CRF, IOPg and IOPcc values than the control group. The pooled mean differences were 1.34Â mmHg (95% confidence interval [CI] 0.60-2.08Â mmHg, Pâ€‰&lt;â€‰0.001) for IOPg and 0.85Â mmHg (95% CI 0.18-1.51Â mmHg, Pâ€‰=â€‰0.013) for IOPcc, 0.38Â mmHg (95% CI 0.01-0.75, Pâ€‰=â€‰0.047) for CH and 0.63Â mmHg (95% CI 0.27-0.98, Pâ€‰=â€‰0.001) for the CRF. Sensitivity analyses using the leave-one-out method showed a consistent significant difference between the groups (all Pâ€‰&lt;â€‰0.001). Corneal biomechanics changed in the patients with DM. High CH, CRF, IOPcc and IOPg values may be associated factors for diabetes mellitus. Future studies are warranted to clarify the underlying mechanisms and explore the relationship between corneal biomechanics, glaucoma and diabetes mellitus. PROSPERO registration No CRD4201705465.","Wang, Xu, Wang, Wang, Chen, He, Chen","Wang, Xu, Wang, Wang, Chen, He, Chen",https://doi.org/10.1007/s00592-020-01481-0,https://doi.org/10.1007/s00592-020-01481-0,2021-08-03
17019.0,pubmed,pubmed,Investigating Bullying as a Predictor of Suicidality in a Clinical Sample of Adolescents with Autism Spectrum Disorder,Investigating Bullying as a Predictor of Suicidality in a Clinical Sample of Adolescents with Autism Spectrum Disorder,"For typically developing adolescents, being bullied is associated with increased risk of suicidality. Although adolescents with autism spectrum disorder (ASD) are at increased risk of both bullying and suicidality, there is very little research that examines the extent to which an experience of being bullied may increase suicidality within this specific population. To address this, we conducted a retrospective cohort study to investigate the longitudinal association between experiencing bullying and suicidality in a clinical population of 680 adolescents with ASD. Electronic health records of adolescents (13-17Ã¢â‚¬â€°years), using mental health services in South London, with a diagnosis of ASD were analyzed. Natural language processing was employed to identify mentions of bullying and suicidality in the free text fields of adolescents' clinical records. Cox regression analysis was employed to investigate the longitudinal relationship between bullying and suicidality outcomes. Reported experience of bullying in the first month of clinical contact was associated with an increased risk suicidality over the follow-up period (hazard ratio = 1.82; 95% confidence interval = 1.28-2.59). In addition, female gender, psychosis, affective disorder diagnoses, and higher intellectual ability were all associated with suicidality at follow-up. This study is the first to demonstrate the strength of longitudinal associations between bullying and suicidality in a clinical population of adolescents with ASD, using automated approaches to detect key life events within clinical records. Our findings provide support for identifying and dealing with bullying in schools, and for antibullying strategy's incorporation into wider suicide prevention programs for young people with ASD. Autism Res 2020, 13: 988-997. Ã‚Â© 2020 The Authors. Autism Research published by International Society for Autism Research published by Wiley Periodicals, Inc. LAY SUMMARY: We investigated the relationship between bullying and suicidality in young people with autism spectrum disorder (ASD). We examined the clinical records of adolescents (aged 13-18Ã¢â‚¬â€°years old) with ASD in South London who were receiving treatment from Child and Adolescent Mental Health Services. We found that if they reported being bullied in the first month after they were first seen by mental health services, they were nearly twice as likely to go on to develop suicidal thoughts or behaviors.","For typically developing adolescents, being bullied is associated with increased risk of suicidality. Although adolescents with autism spectrum disorder (ASD) are at increased risk of both bullying and suicidality, there is very little research that examines the extent to which an experience of being bullied may increase suicidality within this specific population. To address this, we conducted a retrospective cohort study to investigate the longitudinal association between experiencing bullying and suicidality in a clinical population of 680 adolescents with ASD. Electronic health records of adolescents (13-17â€‰years), using mental health services in South London, with a diagnosis of ASD were analyzed. Natural language processing was employed to identify mentions of bullying and suicidality in the free text fields of adolescents' clinical records. Cox regression analysis was employed to investigate the longitudinal relationship between bullying and suicidality outcomes. Reported experience of bullying in the first month of clinical contact was associated with an increased risk suicidality over the follow-up period (hazard ratio = 1.82; 95% confidence interval = 1.28-2.59). In addition, female gender, psychosis, affective disorder diagnoses, and higher intellectual ability were all associated with suicidality at follow-up. This study is the first to demonstrate the strength of longitudinal associations between bullying and suicidality in a clinical population of adolescents with ASD, using automated approaches to detect key life events within clinical records. Our findings provide support for identifying and dealing with bullying in schools, and for antibullying strategy's incorporation into wider suicide prevention programs for young people with ASD. Autism Res 2020, 13: 988-997. Â© 2020 The Authors. Autism Research published by International Society for Autism Research published by Wiley Periodicals, Inc. LAY SUMMARY: We investigated the relationship between bullying and suicidality in young people with autism spectrum disorder (ASD). We examined the clinical records of adolescents (aged 13-18â€‰years old) with ASD in South London who were receiving treatment from Child and Adolescent Mental Health Services. We found that if they reported being bullied in the first month after they were first seen by mental health services, they were nearly twice as likely to go on to develop suicidal thoughts or behaviors.","Holden, Mueller, McGowan, Sanyal, Kikoler, Simonoff, Velupillai, Downs","Holden, Mueller, McGowan, Sanyal, Kikoler, Simonoff, Velupillai, Downs",https://doi.org/10.1002/aur.2292,https://doi.org/10.1002/aur.2292,2021-08-03
17022.0,pubmed,pubmed,A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases,A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases,"Autoimmune diseases are chronic, multifactorial conditions. Through machine learning (ML), a branch of the wider field of artificial intelligence, it is possible to extract patterns within patient data, and exploit these patterns to predict patient outcomes for improved clinical management. Here, we surveyed the use of ML methods to address clinical problems in autoimmune disease. A systematic review was conducted using MEDLINE, embase and computers and applied sciences complete databases. Relevant papers included &quot;machine learning&quot; or &quot;artificial intelligence&quot; and the autoimmune diseases search term(s) in their title, abstract or key words. Exclusion criteria: studies not written in English, no real human patient data included, publication prior to 2001, studies that were not peer reviewed, non-autoimmune disease comorbidity research and review papers. 169 (of 702) studies met the criteria for inclusion. Support vector machines and random forests were the most popular ML methods used. ML models using data on multiple sclerosis, rheumatoid arthritis and inflammatory bowel disease were most common. A small proportion of studies (7.7% or 13/169) combined different data types in the modelling process. Cross-validation, combined with a separate testing set for more robust model evaluation occurred in 8.3% of papers (14/169). The field may benefit from adopting a best practice of validation, cross-validation and independent testing of ML models. Many models achieved good predictive results in simple scenarios (e.g. classification of cases and controls). Progression to more complex predictive models may be achievable in future through integration of multiple data types.","Autoimmune diseases are chronic, multifactorial conditions. Through machine learning (ML), a branch of the wider field of artificial intelligence, it is possible to extract patterns within patient data, and exploit these patterns to predict patient outcomes for improved clinical management. Here, we surveyed the use of ML methods to address clinical problems in autoimmune disease. A systematic review was conducted using MEDLINE, embase and computers and applied sciences complete databases. Relevant papers included ""machine learning"" or ""artificial intelligence"" and the autoimmune diseases search term(s) in their title, abstract or key words. Exclusion criteria: studies not written in English, no real human patient data included, publication prior to 2001, studies that were not peer reviewed, non-autoimmune disease comorbidity research and review papers. 169 (of 702) studies met the criteria for inclusion. Support vector machines and random forests were the most popular ML methods used. ML models using data on multiple sclerosis, rheumatoid arthritis and inflammatory bowel disease were most common. A small proportion of studies (7.7% or 13/169) combined different data types in the modelling process. Cross-validation, combined with a separate testing set for more robust model evaluation occurred in 8.3% of papers (14/169). The field may benefit from adopting a best practice of validation, cross-validation and independent testing of ML models. Many models achieved good predictive results in simple scenarios (e.g. classification of cases and controls). Progression to more complex predictive models may be achievable in future through integration of multiple data types.","Stafford, Kellermann, Mossotto, Beattie, MacArthur, Ennis","Stafford, Kellermann, Mossotto, Beattie, MacArthur, Ennis",https://doi.org/10.1038/s41746-020-0229-3,https://doi.org/10.1038/s41746-020-0229-3,2021-08-03
17028.0,pubmed,pubmed,Pharmacological Treatment of Methamphetamine/Amphetamine Dependence: A Systematic Review,Pharmacological Treatment of Methamphetamine/Amphetamine Dependence: A Systematic Review,"Stimulant drugs are second only to cannabis as the most widely used class of illicit drug globally, accounting for 68 million past-year consumers. Dependence on amphetamines (AMPH) or methamphetamine (MA) is a growing global concern. Yet, there is no established pharmacotherapy for AMPH/MA dependence. A comprehensive assessment of the research literature on pharmacotherapy for AMPH/MA dependence may inform treatment guidelines and future research directions. We systematically reviewed the peer-reviewed literature via the electronic databases PubMed, EMBASE, CINAHL and SCOPUS for randomised controlled trials reported in the English language examining a pharmacological treatment for AMPH/MA dependence or use disorder. We included all studies published to 19 June 2019. The selected studies were evaluated for design; methodology; inclusion and exclusion criteria; sample size; pharmacological and (if included) psychosocial interventions; length of follow-up and follow-up schedules; outcome variables and measures; results; overall conclusions and risk of bias. Outcome measures were any reported impact of treatment related to AMPH/MA use. Our search returned 43 studies that met our criteria, collectively enrolling 4065 participants and reporting on 23 individual pharmacotherapies, alone or in combination. Disparate outcomes and measures (nÃ¢â‚¬â€°=Ã¢â‚¬â€°55 for the primary outcomes) across studies did not allow for meta-analyses. Some studies demonstrated mixed or weak positive signals (often in defined populations, e.g. men who have sex with men), with some variation in efficacy signals dependent on baseline frequency of AMPH/MA use. The most consistent positive findings have been demonstrated with stimulant agonist treatment (dexamphetamine and methylphenidate), naltrexone and topiramate. Less consistent benefits have been shown with the antidepressants bupropion and mirtazapine, the glutamatergic agent riluzole and the corticotropin releasing factor (CRF-1) antagonist pexacerfont; whilst in general, antidepressant medications (e.g. selective serotonin reuptake inhibitors [SSRIs], tricyclic antidepressants [TCAs]) have not been effective in reducing AMPH/MA use. No pharmacotherapy yielded convincing results for the treatment of AMPH/MA dependence; mostly studies were underpowered and had low treatment completion rates. However, there were positive signals from several agents that warrant further investigation in larger scale studies; agonist therapies show promise. Common outcome measures should include change in use days. Future research must address the heterogeneity of AMPH/MA dependence (e.g. coexisting conditions, severity of disorder, differences between MA and AMPH dependence) and the role of psychosocial intervention.","Stimulant drugs are second only to cannabis as the most widely used class of illicit drug globally, accounting for 68 million past-year consumers. Dependence on amphetamines (AMPH) or methamphetamine (MA) is a growing global concern. Yet, there is no established pharmacotherapy for AMPH/MA dependence. A comprehensive assessment of the research literature on pharmacotherapy for AMPH/MA dependence may inform treatment guidelines and future research directions. We systematically reviewed the peer-reviewed literature via the electronic databases PubMed, EMBASE, CINAHL and SCOPUS for randomised controlled trials reported in the English language examining a pharmacological treatment for AMPH/MA dependence or use disorder. We included all studies published to 19 June 2019. The selected studies were evaluated for design; methodology; inclusion and exclusion criteria; sample size; pharmacological and (if included) psychosocial interventions; length of follow-up and follow-up schedules; outcome variables and measures; results; overall conclusions and risk of bias. Outcome measures were any reported impact of treatment related to AMPH/MA use. Our search returned 43 studies that met our criteria, collectively enrolling 4065 participants and reporting on 23 individual pharmacotherapies, alone or in combination. Disparate outcomes and measures (nâ€‰=â€‰55 for the primary outcomes) across studies did not allow for meta-analyses. Some studies demonstrated mixed or weak positive signals (often in defined populations, e.g. men who have sex with men), with some variation in efficacy signals dependent on baseline frequency of AMPH/MA use. The most consistent positive findings have been demonstrated with stimulant agonist treatment (dexamphetamine and methylphenidate), naltrexone and topiramate. Less consistent benefits have been shown with the antidepressants bupropion and mirtazapine, the glutamatergic agent riluzole and the corticotropin releasing factor (CRF-1) antagonist pexacerfont; whilst in general, antidepressant medications (e.g. selective serotonin reuptake inhibitors [SSRIs], tricyclic antidepressants [TCAs]) have not been effective in reducing AMPH/MA use. No pharmacotherapy yielded convincing results for the treatment of AMPH/MA dependence; mostly studies were underpowered and had low treatment completion rates. However, there were positive signals from several agents that warrant further investigation in larger scale studies; agonist therapies show promise. Common outcome measures should include change in use days. Future research must address the heterogeneity of AMPH/MA dependence (e.g. coexisting conditions, severity of disorder, differences between MA and AMPH dependence) and the role of psychosocial intervention.","Siefried, Acheson, Lintzeris, Ezard","Siefried, Acheson, Lintzeris, Ezard",https://doi.org/10.1007/s40263-020-00711-x,https://doi.org/10.1007/s40263-020-00711-x,2021-08-03
17032.0,pubmed,pubmed,Radiomic prediction of mutation status based on MR imaging of lung cancer brain metastases,Radiomic prediction of mutation status based on MR imaging of lung cancer brain metastases,"Lung cancer metastases comprise most of all brain metastases in adults and most brain metastases are diagnosed by magnetic resonance (MR) scans. The purpose of this study was to conduct an MR imaging-based radiomic analysis of brain metastatic lesions from patients with primary lung cancer to classify mutational status of the metastatic disease. We retrospectively identified lung cancer patients with brain metastases treated at our institution between 2009 and 2017 who underwent genotype testing of their primary lung cancer. Brain MR Images were used for segmentation of enhancing tumors and peritumoral edema, and for radiomic feature extraction. The most relevant radiomic features were identified and used with clinical data to train random forest classifiers to classify the mutation status. Of 110 patients in the study cohort (mean age 57.51Ã‚Â Ã‚Â±Ã‚Â 12.32Ã‚Â years; M: FÃ‚Â =Ã‚Â 37:73), 75 had an EGFR mutation, 21 had an ALK translocation, and 15 had a KRAS mutation. One patient had both ALK translocation and EGFR mutation. Majority of radiomic features most relevant for mutation classification were textural. Model building using both radiomic features and clinical data yielded more accurate classifications than using either alone. For classification of EGFR, ALK, and KRAS mutation status, the model built with both radiomic features and clinical data resulted in area-under-the-curve (AUC) values based on cross-validation of 0.912, 0.915, and 0.985, respectively. Our study demonstrated that MR imaging-based radiomic analysis of brain metastases in patients with primary lung cancer may be used to classify mutation status. This approach may be useful for devising treatment strategies and informing prognosis.","Lung cancer metastases comprise most of all brain metastases in adults and most brain metastases are diagnosed by magnetic resonance (MR) scans. The purpose of this study was to conduct an MR imaging-based radiomic analysis of brain metastatic lesions from patients with primary lung cancer to classify mutational status of the metastatic disease. We retrospectively identified lung cancer patients with brain metastases treated at our institution between 2009 and 2017 who underwent genotype testing of their primary lung cancer. Brain MR Images were used for segmentation of enhancing tumors and peritumoral edema, and for radiomic feature extraction. The most relevant radiomic features were identified and used with clinical data to train random forest classifiers to classify the mutation status. Of 110 patients in the study cohort (mean age 57.51Â Â±Â 12.32Â years; M: FÂ =Â 37:73), 75 had an EGFR mutation, 21 had an ALK translocation, and 15 had a KRAS mutation. One patient had both ALK translocation and EGFR mutation. Majority of radiomic features most relevant for mutation classification were textural. Model building using both radiomic features and clinical data yielded more accurate classifications than using either alone. For classification of EGFR, ALK, and KRAS mutation status, the model built with both radiomic features and clinical data resulted in area-under-the-curve (AUC) values based on cross-validation of 0.912, 0.915, and 0.985, respectively. Our study demonstrated that MR imaging-based radiomic analysis of brain metastases in patients with primary lung cancer may be used to classify mutation status. This approach may be useful for devising treatment strategies and informing prognosis.","Chen, Jin, Ye, Mambetsariev, Daniel, Wang, Wong, Rockne, Colen, Holodny, Sampath, Salgia","Chen, Jin, Ye, Mambetsariev, Daniel, Wang, Wong, Rockne, Colen, Holodny, Sampath, Salgia",https://doi.org/10.1016/j.mri.2020.03.002,https://doi.org/10.1016/j.mri.2020.03.002,2021-08-03
17034.0,pubmed,pubmed,Fully automated radiosynthesis and quality control of estrogen receptor targeting radiopharmaceutical 16ÃŽÂ±-[<sup>18</sup>F]fluoroestradiol ([<sup>18</sup>F]FES) for human breast cancer imaging,Fully automated radiosynthesis and quality control of estrogen receptor targeting radiopharmaceutical 16Î±-[<sup>18</sup>F]fluoroestradiol ([<sup>18</sup>F]FES) for human breast cancer imaging,"16ÃŽÂ±-[<sup>18</sup>F]Fluoroestradiol ([<sup>18</sup>F]FES) is the most successful estrogen receptor (ER) targeting radiopharmaceutical to date. [<sup>18</sup>F]FES has been extensively used for positron emission tomography (PET) to assess the ER expression in breast cancer and to monitor the response of breast cancer to antiestrogen therapy. To address local investigator needs for [<sup>18</sup>F]FES-PET, we sought to adapt established literature methods to our in-house multi-purpose <sup>18</sup>F-radiosynthesis module for [<sup>18</sup>F]FES production. Here we describe facile fully automated radiosynthesis and quality control (QC) of [<sup>18</sup>F]FES using our home-built automated multi-purpose <sup>18</sup>F-radiosynthesis module. [<sup>18</sup>F]FES was produced via two-step-one-pot synthesis using cyclic sulfate precursor, and purified by semi-preparative reversed-phase (RP) high performance liquid chromatography (HPLC) with a C18 column followed by solid-phase extraction (SPE) with a C18 Plus Sep-Pak cartridge trap/release formulation. The overall synthesis time was 75-80Ã‚Â min, and the radiochemical yield was 30-35% decay corrected to end of bombardment (EOB), based on H[<sup>18</sup>F]F. The radiochemical purity was &gt;99%, and the molar activity (A<sub>m</sub>) was 182-470 GBq/ÃŽÂ¼mol at EOB. The [<sup>18</sup>F]FES dose meets all QC criteria for clinical use, and is suitable for clinical PET study of breast cancer.","16Î±-[<sup>18</sup>F]Fluoroestradiol ([<sup>18</sup>F]FES) is the most successful estrogen receptor (ER) targeting radiopharmaceutical to date. [<sup>18</sup>F]FES has been extensively used for positron emission tomography (PET) to assess the ER expression in breast cancer and to monitor the response of breast cancer to antiestrogen therapy. To address local investigator needs for [<sup>18</sup>F]FES-PET, we sought to adapt established literature methods to our in-house multi-purpose <sup>18</sup>F-radiosynthesis module for [<sup>18</sup>F]FES production. Here we describe facile fully automated radiosynthesis and quality control (QC) of [<sup>18</sup>F]FES using our home-built automated multi-purpose <sup>18</sup>F-radiosynthesis module. [<sup>18</sup>F]FES was produced via two-step-one-pot synthesis using cyclic sulfate precursor, and purified by semi-preparative reversed-phase (RP) high performance liquid chromatography (HPLC) with a C18 column followed by solid-phase extraction (SPE) with a C18 Plus Sep-Pak cartridge trap/release formulation. The overall synthesis time was 75-80Â min, and the radiochemical yield was 30-35% decay corrected to end of bombardment (EOB), based on H[<sup>18</sup>F]F. The radiochemical purity was &gt;99%, and the molar activity (A<sub>m</sub>) was 182-470 GBq/Î¼mol at EOB. The [<sup>18</sup>F]FES dose meets all QC criteria for clinical use, and is suitable for clinical PET study of breast cancer.","Wang, Glick-Wilson, Zheng","Wang, Glick-Wilson, Zheng",https://doi.org/10.1016/j.apradiso.2020.109109,https://doi.org/10.1016/j.apradiso.2020.109109,2021-08-03
3412.0,,pubmed,Graph-Representation of Patient Data: a Systematic Literature Review,Graph-Representation of Patient Data: a Systematic Literature Review,"Graph theory is a well-established theory with many methods used in mathematics to study graph structures. In the field of medicine, electronic health records (EHR) are commonly used to store and analyze patient data. Consequently, it seems straight-forward to perform research on modeling EHR data as graphs. This systematic literature review aims to investigate the frontiers of the current research in the field of graphs representing and processing patient data. We want to show, which areas of research in this context need further investigation. The databases MEDLINE, Web of Science, IEEE Xplore and ACM digital library were queried by using the search terms health record, graph and related terms. Based on the 'Preferred Reporting Items for Systematic Reviews and Meta-Analysis' (PRISMA) statement guidelines the articles were screened and evaluated using full-text analysis. Eleven out of 383 articles found in systematic literature review were finally included for analysis in this literature review. Most of them use graphs to represent temporal relations, often representing the connection among laboratory data points. Only two papers report that the graph data were further processed by comparing the patient graphs using similarity measurements. Graphs representing individual patients are hardly used in research context, only eleven papers considered such kind of graphs in their investigations. The potential of graph theoretical algorithms, which are already well established, could help increasing this research field, but currently there are too few papers to estimate how this area of research will develop. Altogether, the use of such patient graphs could be a promising technique to develop decision support systems for diagnosis, medication or therapy of patients using similarity measurements or different kinds of analysis.","Graph theory is a well-established theory with many methods used in mathematics to study graph structures. In the field of medicine, electronic health records (EHR) are commonly used to store and analyze patient data. Consequently, it seems straight-forward to perform research on modeling EHR data as graphs. This systematic literature review aims to investigate the frontiers of the current research in the field of graphs representing and processing patient data. We want to show, which areas of research in this context need further investigation. The databases MEDLINE, Web of Science, IEEE Xplore and ACM digital library were queried by using the search terms health record, graph and related terms. Based on the ""Preferred Reporting Items for Systematic Reviews and Meta-Analysis"" (PRISMA) statement guidelines the articles were screened and evaluated using full-text analysis. Eleven out of 383 articles found in systematic literature review were finally included for analysis in this literature review. Most of them use graphs to represent temporal relations, often representing the connection among laboratory data points. Only two papers report that the graph data were further processed by comparing the patient graphs using similarity measurements. Graphs representing individual patients are hardly used in research context, only eleven papers considered such kind of graphs in their investigations. The potential of graph theoretical algorithms, which are already well established, could help increasing this research field, but currently there are too few papers to estimate how this area of research will develop. Altogether, the use of such patient graphs could be a promising technique to develop decision support systems for diagnosis, medication or therapy of patients using similarity measurements or different kinds of analysis.","Schrodt, J.
 and Dudchenko, A.
 and Knaup-Gregori, P.
 and Ganzinger, M.","Schrodt, Dudchenko, Knaup-Gregori, Ganzinger",https://dx.doi.org/10.1007/s10916-020-1538-4,https://doi.org/10.1007/s10916-020-1538-4,2021-08-03
17040.0,pubmed,pubmed,Artificial intelligence and radiomics enhance the positive predictive value of digital chest tomosynthesis for lung cancer detection within SOS clinical trial,Artificial intelligence and radiomics enhance the positive predictive value of digital chest tomosynthesis for lung cancer detection within SOS clinical trial,To enhance the positive predictive value (PPV) of chest digital tomosynthesis (DTS) in the lung cancer detection with the analysis of radiomics features. The investigation was carried out within the SOS clinical trial (NCT03645018) for lung cancer screening with DTS. Lung nodules were identified by visual analysis and then classified using the diameter and the radiological aspect of the nodule following lung-RADS. Haralick texture features were extracted from the segmented nodules. Both semantic variables and radiomics features were used to build a predictive model using logistic regression on a subset of variables selected with backward feature selection and using two machine learning: a Random Forest and a neural network with the whole subset of variables. The methods were applied to a train set and validated on a test set where diagnostic accuracy metrics were calculated. Binary visual analysis had a good sensitivity (0.95) but a low PPV (0.14). Lung-RADS classification increased the PPV (0.19) but with an unacceptable low sensitivity (0.65). Logistic regression showed a mildly increased PPV (0.29) but a lower sensitivity (0.20). Random Forest demonstrated a moderate PPV (0.40) but with a low sensitivity (0.30). Neural network demonstrated to be the best predictor with a high PPV (0.95) and a high sensitivity (0.90). The neural network demonstrated the best PPV. The use of visual analysis along with neural network could help radiologists to reduce the number of false positive in DTS. Ã¢â‚¬Â¢ We investigated several approaches to enhance the positive predictive value of chest digital tomosynthesis in the lung cancer detection. Ã¢â‚¬Â¢ Neural network demonstrated to be the best predictor with a nearly perfect PPV. Ã¢â‚¬Â¢ Neural network could help radiologists to reduce the number of false positive in DTS.,To enhance the positive predictive value (PPV) of chest digital tomosynthesis (DTS) in the lung cancer detection with the analysis of radiomics features. The investigation was carried out within the SOS clinical trial (NCT03645018) for lung cancer screening with DTS. Lung nodules were identified by visual analysis and then classified using the diameter and the radiological aspect of the nodule following lung-RADS. Haralick texture features were extracted from the segmented nodules. Both semantic variables and radiomics features were used to build a predictive model using logistic regression on a subset of variables selected with backward feature selection and using two machine learning: a Random Forest and a neural network with the whole subset of variables. The methods were applied to a train set and validated on a test set where diagnostic accuracy metrics were calculated. Binary visual analysis had a good sensitivity (0.95) but a low PPV (0.14). Lung-RADS classification increased the PPV (0.19) but with an unacceptable low sensitivity (0.65). Logistic regression showed a mildly increased PPV (0.29) but a lower sensitivity (0.20). Random Forest demonstrated a moderate PPV (0.40) but with a low sensitivity (0.30). Neural network demonstrated to be the best predictor with a high PPV (0.95) and a high sensitivity (0.90). The neural network demonstrated the best PPV. The use of visual analysis along with neural network could help radiologists to reduce the number of false positive in DTS. â€¢ We investigated several approaches to enhance the positive predictive value of chest digital tomosynthesis in the lung cancer detection. â€¢ Neural network demonstrated to be the best predictor with a nearly perfect PPV. â€¢ Neural network could help radiologists to reduce the number of false positive in DTS.,"Chauvie, De Maggi, Baralis, Dalmasso, Berchialla, Priotto, Violino, Mazza, Melloni, Grosso, Biggi, Campione, Fortunato, De Maggi, Chauvie, Colantonio, Grosso, Melloni, Mazza, Stanzi, Noceti, Pellegrino, Russi","Chauvie, De Maggi, Baralis, Dalmasso, Berchialla, Priotto, Violino, Mazza, Melloni, Grosso, Biggi, Campione, Fortunato, De Maggi, Chauvie, Colantonio, Grosso, Melloni, Mazza, Stanzi, Noceti, Pellegrino, Russi",https://doi.org/10.1007/s00330-020-06783-z,https://doi.org/10.1007/s00330-020-06783-z,2021-08-03
17043.0,pubmed,pubmed,An immune-centric exploration of BRCA1 and BRCA2 germline mutation related breast and ovarian cancers,An immune-centric exploration of BRCA1 and BRCA2 germline mutation related breast and ovarian cancers,"BRCA1/2 germline mutation related cancers are candidates for new immune therapeutic interventions. This study was a hypothesis generating exploration of genomic data collected at diagnosis for 19 patients. The prominent tumor mutation burden (TMB) in hereditary breast and ovarian cancers in this cohort was not correlated with high global immune activity in their microenvironments. More information is needed about the relationship between genomic instability, phenotypes and immune microenvironments of these hereditary tumors in order to find appropriate markers of immune activity and the most effective anticancer immune strategies. Mining and statistical analyses of the original DNA and RNA sequencing data and The Cancer Genome Atlas data were performed. To interpret the data, we have used published literature and web available resources such as Gene Ontology, The Cancer immunome Atlas and the Cancer Research Institute iAtlas. We found that BRCA1/2 germline related breast and ovarian cancers do not represent a unique phenotypic identity, but they express a range of phenotypes similar to sporadic cancers. All breast and ovarian BRCA1/2 related tumors are characterized by high homologous recombination deficiency (HRD) and low aneuploidy. Interestingly, all sporadic high grade serous ovarian cancers (HGSOC) and most of the subtypes of triple negative breast cancers (TNBC) also express a high degree of HRD. TMB is not associated with the magnitude of the immune response in hereditary BRCA1/2 related breast and ovarian cancers or in sporadic TNBC and sporadic HGSOC. Hereditary tumors express phenotypes as heterogenous as sporadic tumors with various degree of &quot;BRCAness&quot; and various characteristics of the immune microenvironments. The subtyping criteria developed for sporadic tumors can be applied for the classification of hereditary tumors and possibly also characterization of their immune microenvironment. A high HRD score may be a good candidate biomarker for response to platinum, and potentially PARP-inhibition. Phase I Study of the Oral PI3kinase Inhibitor BKM120 or BYL719 and the Oral PARP Inhibitor Olaparib in Patients With Recurrent TNBC or HGSOC (NCT01623349), first posted on June 20, 2012. The design and the outcome of the clinical trial is not in the scope of this study.","BRCA1/2 germline mutation related cancers are candidates for new immune therapeutic interventions. This study was a hypothesis generating exploration of genomic data collected at diagnosis for 19 patients. The prominent tumor mutation burden (TMB) in hereditary breast and ovarian cancers in this cohort was not correlated with high global immune activity in their microenvironments. More information is needed about the relationship between genomic instability, phenotypes and immune microenvironments of these hereditary tumors in order to find appropriate markers of immune activity and the most effective anticancer immune strategies. Mining and statistical analyses of the original DNA and RNA sequencing data and The Cancer Genome Atlas data were performed. To interpret the data, we have used published literature and web available resources such as Gene Ontology, The Cancer immunome Atlas and the Cancer Research Institute iAtlas. We found that BRCA1/2 germline related breast and ovarian cancers do not represent a unique phenotypic identity, but they express a range of phenotypes similar to sporadic cancers. All breast and ovarian BRCA1/2 related tumors are characterized by high homologous recombination deficiency (HRD) and low aneuploidy. Interestingly, all sporadic high grade serous ovarian cancers (HGSOC) and most of the subtypes of triple negative breast cancers (TNBC) also express a high degree of HRD. TMB is not associated with the magnitude of the immune response in hereditary BRCA1/2 related breast and ovarian cancers or in sporadic TNBC and sporadic HGSOC. Hereditary tumors express phenotypes as heterogenous as sporadic tumors with various degree of ""BRCAness"" and various characteristics of the immune microenvironments. The subtyping criteria developed for sporadic tumors can be applied for the classification of hereditary tumors and possibly also characterization of their immune microenvironment. A high HRD score may be a good candidate biomarker for response to platinum, and potentially PARP-inhibition. Phase I Study of the Oral PI3kinase Inhibitor BKM120 or BYL719 and the Oral PARP Inhibitor Olaparib in Patients With Recurrent TNBC or HGSOC (NCT01623349), first posted on June 20, 2012. The design and the outcome of the clinical trial is not in the scope of this study.","Przybytkowski, Davis, Hosny, Eismann, Matulonis, Wulf, Nabavi","Przybytkowski, Davis, Hosny, Eismann, Matulonis, Wulf, Nabavi",https://doi.org/10.1186/s12885-020-6605-1,https://doi.org/10.1186/s12885-020-6605-1,2021-08-03
906.0,,pubmed,"Statistical significance: p value, 005 threshold, and applications to radiomics-reasons for a conservative approach","Statistical significance: p value, 005 threshold, and applications to radiomics-reasons for a conservative approach","Here, we summarise the unresolved debate about p value and its dichotomisation. We present the statement of the American Statistical Association against the misuse of statistical significance as well as the proposals to abandon the use of p value and to reduce the significance threshold from 0.05 to 0.005. We highlight reasons for a conservative approach, as clinical research needs dichotomic answers to guide decision-making, in particular in the case of diagnostic imaging and interventional radiology. With a reduced p value threshold, the cost of research could increase while spontaneous research could be reduced. Secondary evidence from systematic reviews/meta-analyses, data sharing, and cost-effective analyses are better ways to mitigate the false discovery rate and lack of reproducibility associated with the use of the 0.05 threshold. Importantly, when reporting p values, authors should always provide the actual value, not only statements of 'p < 0.05' or 'p >= 0.05', because p values give a measure of the degree of data compatibility with the null hypothesis. Notably, radiomics and big data, fuelled by the application of artificial intelligence, involve hundreds/thousands of tested features similarly to other 'omics' such as genomics, where a reduction in the significance threshold, based on well-known corrections for multiple testing, has been already adopted.","Here, we summarise the unresolved debate about p value and its dichotomisation. We present the statement of the American Statistical Association against the misuse of statistical significance as well as the proposals to abandon the use of p value and to reduce the significance threshold from 0.05 to 0.005. We highlight reasons for a conservative approach, as clinical research needs dichotomic answers to guide decision-making, in particular in the case of diagnostic imaging and interventional radiology. With a reduced p value threshold, the cost of research could increase while spontaneous research could be reduced. Secondary evidence from systematic reviews/meta-analyses, data sharing, and cost-effective analyses are better ways to mitigate the false discovery rate and lack of reproducibility associated with the use of the 0.05 threshold. Importantly, when reporting p values, authors should always provide the actual value, not only statements of ""p &lt; 0.05"" or ""p â‰¥ 0.05"", because p values give a measure of the degree of data compatibility with the null hypothesis. Notably, radiomics and big data, fuelled by the application of artificial intelligence, involve hundreds/thousands of tested features similarly to other ""omics"" such as genomics, where a reduction in the significance threshold, based on well-known corrections for multiple testing, has been already adopted.","Di Leo, G.
 and Sardanelli, F.","Di Leo, Sardanelli",https://dx.doi.org/10.1186/s41747-020-0145-y,https://doi.org/10.1186/s41747-020-0145-y,2021-08-03
3985.0,,pubmed,Temporal information extraction from mental health records to identify duration of untreated psychosis,Temporal information extraction from mental health records to identify duration of untreated psychosis,"BACKGROUND: Duration of untreated psychosis (DUP) is an important clinical construct in the field of mental health, as longer DUP can be associated with worse intervention outcomes. DUP estimation requires knowledge about when psychosis symptoms first started (symptom onset), and when psychosis treatment was initiated. Electronic health records (EHRs) represent a useful resource for retrospective clinical studies on DUP, but the core information underlying this construct is most likely to lie in free text, meaning it is not readily available for clinical research. Natural Language Processing (NLP) is a means to addressing this problem by automatically extracting relevant information in a structured form. As a first step, it is important to identify appropriate documents, i.e., those that are likely to include the information of interest. Next, temporal information extraction methods are needed to identify time references for early psychosis symptoms. This NLP challenge requires solving three different tasks: time expression extraction, symptom extraction, and temporal 'linking'. In this study, we focus on the first step, using two relevant EHR datasets. RESULTS: We applied a rule-based NLP system for time expression extraction that we had previously adapted to a corpus of mental health EHRs from patients with a diagnosis of schizophrenia (first referrals). We extended this work by applying this NLP system to a larger set of documents and patients, to identify additional texts that would be relevant for our long-term goal, and developed a new corpus from a subset of these new texts (early intervention services). Furthermore, we added normalized value annotations ('2011-05') to the annotated time expressions ('May 2011') in both corpora. The finalized corpora were used for further NLP development and evaluation, with promising results (normalization accuracy 71-86%). To highlight the specificities of our annotation task, we also applied the final adapted NLP system to a different temporally annotated clinical corpus. CONCLUSIONS: Developing domain-specific methods is crucial to address complex NLP tasks such as symptom onset extraction and retrospective calculation of duration of a preclinical syndrome. To the best of our knowledge, this is the first clinical text resource annotated for temporal entities in the mental health domain.","Duration of untreated psychosis (DUP) is an important clinical construct in the field of mental health, as longer DUP can be associated with worse intervention outcomes. DUP estimation requires knowledge about when psychosis symptoms first started (symptom onset), and when psychosis treatment was initiated. Electronic health records (EHRs) represent a useful resource for retrospective clinical studies on DUP, but the core information underlying this construct is most likely to lie in free text, meaning it is not readily available for clinical research. Natural Language Processing (NLP) is a means to addressing this problem by automatically extracting relevant information in a structured form. As a first step, it is important to identify appropriate documents, i.e., those that are likely to include the information of interest. Next, temporal information extraction methods are needed to identify time references for early psychosis symptoms. This NLP challenge requires solving three different tasks: time expression extraction, symptom extraction, and temporal ""linking"". In this study, we focus on the first step, using two relevant EHR datasets. We applied a rule-based NLP system for time expression extraction that we had previously adapted to a corpus of mental health EHRs from patients with a diagnosis of schizophrenia (first referrals). We extended this work by applying this NLP system to a larger set of documents and patients, to identify additional texts that would be relevant for our long-term goal, and developed a new corpus from a subset of these new texts (early intervention services). Furthermore, we added normalized value annotations (""2011-05"") to the annotated time expressions (""May 2011"") in both corpora. The finalized corpora were used for further NLP development and evaluation, with promising results (normalization accuracy 71-86%). To highlight the specificities of our annotation task, we also applied the final adapted NLP system to a different temporally annotated clinical corpus. Developing domain-specific methods is crucial to address complex NLP tasks such as symptom onset extraction and retrospective calculation of duration of a preclinical syndrome. To the best of our knowledge, this is the first clinical text resource annotated for temporal entities in the mental health domain.","Viani, N.
 and Kam, J.
 and Yin, L.
 and Bittar, A.
 and Dutta, R.
 and Patel, R.
 and Stewart, R.
 and Velupillai, S.","Viani, Kam, Yin, Bittar, Dutta, Patel, Stewart, Velupillai",not available,https://doi.org/10.1186/s13326-020-00220-2,2021-08-03
17050.0,pubmed,pubmed,Machine Learning techniques in breast cancer prognosis prediction: A primary evaluation,Machine Learning techniques in breast cancer prognosis prediction: A primary evaluation,"More than 750Ã‚Â 000 women in Italy are surviving a diagnosis of breast cancer. A large body of literature tells us which characteristics impact the most on their prognosis. However, the prediction of each disease course and then the establishment of a therapeutic plan and follow-up tailored to the patient is still very complicated. In order to address this issue, a multidisciplinary approach has become widely accepted, while the Multigene Signature Panels and the Nottingham Prognostic Index are still discussed options. The current technological resources permit to gather many data for each patient. Machine Learning (ML) allows us to draw on these data, to discover their mutual relations and to esteem the prognosis for the new instances. This study provides a primary evaluation of the application of ML to predict breast cancer prognosis. We analyzed 1021 patients who underwent surgery for breast cancer in our Institute and we included 610 of them. Three outcomes were chosen: cancer recurrence (both loco-regional and systemic) and death from the disease within 32Ã‚Â months. We developed two types of ML models for every outcome (Artificial Neural Network and Support Vector Machine). Each ML algorithm was tested in accuracy (=95.29%-96.86%), sensitivity (=0.35-0.64), specificity (=0.97-0.99), and AUC (=0.804-0.916). These models might become an additional resource to evaluate the prognosis of breast cancer patients in our daily clinical practice. Before that, we should increase their sensitivity, according to literature, by considering a wider population sample with a longer period of follow-up. However, specificity, accuracy, minimal additional costs, and reproducibility are already encouraging.","More than 750Â 000 women in Italy are surviving a diagnosis of breast cancer. A large body of literature tells us which characteristics impact the most on their prognosis. However, the prediction of each disease course and then the establishment of a therapeutic plan and follow-up tailored to the patient is still very complicated. In order to address this issue, a multidisciplinary approach has become widely accepted, while the Multigene Signature Panels and the Nottingham Prognostic Index are still discussed options. The current technological resources permit to gather many data for each patient. Machine Learning (ML) allows us to draw on these data, to discover their mutual relations and to esteem the prognosis for the new instances. This study provides a primary evaluation of the application of ML to predict breast cancer prognosis. We analyzed 1021 patients who underwent surgery for breast cancer in our Institute and we included 610 of them. Three outcomes were chosen: cancer recurrence (both loco-regional and systemic) and death from the disease within 32Â months. We developed two types of ML models for every outcome (Artificial Neural Network and Support Vector Machine). Each ML algorithm was tested in accuracy (=95.29%-96.86%), sensitivity (=0.35-0.64), specificity (=0.97-0.99), and AUC (=0.804-0.916). These models might become an additional resource to evaluate the prognosis of breast cancer patients in our daily clinical practice. Before that, we should increase their sensitivity, according to literature, by considering a wider population sample with a longer period of follow-up. However, specificity, accuracy, minimal additional costs, and reproducibility are already encouraging.","Boeri, Chiappa, Galli, De Berardinis, Bardelli, Carcano, Rovera","Boeri, Chiappa, Galli, De Berardinis, Bardelli, Carcano, Rovera",https://doi.org/10.1002/cam4.2811,https://doi.org/10.1002/cam4.2811,2021-08-03
1066.0,,pubmed,Watch this space: a systematic review of the use of video-based media as a patient education tool in ophthalmology,Watch this space: a systematic review of the use of video-based media as a patient education tool in ophthalmology,"Effective clinician-patient communication is particularly important in ophthalmology where long-term adherence to treatment is often required. However, in the context of increasingly pressurised clinics, there is a tendency to resort to written information leaflets not suited to patients with visual impairment, non-English speakers or those with low levels of literacy. Video-based media could be harnessed to enhance clinician-patient communication. This systematic review aimed to assess the efficacy of using video-based media for patient education in ophthalmology. A pre-defined search strategy was used by two independent researchers to systematically review the PubMed, MEDLINE, EMBASE and PsycINFO databases. Eligible articles included peer-reviewed studies involving ophthalmology patients, who received a solely video-based educational intervention to assess for improvement in patient knowledge, behaviour and overall health-related outcomes. The search yielded 481 studies of which 31 passed initial screening. Following full-text analysis, 12 studies met the inclusion criteria, of which seven studies (58.3%) were randomised controlled trials. The majority of studies (58.3%) reported outcomes on patient comprehension with 5/7 (71%) showing statistically significant improvement after video intervention. Four studies (33.3%) reported on patient performance in a task (e.g. drop application method) or overall health-related outcome with 2/4 (50%) showing statistically significant improvement after intervention. Though more evidence is needed, the use of video-based media appears to be effective in improving patient understanding and in certain cases may ameliorate overall outcome. There is a paucity of well-designed studies and future research is required to fully examine the role of video-based media in patient education.","Effective clinician-patient communication is particularly important in ophthalmology where long-term adherence to treatment is often required. However, in the context of increasingly pressurised clinics, there is a tendency to resort to written information leaflets not suited to patients with visual impairment, non-English speakers or those with low levels of literacy. Video-based media could be harnessed to enhance clinician-patient communication. This systematic review aimed to assess the efficacy of using video-based media for patient education in ophthalmology. A pre-defined search strategy was used by two independent researchers to systematically review the PubMed, MEDLINE, EMBASE and PsycINFO databases. Eligible articles included peer-reviewed studies involving ophthalmology patients, who received a solely video-based educational intervention to assess for improvement in patient knowledge, behaviour and overall health-related outcomes. The search yielded 481 studies of which 31 passed initial screening. Following full-text analysis, 12 studies met the inclusion criteria, of which seven studies (58.3%) were randomised controlled trials. The majority of studies (58.3%) reported outcomes on patient comprehension with 5/7 (71%) showing statistically significant improvement after video intervention. Four studies (33.3%) reported on patient performance in a task (e.g. drop application method) or overall health-related outcome with 2/4 (50%) showing statistically significant improvement after intervention. Though more evidence is needed, the use of video-based media appears to be effective in improving patient understanding and in certain cases may ameliorate overall outcome. There is a paucity of well-designed studies and future research is required to fully examine the role of video-based media in patient education. æ‘˜è¦: å› çœ¼ç§‘ç–¾ç—…é€šå¸¸éœ€è¦é•¿æœŸæ²»ç–—, æœ‰æ•ˆçš„åŒ»æ‚£æ²Ÿé€šåœ¨çœ¼ç§‘ä¸´åºŠå·¥ä½œä¸­å°¤ä¸ºé‡è¦ã€‚ç„¶è€Œ, æ—¥ç›Šå¢žé•¿çš„ä¸´åºŠå·¥ä½œåŽ‹åŠ›ä½¿æ²Ÿé€šå€¾å‘äºŽä¹¦é¢å½¢å¼, ä½†å¹¶ä¸é€‚åˆè§†åŠ›å·®ã€éžè‹±è¯­ä½¿ç”¨è€…æˆ–è¯†å­—æ°´å¹³ä½Žçš„äººã€‚ä»¥è§†é¢‘ä¸ºåª’ä»‹å¯å¢žè¿›åŒ»æ‚£æ²Ÿé€šã€‚æœ¬æ–‡æ—¨åœ¨è¯„ä¼°çœ¼ç§‘æ‚£è€…æ•™è‚²ä¸­åº”ç”¨è§†é¢‘åª’ä½“çš„æ•ˆæžœã€‚ä¸¤ä½ç ”ç©¶äººå‘˜èƒŒé èƒŒä½¿ç”¨é¢„å…ˆå®šä¹‰æœç´¢ç­–ç•¥, ç³»ç»Ÿåœ°æ£€ç´¢äº†PubMedã€MEDLINEã€EMBASEå’ŒPsychINFOæ•°æ®åº“ã€‚ç¬¦åˆæ¡ä»¶çš„æ–‡çŒ®åŒ…æ‹¬åŒè¡Œè¯„è®®ç ”ç©¶, ç ”ç©¶ä¸­çœ¼ç§‘æ‚£è€…ä»…æŽ¥å—è§†é¢‘æ•™è‚², ä»¥è¯„ä¼°æ‚£è€…çŸ¥è¯†æ°´å¹³ã€è¡Œä¸ºå’Œæ€»ä½“å¥åº·ç›¸å…³çš„æ”¹å–„æƒ…å†µã€‚æœ¬æ–‡å…±æ£€ç´¢481é¡¹ç ”ç©¶, å…¶ä¸­31é¡¹é€šè¿‡åˆç­›ã€‚ç»å…¨æ–‡åˆ†æž, 12é¡¹ç ”ç©¶ç¬¦åˆçº³å…¥æ ‡å‡†, å…¶ä¸­7é¡¹ (58.3%) ä¸ºéšæœºå¯¹ç…§è¯•éªŒã€‚å¤§å¤šæ•°ç ”ç©¶ (58.3%) æŠ¥å‘Šäº†æ‚£è€…æŽ¥å—è§†é¢‘æ•™è‚²åŽç†è§£åŠ›æé«˜çš„ç»“æžœ, å…¶ä¸­5/7 (71%) æ˜¾ç¤ºè§†é¢‘å¹²é¢„åŽæœ‰ç»Ÿè®¡å­¦æ„ä¹‰çš„æ˜¾è‘—æ”¹å–„ã€‚4é¡¹ç ”ç©¶ (33.3%) æŠ¥å‘Šäº†æ‚£è€…é’ˆå¯¹æŸé¡¹ä»»åŠ¡çš„ç†è§£åŠ› (å¦‚çœ¼è¯æ»´è¯æ–¹æ³•) æˆ–æ€»ä½“å¥åº·æƒ…å†µçš„æ”¹å–„æƒ…å†µ, å…¶ä¸­2/4 (50%) æ˜¾ç¤ºå¹²é¢„åŽæœ‰æ˜Žæ˜¾çš„æ”¹å–„ (æœ‰ç»Ÿè®¡å­¦æ„ä¹‰) ã€‚å°½ç®¡éœ€è¦æ›´å¤šè¯æ®, ä½†åº”ç”¨è§†é¢‘åª’ä½“èƒ½æœ‰æ•ˆåœ°æé«˜æ‚£è€…ç†è§£åŠ›, ä»¥åŠåœ¨æŸäº›æƒ…å†µä¸‹, å¯èƒ½æ”¹å–„æ€»ä½“é¢„åŽã€‚ç›®å‰ç¼ºä¹è®¾è®¡è‰¯å¥½çš„ä¸´åºŠå®žéªŒ, æœªæ¥çš„ç ”ç©¶éœ€è¦å…¨é¢è¯„ä¼°è§†é¢‘åª’ä½“åœ¨æ‚£è€…æ•™è‚²ä¸­çš„ä½œç”¨ã€‚.","Farwana, R.
 and Sheriff, A.
 and Manzar, H.
 and Farwana, M.
 and Yusuf, A.
 and Sheriff, I.","Farwana, Sheriff, Manzar, Farwana, Yusuf, Sheriff",https://dx.doi.org/10.1038/s41433-020-0798-z,https://doi.org/10.1038/s41433-020-0798-z,2021-08-03
17052.0,pubmed,pubmed,Natural Language Processing for Mimicking Clinical Trial Recruitment in Critical Care: AÃ‚Â Semi-Automated Simulation Based on the LeoPARDS Trial,Natural Language Processing for Mimicking Clinical Trial Recruitment in Critical Care: AÂ Semi-Automated Simulation Based on the LeoPARDS Trial,"Clinical trials often fail to recruit an adequate number of appropriate patients. Identifying eligible trial participants is resource-intensive when relying on manual review of clinical notes, particularly in critical care settings where the time window is short. Automated review of electronic health records (EHR) may help, but much of the information is in free text rather than a computable form. We applied natural language processing (NLP) to free text EHR data using the CogStack platform to simulate recruitment into the LeoPARDS study, a clinical trial aiming to reduce organ dysfunction in septic shock. We applied an algorithm to identify eligible patients using a moving 1-hour time window, and compared patients identified by our approach with those actually screened and recruited for the trial, for the time period that data were available. We manually reviewed records of a random sample of patients identified by the algorithm but not screened in the original trial. Our method identified 376 patients, including 34 patients with EHR data available who were actually recruited to LeoPARDS in our centre. The sensitivity of CogStack for identifying patients screened was 90% (95% CI 85%, 93%). Of the 203 patients identified by both manual screening and CogStack, the index date matched in 95 (47%) and CogStack was earlier in 94 (47%). In conclusion, analysis of EHR data using NLP could effectively replicate recruitment in a critical care trial, and identify some eligible patients at an earlier stage, potentially improving trial recruitment if implemented in real time.","Clinical trials often fail to recruit an adequate number of appropriate patients. Identifying eligible trial participants is resource-intensive when relying on manual review of clinical notes, particularly in critical care settings where the time window is short. Automated review of electronic health records (EHR) may help, but much of the information is in free text rather than a computable form. We applied natural language processing (NLP) to free text EHR data using the CogStack platform to simulate recruitment into the LeoPARDS study, a clinical trial aiming to reduce organ dysfunction in septic shock. We applied an algorithm to identify eligible patients using a moving 1-hour time window, and compared patients identified by our approach with those actually screened and recruited for the trial, for the time period that data were available. We manually reviewed records of a random sample of patients identified by the algorithm but not screened in the original trial. Our method identified 376 patients, including 34 patients with EHR data available who were actually recruited to LeoPARDS in our centre. The sensitivity of CogStack for identifying patients screened was 90% (95% CI 85%, 93%). Of the 203 patients identified by both manual screening and CogStack, the index date matched in 95 (47%) and CogStack was earlier in 94 (47%). In conclusion, analysis of EHR data using NLP could effectively replicate recruitment in a critical care trial, and identify some eligible patients at an earlier stage, potentially improving trial recruitment if implemented in real time.","Tissot, Shah, Brealey, Harris, Agbakoba, Folarin, Romao, Roguski, Dobson, Asselbergs","Tissot, Shah, Brealey, Harris, Agbakoba, Folarin, Romao, Roguski, Dobson, Asselbergs",https://doi.org/10.1109/JBHI.2020.2977925,https://doi.org/10.1109/JBHI.2020.2977925,2021-08-03
17053.0,pubmed,pubmed,Mobile Peer-Support for Opioid Use Disorders: Refinement of an Innovative Machine Learning Tool,Mobile Peer-Support for Opioid Use Disorders: Refinement of an Innovative Machine Learning Tool,"The majority of individuals with Opioid Use Disorder (OUD) do not receive any formal substance use treatment. Due to limited engagement and access to traditional treatment, there is increasing evidence that patients with OUDs turn to online social platforms to access peer support and obtain health-related information about addiction and recovery. Interacting with peers before and during recovery is a key component of many evidence-based addiction recovery programs, and may improve self-efficacy and treatment engagement as well as reduce relapse. Commonly-used online social platforms are limited in utility and scalability as an adjunct to addiction treatment; lack effective content moderation (e.g., misinformed advice, maliciousness or &quot;trolling&quot;); and lack common security and ethical safeguards inherent to clinical care. This present study will develop a novel, artificial-intelligence (AI) enabled, mobile treatment delivery method that fulfills the need for a robust, secure, technology-based peer support platform to support patients with OUD. Forty adults receiving outpatient buprenorphine treatment for OUD will be asked to pilot a smartphone-based mobile peer support application, the &quot;Marigold App&quot;, for a duration of six weeks. The program will use (1) a prospective cohort study to obtain text message content and feasibility metrics, and (2) qualitative interviews to evaluate usability and acceptability of the mobile platform. The Marigold mobile platform will allow patients to access a tailored chat support group 24/7 as a complement to different forms of clinical OUD treatment. Marigold can keep groups safe and constructive by augmenting chats with AI tools capable of understanding the emotional sentiment in messages, automatically &quot;flagging&quot; critical or clinically relevant content. This project will demonstrate the robustness of these AI tools by adapting them to catch OUD-specific &quot;flags&quot; in peer messages while also examining the adoptability of the platform itself within OUD patients.","The majority of individuals with Opioid Use Disorder (OUD) do not receive any formal substance use treatment. Due to limited engagement and access to traditional treatment, there is increasing evidence that patients with OUDs turn to online social platforms to access peer support and obtain health-related information about addiction and recovery. Interacting with peers before and during recovery is a key component of many evidence-based addiction recovery programs, and may improve self-efficacy and treatment engagement as well as reduce relapse. Commonly-used online social platforms are limited in utility and scalability as an adjunct to addiction treatment; lack effective content moderation (e.g., misinformed advice, maliciousness or ""trolling""); and lack common security and ethical safeguards inherent to clinical care. This present study will develop a novel, artificial-intelligence (AI) enabled, mobile treatment delivery method that fulfills the need for a robust, secure, technology-based peer support platform to support patients with OUD. Forty adults receiving outpatient buprenorphine treatment for OUD will be asked to pilot a smartphone-based mobile peer support application, the ""Marigold App"", for a duration of six weeks. The program will use (1) a prospective cohort study to obtain text message content and feasibility metrics, and (2) qualitative interviews to evaluate usability and acceptability of the mobile platform. The Marigold mobile platform will allow patients to access a tailored chat support group 24/7 as a complement to different forms of clinical OUD treatment. Marigold can keep groups safe and constructive by augmenting chats with AI tools capable of understanding the emotional sentiment in messages, automatically ""flagging"" critical or clinically relevant content. This project will demonstrate the robustness of these AI tools by adapting them to catch OUD-specific ""flags"" in peer messages while also examining the adoptability of the platform itself within OUD patients.","Scherzer, Ranney, Jain, Bommaraju, Patena, Langdon, Nimaja, Jennings, Beaudoin","Scherzer, Ranney, Jain, Bommaraju, Patena, Langdon, Nimaja, Jennings, Beaudoin",https://doi.org/10.20900/jpbs.20200001,https://doi.org/10.20900/jpbs.20200001,2021-08-03
755.0,,pubmed,Future of evidence ecosystem series: 2 Current opportunities and need for better tools and methods,Future of evidence ecosystem series: 2 current opportunities and need for better tools and methods,"To become user-driven and more useful for decision-making, the current evidence synthesis ecosystem requires significant changes (Paper 1.Future of evidence ecosystem series). Reviewers have access to new sources of data (clinical trial registries, protocols, clinical study reports from regulatory agencies or pharmaceutical companies) for more information on randomized control trials. With all these new available data, the management of multiple and scattered trial reports is even more challenging. New types of data are also becoming available: individual patient data and routinely collected data. With the increasing number of diverse sources to be searched and the amount of data to be extracted, the process needs to be rethought. New approaches and tools, such as automation technologies and crowdsourcing, should help accelerate the process. The implementation of these new approaches and methods requires a substantial rethinking and redesign of the current evidence synthesis ecosystem. The concept of a 'living' evidence synthesis enterprise, with living systematic review and living network meta-analysis, has recently emerged. Such an evidence synthesis ecosystem implies conceptualizing evidence synthesis as a continuous process built around a clinical question of interest and no longer as a small team independently answering a specific clinical question at a single point in time.","To become user driven and more useful for decision-making, the current evidence synthesis ecosystem requires significant changes (Paper 1. Future of evidence ecosystem series). Reviewers have access to new sources of data (clinical trial registries, protocols, and clinical study reports from regulatory agencies or pharmaceutical companies) for more information on randomized control trials. With all these newly available data, the management of multiple and scattered trial reports is even more challenging. New types of data are also becoming available: individual patient data and routinely collected data. With the increasing number of diverse sources to be searched and the amount of data to be extracted, the process needs to be rethought. New approaches and tools, such as automation technologies and crowdsourcing, should help accelerate the process. The implementation of these new approaches and methods requires a substantial rethinking and redesign of the current evidence synthesis ecosystem. The concept of a ""living"" evidence synthesis enterprise, with living systematic review and living network meta-analysis, has recently emerged. Such an evidence synthesis ecosystem implies conceptualizing evidence synthesis as a continuous process built around a clinical question of interest and no longer as a small team independently answering a specific clinical question at a single point in time.","Crequit, P.
 and Boutron, I.
 and Meerpohl, J.
 and Williams, H.
 and Craig, J.
 and Ravaud, P.","CrÃ©quit, Boutron, Meerpohl, Williams, Craig, Ravaud",not available,https://doi.org/10.1016/j.jclinepi.2020.01.023,2021-08-03
326.0,,pubmed,The impact of machine learning on patient care: A systematic review,The impact of machine learning on patient care: A systematic review,"BACKGROUND: Despite the expanding use of machine learning (ML) in fields such as finance and marketing, its application in the daily practice of clinical medicine is almost non-existent. In this systematic review, we describe the various areas within clinical medicine that have applied the use of ML to improve patient care. METHODS: A systematic review was performed in accordance with the PRISMA guidelines using Medline(R), EBM Reviews, Embase, Psych Info, and Cochrane Databases, focusing on human studies that used ML to directly address a clinical problem. Included studies were published from January 1, 2000 to May 1, 2018 and provided metrics on the performance of the utilized ML tool. RESULTS: A total of 1909 unique publications were reviewed, with 378 retrospective articles and 8 prospective articles meeting inclusion criteria. Retrospective publications were found to be increasing in frequency, with 61 % of articles published within the last 4 years. Prospective articles comprised only 2 % of the articles meeting our inclusion criteria. These studies utilized a prospective cohort design with an average sample size of 531. CONCLUSION: The majority of literature describing the use of ML in clinical medicine is retrospective in nature and often outlines proof-of-concept approaches to impact patient care. We postulate that identifying and overcoming key translational barriers, including real-time access to clinical data, data security, physician approval of 'black box' generated results, and performance evaluation will allow for a fundamental shift in medical practice, where specialized tools will aid the healthcare team in providing better patient care.","Despite the expanding use of machine learning (ML) in fields such as finance and marketing, its application in the daily practice of clinical medicine is almost non-existent. In this systematic review, we describe the various areas within clinical medicine that have applied the use of ML to improve patient care. A systematic review was performed in accordance with the PRISMA guidelines using Medline(R), EBM Reviews, Embase, Psych Info, and Cochrane Databases, focusing on human studies that used ML to directly address a clinical problem. Included studies were published from January 1, 2000 to May 1, 2018 and provided metrics on the performance of the utilized ML tool. A total of 1909 unique publications were reviewed, with 378 retrospective articles and 8 prospective articles meeting inclusion criteria. Retrospective publications were found to be increasing in frequency, with 61 % of articles published within the last 4 years. Prospective articles comprised only 2 % of the articles meeting our inclusion criteria. These studies utilized a prospective cohort design with an average sample size of 531. The majority of literature describing the use of ML in clinical medicine is retrospective in nature and often outlines proof-of-concept approaches to impact patient care. We postulate that identifying and overcoming key translational barriers, including real-time access to clinical data, data security, physician approval of ""black box"" generated results, and performance evaluation will allow for a fundamental shift in medical practice, where specialized tools will aid the healthcare team in providing better patient care.","Ben-Israel, D.
 and Jacobs, W. B.
 and Casha, S.
 and Lang, S.
 and Ryu, W. H. A.
 and de Lotbiniere-Bassett, M.
 and Cadotte, D. W.","Ben-Israel, Jacobs, Casha, Lang, Ryu, de Lotbiniere-Bassett, Cadotte",https://dx.doi.org/10.1016/j.artmed.2019.101785,https://doi.org/10.1016/j.artmed.2019.101785,2021-08-03
17059.0,pubmed,pubmed,"The greater inflammatory pathway-high clinical potential by innovative predictive, preventive, and personalized medical approach","The greater inflammatory pathway-high clinical potential by innovative predictive, preventive, and personalized medical approach","Impaired wound healing (WH) and chronic inflammation are hallmarks of non-communicable diseases (NCDs). However, despite WH being a recognized player in NCDs, mainstream therapies focus on (un)targeted damping of the inflammatory response, leaving WH largely unaddressed, owing to three main factors.Ã‚Â The first is the complexity of the pathway that links inflammation and wound healing; the second is the dual nature, local and systemic, of WH; and the third is the limited acknowledgement of genetic and contingent causes that disrupt physiologic progression of WH. Here, in the frame of Predictive, Preventive, and Personalized Medicine (PPPM), we integrate and revisit current literature to offer a novel systemic view on the cues that can impact on the fate (acute or chronic inflammation) of WH, beyond the compartmentalization of medical disciplines and with the support of advanced computational biology. This shall open to a broader understanding of the causes for WH going awry, offering new operational criteria for patients' stratification (prediction and personalization). While this may also offer improved options for targeted prevention, we will envisage new therapeutic strategies to reboot and/or boost WH, to enable its progression across its physiological phases, the first of which is a transient acute inflammatory response versus the chronic low-grade inflammation characteristic of NCDs.","Impaired wound healing (WH) and chronic inflammation are hallmarks of non-communicable diseases (NCDs). However, despite WH being a recognized player in NCDs, mainstream therapies focus on (un)targeted damping of the inflammatory response, leaving WH largely unaddressed, owing to three main factors.Â The first is the complexity of the pathway that links inflammation and wound healing; the second is the dual nature, local and systemic, of WH; and the third is the limited acknowledgement of genetic and contingent causes that disrupt physiologic progression of WH. Here, in the frame of Predictive, Preventive, and Personalized Medicine (PPPM), we integrate and revisit current literature to offer a novel systemic view on the cues that can impact on the fate (acute or chronic inflammation) of WH, beyond the compartmentalization of medical disciplines and with the support of advanced computational biology. This shall open to a broader understanding of the causes for WH going awry, offering new operational criteria for patients' stratification (prediction and personalization). While this may also offer improved options for targeted prevention, we will envisage new therapeutic strategies to reboot and/or boost WH, to enable its progression across its physiological phases, the first of which is a transient acute inflammatory response versus the chronic low-grade inflammation characteristic of NCDs.","Maturo, Soligo, Gibson, Manni, Nardini","Maturo, Soligo, Gibson, Manni, Nardini",https://doi.org/10.1007/s13167-019-00195-w,https://doi.org/10.1007/s13167-019-00195-w,2021-08-03
17060.0,pubmed,pubmed,"Venous thromboembolism following 672,495 primary total shoulder and elbow replacements: Meta-analyses of incidence, temporal trends and potential risk factors","Venous thromboembolism following 672,495 primary total shoulder and elbow replacements: Meta-analyses of incidence, temporal trends and potential risk factors","There is wide variability in reported venous thromboembolism (VTE) incidence following total shoulder replacement (TSR) or total elbow replacement (TER). It is uncertain which risk factors influence the risk of VTE following TSR or TER. We conducted a PRISMA compliant meta-analysis to evaluate the incidence, temporal trends and potential risk factors for VTE following primary TSR and TER. MEDLINE, Embase, Web of Science, and Cochrane Library were searched to September 2019 for longitudinal studies reporting VTE outcomes after TSR or TER. Incidence and relative risks (RR) (95% confidence intervals) were estimated. We identified 43 articles with data on 672,495 TSRs and TERs (668,699 TSRs and 3796 TERs). The overall pooled 3-month VTE incidence following TSR was 0.85% (0.39-1.46). For TER, the 3-month incidence of VTE was 0.23% (0.08-0.44). Older age, body mass index (BMI) Ã¢â€°Â¥25Ã‚Â kg/m<sup>2</sup>, and alcohol abuse were each associated with increased VTE risk following TSR. Comorbidities associated with increased VTE risk following TSR were chronic pulmonary disease, previous VTE, heart failure, anaemia, coagulopathy, arrhythmia, epilepsy, urinary tract infection, sleep apnoea, and fluid &amp; electrolyte imbalance. Anatomic and outpatient TSR were each associated with decreased VTE risk. The average 3-month incidence of VTE following TSR or TER is &lt;1%. High risk groups such as older patients, those with a previous VTE history and those undergoing reverse or inpatient TSR may need close monitoring. Modifiable factors such as high BMI, alcohol abuse, and comorbidities could be identified and addressed prior to surgery. PROSPERO 2019: CRD42019134096.","There is wide variability in reported venous thromboembolism (VTE) incidence following total shoulder replacement (TSR) or total elbow replacement (TER). It is uncertain which risk factors influence the risk of VTE following TSR or TER. We conducted a PRISMA compliant meta-analysis to evaluate the incidence, temporal trends and potential risk factors for VTE following primary TSR and TER. MEDLINE, Embase, Web of Science, and Cochrane Library were searched to September 2019 for longitudinal studies reporting VTE outcomes after TSR or TER. Incidence and relative risks (RR) (95% confidence intervals) were estimated. We identified 43 articles with data on 672,495 TSRs and TERs (668,699 TSRs and 3796 TERs). The overall pooled 3-month VTE incidence following TSR was 0.85% (0.39-1.46). For TER, the 3-month incidence of VTE was 0.23% (0.08-0.44). Older age, body mass index (BMI) â‰¥25Â kg/m<sup>2</sup>, and alcohol abuse were each associated with increased VTE risk following TSR. Comorbidities associated with increased VTE risk following TSR were chronic pulmonary disease, previous VTE, heart failure, anaemia, coagulopathy, arrhythmia, epilepsy, urinary tract infection, sleep apnoea, and fluid &amp; electrolyte imbalance. Anatomic and outpatient TSR were each associated with decreased VTE risk. The average 3-month incidence of VTE following TSR or TER is &lt;1%. High risk groups such as older patients, those with a previous VTE history and those undergoing reverse or inpatient TSR may need close monitoring. Modifiable factors such as high BMI, alcohol abuse, and comorbidities could be identified and addressed prior to surgery. PROSPERO 2019: CRD42019134096.","Kunutsor, Barrett, Whitehouse, Blom","Kunutsor, Barrett, Whitehouse, Blom",https://doi.org/10.1016/j.thromres.2020.02.018,https://doi.org/10.1016/j.thromres.2020.02.018,2021-08-03
2584.0,,pubmed,Recent advances in hearing conservation programmes: A systematic review,Recent advances in hearing conservation programmes: A systematic review,"BACKGROUND: Current evidence from low- and middle-income (LAMI) countries, such as South Africa, indicates that occupational noise-induced hearing loss (ONIHL) continues to be a health and safety challenge for the mining industry. There is also evidence of hearing conservation programmes (HCPs) being implemented with limited success. OBJECTIVES: The aim of this study was to explore and document current evidence reflecting recent advances in HCPs in order to identify gaps within the South African HCPs. METHOD: A systematic literature review was conducted in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analysis. Electronic databases including Sage, Science Direct, PubMed, Scopus MEDLINE, ProQuest and Google Scholar were searched for potential studies published in English between 2010 and 2019 reporting on recent advances in HCPs within the mining industry. RESULTS: The study findings revealed a number of important recent advances internationally, which require deliberation for possible implementation within the South African HCPs context. These advances have been presented under seven themes: (1) the use of metrics, (2) pharmacological interventions and hair cell regeneration, (3) artificial neural network, (4) audiology assessment measures, (5) noise monitoring advances, (6) conceptual approaches to HCPs and (7) buying quiet. CONCLUSION: The study findings raise important advances that may have significant implications for HCPs in LAMI countries where ONIHL remains a highly prevalent occupational health challenge. Establishing feasibility and efficacy of these advances in these contexts to ensure contextual relevance and responsiveness is one of the recommendations to facilitate the success of HCPs targets.","Current evidence from low- and middle-income (LAMI) countries, such as South Africa, indicates that occupational noise-induced hearing loss (ONIHL) continues to be a health and safety challenge for the mining industry. There is also evidence of hearing conservation programmes (HCPs) being implemented with limited success. The aim of this study was to explore and document current evidence reflecting recent advances in HCPs in order to identify gaps within the South African HCPs. A systematic literature review was conducted in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analysis. Electronic databases including Sage, Science Direct, PubMed, Scopus MEDLINE, ProQuest and Google Scholar were searched for potential studies published in English between 2010 and 2019 reporting on recent advances in HCPs within the mining industry. The study findings revealed a number of important recent advances internationally, which require deliberation for possible implementation within the South African HCPs context. These advances have been presented under seven themes: (1) the use of metrics, (2) pharmacological interventions and hair cell regeneration, (3) artificial neural network, (4) audiology assessment measures, (5) noise monitoring advances, (6) conceptual approaches to HCPs and (7) buying quiet. The study findings raise important advances that may have significant implications for HCPs in LAMI countries where ONIHL remains a highly prevalent occupational health challenge. Establishing feasibility and efficacy of these advances in these contexts to ensure contextual relevance and responsiveness is one of the recommendations to facilitate the success of HCPs targets.","Moroe, N. F.
 and Khoza-Shangase, K.","Moroe, Khoza-Shangase",https://dx.doi.org/10.4102/sajcd.v67i2.675,https://doi.org/10.4102/sajcd.v67i2.675,2021-08-03
17070.0,pubmed,pubmed,Interpretable machine learning models for classifying low back pain status using functional physiological variables,Interpretable machine learning models for classifying low back pain status using functional physiological variables,"To evaluate the predictive performance of statistical models which distinguishes different low back pain (LBP) sub-types and healthy controls, using as input predictors the time-varying signals of electromyographic and kinematic variables, collected during low-load lifting. Motion capture with electromyography (EMG) assessment was performed on 49 participants [healthy control (con)Ã¢â‚¬â€°=Ã¢â‚¬â€°16, remission LBP (rmLBP)Ã¢â‚¬â€°=Ã¢â‚¬â€°16, current LBP (LBP)Ã¢â‚¬â€°=Ã¢â‚¬â€°17], whilst performing a low-load lifting task, to extract a total of 40 predictors (kinematic and electromyographic variables). Three statistical models were developed using functional data boosting (FDboost), for binary classification of LBP statuses (model 1: con vs. LBP; model 2: con vs. rmLBP; model 3: rmLBP vs. LBP). After removing collinear predictors (i.e. a correlation of &gt;Ã¢â‚¬â€°0.7 with other predictors) and inclusion of the covariate sex, 31 predictors were included for fitting model 1, 31 predictors for model 2, and 32 predictors for model 3. Seven EMG predictors were selected in model 1 (area under the receiver operator curve [AUC] of 90.4%), nine predictors in model 2 (AUC of 91.2%), and seven predictors in model 3 (AUC of 96.7%). The most influential predictor was the biceps femoris muscle (peak [Formula: see text]Ã¢â‚¬â€° =Ã¢â‚¬â€°0.047) in model 1, the deltoid muscle (peak [Formula: see text]Ã¢â‚¬â€°=Ã¢â‚¬â€°Ã¢â‚¬â€°0.052) in model 2, and the iliocostalis muscle (peakÃ¢â‚¬â€°[Formula: see text] =Ã¢â‚¬â€°Ã‚Â 0.16) in model 3. The ability to transform time-varying physiological differences into clinical differences could be used in future prospective prognostic research to identify the dominant movement impairments that drive the increased risk. These slides can be retrieved under Electronic Supplementary Material.","To evaluate the predictive performance of statistical models which distinguishes different low back pain (LBP) sub-types and healthy controls, using as input predictors the time-varying signals of electromyographic and kinematic variables, collected during low-load lifting. Motion capture with electromyography (EMG) assessment was performed on 49 participants [healthy control (con)â€‰=â€‰16, remission LBP (rmLBP)â€‰=â€‰16, current LBP (LBP)â€‰=â€‰17], whilst performing a low-load lifting task, to extract a total of 40 predictors (kinematic and electromyographic variables). Three statistical models were developed using functional data boosting (FDboost), for binary classification of LBP statuses (model 1: con vs. LBP; model 2: con vs. rmLBP; model 3: rmLBP vs. LBP). After removing collinear predictors (i.e. a correlation of &gt;â€‰0.7 with other predictors) and inclusion of the covariate sex, 31 predictors were included for fitting model 1, 31 predictors for model 2, and 32 predictors for model 3. Seven EMG predictors were selected in model 1 (area under the receiver operator curve [AUC] of 90.4%), nine predictors in model 2 (AUC of 91.2%), and seven predictors in model 3 (AUC of 96.7%). The most influential predictor was the biceps femoris muscle (peak [Formula: see text]â€‰ =â€‰0.047) in model 1, the deltoid muscle (peak [Formula: see text]â€‰=â€‰â€‰0.052) in model 2, and the iliocostalis muscle (peakâ€‰[Formula: see text] =â€‰Â 0.16) in model 3. The ability to transform time-varying physiological differences into clinical differences could be used in future prospective prognostic research to identify the dominant movement impairments that drive the increased risk. These slides can be retrieved under Electronic Supplementary Material.","Liew, Rugamer, De Nunzio, Falla","Liew, Rugamer, De Nunzio, Falla",https://doi.org/10.1007/s00586-020-06356-0,https://doi.org/10.1007/s00586-020-06356-0,2021-08-03
2370.0,,pubmed,The applications of machine learning in plastic and reconstructive surgery: protocol of a systematic review,The applications of machine learning in plastic and reconstructive surgery: protocol of a systematic review,"BACKGROUND: Machine learning, a subset of artificial intelligence, is a set of models and methods that can automatically detect patterns in vast amounts of data, extract information and use it to perform various kinds of decision-making under uncertain conditions. This can assist surgeons in clinical decision-making by identifying patient cohorts that will benefit from surgery prior to treatment. The aim of this review is to evaluate the applications of machine learning in plastic and reconstructive surgery. METHODS: A literature review will be undertaken of EMBASE, MEDLINE and CENTRAL (1990 up to September 2019) to identify studies relevant for the review. Studies in which machine learning has been employed in the clinical setting of plastic surgery will be included. Primary outcomes will be the evaluation of the accuracy of machine learning models in predicting a clinical diagnosis and post-surgical outcomes. Secondary outcomes will include a cost analysis of those models. This protocol has been prepared using the Preferred Items for Systematic Review and Meta-Analysis Protocols (PRISMA-P) guidelines. DISCUSSION: This will be the first systematic review in available literature that summarises the published work on the applications of machine learning in plastic surgery. Our findings will provide the basis of future research in developing artificial intelligence interventions in the specialty. Systematic review registration: prospero crd42019140924.","Machine learning, a subset of artificial intelligence, is a set of models and methods that can automatically detect patterns in vast amounts of data, extract information and use it to perform various kinds of decision-making under uncertain conditions. This can assist surgeons in clinical decision-making by identifying patient cohorts that will benefit from surgery prior to treatment. The aim of this review is to evaluate the applications of machine learning in plastic and reconstructive surgery. A literature review will be undertaken of EMBASE, MEDLINE and CENTRAL (1990 up to September 2019) to identify studies relevant for the review. Studies in which machine learning has been employed in the clinical setting of plastic surgery will be included. Primary outcomes will be the evaluation of the accuracy of machine learning models in predicting a clinical diagnosis and post-surgical outcomes. Secondary outcomes will include a cost analysis of those models. This protocol has been prepared using the Preferred Items for Systematic Review and Meta-Analysis Protocols (PRISMA-P) guidelines. This will be the first systematic review in available literature that summarises the published work on the applications of machine learning in plastic surgery. Our findings will provide the basis of future research in developing artificial intelligence interventions in the specialty. PROSPERO CRD42019140924.","Mantelakis, A.
 and Khajuria, A.","Mantelakis, Khajuria",https://dx.doi.org/10.1186/s13643-020-01304-x,https://doi.org/10.1186/s13643-020-01304-x,2021-08-03
2673.0,,pubmed,Evolving Role and Future Directions of Natural Language Processing in Gastroenterology,Evolving Role and Future Directions of Natural Language Processing in Gastroenterology,"In line with the current trajectory of healthcare reform, significant emphasis has been placed on improving the utilization of data collected during a clinical encounter. Although the structured fields of electronic health records have provided a convenient foundation on which to begin such efforts, it was well understood that a substantial portion of relevant information is confined in the free-text narratives documenting care. Unfortunately, extracting meaningful information from such narratives is a non-trivial task, traditionally requiring significant manual effort. Today, computational approaches from a field known as Natural Language Processing (NLP) are poised to make a transformational impact in the analysis and utilization of these documents across healthcare practice and research, particularly in procedure-heavy sub-disciplines such as gastroenterology (GI). As such, this manuscript provides a clinically focused review of NLP systems in GI practice. It begins with a detailed synopsis around the state of NLP techniques, presenting state-of-the-art methods and typical use cases in both clinical settings and across other domains. Next, it will present a robust literature review around current applications of NLP within four prominent areas of gastroenterology including endoscopy, inflammatory bowel disease, pancreaticobiliary, and liver diseases. Finally, it concludes with a discussion of open problems and future opportunities of this technology in the field of gastroenterology and health care as a whole.","In line with the current trajectory of healthcare reform, significant emphasis has been placed on improving the utilization of data collected during a clinical encounter. Although the structured fields of electronic health records have provided a convenient foundation on which to begin such efforts, it was well understood that a substantial portion of relevant information is confined in the free-text narratives documenting care. Unfortunately, extracting meaningful information from such narratives is a non-trivial task, traditionally requiring significant manual effort. Today, computational approaches from a field known as Natural Language Processing (NLP) are poised to make a transformational impact in the analysis and utilization of these documents across healthcare practice and research, particularly in procedure-heavy sub-disciplines such as gastroenterology (GI). As such, this manuscript provides a clinically focused review of NLP systems in GI practice. It begins with a detailed synopsis around the state of NLP techniques, presenting state-of-the-art methods and typical use cases in both clinical settings and across other domains. Next, it will present a robust literature review around current applications of NLP within four prominent areas of gastroenterology including endoscopy, inflammatory bowel disease, pancreaticobiliary, and liver diseases. Finally, it concludes with a discussion of open problems and future opportunities of this technology in the field of gastroenterology and health care as a whole.","Nehme, F.
 and Feldman, K.","Nehme, Feldman",https://dx.doi.org/10.1007/s10620-020-06156-y,https://doi.org/10.1007/s10620-020-06156-y,2021-08-03
17084.0,pubmed,pubmed,Cervical vertebral maturation assessment on lateral cephalometric radiographs using artificial intelligence: comparison of machine learning classifier models,Cervical vertebral maturation assessment on lateral cephalometric radiographs using artificial intelligence: comparison of machine learning classifier models,"This study aimed to develop five different supervised machine learning (ML) classifier models using artificial intelligence (AI) techniques and to compare their performance for cervical vertebral maturation (CVM) analysis. A clinical decision support system (CDSS) was developed for more objective results. A total of 647 digital lateral cephalometric radiographs with visible C2, C3, C4 and C5 vertebrae were chosen. Newly developed software was used for manually labelling the samples, with the integrated CDSS developed by evaluation of 100 radiographs. On each radiograph, 26 points were marked, and the CDSS generated a suggestion according to the points and CVM analysis performed by the human observer. For each sample, 54 features were saved in text format and classified using logistic regression (LR), support vector machine, random forest, artificial neural network (ANN) and decision tree (DT) models. The weighted ÃŽÂº coefficient was used to evaluate the concordance of classification and expert visual evaluation results. Among the CVM stage classifier models, the best result was achieved using the ANN model (ÃŽÂº = 0.926). Among cervical vertebrae morphology classifier models, the best result was achieved using the LR model (ÃŽÂº = 0.968) for the presence of concavity, and the DT model (ÃŽÂº = 0.949) for vertebral body shapes. This study has proposed ML models for CVM assessment on lateral cephalometric radiographs, which can be used for the prediction of cervical vertebrae morphology. Further studies should be done especially of forensic applications of AI models through CVM evaluations.","This study aimed to develop five different supervised machine learning (ML) classifier models using artificial intelligence (AI) techniques and to compare their performance for cervical vertebral maturation (CVM) analysis. A clinical decision support system (CDSS) was developed for more objective results. A total of 647 digital lateral cephalometric radiographs with visible C2, C3, C4 and C5 vertebrae were chosen. Newly developed software was used for manually labelling the samples, with the integrated CDSS developed by evaluation of 100 radiographs. On each radiograph, 26 points were marked, and the CDSS generated a suggestion according to the points and CVM analysis performed by the human observer. For each sample, 54 features were saved in text format and classified using logistic regression (LR), support vector machine, random forest, artificial neural network (ANN) and decision tree (DT) models. The weighted Îº coefficient was used to evaluate the concordance of classification and expert visual evaluation results. Among the CVM stage classifier models, the best result was achieved using the ANN model (Îº = 0.926). Among cervical vertebrae morphology classifier models, the best result was achieved using the LR model (Îº = 0.968) for the presence of concavity, and the DT model (Îº = 0.949) for vertebral body shapes. This study has proposed ML models for CVM assessment on lateral cephalometric radiographs, which can be used for the prediction of cervical vertebrae morphology. Further studies should be done especially of forensic applications of AI models through CVM evaluations.","Amasya, Yildirim, Aydogan, Kemaloglu, Orhan","Amasya, Yildirim, Aydogan, Kemaloglu, Orhan",https://doi.org/10.1259/dmfr.20190441,https://doi.org/10.1259/dmfr.20190441,2021-08-03
17085.0,pubmed,pubmed,Cost-Effectiveness of Pharmacist-Led Deprescribing of NSAIDs in Community-Dwelling Older Adults,Cost-Effectiveness of Pharmacist-Led Deprescribing of NSAIDs in Community-Dwelling Older Adults,"Older adults are often prescribed potentially inappropriate medications associated with adverse health outcomes and increased health services utilization. Developing Pharmacist-led Research to Educate and Sensitize Community Residents to the Inappropriate Prescriptions Burden in the Elderly (D-PRESCRIBE), a pragmatic randomized clinical trial, demonstrated how a community pharmacist-led evidence-based educational intervention successfully empowered community-dwelling older adults and their physicians to reduce chronic use of inappropriate medications. The objective of this study was to evaluate the cost-effectiveness of the D-PRESCRIBE intervention for discontinuing nonsteroidal anti-inflammatory drugs (NSAIDs). Cost-effectiveness analysis. Canada. Community-dwelling adults aged 65 years and older. Decision analysis combining decision tree and Markov state transition modeling was developed to estimate the cost-effectiveness of D-PRESCRIBE (NSAIDs) compared with usual care from a Canadian healthcare system perspective with a time horizon of 1 year. Data from the D-PRESCRIBE trial and published literature were used to calculate effectiveness, utilities, and costs. Reference case and scenario analyses were conducted using probabilistic modeling. Sensitivity analyses assessed the robustness of the reference case model. D-PRESCRIBE (NSAIDs) was less costly (-$1008.61) and more effective (.11 quality-adjusted life-years [QALYs]) than usual care and was the dominant strategy. At willingness-to-pay thresholds of $50Ã¢â‚¬â€°000 per QALY and $100Ã¢â‚¬â€°000 per QALY, D-PRESCRIBE (NSAIDs) incurred a positive incremental net benefit compared with usual care, suggesting it is cost-effective. Compared with the reference case, scenario analyses gave comparable QALYs with modest variation in cost estimates. For community-dwelling older adults, D-PRESCRIBE (NSAIDs) provides greater benefits at lower system costs, making it a compelling strategy to reduce the use and harms associated with chronic NSAID consumption. Our findings support reimbursing community pharmacists' clinical professional services for deprescribing inappropriate NSAIDs in community-dwelling older adults. J Am Geriatr Soc 68:1090-1097, 2020.","Older adults are often prescribed potentially inappropriate medications associated with adverse health outcomes and increased health services utilization. Developing Pharmacist-led Research to Educate and Sensitize Community Residents to the Inappropriate Prescriptions Burden in the Elderly (D-PRESCRIBE), a pragmatic randomized clinical trial, demonstrated how a community pharmacist-led evidence-based educational intervention successfully empowered community-dwelling older adults and their physicians to reduce chronic use of inappropriate medications. The objective of this study was to evaluate the cost-effectiveness of the D-PRESCRIBE intervention for discontinuing nonsteroidal anti-inflammatory drugs (NSAIDs). Cost-effectiveness analysis. Canada. Community-dwelling adults aged 65 years and older. Decision analysis combining decision tree and Markov state transition modeling was developed to estimate the cost-effectiveness of D-PRESCRIBE (NSAIDs) compared with usual care from a Canadian healthcare system perspective with a time horizon of 1 year. Data from the D-PRESCRIBE trial and published literature were used to calculate effectiveness, utilities, and costs. Reference case and scenario analyses were conducted using probabilistic modeling. Sensitivity analyses assessed the robustness of the reference case model. D-PRESCRIBE (NSAIDs) was less costly (-$1008.61) and more effective (.11 quality-adjusted life-years [QALYs]) than usual care and was the dominant strategy. At willingness-to-pay thresholds of $50â€‰000 per QALY and $100â€‰000 per QALY, D-PRESCRIBE (NSAIDs) incurred a positive incremental net benefit compared with usual care, suggesting it is cost-effective. Compared with the reference case, scenario analyses gave comparable QALYs with modest variation in cost estimates. For community-dwelling older adults, D-PRESCRIBE (NSAIDs) provides greater benefits at lower system costs, making it a compelling strategy to reduce the use and harms associated with chronic NSAID consumption. Our findings support reimbursing community pharmacists' clinical professional services for deprescribing inappropriate NSAIDs in community-dwelling older adults. J Am Geriatr Soc 68:1090-1097, 2020.","Sanyal, Turner, Martin, Tannenbaum","Sanyal, Turner, Martin, Tannenbaum",https://doi.org/10.1111/jgs.16388,https://doi.org/10.1111/jgs.16388,2021-08-03
17088.0,pubmed,pubmed,Identification of patients with carotid stenosis using natural language processing,Identification of patients with carotid stenosis using natural language processing,"The highly structured nature of medical reports makes them feasible for automated large-scale patient identification. This study aimed to develop a natural language processing (NLP) model to retrospectively retrieve patients with presence and history of carotid stenosis (CS) using their ultrasound reports. Ultrasound reports from our institution between January 2016 and December 2017 were selected. To process the texts, we developed a parser to divide the raw text into fields. For baseline method, we used bag-of-n-grams and term frequency inverse document frequency as the features and used linear classifiers. Logistic regression was performed as the baseline model. Convolution and recurrent neural networks (CNN; RNN) with attention mechanism were applied to the dataset to improve the classification accuracy. We had 1220 ultrasound reports for training and 307 for testing, totaling to 1527 reports. For predicting history of CS, both CNN and RNN-attention models had a significantly higher specificity than logistic regression. In addition, RNN-attention also had a significantly higher F1 score and accuracy. For predicting presence of carotid stenosis, all models achieved above 93% accuracy. RNN-attention achieved a 95.4% accuracy, although the difference with logistic regression was not statistically significant. RNN-attention had a statistically significant higher specificity than logistic regression. We developed linear, CNN, and RNN models to predict history and presence of CS from ultrasound reports. We have demonstrated NLP to be an efficient, accurate approach for large-scale retrospective patient identification, with applications in long-term follow-up of patients and clinical research studies. Ã¢â‚¬Â¢ Natural language processing models using both linear classifiers and neural networks can achieve a good performance, with an overall accuracy above 90% in predicting history and presence of carotid stenosis. Ã¢â‚¬Â¢ Convolution and recurrent neural networks, especially with additional features including field awareness and attention mechanism, have superior performance than traditional linear classifiers. Ã¢â‚¬Â¢ NLP is shown to be an efficient approach for large-scale retrospective patient identification, with applications in long-term follow-up of patients and further clinical research studies.","The highly structured nature of medical reports makes them feasible for automated large-scale patient identification. This study aimed to develop a natural language processing (NLP) model to retrospectively retrieve patients with presence and history of carotid stenosis (CS) using their ultrasound reports. Ultrasound reports from our institution between January 2016 and December 2017 were selected. To process the texts, we developed a parser to divide the raw text into fields. For baseline method, we used bag-of-n-grams and term frequency inverse document frequency as the features and used linear classifiers. Logistic regression was performed as the baseline model. Convolution and recurrent neural networks (CNN; RNN) with attention mechanism were applied to the dataset to improve the classification accuracy. We had 1220 ultrasound reports for training and 307 for testing, totaling to 1527 reports. For predicting history of CS, both CNN and RNN-attention models had a significantly higher specificity than logistic regression. In addition, RNN-attention also had a significantly higher F1 score and accuracy. For predicting presence of carotid stenosis, all models achieved above 93% accuracy. RNN-attention achieved a 95.4% accuracy, although the difference with logistic regression was not statistically significant. RNN-attention had a statistically significant higher specificity than logistic regression. We developed linear, CNN, and RNN models to predict history and presence of CS from ultrasound reports. We have demonstrated NLP to be an efficient, accurate approach for large-scale retrospective patient identification, with applications in long-term follow-up of patients and clinical research studies. â€¢ Natural language processing models using both linear classifiers and neural networks can achieve a good performance, with an overall accuracy above 90% in predicting history and presence of carotid stenosis. â€¢ Convolution and recurrent neural networks, especially with additional features including field awareness and attention mechanism, have superior performance than traditional linear classifiers. â€¢ NLP is shown to be an efficient approach for large-scale retrospective patient identification, with applications in long-term follow-up of patients and further clinical research studies.","Wu, Zhao, Radev, Malhotra","Wu, Zhao, Radev, Malhotra",https://doi.org/10.1007/s00330-020-06721-z,https://doi.org/10.1007/s00330-020-06721-z,2021-08-03
17089.0,pubmed,pubmed,Radiomics: from qualitative to quantitative imaging,Radiomics: from qualitative to quantitative imaging,"Historically, medical imaging has been a qualitative or semi-quantitative modality. It is difficult to quantify what can be seen in an image, and to turn it into valuable predictive outcomes. As a result of advances in both computational hardware and machine learning algorithms, computers are making great strides in obtaining quantitative information from imaging and correlating it with outcomes. Radiomics, in its two forms &quot;handcrafted and deep,&quot; is an emerging field that translates medical images into quantitative data to yield biological information and enable radiologic phenotypic profiling for diagnosis, theragnosis, decision support, and monitoring. Handcrafted radiomics is a multistage process in which features based on shape, pixel intensities, and texture are extracted from radiographs. Within this review, we describe the steps: starting with quantitative imaging data, how it can be extracted, how to correlate it with clinical and biological outcomes, resulting in models that can be used to make predictions, such as survival, or for detection and classification used in diagnostics. The application of deep learning, the second arm of radiomics, and its place in the radiomics workflow is discussed, along with its advantages and disadvantages. To better illustrate the technologies being used, we provide real-world clinical applications of radiomics in oncology, showcasing research on the applications of radiomics, as well as covering its limitations and its future direction.","Historically, medical imaging has been a qualitative or semi-quantitative modality. It is difficult to quantify what can be seen in an image, and to turn it into valuable predictive outcomes. As a result of advances in both computational hardware and machine learning algorithms, computers are making great strides in obtaining quantitative information from imaging and correlating it with outcomes. Radiomics, in its two forms ""handcrafted and deep,"" is an emerging field that translates medical images into quantitative data to yield biological information and enable radiologic phenotypic profiling for diagnosis, theragnosis, decision support, and monitoring. Handcrafted radiomics is a multistage process in which features based on shape, pixel intensities, and texture are extracted from radiographs. Within this review, we describe the steps: starting with quantitative imaging data, how it can be extracted, how to correlate it with clinical and biological outcomes, resulting in models that can be used to make predictions, such as survival, or for detection and classification used in diagnostics. The application of deep learning, the second arm of radiomics, and its place in the radiomics workflow is discussed, along with its advantages and disadvantages. To better illustrate the technologies being used, we provide real-world clinical applications of radiomics in oncology, showcasing research on the applications of radiomics, as well as covering its limitations and its future direction.","Rogers, Thulasi Seetha, Refaee, Lieverse, Granzier, Ibrahim, Keek, Sanduleanu, Primakov, Beuque, Marcus, van der Wiel, Zerka, Oberije, van Timmeren, Woodruff, Lambin","Rogers, Thulasi Seetha, Refaee, Lieverse, Granzier, Ibrahim, Keek, Sanduleanu, Primakov, Beuque, Marcus, van der Wiel, Zerka, Oberije, van Timmeren, Woodruff, Lambin",https://doi.org/10.1259/bjr.20190948,https://doi.org/10.1259/bjr.20190948,2021-08-03
17091.0,pubmed,pubmed,Effect of Drinking Oxygenated Water Assessed by in vivo MRI Relaxometry,Effect of Drinking Oxygenated Water Assessed by in vivo MRI Relaxometry,"This project was funded by the Research Council of Norway. Oxygen uptake through the gastrointestinal tract after oral administration of oxygenated water in humans is not well studied and is debated in the literature. Due to the paramagnetic properties of oxygen and deoxyhemoglobin, MRI as a technique might be able to detect changes in relaxometry values caused by increased oxygen levels in the blood. To assess whether oxygen dissolved in water is absorbed from the gastrointestinal tract and transported into the bloodstream after oral administration. A randomized, double-blinded, placebo-controlled crossover trial. Thirty healthy male volunteers age 20-35. 3T/Modified Look-Locker inversion recovery (MOLLI) T<sub>1</sub> -mapping and multi fast field echo (mFFE) T<sub>2</sub> *-mapping. Each volunteer was scanned in two separate sessions. T<sub>1</sub> and T<sub>2</sub> * maps were acquired repeatedly covering the hepatic portal vein (HPV) and vena cava inferior (VCI, control vein) before and after intake of oxygenated or control water. Assessments were done by placing a region of interest in the HPV and VCI. A mixed linear model was performed to the compare control vs. oxygen group. Drinking caused a mean 1.6% 95% CI (1.1-2.0% PÃ¢â‚¬â€°&lt;Ã¢â‚¬â€°0.001) increase in T<sub>1</sub> of HPV blood and water oxygenation attributed another 0.70% 95% confidence interval (CI) (0.07-1.3% P = 0.028) increase. Oxygenation did not change T<sub>1</sub> in VCI blood. Mean T<sub>2</sub> * increased 9.6% 95% CI (1.7-17.5% P = 0.017) after ingestion of oxygenated water and 1.2% 95% CI (-4.3-6.8% P = 0.661) after ingestion of control water. The corresponding changes in VCI blood were not significant. Ingestion of water caused changes in T<sub>1</sub> and T<sub>2</sub> * of HPV blood compatible with dilution due to water absorption. The effects were enhanced by oxygen. Assessment of oxygen enrichment of HPV blood was not possible due to the dilution effect. 2 TECHNICAL EFFICACY STAGE: 2 J. Magn. Reson. Imaging 2020;52:720-728.","This project was funded by the Research Council of Norway. Oxygen uptake through the gastrointestinal tract after oral administration of oxygenated water in humans is not well studied and is debated in the literature. Due to the paramagnetic properties of oxygen and deoxyhemoglobin, MRI as a technique might be able to detect changes in relaxometry values caused by increased oxygen levels in the blood. To assess whether oxygen dissolved in water is absorbed from the gastrointestinal tract and transported into the bloodstream after oral administration. A randomized, double-blinded, placebo-controlled crossover trial. Thirty healthy male volunteers age 20-35. 3T/Modified Look-Locker inversion recovery (MOLLI) T<sub>1</sub> -mapping and multi fast field echo (mFFE) T<sub>2</sub> *-mapping. Each volunteer was scanned in two separate sessions. T<sub>1</sub> and T<sub>2</sub> * maps were acquired repeatedly covering the hepatic portal vein (HPV) and vena cava inferior (VCI, control vein) before and after intake of oxygenated or control water. Assessments were done by placing a region of interest in the HPV and VCI. A mixed linear model was performed to the compare control vs. oxygen group. Drinking caused a mean 1.6% 95% CI (1.1-2.0% Pâ€‰&lt;â€‰0.001) increase in T<sub>1</sub> of HPV blood and water oxygenation attributed another 0.70% 95% confidence interval (CI) (0.07-1.3% P = 0.028) increase. Oxygenation did not change T<sub>1</sub> in VCI blood. Mean T<sub>2</sub> * increased 9.6% 95% CI (1.7-17.5% P = 0.017) after ingestion of oxygenated water and 1.2% 95% CI (-4.3-6.8% P = 0.661) after ingestion of control water. The corresponding changes in VCI blood were not significant. Ingestion of water caused changes in T<sub>1</sub> and T<sub>2</sub> * of HPV blood compatible with dilution due to water absorption. The effects were enhanced by oxygen. Assessment of oxygen enrichment of HPV blood was not possible due to the dilution effect. 2 TECHNICAL EFFICACY STAGE: 2 J. Magn. Reson. Imaging 2020;52:720-728.","Vatnehol, Hol, BjÃƒÂ¸rnerud, Amiry-Moghaddam, HaglerÃƒÂ¸d, StorÃƒÂ¥s","Vatnehol, Hol, BjÃ¸rnerud, Amiry-Moghaddam, HaglerÃ¸d, StorÃ¥s",https://doi.org/10.1002/jmri.27104,https://doi.org/10.1002/jmri.27104,2021-08-03
3675.0,,pubmed,OmixLitMiner: A Bioinformatics Tool for Prioritizing Biological Leads from 'Omics Data Using Literature Retrieval and Data Mining,OmixLitMiner: A Bioinformatics Tool for Prioritizing Biological Leads from 'Omics Data Using Literature Retrieval and Data Mining,"Proteomics and genomics discovery experiments generate increasingly large result tables, necessitating more researcher time to convert the biological data into new knowledge. Literature review is an important step in this process and can be tedious for large scale experiments. An informed and strategic decision about which biomolecule targets should be pursued for follow-up experiments thus remains a considerable challenge. To streamline and formalise this process of literature retrieval and analysis of discovery based 'omics data and as a decision-facilitating support tool for follow-up experiments we present OmixLitMiner, a package written in the computational language R. The tool automates the retrieval of literature from PubMed based on UniProt protein identifiers, gene names and their synonyms, combined with user defined contextual keyword search (i.e., gene ontology based). The search strategy is programmed to allow either strict or more lenient literature retrieval and the outputs are assigned to three categories describing how well characterized a regulated gene or protein is. The category helps to meet a decision, regarding which gene/protein follow-up experiments may be performed for gaining new knowledge and to exclude following already known biomarkers. We demonstrate the tool's usefulness in this retrospective study assessing three cancer proteomics and one cancer genomics publication. Using the tool, we were able to corroborate most of the decisions in these papers as well as detect additional biomolecule leads that may be valuable for future research.","Proteomics and genomics discovery experiments generate increasingly large result tables, necessitating more researcher time to convert the biological data into new knowledge. Literature review is an important step in this process and can be tedious for large scale experiments. An informed and strategic decision about which biomolecule targets should be pursued for follow-up experiments thus remains a considerable challenge. To streamline and formalise this process of literature retrieval and analysis of discovery based 'omics data and as a decision-facilitating support tool for follow-up experiments we present OmixLitMiner, a package written in the computational language R. The tool automates the retrieval of literature from PubMed based on UniProt protein identifiers, gene names and their synonyms, combined with user defined contextual keyword search (i.e., gene ontology based). The search strategy is programmed to allow either strict or more lenient literature retrieval and the outputs are assigned to three categories describing how well characterized a regulated gene or protein is. The category helps to meet a decision, regarding which gene/protein follow-up experiments may be performed for gaining new knowledge and to exclude following already known biomarkers. We demonstrate the tool's usefulness in this retrospective study assessing three cancer proteomics and one cancer genomics publication. Using the tool, we were able to corroborate most of the decisions in these papers as well as detect additional biomolecule leads that may be valuable for future research.","Steffen, P.
 and Wu, J.
 and Hariharan, S.
 and Voss, H.
 and Raghunath, V.
 and Molloy, M. P.
 and Schluter, H.","Steffen, Wu, Hariharan, Voss, Raghunath, Molloy, SchlÃ¼ter",https://dx.doi.org/10.3390/ijms21041374,https://doi.org/10.3390/ijms21041374,2021-08-03
2054.0,,pubmed,Mobile Sensing in Substance Use Research: A Scoping Review,Mobile Sensing in Substance Use Research: A Scoping Review,"<b>Background:</b> Addictive disorders and substance use are significant health challenges worldwide, and relapse is a core component of addictive disorders. The dynamics surrounding relapse and especially the immediate period before it occurs is only partly understood, much due to difficulties collecting reliable and sufficient data from this narrow period. Mobile sensing has been an important way to improve data quality and enhance predictive capabilities for symptom worsening within physical and mental health care, but is less developed within substance use research. <b>Methodology: </b> This scoping review aimed to reviewing the currently available research on mobile sensing of substance use and relapse in substance use disorders. The search was conducted in January 2019 using PubMed and Web of Science. <b>Results:</b> Six articles were identified, all concerning subjects using alcohol. In the studies a range of mobile sensors and derived aggregated features were employed. Data collected through mobile sensing were predominantly used to make dichotomous inference on ongoing substance use or not and in some cases on the quantity of substance intake. Only one of the identified studies predicted later substance use. A range of statistical machine learning techniques was employed. <b>Conclusions:</b> The research on mobile sensing in this field remains scarce. The issues requiring further attention include more research on clinical populations in naturalistic settings, use of a priori knowledge in statistical modeling, focus on prediction of substance use rather than purely identification, and finally research on other substances than alcohol.","<b><i>Background:</i></b> <i>Addictive disorders and substance use are significant health challenges worldwide, and relapse is a core component of addictive disorders. The dynamics surrounding relapse and especially the immediate period before it occurs is only partly understood, much due to difficulties collecting reliable and sufficient data from this narrow period. Mobile sensing has been an important way to improve data quality and enhance predictive capabilities for symptom worsening within physical and mental health care, but is less developed within substance use research.</i> <b><i>Methodology:</i></b> <i>This scoping review aimed to reviewing the currently available research on mobile sensing of substance use and relapse in substance use disorders. The search was conducted in January 2019 using PubMed and Web of Science.</i> <b><i>Results:</i></b> <i>Six articles were identified, all concerning subjects using alcohol. In the studies a range of mobile sensors and derived aggregated features were employed. Data collected through mobile sensing were predominantly used to make dichotomous inference on ongoing substance use or not and in some cases on the quantity of substance intake. Only one of the identified studies predicted later substance use. A range of statistical machine learning techniques was employed.</i> <b><i>Conclusions:</i></b> <i>The research on mobile sensing in this field remains scarce. The issues requiring further attention include more research on clinical populations in naturalistic settings, use of</i> a priori <i>knowledge in statistical modeling, focus on prediction of substance use rather than purely identification, and finally research on other substances than alcohol.</i>","Lauvsnes, A. D. F.
 and Langaas, M.
 and Toussaint, P.
 and Grawe, R. W.","Lauvsnes, Langaas, Toussaint, GrÃ¥we",https://dx.doi.org/10.1089/tmj.2019.0241,https://doi.org/10.1089/tmj.2019.0241,2021-08-03
3605.0,,pubmed,"The effectiveness of high-intensity interval training on body composition, cardiorespiratory fitness, and cardiovascular risk factors in children: A protocol for a systematic review","The effectiveness of high-intensity interval training on body composition, cardiorespiratory fitness, and cardiovascular risk factors in children: A protocol for a systematic review","BACKGROUND: No previous systematic review has examined the effect of high-intensity interval training (HIIT) interventions on body composition, cardiometabolic risk factors and cardiorespiratory fitness (CRF) in healthy schoolchildren from 5 to 12 years old. METHODS: This study will be conducted by following the guideline of the preferred reporting items for systematic review and meta-analysis protocols. An electronic search in MEDLINE (via PubMed), EMBASE (via Scopus), SPORTDiscus, Cochrane Library and Web of Science databases of all dates from inception will be conducted. We will include randomized controlled trials aimed to assess the effectiveness of HIIT to improve cardiometabolic risk factors, body composition, and CRF in children. Two authors will perform the study selection and data collection; disagreements will be solved by a third reviewer. The methodological quality of studies will be assessed by the Cochrane Collaboration's tool for assessing risk of bias (RoB2). Data analysis and synthesis will be performed by Comprehensive Meta-analysis Software and StataSE software, version 15. CONCLUSION: The results should be disseminated through publication in a peer-reviewed journal. Since the data used in systematic reviews of this type will be extracted exclusively from published studies, approval form and ethics committee will not be required.","No previous systematic review has examined the effect of high-intensity interval training (HIIT) interventions on body composition, cardiometabolic risk factors and cardiorespiratory fitness (CRF) in healthy schoolchildren from 5 to 12 years old. This study will be conducted by following the guideline of the preferred reporting items for systematic review and meta-analysis protocols. An electronic search in MEDLINE (via PubMed), EMBASE (via Scopus), SPORTDiscus, Cochrane Library and Web of Science databases of all dates from inception will be conducted. We will include randomized controlled trials aimed to assess the effectiveness of HIIT to improve cardiometabolic risk factors, body composition, and CRF in children. Two authors will perform the study selection and data collection; disagreements will be solved by a third reviewer. The methodological quality of studies will be assessed by the Cochrane Collaboration's tool for assessing risk of bias (RoB2). Data analysis and synthesis will be performed by Comprehensive Meta-analysis Software and StataSE software, version 15. The results should be disseminated through publication in a peer-reviewed journal. Since the data used in systematic reviews of this type will be extracted exclusively from published studies, approval form and ethics committee will not be required.","Solera-Martinez, M.
 and Diez-Fernandez, A.
 and Gonzalez-Garcia, A.
 and Manzanares-Dominguez, I.
 and Martinez-Vizcaino, V.
 and Pozuelo-Carrascosa, D. P.","Solera-MartÃ­nez, DÃ­ez-FernÃ¡ndez, GonzÃ¡lez-GarcÃ­a, Manzanares-DomÃ­nguez, MartÃ­nez-VizcaÃ­no, Pozuelo-Carrascosa",not available,https://doi.org/10.1097/MD.0000000000019233,2021-08-03
17099.0,pubmed,pubmed,Combination of Peri- and Intratumoral Radiomic Features on Baseline CT Scans Predicts Response to Chemotherapy in Lung Adenocarcinoma,Combination of Peri- and Intratumoral Radiomic Features on Baseline CT Scans Predicts Response to Chemotherapy in Lung Adenocarcinoma,"To identify the role of radiomics texture features both within and outside the nodule in predicting <i>(a)</i> time to progression (TTP) and overall survival (OS) as well as <i>(b)</i> response to chemotherapy in patients with non-small cell lung cancer (NSCLC). Data in a total of 125 patients who had been treated with pemetrexed-based platinum doublet chemotherapy at Cleveland Clinic were retrospectively analyzed. The patients were divided randomly into two sets with the constraint that there were an equal number of responders and nonresponders in the training set. The training set comprised 53 patients with NSCLC, and the validation set comprised 72 patients. A machine learning classifier trained with radiomic texture features extracted from intra- and peritumoral regions of non-contrast-enhanced CT images was used to predict response to chemotherapy. The radiomic risk-score signature was generated by using least absolute shrinkage and selection operator with the Cox regression model; association of the radiomic signature with TTP and OS was also evaluated. A combination of radiomic features in conjunction with a quadratic discriminant analysis classifier yielded a mean maximum area under the receiver operating characteristic curve (AUC) of 0.82 Ã‚Â± 0.09 (standard deviation) in the training set and a corresponding AUC of 0.77 in the independent testing set. The radiomics signature was also significantly associated with TTP (hazard ratio [HR], 2.8; 95% confidence interval [CI]: 1.95, 4.00; <i>P</i> &lt; .0001) and OS (HR, 2.35; 95% CI: 1.41, 3.94; <i>P</i> = .0011). Additionally, decision curve analysis demonstrated that in terms of clinical usefulness, the radiomics signature had a higher overall net benefit in prediction of high-risk patients to receive treatment than the clinicopathologic measurements. This study suggests that radiomic texture features extracted from within and around the nodule on baseline CT scans are <i>(a)</i> predictive of response to chemotherapy and <i>(b)</i> associated with TTP and OS for patients with NSCLC.Ã‚Â© RSNA, 2019<i>Supplemental material is available for this article.</i>","To identify the role of radiomics texture features both within and outside the nodule in predicting <i>(a)</i> time to progression (TTP) and overall survival (OS) as well as <i>(b)</i> response to chemotherapy in patients with non-small cell lung cancer (NSCLC). Data in a total of 125 patients who had been treated with pemetrexed-based platinum doublet chemotherapy at Cleveland Clinic were retrospectively analyzed. The patients were divided randomly into two sets with the constraint that there were an equal number of responders and nonresponders in the training set. The training set comprised 53 patients with NSCLC, and the validation set comprised 72 patients. A machine learning classifier trained with radiomic texture features extracted from intra- and peritumoral regions of non-contrast-enhanced CT images was used to predict response to chemotherapy. The radiomic risk-score signature was generated by using least absolute shrinkage and selection operator with the Cox regression model; association of the radiomic signature with TTP and OS was also evaluated. A combination of radiomic features in conjunction with a quadratic discriminant analysis classifier yielded a mean maximum area under the receiver operating characteristic curve (AUC) of 0.82 Â± 0.09 (standard deviation) in the training set and a corresponding AUC of 0.77 in the independent testing set. The radiomics signature was also significantly associated with TTP (hazard ratio [HR], 2.8; 95% confidence interval [CI]: 1.95, 4.00; <i>P</i> &lt; .0001) and OS (HR, 2.35; 95% CI: 1.41, 3.94; <i>P</i> = .0011). Additionally, decision curve analysis demonstrated that in terms of clinical usefulness, the radiomics signature had a higher overall net benefit in prediction of high-risk patients to receive treatment than the clinicopathologic measurements. This study suggests that radiomic texture features extracted from within and around the nodule on baseline CT scans are <i>(a)</i> predictive of response to chemotherapy and <i>(b)</i> associated with TTP and OS for patients with NSCLC.Â© RSNA, 2019<i>Supplemental material is available for this article.</i>","Khorrami, Khunger, Zagouras, Patil, Thawani, Bera, Rajiah, Fu, Velcheti, Madabhushi","Khorrami, Khunger, Zagouras, Patil, Thawani, Bera, Rajiah, Fu, Velcheti, Madabhushi",https://doi.org/10.1148/ryai.2019180012,https://doi.org/10.1148/ryai.2019180012,2021-08-03
179.0,,pubmed,Comparing machine and human reviewers to evaluate the risk of bias in randomized controlled trials,Comparing machine and human reviewers to evaluate the risk of bias in randomized controlled trials,"BACKGROUND: Evidence from new health technologies is growing, along with demands for evidence to inform policy decisions, creating challenges in completing health technology assessments (HTAs)/systematic reviews (SRs) in a timely manner. Software can decrease the time and burden by automating the process, but evidence validating such software is limited. We tested the accuracy of RobotReviewer, a semi-autonomous risk of bias (RoB) assessment tool, and its agreement with human reviewers. METHODS: Two reviewers independently conducted RoB assessments on a sample of randomized controlled trials (RCTs), and their consensus ratings were compared with those generated by RobotReviewer. Agreement with the human reviewers was assessed using percent agreement and weighted kappa (kappa). The accuracy of RobotReviewer was also assessed by calculating the sensitivity, specificity, and area under the curve in comparison to the consensus agreement of the human reviewers. RESULTS: The study included 372 RCTs. Inter-rater reliability ranged from kappa = -0.06 (no agreement) for blinding of participants and personnel to kappa = 0.62 (good agreement) for random sequence generation (excluding overall RoB). RobotReviewer was found to use a high percentage of 'irrelevant supporting quotations' to complement RoB assessments for blinding of participants and personnel (72.6%), blinding of outcome assessment (70.4%), and allocation concealment (54.3%). CONCLUSION: RobotReviewer can help with risk of bias assessment of RCTs but cannot replace human evaluations. Thus, reviewers should check and validate RoB assessments from RobotReviewer by consulting the original article when not relevant supporting quotations are provided by RobotReviewer. This consultation is in line with the recommendation provided by the developers.","Evidence from new health technologies is growing, along with demands for evidence to inform policy decisions, creating challenges in completing health technology assessments (HTAs)/systematic reviews (SRs) in a timely manner. Software can decrease the time and burden by automating the process, but evidence validating such software is limited. We tested the accuracy of RobotReviewer, a semi-autonomous risk of bias (RoB) assessment tool, and its agreement with human reviewers. Two reviewers independently conducted RoB assessments on a sample of randomized controlled trials (RCTs), and their consensus ratings were compared with those generated by RobotReviewer. Agreement with the human reviewers was assessed using percent agreement and weighted kappa (Îº). The accuracy of RobotReviewer was also assessed by calculating the sensitivity, specificity, and area under the curve in comparison to the consensus agreement of the human reviewers. The study included 372 RCTs. Inter-rater reliability ranged from Îº = -0.06 (no agreement) for blinding of participants and personnel to Îº = 0.62 (good agreement) for random sequence generation (excluding overall RoB). RobotReviewer was found to use a high percentage of ""irrelevant supporting quotations"" to complement RoB assessments for blinding of participants and personnel (72.6%), blinding of outcome assessment (70.4%), and allocation concealment (54.3%). RobotReviewer can help with risk of bias assessment of RCTs but cannot replace human evaluations. Thus, reviewers should check and validate RoB assessments from RobotReviewer by consulting the original article when not relevant supporting quotations are provided by RobotReviewer. This consultation is in line with the recommendation provided by the developers.","Armijo-Olivo, S.
 and Craig, R.
 and Campbell, S.","Armijo-Olivo, Craig, Campbell",not available,https://doi.org/10.1002/jrsm.1398,2021-08-03
17104.0,pubmed,pubmed,A customizable deep learning model for nosocomial risk prediction from critical care notes with indirect supervision,A customizable deep learning model for nosocomial risk prediction from critical care notes with indirect supervision,"Reliable longitudinal risk prediction for hospitalized patients is needed to provide quality care. Our goal is to develop a generalizable model capable of leveraging clinical notes to predict healthcare-associated diseases 24-96 hours in advance. We developed a reCurrent Additive Network for Temporal RIsk Prediction (CANTRIP) to predict the risk of hospital acquired (occurring Ã¢â€°Â¥ 48 hours after admission) acute kidney injury, pressure injury, or anemia Ã¢â€°Â¥ 24 hours before it is implicated by the patient's chart, labs, or notes. We rely on the MIMIC III critical care database and extract distinct positive and negative cohorts for each disease. We retrospectively determine the date-of-event using structured and unstructured criteria and use it as a form of indirect supervision to train and evaluate CANTRIP to predict disease risk using clinical notes. Our experiments indicate that CANTRIP, operating on text alone, obtains 74%-87% area under the curve and 77%-85% Specificity. Baseline shallow models showed lower performance on all metrics, while bidirectional long short-term memory obtained the highest Sensitivity at the cost of significantly lower Specificity and Precision. Proper model architecture allows clinical text to be successfully harnessed to predict nosocomial disease, outperforming shallow models and obtaining similar performance to disease-specific models reported in the literature. Clinical text on its own can provide a competitive alternative to traditional structured features (eg, lab values, vital signs). CANTRIP is able to generalize across nosocomial diseases without disease-specific feature extraction and is available at https://github.com/h4ste/cantrip.","Reliable longitudinal risk prediction for hospitalized patients is needed to provide quality care. Our goal is to develop a generalizable model capable of leveraging clinical notes to predict healthcare-associated diseases 24-96 hours in advance. We developed a reCurrent Additive Network for Temporal RIsk Prediction (CANTRIP) to predict the risk of hospital acquired (occurring â‰¥ 48 hours after admission) acute kidney injury, pressure injury, or anemia â‰¥ 24 hours before it is implicated by the patient's chart, labs, or notes. We rely on the MIMIC III critical care database and extract distinct positive and negative cohorts for each disease. We retrospectively determine the date-of-event using structured and unstructured criteria and use it as a form of indirect supervision to train and evaluate CANTRIP to predict disease risk using clinical notes. Our experiments indicate that CANTRIP, operating on text alone, obtains 74%-87% area under the curve and 77%-85% Specificity. Baseline shallow models showed lower performance on all metrics, while bidirectional long short-term memory obtained the highest Sensitivity at the cost of significantly lower Specificity and Precision. Proper model architecture allows clinical text to be successfully harnessed to predict nosocomial disease, outperforming shallow models and obtaining similar performance to disease-specific models reported in the literature. Clinical text on its own can provide a competitive alternative to traditional structured features (eg, lab values, vital signs). CANTRIP is able to generalize across nosocomial diseases without disease-specific feature extraction and is available at https://github.com/h4ste/cantrip.","Goodwin, Demner-Fushman","Goodwin, Demner-Fushman",https://doi.org/10.1093/jamia/ocaa004,https://doi.org/10.1093/jamia/ocaa004,2021-08-03
17110.0,pubmed,pubmed,Systems biology-based approaches to summarize and identify novel genes and pathways associated with acute and chronic postsurgical pain,Systems biology-based approaches to summarize and identify novel genes and pathways associated with acute and chronic postsurgical pain,"To employ systems biology-based machine learning to identify biologic processes over-represented with genetic variants (gene enrichment) implicated in post-surgical pain. Informed systems biology based integrative computational analyses. Pediatric research and teaching institution. Pubmed search (01/01/2001-10/31/2017) was performed to identify &quot;training&quot; genes associated with postoperative pain in humans. Candidate genes were identified and prioritized using Toppgene suite, based on functional enrichment using several gene ontology annotations, and curated gene sets associated with mouse phenotype-knockout studies. Computationally top-ranked candidate genes and literature-curated genes were included in pathway enrichment analyses. Hierarchical clustering was used to visualize select functional enrichment results between the two phenotypes. Literature review identified 38 training genes associated with postoperative pain and 31 with CPSP. We identified 2610 prioritized novel candidate genes likely associated with acute and chronic postsurgical pain, the top 10th percentile jointly enriched (p 0.05; Benjamini-Hochberg correction) several pathways, topmost being cAMP response element-binding protein and ion channel pathways. Heat maps demonstrated enrichment of inflammatory/drug metabolism processes in acute postoperative pain and immune mechanisms in CPSP. High interindividual variability in pain responses immediately after surgery and risk for CPSP suggests genetic susceptibility. Lack of large homogenous sample sizes have led to underpowered genetic association studies. Systems biology can be leveraged to integrate genetic-level data with biologic processes to generate prioritized candidate gene lists and understand novel biological pathways involved in acute postoperative pain and CPSP. Such data would be key to informing future polygenic studies with targeted genome wide profiling. This study demonstrates the utility of functional annotation - based prioritization and enrichment approaches and identifies novel genes and unique/shared biological processes involved in acute and chronic postoperative pain. Results provide framework for future targeted genetic profiling of CPSP risk, to enable preventive and therapeutic approaches.","To employ systems biology-based machine learning to identify biologic processes over-represented with genetic variants (gene enrichment) implicated in post-surgical pain. Informed systems biology based integrative computational analyses. Pediatric research and teaching institution. Pubmed search (01/01/2001-10/31/2017) was performed to identify ""training"" genes associated with postoperative pain in humans. Candidate genes were identified and prioritized using Toppgene suite, based on functional enrichment using several gene ontology annotations, and curated gene sets associated with mouse phenotype-knockout studies. Computationally top-ranked candidate genes and literature-curated genes were included in pathway enrichment analyses. Hierarchical clustering was used to visualize select functional enrichment results between the two phenotypes. Literature review identified 38 training genes associated with postoperative pain and 31 with CPSP. We identified 2610 prioritized novel candidate genes likely associated with acute and chronic postsurgical pain, the top 10th percentile jointly enriched (p 0.05; Benjamini-Hochberg correction) several pathways, topmost being cAMP response element-binding protein and ion channel pathways. Heat maps demonstrated enrichment of inflammatory/drug metabolism processes in acute postoperative pain and immune mechanisms in CPSP. High interindividual variability in pain responses immediately after surgery and risk for CPSP suggests genetic susceptibility. Lack of large homogenous sample sizes have led to underpowered genetic association studies. Systems biology can be leveraged to integrate genetic-level data with biologic processes to generate prioritized candidate gene lists and understand novel biological pathways involved in acute postoperative pain and CPSP. Such data would be key to informing future polygenic studies with targeted genome wide profiling. This study demonstrates the utility of functional annotation - based prioritization and enrichment approaches and identifies novel genes and unique/shared biological processes involved in acute and chronic postoperative pain. Results provide framework for future targeted genetic profiling of CPSP risk, to enable preventive and therapeutic approaches.","Chidambaran, Ashton, Martin, Jegga","Chidambaran, Ashton, Martin, Jegga",https://doi.org/10.1016/j.jclinane.2020.109738,https://doi.org/10.1016/j.jclinane.2020.109738,2021-08-03
3140.0,,pubmed,The impact of flipped classroom andragogy on student assessment performance and perception of learning experience in two advanced physiology subjects,The impact of flipped classroom andragogy on student assessment performance and perception of learning experience in two advanced physiology subjects,"Flipped classroom teaching has been used by many educators to promote active learning in higher education. This andragogy is thought to increase student engagement by making them more accountable for their learning and increase time on task in the classroom. While there are several systematic reviews that point to improved student results, it remains unclear if flipped classrooms have positive learning effects in physiology education. Flipped classroom teaching was introduced in two advanced physiology subjects (advanced neuroscience, semester 1, and cardiorespiratory and renal physiology, semester 2). Changing the mode of content delivery reduced the time students needed to spend listening to lectures by one-third, without sacrificing either learning content or academic standards. Higher pass rates were observed with larger number of students earning distinction and high-distinction grades. Statistically significant improvements in final grades were observed from both subjects (semester 1: 2017, 49.28 +/- 20.16; 2018, 53.29 +/- 19.77, t<sub>268</sub> = 2.058, P = 0.0405; semester 2: 2017, 58.87 +/- 21.19; 2018, 67.91 +/- 20.40, t<sub>111</sub> = 2.306, P = 0.023). Finally, students' perception of their learning experience remained at or above the university benchmarks (median score of >80% for all iterations of the subjects). While the most frequent and persistent area that students suggested could be improved was reduction of content, equal numbers of students commented that no improvement in the subjects was required. Despite the generally positive attitude to recorded didactic teaching content, classroom attendance remained very low, and students did not engage with the active learning content. This suggest that more emphasis needs to be placed on promoting class attendance by developing better active learning content.","Flipped classroom teaching has been used by many educators to promote active learning in higher education. This andragogy is thought to increase student engagement by making them more accountable for their learning and increase time on task in the classroom. While there are several systematic reviews that point to improved student results, it remains unclear if flipped classrooms have positive learning effects in physiology education. Flipped classroom teaching was introduced in two advanced physiology subjects (advanced neuroscience, <i>semester 1</i>, and cardiorespiratory and renal physiology, <i>semester 2</i>). Changing the mode of content delivery reduced the time students needed to spend listening to lectures by one-third, without sacrificing either learning content or academic standards. Higher pass rates were observed with larger number of students earning distinction and high-distinction grades. Statistically significant improvements in final grades were observed from both subjects (<i>semester 1</i>: 2017, 49.28â€‰Â±â€‰20.16; 2018, 53.29â€‰Â±â€‰19.77, <i>t</i><sub>268</sub>â€‰=â€‰2.058, <i>P</i> = 0.0405; <i>semester 2</i>: 2017, 58.87â€‰Â±â€‰21.19; 2018, 67.91â€‰Â±â€‰20.40, <i>t</i><sub>111</sub>â€‰=â€‰2.306, <i>P</i> = 0.023). Finally, students' perception of their learning experience remained at or above the university benchmarks (median score of &gt;80% for all iterations of the subjects). While the most frequent and persistent area that students suggested could be improved was reduction of content, equal numbers of students commented that no improvement in the subjects was required. Despite the generally positive attitude to recorded didactic teaching content, classroom attendance remained very low, and students did not engage with the active learning content. This suggest that more emphasis needs to be placed on promoting class attendance by developing better active learning content.","Rathner, J. A.
 and Schier, M. A.","Rathner, Schier",https://dx.doi.org/10.1152/advan.00125.2019,https://doi.org/10.1152/advan.00125.2019,2021-08-03
3540.0,,pubmed,Feeling down? A systematic review of the gut microbiota in anxiety/depression and irritable bowel syndrome,Feeling down? A systematic review of the gut microbiota in anxiety/depression and irritable bowel syndrome,"Background Anxiety/depression and irritable bowel syndrome (IBS) are highly prevalent and burdensome conditions, whose co-occurrence is estimated between 44 and 84%. Shared gut microbiota alterations have been identified in these separate disorders relative to controls; however, studies have not adequately considered their comorbidity. This review set out to identify case-control studies comparing the gut microbiota in anxiety/depression, IBS, and both conditions comorbidly relative to each other and to controls, as well as gut microbiota investigations including measures of both IBS and anxiety/depression. Methods Four databases were systematically searched using comprehensive search terms (OVID Medline, Embase, PsycINFO, and PubMed), following PRISMA guidelines. Results Systematic review identified 17 studies (10 human, 7 animal). Most studies investigated the gut microbiota and anxiety/depression symptoms in IBS cohorts. Participants with IBS and high anxiety/depression symptoms had lower alpha diversity compared to controls and IBS-only cohorts. Machine learning and beta diversity distinguished between IBS participants with and without anxiety/depression by their gut microbiota. Comorbid IBS and anxiety/depression also had higher abundance of Proteobacteria, Prevotella/Prevotellaceae, Bacteroides and lower Lachnospiraceae relative to controls. Limitations A large number of gut microbiota estimation methods and statistical techniques were utilized; therefore, meta-analysis was not possible. Conclusions Well-designed case-control and longitudinal studies are required to disentangle whether the gut microbiota is predicted as a continuum of gastrointestinal and anxiety/depression symptom severity, or whether reported dysbiosis is unique to IBS and anxiety/depression comorbidity. These findings may inform the development of targeted treatment through the gut microbiota for individuals with both anxiety/depression and IBS.","Background Anxiety/depression and irritable bowel syndrome (IBS) are highly prevalent and burdensome conditions, whose co-occurrence is estimated between 44 and 84%. Shared gut microbiota alterations have been identified in these separate disorders relative to controls; however, studies have not adequately considered their comorbidity. This review set out to identify case-control studies comparing the gut microbiota in anxiety/depression, IBS, and both conditions comorbidly relative to each other and to controls, as well as gut microbiota investigations including measures of both IBS and anxiety/depression. Methods Four databases were systematically searched using comprehensive search terms (OVID Medline, Embase, PsycINFO, and PubMed), following PRISMA guidelines. Results Systematic review identified 17 studies (10 human, 7 animal). Most studies investigated the gut microbiota and anxiety/depression symptoms in IBS cohorts. Participants with IBS and high anxiety/depression symptoms had lower alpha diversity compared to controls and IBS-only cohorts. Machine learning and beta diversity distinguished between IBS participants with and without anxiety/depression by their gut microbiota. Comorbid IBS and anxiety/depression also had higher abundance of Proteobacteria, Prevotella/Prevotellaceae, Bacteroides and lower Lachnospiraceae relative to controls. Limitations A large number of gut microbiota estimation methods and statistical techniques were utilized; therefore, meta-analysis was not possible. Conclusions Well-designed case-control and longitudinal studies are required to disentangle whether the gut microbiota is predicted as a continuum of gastrointestinal and anxiety/depression symptom severity, or whether reported dysbiosis is unique to IBS and anxiety/depression comorbidity. These findings may inform the development of targeted treatment through the gut microbiota for individuals with both anxiety/depression and IBS.","Simpson, C. A.
 and Mu, A.
 and Haslam, N.
 and Schwartz, O. S.
 and Simmons, J. G.","Simpson, Mu, Haslam, Schwartz, Simmons",https://dx.doi.org/10.1016/j.jad.2020.01.124,https://doi.org/10.1016/j.jad.2020.01.124,2021-08-03
4279.0,,pubmed,The diagnostic accuracy of artificial intelligence in thoracic diseases: A protocol for systematic review and meta-analysis,The diagnostic accuracy of artificial intelligence in thoracic diseases: A protocol for systematic review and meta-analysis,"INTRODUCTION: Thoracic diseases include a variety of common human primary malignant tumors, among which lung cancer and esophageal cancer are among the top 10 in cancer incidence and mortality. Early diagnosis is an important part of cancer treatment, so artificial intelligence (AI) systems have been developed for the accurate and automated detection and diagnosis of thoracic tumors. However, the complicated AI structure and image processing made the diagnosis result of AI-based system unstable. The purpose of this study is to systematically review published evidence to explore the accuracy of AI systems in diagnosing thoracic cancers. METHODS AND ANALYSIS: We will conduct a systematic review and meta-analysis of the diagnostic accuracy of AI systems for the prediction of thoracic diseases. The primary objective is to assess the diagnostic accuracy of thoracic cancers, including assessing potential biases and calculating combined estimates of sensitivity, specificity, and area under the receiver operating characteristic curve (AUC). The secondary objective is to evaluate the factors associated with different models, classifiers, and radiomics information. We will search databases such as PubMed/MEDLINE, Embase (via OVID), and the Cochrane Library. Two reviewers will independently screen titles and abstracts, perform full article reviews and extract study data. We will report study characteristics and assess methodological quality using the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) tool. RevMan 5.3 and Meta-disc 1.4 software will be used for data synthesis. If pooling is appropriate, we will produce summary receiver operating characteristic (SROC) curves, summary operating points (pooled sensitivity and specificity), and 95% confidence intervals around the summary operating points. Methodological subgroup and sensitivity analyses will be performed to explore heterogeneity. Prospero registration number: crd42019135247.","Thoracic diseases include a variety of common human primary malignant tumors, among which lung cancer and esophageal cancer are among the top 10 in cancer incidence and mortality. Early diagnosis is an important part of cancer treatment, so artificial intelligence (AI) systems have been developed for the accurate and automated detection and diagnosis of thoracic tumors. However, the complicated AI structure and image processing made the diagnosis result of AI-based system unstable. The purpose of this study is to systematically review published evidence to explore the accuracy of AI systems in diagnosing thoracic cancers. We will conduct a systematic review and meta-analysis of the diagnostic accuracy of AI systems for the prediction of thoracic diseases. The primary objective is to assess the diagnostic accuracy of thoracic cancers, including assessing potential biases and calculating combined estimates of sensitivity, specificity, and area under the receiver operating characteristic curve (AUC). The secondary objective is to evaluate the factors associated with different models, classifiers, and radiomics information. We will search databases such as PubMed/MEDLINE, Embase (via OVID), and the Cochrane Library. Two reviewers will independently screen titles and abstracts, perform full article reviews and extract study data. We will report study characteristics and assess methodological quality using the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) tool. RevMan 5.3 and Meta-disc 1.4 software will be used for data synthesis. If pooling is appropriate, we will produce summary receiver operating characteristic (SROC) curves, summary operating points (pooled sensitivity and specificity), and 95% confidence intervals around the summary operating points. Methodological subgroup and sensitivity analyses will be performed to explore heterogeneity. CRD42019135247.","Yang, Y.
 and Jin, G.
 and Pang, Y.
 and Wang, W.
 and Zhang, H.
 and Tuo, G.
 and Wu, P.
 and Wang, Z.
 and Zhu, Z.","Yang, Jin, Pang, Wang, Zhang, Tuo, Wu, Wang, Zhu",https://dx.doi.org/10.1097/MD.0000000000019114,https://doi.org/10.1097/MD.0000000000019114,2021-08-03
17115.0,pubmed,pubmed,A Corticosteroid-Eluting Sinus Implant Following Endoscopic Sinus Surgery for Chronic Rhinosinusitis: A UK-Based Cost-Effectiveness Analysis,A Corticosteroid-Eluting Sinus Implant Following Endoscopic Sinus Surgery for Chronic Rhinosinusitis: A UK-Based Cost-Effectiveness Analysis,"Chronic rhinosinusitis (CRS) is one of the commonest chronic health problems among adults in the UK. Around 15% of CRS patients undergo functional endoscopic sinus surgery (FESS) annually after failing medical treatment. However, as incomplete resolution of symptoms or complications post-operatively is common, the post-operative management is considered to be as important as the surgery itself. A bioabsorbable corticosteroid-eluting sinus implant (CESI) (Propel<sup>Ã‚Â®</sup>, mometasone furoate 370Ã‚Â Ã‚Âµg) has been used as an alternative post-FESS treatment. The objective of this study was to assess the cost effectiveness of the corticosteroid-eluting implant versus non-corticosteroid-eluting spacer following FESS for treatment of patients with CRS. A decision tree model was developed to estimate the cost and effectiveness in each strategy. Costs and effects were estimated from a UK National Health Service (NHS) and personal social services perspective over a 6-month time horizon. Model pathways and parameters were informed by existing clinical guidelines and literature and sensitivity analyses were conducted to explore uncertainties in base-case assumptions. Over a 6-month time horizon, inserting CESI at the end of FESS is less costly (Ã‚Â£4646 vs. Ã‚Â£4655 per patient) and is the more effective intervention [total quality-adjusted life-years (QALYs) over 6Ã‚Â months 0.443 vs. 0.444] than non-corticosteroid-eluting spacers; hence, it is a dominant strategy. The probabilistic analysis results indicate that CESI following FESS has a 62% probability of being cost effective at the Ã‚Â£20,000/per QALY willingness-to-pay threshold and 56% probability of being a cost-saving intervention. The use of CESI after FESS results in fewer post-operative complications than non-corticosteroid-eluting implants and may be a cost-saving technology over a 6-month time horizon. Although the cost of initial treatment with the CESI is greater, cost savings are made due to a reduction in the number of complications experienced.","Chronic rhinosinusitis (CRS) is one of the commonest chronic health problems among adults in the UK. Around 15% of CRS patients undergo functional endoscopic sinus surgery (FESS) annually after failing medical treatment. However, as incomplete resolution of symptoms or complications post-operatively is common, the post-operative management is considered to be as important as the surgery itself. A bioabsorbable corticosteroid-eluting sinus implant (CESI) (Propel<sup>Â®</sup>, mometasone furoate 370Â Âµg) has been used as an alternative post-FESS treatment. The objective of this study was to assess the cost effectiveness of the corticosteroid-eluting implant versus non-corticosteroid-eluting spacer following FESS for treatment of patients with CRS. A decision tree model was developed to estimate the cost and effectiveness in each strategy. Costs and effects were estimated from a UK National Health Service (NHS) and personal social services perspective over a 6-month time horizon. Model pathways and parameters were informed by existing clinical guidelines and literature and sensitivity analyses were conducted to explore uncertainties in base-case assumptions. Over a 6-month time horizon, inserting CESI at the end of FESS is less costly (Â£4646 vs. Â£4655 per patient) and is the more effective intervention [total quality-adjusted life-years (QALYs) over 6Â months 0.443 vs. 0.444] than non-corticosteroid-eluting spacers; hence, it is a dominant strategy. The probabilistic analysis results indicate that CESI following FESS has a 62% probability of being cost effective at the Â£20,000/per QALY willingness-to-pay threshold and 56% probability of being a cost-saving intervention. The use of CESI after FESS results in fewer post-operative complications than non-corticosteroid-eluting implants and may be a cost-saving technology over a 6-month time horizon. Although the cost of initial treatment with the CESI is greater, cost savings are made due to a reduction in the number of complications experienced.","Javanbakht, Saleh, Hemami, Branagan-Harris, Boiano","Javanbakht, Saleh, Hemami, Branagan-Harris, Boiano",https://doi.org/10.1007/s41669-020-00198-8,https://doi.org/10.1007/s41669-020-00198-8,2021-08-03
3229.0,,pubmed,"Placebo response in trials of drug treatments for cancer-related fatigue: a systematic review, meta-analysis and meta-regression","Placebo response in trials of drug treatments for cancer-related fatigue: a systematic review, meta-analysis and meta-regression","BACKGROUND: Cancer-related fatigue (CRF) is one of the most distressing symptoms experienced by patients. There is no gold standard treatment, although multiple drugs have been tested with little evidence of efficacy. Randomised controlled trials (RCTs) of these drugs have commented on the existence or size of the placebo response (PR). The objective of this systematic review was to establish the magnitude of the PR in RCTs of drugs to relieve CRF and to identify contributing factors. METHOD: RCTs were included in which the objective was to treat CRF. A meta-analysis was conducted using the standardised mean change (SMC) between baseline and final measurement in the placebo group. To explore factors that may be associated with the PR (eg, population or drug), a meta-regression was undertaken. Risk of bias was assessed using the revised Cochrane tool. RESULTS: From 3916 citations, 30 relevant RCTs were identified. All had limitations that increased their risk of bias. The pooled SMC in reduction in fatigue status in placebo groups was -0.23 (95% confidence intervals -0.42 to -0.04). None of the variables analysed in the meta-regression were statistically significant related to PR. CONCLUSION: There is some evidence, based on trials with small samples, that the PR in trials testing drugs for CRF is non-trivial in size and statistically significant. We recommend that researchers planning drug studies in CRF should consider implementing alternative trial designs to better account for PR and decrease impact on the study results.","Cancer-related fatigue (CRF) is one of the most distressing symptoms experienced by patients. There is no gold standard treatment, although multiple drugs have been tested with little evidence of efficacy. Randomised controlled trials (RCTs) of these drugs have commented on the existence or size of the placebo response (PR). The objective of this systematic review was to establish the magnitude of the PR in RCTs of drugs to relieve CRF and to identify contributing factors. RCTs were included in which the objective was to treat CRF. A meta-analysis was conducted using the standardised mean change (SMC) between baseline and final measurement in the placebo group. To explore factors that may be associated with the PR (eg, population or drug), a meta-regression was undertaken. Risk of bias was assessed using the revised Cochrane tool. From 3916 citations, 30 relevant RCTs were identified. All had limitations that increased their risk of bias. The pooled SMC in reduction in fatigue status in placebo groups was -0.23 (95% confidence intervals -0.42 to -0.04). None of the variables analysed in the meta-regression were statistically significant related to PR. There is some evidence, based on trials with small samples, that the PR in trials testing drugs for CRF is non-trivial in size and statistically significant. We recommend that researchers planning drug studies in CRF should consider implementing alternative trial designs to better account for PR and decrease impact on the study results.","Roji, R.
 and Stone, P.
 and Ricciardi, F.
 and Candy, B.","Roji, Stone, Ricciardi, Candy",https://dx.doi.org/10.1136/bmjspcare-2019-002163,https://doi.org/10.1136/bmjspcare-2019-002163,2021-08-03
774.0,,pubmed,A systematic review of automated feeder detection software for locoregional treatment of hepatic tumors,A systematic review of automated feeder detection software for locoregional treatment of hepatic tumors,"PURPOSE: The purpose of this study was to perform a systematic review of current literature describing the efficacy and technical outcomes of transarterial liver therapies using automated feeder detection (AFD) software. MATERIALS AND METHODS: This systematic review was conducted in accordance with the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) guidelines. A structured search was performed in the PubMed, SCOPUS, and Embase databases of patients undergoing locoregional therapy of liver tumors utilizing AFD software. Demographic data, procedure data (including radiometrics) and tumor response rate were recorded. Where available, performance of AFD was compared to conventional digital subtraction angiography (DSA) and cone-beam CT (CBCT) without AFD. RESULTS: A total of 14 full-text manuscripts met inclusion criteria, comprising 1042 tumors in 604 patients (305 men, 156 women; mean age, 68.6+/-6.0 [SD] years), including 537 patients with hepatocellular carcinoma, 8 with metastases from neuroendocrine tumors, and 59 patients without reported etiology. Reported sensitivity of AFD ranged between 86% and 98.5%, compared to DSA alone (38% - 64%) or DSA in combination with CBCT (69% - 81%). Three studies reported tumor response by modified response evaluation criteria in solid tumors (mRECIST) guidelines, with complete response in the range of 60% - 69%. CONCLUSION: AFD is a promising new technology for the identification of intrahepatic and extrahepatic tumor-feeding arteries and should be considered a useful adjunct to conventional DSA and CBCT in the treatment of liver tumors.","The purpose of this study was to perform a systematic review of current literature describing the efficacy and technical outcomes of transarterial liver therapies using automated feeder detection (AFD) software. This systematic review was conducted in accordance with the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) guidelines. A structured search was performed in the PubMed, SCOPUS, and Embase databases of patients undergoing locoregional therapy of liver tumors utilizing AFD software. Demographic data, procedure data (including radiometrics) and tumor response rate were recorded. Where available, performance of AFD was compared to conventional digital subtraction angiography (DSA) and cone-beam CT (CBCT) without AFD. A total of 14 full-text manuscripts met inclusion criteria, comprising 1042 tumors in 604 patients (305 men, 156 women; mean age, 68.6Â±6.0 [SD] years), including 537 patients with hepatocellular carcinoma, 8 with metastases from neuroendocrine tumors, and 59 patients without reported etiology. Reported sensitivity of AFD ranged between 86% and 98.5%, compared to DSA alone (38% - 64%) or DSA in combination with CBCT (69% - 81%). Three studies reported tumor response by modified response evaluation criteria in solid tumors (mRECIST) guidelines, with complete response in the range of 60% - 69%. AFD is a promising new technology for the identification of intrahepatic and extrahepatic tumor-feeding arteries and should be considered a useful adjunct to conventional DSA and CBCT in the treatment of liver tumors.","Cui, Z.
 and Shukla, P. A.
 and Habibollahi, P.
 and Park, H. S.
 and Fischman, A.
 and Kolber, M. K.","Cui, Shukla, Habibollahi, Park, Fischman, Kolber",https://dx.doi.org/10.1016/j.diii.2020.01.011,https://doi.org/10.1016/j.diii.2020.01.011,2021-08-03
2656.0,,pubmed,Application of Raw Accelerometer Data and Machine-Learning Techniques to Characterize Human Movement Behavior: A Systematic Scoping Review,Application of Raw Accelerometer Data and Machine-Learning Techniques to Characterize Human Movement Behavior: A Systematic Scoping Review,"BACKGROUND: Application of machine learning for classifying human behavior is increasingly common as access to raw accelerometer data improves. The aims of this scoping review are (1) to examine if machine-learning techniques can accurately identify human activity behaviors from raw accelerometer data and (2) to summarize the practical implications of these machine-learning techniques for future work. METHODS: Keyword searches were performed in Scopus, Web of Science, and EBSCO databases in 2018. Studies that applied supervised machine-learning techniques to raw accelerometer data and estimated components of physical activity were included. Information on study characteristics, machine-learning techniques, and key study findings were extracted from included studies. RESULTS: Of the 53 studies included in the review, 75% were published in the last 5 years. Most studies predicted postures and activity type, rather than intensity, and were conducted in controlled environments using 1 or 2 devices. The most common models were support vector machine, random forest, and artificial neural network. Overall, classification accuracy ranged from 62% to 99.8%, although nearly 80% of studies achieved an overall accuracy above 85%. CONCLUSIONS: Machine-learning algorithms demonstrate good accuracy when predicting physical activity components; however, their application to free-living settings is currently uncertain.","Application of machine learning for classifying human behavior is increasingly common as access to raw accelerometer data improves. The aims of this scoping review are (1)Â to examine if machine-learning techniques can accurately identify human activity behaviors from raw accelerometer data and (2)Â to summarize the practical implications of these machine-learning techniques for future work. Keyword searches were performed in Scopus, Web of Science, and EBSCO databases in 2018. Studies that applied supervised machine-learning techniques to raw accelerometer data and estimated components of physical activity were included. Information on study characteristics, machine-learning techniques, and key study findings were extracted from included studies. Of the 53 studies included in the review, 75% were published in the last 5 years. Most studies predicted postures and activity type, rather than intensity, and were conducted in controlled environments using 1 or 2 devices. The most common models were support vector machine, random forest, and artificial neural network. Overall, classification accuracy ranged from 62% to 99.8%, although nearly 80% of studies achieved an overall accuracy above 85%. Machine-learning algorithms demonstrate good accuracy when predicting physical activity components; however, their application to free-living settings is currently uncertain.","Narayanan, A.
 and Desai, F.
 and Stewart, T.
 and Duncan, S.
 and Mackay, L.","Narayanan, Desai, Stewart, Duncan, Mackay",https://dx.doi.org/10.1123/jpah.2019-0088,https://doi.org/10.1123/jpah.2019-0088,2021-08-03
3733.0,,pubmed,A systematic review on the accuracy of zirconia crowns and fixed dental prostheses,A systematic review on the accuracy of zirconia crowns and fixed dental prostheses,"<b>Purpose:</b> The aim of this study was to review the fit and assess the accuracy of tooth-supported single and multi-unit zirconia fixed dental prostheses. <b>Background:</b> The fit of zirconia restorations has been reported in several studies, but the accuracy of the manufacturing process is seldom discussed or used when drawing conclusions on the fit. <b>Materials and methods:</b> A literature search of articles published in PubMed between 2 March 2013 and 1 February 2018 was performed using clearly defined inclusion and exclusion criteria. 841 articles were found and 767 were excluded after screening the title and abstract. After full-text analysis another 60 articles were excluded which left 14 articles to be included for data extraction. Fit was the mean of distances reported in the studies and accuracy was the fit minus the pre-set spacer <b>Results:</b> For marginal gap of single crowns and multi-unit FDPs combined, the fit was 83 mum and the accuracy was 59 mum. The internal gap fit was 111 mum and the accuracy 61 mum. For the total gap, the fit was 101 mum, and the accuracy of the zirconia restorations was 53 mum.","<b>Purpose:</b> The aim of this study was to review the fit and assess the accuracy of tooth-supported single and multi-unit zirconia fixed dental prostheses. <b>Background:</b> The fit of zirconia restorations has been reported in several studies, but the accuracy of the manufacturing process is seldom discussed or used when drawing conclusions on the fit. <b>Materials and methods:</b> A literature search of articles published in PubMed between 2 March 2013 and 1 February 2018 was performed using clearly defined inclusion and exclusion criteria. 841 articles were found and 767 were excluded after screening the title and abstract. After full-text analysis another 60 articles were excluded which left 14 articles to be included for data extraction. Fit was the mean of distances reported in the studies and accuracy was the fit minus the pre-set spacer <b>Results:</b> For marginal gap of single crowns and multi-unit FDPs combined, the fit was 83â€‰Î¼m and the accuracy was 59â€‰Î¼m. The internal gap fit was 111â€‰Î¼m and the accuracy 61â€‰Î¼m. For the total gap, the fit was 101â€‰Î¼m, and the accuracy of the zirconia restorations was 53â€‰Î¼m. <b>Conclusions:</b> Within the limitations of the present systematic review the fit of zirconia single crowns and multi-unit FDPs may be regarded as clinically acceptable, and the accuracy of the manufacturing of zirconia is âˆ¼60â€‰Î¼m for marginal, internal, and total gap. Also, digital impressions seem to be associated with a smaller gap value.","Svanborg, P.",Svanborg,https://dx.doi.org/10.1080/26415275.2019.1708202,https://doi.org/10.1080/26415275.2019.1708202,2021-08-03
3632.0,,pubmed,Deep Learning for Natural Language Processing in Radiology-Fundamentals and a Systematic Review,Deep Learning for Natural Language Processing in Radiology-Fundamentals and a Systematic Review,"PURPOSE: Natural language processing (NLP) enables conversion of free text into structured data. Recent innovations in deep learning technology provide improved NLP performance. We aimed to survey deep learning NLP fundamentals and review radiology-related research. METHODS: This systematic review was reported according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. We searched for deep learning NLP radiology studies published up to September 2019. MEDLINE, Scopus, and Google Scholar were used as search databases. RESULTS: Ten relevant studies published between 2018 and 2019 were identified. Deep learning models applied for NLP in radiology are convolutional neural networks, recurrent neural networks, long short-term memory networks, and attention networks. Deep learning NLP applications in radiology include flagging of diagnoses such as pulmonary embolisms and fractures, labeling follow-up recommendations, and automatic selection of imaging protocols. Deep learning NLP models perform as well as or better than traditional NLP models. CONCLUSION: Research and use of deep learning NLP in radiology is increasing. Acquaintance with this technology can help prepare radiologists for the coming changes in their field.","Natural language processing (NLP) enables conversion of free text into structured data. Recent innovations in deep learning technology provide improved NLP performance. We aimed to survey deep learning NLP fundamentals and review radiology-related research. This systematic review was reported according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. We searched for deep learning NLP radiology studies published up to September 2019. MEDLINE, Scopus, and Google Scholar were used as search databases. Ten relevant studies published between 2018 and 2019 were identified. Deep learning models applied for NLP in radiology are convolutional neural networks, recurrent neural networks, long short-term memory networks, and attention networks. Deep learning NLP applications in radiology include flagging of diagnoses such as pulmonary embolisms and fractures, labeling follow-up recommendations, and automatic selection of imaging protocols. Deep learning NLP models perform as well as or better than traditional NLP models. Research and use of deep learning NLP in radiology is increasing. Acquaintance with this technology can help prepare radiologists for the coming changes in their field.","Sorin, V.
 and Barash, Y.
 and Konen, E.
 and Klang, E.","Sorin, Barash, Konen, Klang",https://dx.doi.org/10.1016/j.jacr.2019.12.026,https://doi.org/10.1016/j.jacr.2019.12.026,2021-08-03
3817.0,,pubmed,Comparison of aggregate and individual participant data approaches to meta-analysis of randomised trials: An observational study,Comparison of aggregate and individual participant data approaches to meta-analysis of randomised trials: An observational study,"BACKGROUND: It remains unclear when standard systematic reviews and meta-analyses that rely on published aggregate data (AD) can provide robust clinical conclusions. We aimed to compare the results from a large cohort of systematic reviews and meta-analyses based on individual participant data (IPD) with meta-analyses of published AD, to establish when the latter are most likely to be reliable and when the IPD approach might be required. METHODS AND FINDINGS: We used 18 cancer systematic reviews that included IPD meta-analyses: all of those completed and published by the Meta-analysis Group of the MRC Clinical Trials Unit from 1991 to 2010. We extracted or estimated hazard ratios (HRs) and standard errors (SEs) for survival from trial reports and compared these with IPD equivalents at both the trial and meta-analysis level. We also extracted or estimated the number of events. We used paired t tests to assess whether HRs and SEs from published AD differed on average from those from IPD. We assessed agreement, and whether this was associated with trial or meta-analysis characteristics, using the approach of Bland and Altman. The 18 systematic reviews comprised 238 unique trials or trial comparisons, including 37,082 participants. A HR and SE could be generated for 127 trials, representing 53% of the trials and approximately 79% of eligible participants. On average, trial HRs derived from published AD were slightly more in favour of the research interventions than those from IPD (HRAD to HRIPD ratio = 0.95, p = 0.007), but the limits of agreement show that for individual trials, the HRs could deviate substantially. These limits narrowed with an increasing number of participants (p < 0.001) or a greater number (p < 0.001) or proportion (p < 0.001) of events in the AD. On average, meta-analysis HRs from published AD slightly tended to favour the research interventions whether based on fixed-effect (HRAD to HRIPD ratio = 0.97, p = 0.088) or random-effects (HRAD to HRIPD ratio = 0.96, p = 0.044) models, but the limits of agreement show that for individual meta-analyses, agreement was much more variable. These limits tended to narrow with an increasing number (p = 0.077) or proportion of events (p = 0.11) in the AD. However, even when the information size of the AD was large, individual meta-analysis HRs could still differ from their IPD equivalents by a relative 10% in favour of the research intervention to 5% in favour of control. We utilised the results to construct a decision tree for assessing whether an AD meta-analysis includes sufficient information, and when estimates of effects are most likely to be reliable. A lack of power at the meta-analysis level may have prevented us identifying additional factors associated with the reliability of AD meta-analyses, and we cannot be sure that our results are generalisable to all outcomes and effect measures. CONCLUSIONS: In this study we found that HRs from published AD were most likely to agree with those from IPD when the information size was large. Based on these findings, we provide guidance for determining systematically when standard AD meta-analysis will likely generate robust clinical conclusions, and when the IPD approach will add considerable value.","It remains unclear when standard systematic reviews and meta-analyses that rely on published aggregate data (AD) can provide robust clinical conclusions. We aimed to compare the results from a large cohort of systematic reviews and meta-analyses based on individual participant data (IPD) with meta-analyses of published AD, to establish when the latter are most likely to be reliable and when the IPD approach might be required. We used 18 cancer systematic reviews that included IPD meta-analyses: all of those completed and published by the Meta-analysis Group of the MRC Clinical Trials Unit from 1991 to 2010. We extracted or estimated hazard ratios (HRs) and standard errors (SEs) for survival from trial reports and compared these with IPD equivalents at both the trial and meta-analysis level. We also extracted or estimated the number of events. We used paired t tests to assess whether HRs and SEs from published AD differed on average from those from IPD. We assessed agreement, and whether this was associated with trial or meta-analysis characteristics, using the approach of Bland and Altman. The 18 systematic reviews comprised 238 unique trials or trial comparisons, including 37,082 participants. A HR and SE could be generated for 127 trials, representing 53% of the trials and approximately 79% of eligible participants. On average, trial HRs derived from published AD were slightly more in favour of the research interventions than those from IPD (HRAD to HRIPD ratio = 0.95, p = 0.007), but the limits of agreement show that for individual trials, the HRs could deviate substantially. These limits narrowed with an increasing number of participants (p &lt; 0.001) or a greater number (p &lt; 0.001) or proportion (p &lt; 0.001) of events in the AD. On average, meta-analysis HRs from published AD slightly tended to favour the research interventions whether based on fixed-effect (HRAD to HRIPD ratio = 0.97, p = 0.088) or random-effects (HRAD to HRIPD ratio = 0.96, p = 0.044) models, but the limits of agreement show that for individual meta-analyses, agreement was much more variable. These limits tended to narrow with an increasing number (p = 0.077) or proportion of events (p = 0.11) in the AD. However, even when the information size of the AD was large, individual meta-analysis HRs could still differ from their IPD equivalents by a relative 10% in favour of the research intervention to 5% in favour of control. We utilised the results to construct a decision tree for assessing whether an AD meta-analysis includes sufficient information, and when estimates of effects are most likely to be reliable. A lack of power at the meta-analysis level may have prevented us identifying additional factors associated with the reliability of AD meta-analyses, and we cannot be sure that our results are generalisable to all outcomes and effect measures. In this study we found that HRs from published AD were most likely to agree with those from IPD when the information size was large. Based on these findings, we provide guidance for determining systematically when standard AD meta-analysis will likely generate robust clinical conclusions, and when the IPD approach will add considerable value.","Tierney, J. F.
 and Fisher, D. J.
 and Burdett, S.
 and Stewart, L. A.
 and Parmar, M. K. B.","Tierney, Fisher, Burdett, Stewart, Parmar",https://dx.doi.org/10.1371/journal.pmed.1003019,https://doi.org/10.1371/journal.pmed.1003019,2021-08-03
3694.0,,pubmed,Improving palliative and end-of-life care with machine learning and routine data: a rapid review,Improving palliative and end-of-life care with machine learning and routine data: a rapid review,"<b>Introduction:</b> Improving end-of-life (EOL) care is a priority worldwide as this population experiences poor outcomes and accounts disproportionately for costs. In clinical practice, physician judgement is the core method of identifying EOL care needs but has important limitations. Machine learning (ML) is a subset of artificial intelligence advancing capacity to identify patterns and make predictions using large datasets. ML approaches have the potential to improve clinical decision-making and policy design, but there has been no systematic assembly of current evidence.","<b>Introduction:</b> Improving end-of-life (EOL) care is a priority worldwide as this population experiences poor outcomes and accounts disproportionately for costs. In clinical practice, physician judgement is the core method of identifying EOL care needs but has important limitations. Machine learning (ML) is a subset of artificial intelligence advancing capacity to identify patterns and make predictions using large datasets.Â ML approaches have the potential to improve clinical decision-making and policy design, but there has been no systematic assembly of current evidence. <b>Methods:</b> We conducted a rapid review, searching systematically seven databases from inception to December 31st, 2018: EMBASE, MEDLINE, Cochrane Library, PsycINFO, WOS, SCOPUS and ECONLIT.Â We included peer-reviewed studies that used ML approaches on routine data to improve palliative and EOL care for adults.Â Our specified outcomes were survival, quality of life (QoL), place of death, costs, and receipt of high-intensity treatment near end of life.Â We did not search grey literature and excluded material that was not a peer-reviewed article. <b>Results:</b> The database search identified 426 citations. We discarded 162 duplicates and screened 264 unique title/abstracts, of which 22 were forwarded for full text review.Â Three papers were included, 18 papers were excluded and one full text was sought but unobtainable.Â One paper predicted six-month mortality, one paper predicted 12-month mortality and one paper cross-referenced predicted 12-month mortality with healthcare spending.Â ML-informed models outperformed logistic regression in predicting mortality but poor prognosis is a weak driver of costs.Â Models using only routine administrative data had limited benefit from ML methods. <b>Conclusion:</b> While ML can in principle help to identify those at risk of adverse outcomes and inappropriate treatment near EOL, applications to policy and practice are formative.Â Future research must not only expand scope to other outcomes and longer timeframes, but also engage with individual preferences and ethical challenges.","Storick, V.
 and O'Herlihy, A.
 and Abdelhafeez, S.
 and Ahmed, R.
 and May, P.","Storick, O'Herlihy, Abdelhafeez, Ahmed, May",https://dx.doi.org/10.12688/hrbopenres.12923.2,https://doi.org/10.12688/hrbopenres.12923.2,2021-08-03
1401.0,,pubmed,Lung Nodule Detection from Feature Engineering to Deep Learning in Thoracic CT Images: a Comprehensive Review,Lung Nodule Detection from Feature Engineering to Deep Learning in Thoracic CT Images: a Comprehensive Review,"This paper presents a systematic review of the literature focused on the lung nodule detection in chest computed tomography (CT) images. Manual detection of lung nodules by the radiologist is a sequential and time-consuming process. The detection is subjective and depends on the radiologist's experiences. Owing to the variation in shapes and appearances of a lung nodule, it is very difficult to identify the proper location of the nodule from a huge number of slices generated by the CT scanner. Small nodules (< 10 mm in diameter) may be missed by this manual detection process. Therefore, computer-aided diagnosis (CAD) system acts as a 'second opinion' for the radiologists, by making final decision quickly with higher accuracy and greater confidence. The goal of this survey work is to present the current state of the artworks and their progress towards lung nodule detection to the researchers and readers in this domain. This review paper has covered the published works from 2009 to April 2018. Different nodule detection approaches are described elaborately in this work. Recently, it is observed that deep learning (DL)-based approaches are applied extensively for nodule detection and characterization. Therefore, emphasis has been given to convolutional neural network (CNN)-based DL approaches by describing different CNN-based networks.","This paper presents a systematic review of the literature focused on the lung nodule detection in chest computed tomography (CT) images. Manual detection of lung nodules by the radiologist is a sequential and time-consuming process. The detection is subjective and depends on the radiologist's experiences. Owing to the variation in shapes and appearances of a lung nodule, it is very difficult to identify the proper location of the nodule from a huge number of slices generated by the CT scanner. Small nodules (&lt;â€‰10Â mm in diameter) may be missed by this manual detection process. Therefore, computer-aided diagnosis (CAD) system acts as a ""second opinion"" for the radiologists, by making final decision quickly with higher accuracy and greater confidence. The goal of this survey work is to present the current state of the artworks and their progress towards lung nodule detection to the researchers and readers in this domain. This review paper has covered the published works from 2009 to April 2018. Different nodule detection approaches are described elaborately in this work. Recently, it is observed that deep learning (DL)-based approaches are applied extensively for nodule detection and characterization. Therefore, emphasis has been given to convolutional neural network (CNN)-based DL approaches by describing different CNN-based networks.","Halder, A.
 and Dey, D.
 and Sadhu, A. K.","Halder, Dey, Sadhu",https://dx.doi.org/10.1007/s10278-020-00320-6,https://doi.org/10.1007/s10278-020-00320-6,2021-08-03
1501.0,,pubmed,Clinical Scenarios for Which Cervical Mobilization and Manipulation Are Considered by an Expert Panel to Be Appropriate (and Inappropriate) for Patients With Chronic Neck Pain,Clinical Scenarios for Which Cervical Mobilization and Manipulation Are Considered by an Expert Panel to Be Appropriate (and Inappropriate) for Patients With Chronic Neck Pain,"OBJECTIVES: Cervical mobilization and manipulation are 2 therapies commonly used for chronic neck pain (CNP). However, safety, especially of cervical manipulation, is controversial. This study identifies the clinical scenarios for which an expert panel rated cervical mobilization and manipulation as appropriate and inappropriate. METHODS: An expert panel, following a well-validated modified-Delphi approach, used an evidence synthesis and clinical acumen to develop and then rate the appropriateness of cervical mobilization and manipulation for each of an exhaustive list of clinical scenarios for CNP. Key patient characteristics were identified using decision tree analysis (DTA). RESULTS: Three hundred seventy-two clinical scenarios were defined and rated by an 11-member expert panel as to the appropriateness of cervical mobilization and manipulation. Across clinical scenarios more were rated inappropriate than appropriate for both therapies, and more scenarios were rated inappropriate for manipulation than mobilization. However, the number of patients presenting with each scenario is not yet known. Nevertheless, DTA indicates that all clinical scenarios that included red flags (eg, fever, cancer, inflammatory arthritides, or vasculitides), and some others involving major neurological findings, especially if previous manual therapy was unfavorable, were rated as inappropriate for both cervical mobilization and manipulation. DTA also identified the absence of cervical disk herniation, stenosis, or foraminal osteophytosis on additional testing as the most important patient characteristic in predicting ratings of appropriate. CONCLUSIONS: Clinical guidelines for CNP should include information on the clinical scenarios for which cervical mobilization and manipulation were found inappropriate, including those with red flags, and others involving major neurological findings if previous manual therapy was unfavorable.","Cervical mobilization and manipulation are 2 therapies commonly used for chronic neck pain (CNP). However, safety, especially of cervical manipulation, is controversial. This study identifies the clinical scenarios for which an expert panel rated cervical mobilization and manipulation as appropriate and inappropriate. An expert panel, following a well-validated modified-Delphi approach, used an evidence synthesis and clinical acumen to develop and then rate the appropriateness of cervical mobilization and manipulation for each of an exhaustive list of clinical scenarios for CNP. Key patient characteristics were identified using decision tree analysis (DTA). Three hundred seventy-two clinical scenarios were defined and rated by an 11-member expert panel as to the appropriateness of cervical mobilization and manipulation. Across clinical scenarios more were rated inappropriate than appropriate for both therapies, and more scenarios were rated inappropriate for manipulation than mobilization. However, the number of patients presenting with each scenario is not yet known. Nevertheless, DTA indicates that all clinical scenarios that included red flags (eg, fever, cancer, inflammatory arthritides, or vasculitides), and some others involving major neurological findings, especially if previous manual therapy was unfavorable, were rated as inappropriate for both cervical mobilization and manipulation. DTA also identified the absence of cervical disk herniation, stenosis, or foraminal osteophytosis on additional testing as the most important patient characteristic in predicting ratings of appropriate. Clinical guidelines for CNP should include information on the clinical scenarios for which cervical mobilization and manipulation were found inappropriate, including those with red flags, and others involving major neurological findings if previous manual therapy was unfavorable.","Herman, P. M.
 and Vernon, H.
 and Hurwitz, E. L.
 and Shekelle, P. G.
 and Whitley, M. D.
 and Coulter, I. D.","Herman, Vernon, Hurwitz, Shekelle, Whitley, Coulter",https://dx.doi.org/10.1097/AJP.0000000000000800,https://doi.org/10.1097/AJP.0000000000000800,2021-08-03
2923.0,,pubmed,"Title, abstract, and keyword searching resulted in poor recovery of articles in systematic reviews of epidemiologic practice","Title, abstract, and keyword searching resulted in poor recovery of articles in systematic reviews of epidemiologic practice","OBJECTIVE: Article full texts are often inaccessible via the standard search engines of biomedical literature, such as PubMed and Embase, which are commonly used for systematic reviews. Excluding the full-text bodies from a literature search may result in a small or selective subset of articles being included in the review because of the limited information that is available in only title, abstract, and keywords. This article describes a comparison of search strategies based on a systematic literature review of all articles published in 5 top-ranked epidemiology journals between 2000 and 2017. STUDY DESIGN AND SETTING: Based on a text-mining approach, we studied how nine different methodological topics were mentioned across text fields (title, abstract, keywords, and text body). The following methodological topics were studied: propensity score methods, inverse probability weighting, marginal structural modeling, multiple imputation, Kaplan-Meier estimation, number needed to treat, measurement error, randomized controlled trial, and latent class analysis. RESULTS: In total, 31,641 Hypertext Markup Language (HTML) files were downloaded from the journals' websites. For all methodological topics and journals, at most 50% of articles with a mention of a topic in the text body also mentioned the topic in the title, abstract, or keywords. For several topics, a gradual decrease over calendar time was observed of reporting in the title, abstract, or keywords. CONCLUSION: Literature searches based on title, abstract, and keywords alone may not be sufficiently sensitive for studies of epidemiological research practice. This study also illustrates the potential value of full-text literature searches, provided there is accessibility of full-text bodies for literature searches.","Article full texts are often inaccessible via the standard search engines of biomedical literature, such as PubMed and Embase, which are commonly used for systematic reviews. Excluding the full-text bodies from a literature search may result in a small or selective subset of articles being included in the review because of the limited information that is available in only title, abstract, and keywords. This article describes a comparison of search strategies based on a systematic literature review of all articles published in 5 top-ranked epidemiology journals between 2000 andÂ 2017. Based on a text-mining approach, we studied how nine different methodological topics were mentioned across text fields (title, abstract, keywords, and text body). The following methodological topics were studied: propensity score methods, inverse probability weighting, marginal structural modeling, multiple imputation, Kaplan-Meier estimation, number needed to treat, measurement error, randomized controlled trial, and latent class analysis. In total, 31,641 Hypertext Markup Language (HTML) files were downloaded from the journals' websites. For all methodological topics and journals, at most 50% of articles with a mention of a topic in the text body also mentioned the topic in the title, abstract, or keywords. For several topics, a gradual decrease over calendar time was observed of reporting in the title, abstract, or keywords. Literature searches based on title, abstract, and keywords alone may not be sufficiently sensitive for studies of epidemiological research practice. This study also illustrates the potential value of full-text literature searches, provided there is accessibility of full-text bodies for literature searches.","Penning de Vries, B. B. L.
 and van Smeden, M.
 and Rosendaal, F. R.
 and Groenwold, R. H. H.","Penning de Vries, van Smeden, Rosendaal, Groenwold",not available,https://doi.org/10.1016/j.jclinepi.2020.01.009,2021-08-03
4355.0,,pubmed,Pressure injury image analysis with machine learning techniques: A systematic review on previous and possible future methods,Pressure injury image analysis with machine learning techniques: A systematic review on previous and possible future methods,"Pressure injuries represent a tremendous healthcare challenge in many nations. Elderly and disabled people are the most affected by this fast growing disease. Hence, an accurate diagnosis of pressure injuries is paramount for efficient treatment. The characteristics of these wounds are crucial indicators for the progress of the healing. While invasive methods to retrieve information are not only painful to the patients but may also increase the risk of infections, non-invasive techniques by means of imaging systems provide a better monitoring of the wound healing processes without causing any harm to the patients. These systems should include an accurate segmentation of the wound, the classification of its tissue types, the metrics including the diameter, area and volume, as well as the healing evaluation. Therefore, the aim of this survey is to provide the reader with an overview of imaging techniques for the analysis and monitoring of pressure injuries as an aid to their diagnosis, and proof of the efficiency of Deep Learning to overcome this problem and even outperform the previous methods. In this paper, 114 out of 199 papers retrieved from 8 databases have been analyzed, including also contributions on chronic wounds and skin lesions.","Pressure injuries represent a tremendous healthcare challenge in many nations. Elderly and disabled people are the most affected by this fast growing disease. Hence, an accurate diagnosis of pressure injuries is paramount for efficient treatment. The characteristics of these wounds are crucial indicators for the progress of the healing. While invasive methods to retrieve information are not only painful to the patients but may also increase the risk of infections, non-invasive techniques by means of imaging systems provide a better monitoring of the wound healing processes without causing any harm to the patients. These systems should include an accurate segmentation of the wound, the classification of its tissue types, the metrics including the diameter, area and volume, as well as the healing evaluation. Therefore, the aim of this survey is to provide the reader with an overview of imaging techniques for the analysis and monitoring of pressure injuries as an aid to their diagnosis, and proof of the efficiency of Deep Learning to overcome this problem and even outperform the previous methods. In this paper, 114 out of 199 papers retrieved from 8 databases have been analyzed, including also contributions on chronic wounds and skin lesions.","Zahia, S.
 and Garcia Zapirain, M. B.
 and Sevillano, X.
 and Gonzalez, A.
 and Kim, P. J.
 and Elmaghraby, A.","Zahia, Garcia Zapirain, Sevillano, GonzÃ¡lez, Kim, Elmaghraby",https://dx.doi.org/10.1016/j.artmed.2019.101742,https://doi.org/10.1016/j.artmed.2019.101742,2021-08-03
1131.0,,pubmed,Machine learning for the prediction of sepsis: a systematic review and meta-analysis of diagnostic test accuracy,Machine learning for the prediction of sepsis: a systematic review and meta-analysis of diagnostic test accuracy,"PURPOSE: Early clinical recognition of sepsis can be challenging. With the advancement of machine learning, promising real-time models to predict sepsis have emerged. We assessed their performance by carrying out a systematic review and meta-analysis. METHODS: A systematic search was performed in PubMed, Embase.com and Scopus. Studies targeting sepsis, severe sepsis or septic shock in any hospital setting were eligible for inclusion. The index test was any supervised machine learning model for real-time prediction of these conditions. Quality of evidence was assessed using the Grading of Recommendations Assessment, Development and Evaluation (GRADE) methodology, with a tailored Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) checklist to evaluate risk of bias. Models with a reported area under the curve of the receiver operating characteristic (AUROC) metric were meta-analyzed to identify strongest contributors to model performance. RESULTS: After screening, a total of 28 papers were eligible for synthesis, from which 130 models were extracted. The majority of papers were developed in the intensive care unit (ICU, n = 15; 54%), followed by hospital wards (n = 7; 25%), the emergency department (ED, n = 4; 14%) and all of these settings (n = 2; 7%). For the prediction of sepsis, diagnostic test accuracy assessed by the AUROC ranged from 0.68-0.99 in the ICU, to 0.96-0.98 in-hospital and 0.87 to 0.97 in the ED. Varying sepsis definitions limit pooling of the performance across studies. Only three papers clinically implemented models with mixed results. In the multivariate analysis, temperature, lab values, and model type contributed most to model performance. CONCLUSION: This systematic review and meta-analysis show that on retrospective data, individual machine learning models can accurately predict sepsis onset ahead of time. Although they present alternatives to traditional scoring systems, between-study heterogeneity limits the assessment of pooled results. Systematic reporting and clinical implementation studies are needed to bridge the gap between bytes and bedside.","Early clinical recognition of sepsis can be challenging. With the advancement of machine learning, promising real-time models to predict sepsis have emerged. We assessed their performance by carrying out a systematic review and meta-analysis. A systematic search was performed in PubMed, Embase.com and Scopus. Studies targeting sepsis, severe sepsis or septic shock in any hospital setting were eligible for inclusion. The index test was any supervised machine learning model for real-time prediction of these conditions. Quality of evidence was assessed using the Grading of Recommendations Assessment, Development and Evaluation (GRADE) methodology, with a tailored Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) checklist to evaluate risk of bias. Models with a reported area under the curve of the receiver operating characteristic (AUROC) metric were meta-analyzed to identify strongest contributors to model performance. After screening, a total of 28 papers were eligible for synthesis, from which 130 models were extracted. The majority of papers were developed in the intensive care unit (ICU, nâ€‰=â€‰15; 54%), followed by hospital wards (nâ€‰=â€‰7; 25%), the emergency department (ED, nâ€‰=â€‰4; 14%) and all of these settings (nâ€‰=â€‰2; 7%). For the prediction of sepsis, diagnostic test accuracy assessed by the AUROC ranged from 0.68-0.99 in the ICU, to 0.96-0.98 in-hospital and 0.87 to 0.97 in the ED. Varying sepsis definitions limit pooling of the performance across studies. Only three papers clinically implemented models with mixed results. In the multivariate analysis, temperature, lab values, and model type contributed most to model performance. This systematic review and meta-analysis show that on retrospective data, individual machine learning models can accurately predict sepsis onset ahead of time. Although they present alternatives to traditional scoring systems, between-study heterogeneity limits the assessment of pooled results. Systematic reporting and clinical implementation studies are needed to bridge the gap between bytes and bedside.","Fleuren, L. M.
 and Klausch, T. L. T.
 and Zwager, C. L.
 and Schoonmade, L. J.
 and Guo, T.
 and Roggeveen, L. F.
 and Swart, E. L.
 and Girbes, A. R. J.
 and Thoral, P.
 and Ercole, A.
 and Hoogendoorn, M.
 and Elbers, P. W. G.","Fleuren, Klausch, Zwager, Schoonmade, Guo, Roggeveen, Swart, Girbes, Thoral, Ercole, Hoogendoorn, Elbers",https://dx.doi.org/10.1007/s00134-019-05872-y,https://doi.org/10.1007/s00134-019-05872-y,2021-08-03
3219.0,,pubmed,Decision-Analytic Modeling Studies in Prevention and Treatment of Iodine Deficiency and Thyroid Disorders: A Systematic Overview,Decision-Analytic Modeling Studies in Prevention and Treatment of Iodine Deficiency and Thyroid Disorders: A Systematic Overview,"<b>Background:</b> Prevention and treatment of iodine deficiency-related diseases remain an important public health challenge. Iodine deficiency can have severe health consequences, such as cretinism, goiter, or other thyroid disorders, and it has economic implications. Our aim was to give an overview of studies applying decision-analytic modeling to evaluate the effectiveness and/or cost-effectiveness of iodine deficiency-related prevention strategies or treatments related to thyroid disorders. <b>Methods:</b> We performed a systematic literature search in PubMed/MEDLINE (Medical Literature Analysis and Retrieval System Online), EMBASE (Excerpta Medica Database), Tuft's Cost-Effectiveness Analysis Registry, and National Health System Economic Evaluation Database (NHS EED) to identify studies published between 1985 and 2018 comparing different prevention or treatment strategies for iodine deficiency and thyroid disorders by applying a mathematical decision-analytic model. Studies were required to evaluate patient-relevant health outcomes (e.g., remaining life years, quality-adjusted life years [QALYs]). <b>Results:</b> Overall, we found 3950 studies. After removal of duplicates, abstract/title, and full-text screening, 17 studies were included. Eleven studies evaluated screening programs (mainly newborns and pregnant women), five studies focused on treatment approaches (Graves' disease, toxic thyroid adenoma), and one study was about primary prevention (consequences of iodine supplementation on offspring). Most of the studies were conducted within the U.S. health care context (n = 7). Seven studies were based on a Markov state-transition model, nine studies on a decision tree model, and in one study, an initial decision tree and a long-term Markov state-transition model were combined. The analytic time horizon ranged from 1 year to lifetime. QALYs were evaluated as health outcome measure in 15 of the included studies. In all studies, a cost-effectiveness analysis was performed. None of the models reported a formal model validation. In most cases, the authors of the modeling studies concluded that screening is potentially cost-effective or even cost-saving. The recommendations for treatment approaches were rather heterogeneous and depending on the specific research question, population, and setting. <b>Conclusions:</b> Overall, we predominantly identified decision-analytic modeling studies evaluating specific screening programs or treatment approaches; however, there was no model evaluating primary prevention programs on a population basis. Conclusions deriving from these studies, for example, that prevention is cost-saving, need to be carefully interpreted as they rely on many assumptions.","<b><i>Background:</i></b> Prevention and treatment of iodine deficiency-related diseases remain an important public health challenge. Iodine deficiency can have severe health consequences, such as cretinism, goiter, or other thyroid disorders, and it has economic implications. Our aim was to give an overview of studies applying decision-analytic modeling to evaluate the effectiveness and/or cost-effectiveness of iodine deficiency-related prevention strategies or treatments related to thyroid disorders. <b><i>Methods:</i></b> We performed a systematic literature search in PubMed/MEDLINE (Medical Literature Analysis and Retrieval System Online), EMBASE (Excerpta Medica Database), Tuft's Cost-Effectiveness Analysis Registry, and National Health System Economic Evaluation Database (NHS EED) to identify studies published between 1985 and 2018 comparing different prevention or treatment strategies for iodine deficiency and thyroid disorders by applying a mathematical decision-analytic model. Studies were required to evaluate patient-relevant health outcomes (e.g., remaining life years, quality-adjusted life years [QALYs]). <b><i>Results:</i></b> Overall, we found 3950 studies. After removal of duplicates, abstract/title, and full-text screening, 17 studies were included. Eleven studies evaluated screening programs (mainly newborns and pregnant women), five studies focused on treatment approaches (Graves' disease, toxic thyroid adenoma), and one study was about primary prevention (consequences of iodine supplementation on offspring). Most of the studies were conducted within the U.S. health care context (<i>n</i>â€‰=â€‰7). Seven studies were based on a Markov state-transition model, nine studies on a decision tree model, and in one study, an initial decision tree and a long-term Markov state-transition model were combined. The analytic time horizon ranged from 1 year to lifetime. QALYs were evaluated as health outcome measure in 15 of the included studies. In all studies, a cost-effectiveness analysis was performed. None of the models reported a formal model validation. In most cases, the authors of the modeling studies concluded that screening is potentially cost-effective or even cost-saving. The recommendations for treatment approaches were rather heterogeneous and depending on the specific research question, population, and setting. <b><i>Conclusions:</i></b> Overall, we predominantly identified decision-analytic modeling studies evaluating specific screening programs or treatment approaches; however, there was no model evaluating primary prevention programs on a population basis. Conclusions deriving from these studies, for example, that prevention is cost-saving, need to be carefully interpreted as they rely on many assumptions.","Rochau, U.
 and Qerimi Rushaj, V.
 and Schaffner, M.
 and Schonhensch, M.
 and Stojkov, I.
 and Jahn, B.
 and Hubalewska-Dydejczyk, A.
 and Erlund, I.
 and Thuesen, B. H.
 and Zimmermann, M.
 and Moreno-Reyes, R.
 and Lazarus, J. H.
 and Volzke, H.
 and Siebert, U.","Rochau, Qerimi Rushaj, Schaffner, SchÃ¶nhensch, Stojkov, Jahn, Hubalewska-Dydejczyk, Erlund, Thuesen, Zimmermann, Moreno-Reyes, Lazarus, VÃ¶lzke, Siebert",not available,https://doi.org/10.1089/thy.2018.0776,2021-08-03
757.0,,pubmed,Investigating the use of data-driven artificial intelligence in computerised decision support systems for health and social care: A systematic review,Investigating the use of data-driven artificial intelligence in computerised decision support systems for health and social care: A systematic review,"There is growing interest in the potential of artificial intelligence to support decision-making in health and social care settings. There is, however, currently limited evidence of the effectiveness of these systems. The aim of this study was to investigate the effectiveness of artificial intelligence-based computerised decision support systems in health and social care settings. We conducted a systematic literature review to identify relevant randomised controlled trials conducted between 2013 and 2018. We searched the following databases: MEDLINE, EMBASE, CINAHL, PsycINFO, Web of Science, Cochrane Library, ASSIA, Emerald, Health Business Fulltext Elite, ProQuest Public Health, Social Care Online, and grey literature sources. Search terms were conceptualised into three groups: artificial intelligence-related terms, computerised decision support -related terms, and terms relating to health and social care. Terms within groups were combined using the Boolean operator OR, and groups were combined using the Boolean operator AND. Two reviewers independently screened studies against the eligibility criteria and two independent reviewers extracted data on eligible studies onto a customised sheet. We assessed the quality of studies through the Critical Appraisal Skills Programme checklist for randomised controlled trials. We then conducted a narrative synthesis. We identified 68 hits of which five studies satisfied the inclusion criteria. These studies varied substantially in relation to quality, settings, outcomes, and technologies. None of the studies was conducted in social care settings, and three randomised controlled trials showed no difference in patient outcomes. Of these, one investigated the use of Bayesian triage algorithms on forced expiratory volume in 1 second (FEV1) and health-related quality of life in lung transplant patients. Another investigated the effect of image pattern recognition on neonatal development outcomes in pregnant women, and another investigated the effect of the Kalman filter technique for warfarin dosing suggestions on time in therapeutic range. The remaining two randomised controlled trials, investigating computer vision and neural networks on medication adherence and the impact of learning algorithms on assessment time of patients with gestational diabetes, showed statistically significant and clinically important differences to the control groups receiving standard care. However, these studies tended to be of low quality lacking detailed descriptions of methods and only one study used a double-blind design. Although the evidence of effectiveness of data-driven artificial intelligence to support decision-making in health and social care settings is limited, this work provides important insights on how a meaningful evidence base in this emerging field needs to be developed going forward. It is unlikely that any single overall message surrounding effectiveness will emerge - rather effectiveness of interventions is likely to be context-specific and calls for inclusion of a range of study designs to investigate mechanisms of action.","There is growing interest in the potential of artificial intelligence to support decision-making in health and social care settings. There is, however, currently limited evidence of the effectiveness of these systems. The aim of this study was to investigate the effectiveness of artificial intelligence-based computerised decision support systems in health and social care settings. We conducted a systematic literature review to identify relevant randomised controlled trials conducted between 2013 and 2018. We searched the following databases: MEDLINE, EMBASE, CINAHL, PsycINFO, Web of Science, Cochrane Library, ASSIA, Emerald, Health Business Fulltext Elite, ProQuest Public Health, Social Care Online, and grey literature sources. Search terms were conceptualised into three groups: artificial intelligence-related terms, computerised decision support -related terms, and terms relating to health and social care. Terms within groups were combined using the Boolean operator OR, and groups were combined using the Boolean operator AND. Two reviewers independently screened studies against the eligibility criteria and two independent reviewers extracted data on eligible studies onto a customised sheet. We assessed the quality of studies through the Critical Appraisal Skills Programme checklist for randomised controlled trials. We then conducted a narrative synthesis. We identified 68 hits of which five studies satisfied the inclusion criteria. These studies varied substantially in relation to quality, settings, outcomes, and technologies. None of the studies was conducted in social care settings, and three randomised controlled trials showed no difference in patient outcomes. Of these, one investigated the use of Bayesian triage algorithms on forced expiratory volume in 1 second (FEV1) and health-related quality of life in lung transplant patients. Another investigated the effect of image pattern recognition on neonatal development outcomes in pregnant women, and another investigated the effect of the Kalman filter technique for warfarin dosing suggestions on time in therapeutic range. The remaining two randomised controlled trials, investigating computer vision and neural networks on medication adherence and the impact of learning algorithms on assessment time of patients with gestational diabetes, showed statistically significant and clinically important differences to the control groups receiving standard care. However, these studies tended to be of low quality lacking detailed descriptions of methods and only one study used a double-blind design. Although the evidence of effectiveness of data-driven artificial intelligence to support decision-making in health and social care settings is limited, this work provides important insights on how a meaningful evidence base in this emerging field needs to be developed going forward. It is unlikely that any single overall message surrounding effectiveness will emerge - rather effectiveness of interventions is likely to be context-specific and calls for inclusion of a range of study designs to investigate mechanisms of action.","Cresswell, K.
 and Callaghan, M.
 and Khan, S.
 and Sheikh, Z.
 and Mozaffar, H.
 and Sheikh, A.","Cresswell, Callaghan, Khan, Sheikh, Mozaffar, Sheikh",https://dx.doi.org/10.1177/1460458219900452,https://doi.org/10.1177/1460458219900452,2021-08-03
3973.0,,pubmed,Automated screening of natural language in electronic health records for the diagnosis septic shock is feasible and outperforms an approach based on explicit administrative codes,Automated screening of natural language in electronic health records for the diagnosis septic shock is feasible and outperforms an approach based on explicit administrative codes,"PURPOSE: Identification of patients for epidemiologic research through administrative coding has important limitations. We investigated the feasibility of a search based on natural language processing (NLP) on the text sections of electronic health records for identification of patients with septic shock. MATERIALS AND METHODS: Results of an explicit search strategy (using explicit concept retrieval) and a combined search strategy (using both explicit and implicit concept retrieval) were compared to hospital ICD-9 based administrative coding and to our department's own prospectively compiled infection database. RESULTS: Of 8911 patients admitted to the medical or surgical ICU, 1023 (11.5%) suffered from septic shock according to the combined search strategy. This was significantly more than those identified by the explicit strategy (518, 5.8%), by hospital administrative coding (549, 5.8%) or by our own prospectively compiled database (609, 6.8%) (p < .001). Sensitivity and specificity of the automated combined search strategy were 72.7% (95%CI 69.0%-76.2%) and 93.0% (95%CI 92.4%-93.6%), compared to 56.0% (95%CI 52.0%-60.0%) and 97.5% (95%CI 97.1%-97.8%) for hospital administrative coding. CONCLUSIONS: An automated search strategy based on a combination of explicit and implicit concept retrieval is feasible to screen electronic health records for septic shock and outperforms an administrative coding based explicit approach.","Identification of patients for epidemiologic research through administrative coding has important limitations. We investigated the feasibility of a search based on natural language processing (NLP) on the text sections of electronic health records for identification of patients with septic shock. Results of an explicit search strategy (using explicit concept retrieval) and a combined search strategy (using both explicit and implicit concept retrieval) were compared to hospital ICD-9 based administrative coding and to our department's own prospectively compiled infection database. Of 8911 patients admitted to the medical or surgical ICU, 1023 (11.5%) suffered from septic shock according to the combined search strategy. This was significantly more than those identified by the explicit strategy (518, 5.8%), by hospital administrative coding (549, 5.8%) or by our own prospectively compiled database (609, 6.8%) (pÂ &lt;Â .001). Sensitivity and specificity of the automated combined search strategy were 72.7% (95%CI 69.0%-76.2%) and 93.0% (95%CI 92.4%-93.6%), compared to 56.0% (95%CI 52.0%-60.0%) and 97.5% (95%CI 97.1%-97.8%) for hospital administrative coding. An automated search strategy based on a combination of explicit and implicit concept retrieval is feasible to screen electronic health records for septic shock and outperforms an administrative coding based explicit approach.","Vermassen, J.
 and Colpaert, K.
 and De Bus, L.
 and Depuydt, P.
 and Decruyenaere, J.","Vermassen, Colpaert, De Bus, Depuydt, Decruyenaere",not available,https://doi.org/10.1016/j.jcrc.2020.01.007,2021-08-03
2391.0,,pubmed,Semi-Automated evidence synthesis in health psychology: current methods and future prospects,Semi-Automated evidence synthesis in health psychology: current methods and future prospects,"The evidence base in health psychology is vast and growing rapidly. These factors make it difficult (and sometimes practically impossible) to consider all available evidence when making decisions about the state of knowledge on a given phenomenon (e.g., associations of variables, effects of interventions on particular outcomes). Systematic reviews, meta-analyses, and other rigorous syntheses of the research mitigate this problem by providing concise, actionable summaries of knowledge in a given area of study. Yet, conducting these syntheses has grown increasingly laborious owing to the fast accumulation of new evidence; existing, manual methods for synthesis do not scale well. In this article, we discuss how semi-automation via machine learning and natural language processing methods may help researchers and practitioners to review evidence more efficiently. We outline concrete examples in health psychology, highlighting practical, open-source technologies available now. We indicate the potential of more advanced methods and discuss how to avoid the pitfalls of automated reviews.","The evidence base in health psychology is vast and growing rapidly. These factors make it difficult (and sometimes practically impossible) to consider all available evidence when making decisions about the state of knowledge on a given phenomenon (e.g., associations of variables, effects of interventions on particular outcomes). Systematic reviews, meta-analyses, and other rigorous syntheses of the research mitigate this problem by providing concise, actionable summaries of knowledge in a given area of study. Yet, conducting these syntheses has grown increasingly laborious owing to the fast accumulation of new evidence; existing, manual methods for synthesis do not scale well. In this article, we discuss how semi-automation via machine learning and natural language processing methods may help researchers and practitioners to review evidence more efficiently. We outline concrete examples in health psychology, highlighting practical, open-source technologies available now. We indicate the potential of more advanced methods and discuss how to avoid the pitfalls of automated reviews.","Marshall, I. J.
 and Johnson, B. T.
 and Wang, Z.
 and Rajasekaran, S.
 and Wallace, B. C.","Marshall, Johnson, Wang, Rajasekaran, Wallace",https://dx.doi.org/10.1080/17437199.2020.1716198,https://doi.org/10.1080/17437199.2020.1716198,2021-08-03
976.0,,pubmed,Lead-I ECG for detecting atrial fibrillation in patients with an irregular pulse using single time point testing: a systematic review and economic evaluation,Lead-I ECG for detecting atrial fibrillation in patients with an irregular pulse using single time point testing: a systematic review and economic evaluation,"BACKGROUND: Atrial fibrillation (AF) is the most common type of cardiac arrhythmia and is associated with an increased risk of stroke and congestive heart failure. Lead-I electrocardiogram (ECG) devices are handheld instruments that can be used to detect AF at a single time point in people who present with relevant signs or symptoms. OBJECTIVE: To assess the diagnostic test accuracy, clinical impact and cost-effectiveness of using single time point lead-I ECG devices for the detection of AF in people presenting to primary care with relevant signs or symptoms, and who have an irregular pulse compared with using manual pulse palpation (MPP) followed by a 12-lead ECG in primary or secondary care. DATA SOURCES: MEDLINE, MEDLINE Epub Ahead of Print and MEDLINE In-Process & Other Non-Indexed Citations, EMBASE, PubMed, Cochrane Databases of Systematic Reviews, Cochrane Central Database of Controlled Trials, Database of Abstracts of Reviews of Effects and the Health Technology Assessment Database. METHODS: The systematic review methods followed published guidance. Two reviewers screened the search results (database inception to April 2018), extracted data and assessed the quality of the included studies. Summary estimates of diagnostic accuracy were calculated using bivariate models. An economic model consisting of a decision tree and two cohort Markov models was developed to evaluate the cost-effectiveness of lead-I ECG devices. RESULTS: No studies were identified that evaluated the use of lead-I ECG devices for patients with signs or symptoms of AF. Therefore, the diagnostic accuracy and clinical impact results presented are derived from an asymptomatic population (used as a proxy for people with signs or symptoms of AF). The summary sensitivity of lead-I ECG devices was 93.9% [95% confidence interval (CI) 86.2% to 97.4%] and summary specificity was 96.5% (95% CI 90.4% to 98.8%). One study reported limited clinical outcome data. Acceptability of lead-I ECG devices was reported in four studies, with generally positive views. The de novo economic model yielded incremental cost-effectiveness ratios (ICERs) per quality-adjusted life-year (QALY) gained. The results of the pairwise analysis show that all lead-I ECG devices generated ICERs per QALY gained below the 20,000-30,000 threshold. Kardia Mobile (AliveCor Ltd, Mountain View, CA, USA) is the most cost-effective option in a full incremental analysis. LIMITATIONS: No published data evaluating the diagnostic accuracy, clinical impact or cost-effectiveness of lead-I ECG devices for the population of interest are available. CONCLUSIONS: Single time point lead-I ECG devices for the detection of AF in people with signs or symptoms of AF and an irregular pulse appear to be a cost-effective use of NHS resources compared with MPP followed by a 12-lead ECG in primary or secondary care, given the assumptions used in the base-case model. FUTURE WORK: Studies assessing how the use of lead-I ECG devices in this population affects the number of people diagnosed with AF when compared with current practice would be useful. STUDY REGISTRATION: This study is registered as PROSPERO CRD42018090375. FUNDING: The National Institute for Health Research Health Technology Assessment programme.","Atrial fibrillation (AF) is the most common type of cardiac arrhythmia and is associated with an increased risk of stroke and congestive heart failure. Lead-I electrocardiogram (ECG) devices are handheld instruments that can be used to detect AF at a single time point in people who present with relevant signs or symptoms. To assess the diagnostic test accuracy, clinical impact and cost-effectiveness of using single time point lead-I ECG devices for the detection of AF in people presenting to primary care with relevant signs or symptoms, and who have an irregular pulse compared with using manual pulse palpation (MPP) followed by a 12-lead ECG in primary or secondary care. MEDLINE, MEDLINE Epub Ahead of Print and MEDLINE In-Process &amp; Other Non-Indexed Citations, EMBASE, PubMed, Cochrane Databases of Systematic Reviews, Cochrane Central Database of Controlled Trials, Database of Abstracts of Reviews of Effects and the Health Technology Assessment Database. The systematic review methods followed published guidance. Two reviewers screened the search results (database inception to April 2018), extracted data and assessed the quality of the included studies. Summary estimates of diagnostic accuracy were calculated using bivariate models. An economic model consisting of a decision tree and two cohort Markov models was developed to evaluate the cost-effectiveness of lead-I ECG devices. No studies were identified that evaluated the use of lead-I ECG devices for patients with signs or symptoms of AF. Therefore, the diagnostic accuracy and clinical impact results presented are derived from an asymptomatic population (used as a proxy for people with signs or symptoms of AF). The summary sensitivity of lead-I ECG devices was 93.9% [95% confidence interval (CI) 86.2% to 97.4%] and summary specificity was 96.5% (95% CI 90.4% to 98.8%). One study reported limited clinical outcome data. Acceptability of lead-I ECG devices was reported in four studies, with generally positive views. The de novo economic model yielded incremental cost-effectiveness ratios (ICERs) per quality-adjusted life-year (QALY) gained. The results of the pairwise analysis show that all lead-I ECG devices generated ICERs per QALY gained below the Â£20,000-30,000 threshold. Kardia Mobile (AliveCor Ltd, Mountain View, CA, USA) is the most cost-effective option in a full incremental analysis. No published data evaluating the diagnostic accuracy, clinical impact or cost-effectiveness of lead-I ECG devices for the population of interest are available. Single time point lead-I ECG devices for the detection of AF in people with signs or symptoms of AF and an irregular pulse appear to be a cost-effective use of NHS resources compared with MPP followed by a 12-lead ECG in primary or secondary care, given the assumptions used in the base-case model. Studies assessing how the use of lead-I ECG devices in this population affects the number of people diagnosed with AF when compared with current practice would be useful. This study is registered as PROSPERO CRD42018090375. The National Institute for Health Research Health Technology Assessment programme. Atrial fibrillation (AF) is the most common type of abnormal heart rhythm. People with AF are more likely to have a serious stroke or die than people without the condition. Many people go to their general practitioner (GP) with the signs or symptoms commonly linked to AF, such as feeling dizzy, being short of breath, feeling tired and having heart palpitations. GPs check for AF by taking the patientâ€™s pulse by hand. If the GP thinks that the patient might have AF, a 12-lead electrocardiogram (ECG) test is arranged. Lead-I (i.e. one lead) ECGs are handheld electronic devices that could detect AF more accurately than a manual pulse check. If GPs were to routinely use lead-I ECG devices, people with suspected AF could receive treatment while waiting for the AF diagnosis to be confirmed by a 12-lead ECG. This study aimed to assess whether or not the use of lead-I ECGs in GP surgeries could benefit these patients and offer good value for money to the NHS. All studies that examined how well lead-I ECGs identified people with AF were reviewed, and the economic value of using these devices was assessed. No evidence was found that examined the use of lead-I ECGs for people with signs or symptoms of AF. As an alternative, evidence for the use of lead-I ECGs for people with no symptoms of AF was searched for and these data were used to assess value for money. The study found that using a manual pulse check followed by a lead-I ECG offers value for money when compared with a manual pulse check followed by a 12-lead ECG. This is mostly because patients with AF can begin treatment earlier when a GP has access to a lead-I ECG device.","Duarte, R.
 and Stainthorpe, A.
 and Greenhalgh, J.
 and Richardson, M.
 and Nevitt, S.
 and Mahon, J.
 and Kotas, E.
 and Boland, A.
 and Thom, H.
 and Marshall, T.
 and Hall, M.
 and Takwoingi, Y.","Duarte, Stainthorpe, Greenhalgh, Richardson, Nevitt, Mahon, Kotas, Boland, Thom, Marshall, Hall, Takwoingi",https://dx.doi.org/10.3310/hta24030,https://doi.org/10.3310/hta24030,2021-08-03
2672.0,,pubmed,Application of artificial neural networks for automated analysis of cystoscopic images: a review of the current status and future prospects,Application of artificial neural networks for automated analysis of cystoscopic images: a review of the current status and future prospects,"BACKGROUND: Optimal detection and surveillance of bladder cancer (BCa) rely primarily on the cystoscopic visualization of bladder lesions. AI-assisted cystoscopy may improve image recognition and accelerate data acquisition. OBJECTIVE: To provide a comprehensive review of machine learning (ML), deep learning (DL) and convolutional neural network (CNN) applications in cystoscopic image recognition. EVIDENCE ACQUISITION: A detailed search of original articles was performed using the PubMed-MEDLINE database to identify recent English literature relevant to ML, DL and CNN applications in cystoscopic image recognition. EVIDENCE SYNTHESIS: In total, two articles and one conference abstract were identified addressing the application of AI methods in cystoscopic image recognition. These investigations showed accuracies exceeding 90% for tumor detection; however, future work is necessary to incorporate these methods into AI-aided cystoscopy and compared to other tumor visualization tools. Furthermore, we present results from the RaVeNNA-4pi consortium initiative which has extracted 4200 frames from 62 videos, analyzed them with the U-Net network and achieved an average dice score of 0.67. Improvements in its precision can be achieved by augmenting the video/frame database. CONCLUSION: AI-aided cystoscopy has the potential to outperform urologists at recognizing and classifying bladder lesions. To ensure their real-life implementation, however, these algorithms require external validation to generalize their results across other data sets.","Optimal detection and surveillance of bladder cancer (BCa) rely primarily on the cystoscopic visualization of bladder lesions. AI-assisted cystoscopy may improve image recognition and accelerate data acquisition. To provide a comprehensive review of machine learning (ML), deep learning (DL) and convolutional neural network (CNN) applications in cystoscopic image recognition. A detailed search of original articles was performed using the PubMed-MEDLINE database to identify recent English literature relevant to ML, DL and CNN applications in cystoscopic image recognition. In total, two articles and one conference abstract were identified addressing the application of AI methods in cystoscopic image recognition. These investigations showed accuracies exceeding 90% for tumor detection; however, future work is necessary to incorporate these methods into AI-aided cystoscopy and compared to other tumor visualization tools. Furthermore, we present results from the RaVeNNA-4pi consortium initiative which has extracted 4200 frames from 62 videos, analyzed them with the U-Net network and achieved an average dice score of 0.67. Improvements in its precision can be achieved by augmenting the video/frame database. AI-aided cystoscopy has the potential to outperform urologists at recognizing and classifying bladder lesions. To ensure their real-life implementation, however, these algorithms require external validation to generalize their results across other data sets.","Negassi, M.
 and Suarez-Ibarrola, R.
 and Hein, S.
 and Miernik, A.
 and Reiterer, A.","Negassi, Suarez-Ibarrola, Hein, Miernik, Reiterer",https://dx.doi.org/10.1007/s00345-019-03059-0,https://doi.org/10.1007/s00345-019-03059-0,2021-08-03
2818.0,,pubmed,Bio-AnswerFinder: a system to find answers to questions from biomedical texts,Bio-AnswerFinder: a system to find answers to questions from biomedical texts,"The ever accelerating pace of biomedical research results in corresponding acceleration in the volume of biomedical literature created. Since new research builds upon existing knowledge, the rate of increase in the available knowledge encoded in biomedical literature makes the easy access to that implicit knowledge more vital over time. Toward the goal of making implicit knowledge in the biomedical literature easily accessible to biomedical researchers, we introduce a question answering system called Bio-AnswerFinder. Bio-AnswerFinder uses a weighted-relaxed word mover's distance based similarity on word/phrase embeddings learned from PubMed abstracts to rank answers after question focus entity type filtering. Our approach retrieves relevant documents iteratively via enhanced keyword queries from a traditional search engine. To improve document retrieval performance, we introduced a supervised long short term memory neural network to select keywords from the question to facilitate iterative keyword search. Our unsupervised baseline system achieves a mean reciprocal rank score of 0.46 and Precision@1 of 0.32 on 936 questions from BioASQ. The answer sentences are further ranked by a fine-tuned bidirectional encoder representation from transformers (BERT) classifier trained using 100 answer candidate sentences per question for 492 BioASQ questions. To test ranking performance, we report a blind test on 100 questions that three independent annotators scored. These experts preferred BERT based reranking with 7% improvement on MRR and 13% improvement on Precision@1 scores on average.","The ever accelerating pace of biomedical research results in corresponding acceleration in the volume of biomedical literature created. Since new research builds upon existing knowledge, the rate of increase in the available knowledge encoded in biomedical literature makes the easy access to that implicit knowledge more vital over time. Toward the goal of making implicit knowledge in the biomedical literature easily accessible to biomedical researchers, we introduce a question answering system called Bio-AnswerFinder. Bio-AnswerFinder uses a weighted-relaxed word mover's distance based similarity on word/phrase embeddings learned from PubMed abstracts to rank answers after question focus entity type filtering. Our approach retrieves relevant documents iteratively via enhanced keyword queries from a traditional search engine. To improve document retrieval performance, we introduced a supervised long short term memory neural network to select keywords from the question to facilitate iterative keyword search. Our unsupervised baseline system achieves a mean reciprocal rank score of 0.46 and Precision@1 of 0.32 on 936 questions from BioASQ. The answer sentences are further ranked by a fine-tuned bidirectional encoder representation from transformers (BERT) classifier trained using 100 answer candidate sentences per question for 492 BioASQ questions. To test ranking performance, we report a blind test on 100 questions that three independent annotators scored. These experts preferred BERT based reranking with 7% improvement on MRR and 13% improvement on Precision@1 scores on average.","Ozyurt, I. B.
 and Bandrowski, A.
 and Grethe, J. S.","Ozyurt, Bandrowski, Grethe",https://dx.doi.org/10.1093/database/baz137,https://doi.org/10.1093/database/baz137,2021-08-03
2854.0,,pubmed,Enablers and barriers to the implementation of socially assistive humanoid robots in health and social care: a systematic review,Enablers and barriers to the implementation of socially assistive humanoid robots in health and social care: a systematic review,"OBJECTIVES: Socially assistive humanoid robots are considered a promising technology to tackle the challenges in health and social care posed by the growth of the ageing population. The purpose of our study was to explore the current evidence on barriers and enablers for the implementation of humanoid robots in health and social care. DESIGN: Systematic review of studies entailing hands-on interactions with a humanoid robot. SETTING: From April 2018 to June 2018, databases were searched using a combination of the same search terms for articles published during the last decade. Data collection was conducted by using the Rayyan software, a standardised predefined grid, and a risk of bias and a quality assessment tool. PARTICIPANTS: Post-experimental data were collected and analysed for a total of 420 participants. Participants comprised: older adults (n=307) aged >=60 years, with no or some degree of age-related cognitive impairment, residing either in residential care facilities or at their home; care home staff (n=106); and informal caregivers (n=7). PRIMARY OUTCOMES: Identification of enablers and barriers to the implementation of socially assistive humanoid robots in health and social care, and consequent insights and impact. Future developments to inform further research. RESULTS: Twelve studies met the eligibility criteria and were included. None of the selected studies had an experimental design; hence overall quality was low, with high risks of biases. Several studies had no comparator, no baseline, small samples, and self-reported measures only. Within this limited evidence base, the enablers found were enjoyment, usability, personalisation and familiarisation. Barriers were related to technical problems, to the robots' limited capabilities and the negative preconceptions towards the use of robots in healthcare. Factors which produced mixed results were the robot's human-like attributes, previous experience with technology and views of formal and informal carers. CONCLUSIONS: The available evidence related to implementation factors of socially assistive humanoid robots for older adults is limited, mainly focusing on aspects at individual level, and exploring acceptance of this technology. Investigation of elements linked to the environment, organisation, societal and cultural milieu, policy and legal framework is necessary. Prospero registration number: crd42018092866.","Socially assistive humanoid robots are considered a promising technology to tackle the challenges in health and social care posed by the growth of the ageing population. The purpose of our study was to explore the current evidence on barriers and enablers for the implementation of humanoid robots in health and social care. Systematic review of studies entailing hands-on interactions with a humanoid robot. From April 2018 to June 2018, databases were searched using a combination of the same search terms for articles published during the last decade. Data collection was conducted by using the <i>Rayyan</i> software, a standardised predefined grid, and a risk of bias and a quality assessment tool. Post-experimental data were collected and analysed for a total of 420 participants. Participants comprised: older adults (n=307) aged â‰¥60 years, with no or some degree of age-related cognitive impairment, residing either in residential care facilities or at their home; care home staff (n=106); and informal caregivers (n=7). Identification of enablers and barriers to the implementation of socially assistive humanoid robots in health and social care, and consequent insights and impact. Future developments to inform further research. Twelve studies met the eligibility criteria and were included. None of the selected studies had an experimental design; hence overall quality was low, with high risks of biases. Several studies had no comparator, no baseline, small samples, and self-reported measures only. Within this limited evidence base, the enablers found were enjoyment, usability, personalisation and familiarisation. Barriers were related to technical problems, to the robots' limited capabilities and the negative preconceptions towards the use of robots in healthcare. Factors which produced mixed results were the robot's human-like attributes, previous experience with technology and views of formal and informal carers. The available evidence related to implementation factors of socially assistive humanoid robots for older adults is limited, mainly focusing on aspects at individual level, and exploring acceptance of this technology. Investigation of elements linked to the environment, organisation, societal and cultural milieu, policy and legal framework is necessary. CRD42018092866.","Papadopoulos, I.
 and Koulouglioti, C.
 and Lazzarino, R.
 and Ali, S.","Papadopoulos, Koulouglioti, Lazzarino, Ali",https://dx.doi.org/10.1136/bmjopen-2019-033096,https://doi.org/10.1136/bmjopen-2019-033096,2021-08-03
1721.0,,pubmed,A mixed-studies systematic review and meta-analysis of school-based interventions to promote physical activity and/or reduce sedentary time in children,A mixed-studies systematic review and meta-analysis of school-based interventions to promote physical activity and/or reduce sedentary time in children,"Purpose: The aim of this mixed-studies systematic review was to ascertain the effectiveness of school-based interventions in increasing physical activity (PA) and/or reducing sedentary time (ST) in children aged 5-11 years, as well as to explore their effectiveness in relation to categories of the theory of expanded, extended, and enhanced opportunity (TEO). Methods: Adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, 5 databases were searched using predefined search terms. Following title and abstract screening of 1115 records, the removal of duplicates (n=584) and articles that did not meet the inclusion criteria agreed to a priori (n=419) resulted in 112 records that were full-text screened. Two independent reviewers subsequently used the mixed-methods appraisal tool to assess the methodological quality of 57 full-text studies that met the inclusion criteria after full-text screening. The interventions were summarised using the TIDierR checklist and TEO. The strength of evidence was determined using a 5-level rating system utilising a published decision tree. Results: Overall evidence ratings for interventions implemented within school settings were: no evidence of effects on moderate-to-vigorous physical activity (MVPA) and inconclusive evidence of effects on sedentary time. In relation to the TEO, expansion of PA appeared to be the most promising intervention type for MVPA, with moderate evidence of effect, whereas extension and enhancement of PA opportunity demonstrated no evidence of effect. A critical issue of possible compensatory behavior was identified by analysis of intervention effect in relation to PA measurement duration; when studies measured changes in PA during the actual intervention, there was moderate evidence of effect, whereas those that measured changes in PA during the school day presented inconclusive evidence of effect, and those that measured changes in PA over a whole day yielded no evidence of effect. Two meta-analyses of those studies using a whole-day accelerometer measure for MVPA or ST showed a significant but moderate effect for MVPA (effect size=0.51; 95% confidence interval (CI): 0.02-0.99) and a large but nonsignificant effect for ST (effect size=1.15; 95%CI: -1.03 to 3.33); both meta-analyses demonstrated low precision, considerable inconsistency, and high heterogeneity. Conclusion: The findings have important implications for future intervention research in terms of intervention design, implementation, and evaluation.","The aim of this mixed-studies systematic review was to ascertain the effectiveness of school-based interventions in increasing physical activity (PA) and/or reducing sedentary time (ST) in children aged 5-11 years, as well as to explore their effectiveness in relation to categories of the theory of expanded, extended, and enhanced opportunity (TEO). Adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, 5 databases were searched using predefined search terms. Following title and abstract screening of 1115 records, the removal of duplicates (<i>n</i>â€¯=â€¯584) and articles that did not meet the inclusion criteria agreed to <i>a priori</i> (<i>n</i>â€¯=â€¯419) resulted in 112 records that were full-text screened. Two independent reviewers subsequently used the mixed-methods appraisal tool to assess the methodological quality of 57 full-text studies that met the inclusion criteria after full-text screening. The interventions were summarised using the TIDierR checklist and TEO. The strength of evidence was determined using a 5-level rating system utilising a published decision tree. Overall evidence ratings for interventions implemented within school settings were: no evidence of effects on moderate-to-vigorous physical activity (MVPA) and inconclusive evidence of effects on sedentary time. In relation to the TEO, expansion of PA appeared to be the most promising intervention type for MVPA, with moderate evidence of effect, whereas extension and enhancement of PA opportunity demonstrated no evidence of effect. A critical issue of possible compensatory behavior was identified by analysis of intervention effect in relation to PA measurement duration; when studies measured changes in PA during the actual intervention, there was moderate evidence of effect, whereas those that measured changes in PA during the school day presented inconclusive evidence of effect, and those that measured changes in PA over a whole day yielded no evidence of effect. Two meta-analyses of those studies using a whole-day accelerometer measure for MVPA or ST showed a significant but moderate effect for MVPA (effect sizeâ€¯=â€¯0.51; 95% confidence interval (CI): 0.02-0.99) and a large but nonsignificant effect for ST (effect sizeâ€¯=â€¯1.15; 95%CI: -1.03 to 3.33); both meta-analyses demonstrated low precision, considerable inconsistency, and high heterogeneity. The findings have important implications for future intervention research in terms of intervention design, implementation, and evaluation.","Jones, M.
 and Defever, E.
 and Letsinger, A.
 and Steele, J.
 and Mackintosh, K. A.","Jones, Defever, Letsinger, Steele, Mackintosh",not available,https://doi.org/10.1016/j.jshs.2019.06.009,2021-08-03
1575.0,,pubmed,Effectiveness and safety of warm needle acupuncture on chronic renal failure: Protocol for a systematic review and meta-analysis,Effectiveness and safety of warm needle acupuncture on chronic renal failure: Protocol for a systematic review and meta-analysis,"BACKGROUND: Warm needle acupuncture (WNA) is an integral part of the acupuncture therapy. Chronic renal failure (CRF) is a common disease, which is a type of kidney disease characterized by a slow and progressive decline in renal function. The clinical practice indicates that WNA has a therapeutic effect on CRF. Therefore, we will provide a protocol to explore the effectiveness and safety of WNA for CRF. METHODS: We will search the randomized controlled trials literatures of WNA for CRF in 5 English databases (PubMed, Web of Science, EMBASE, the Cochrane Central Register of Controlled Trials [Cochrane Library], and World Health Organization International Clinical Trials Registry Platform) and 4 Chinese databases (Chinese national knowledge infrastructure, Chinese Scientific Journal Database Information, Wanfang Database, and Chinese biomedical literature database). The renal function will be considered as the primary outcome and the secondary outcome will include curative effect, security, syndrome according to standards for assessing traditional Chinese medicine, adverse events caused by WNA, such as dizziness, nausea, vomiting, weariness, and so on. We will use EndnoteX7 software to perform the selection of the studies. And all analyses will be conducted by using RevMan software V5.3. RESULT: This study will provide a rational synthesis of current evidences for WNM on CRF. CONCLUSION: The conclusion of this study will provide evidence to judge the effectiveness and safety of WNA on CRF.Registration: PROS-PERO CRD42019144530.","Warm needle acupuncture (WNA) is an integral part of the acupuncture therapy. Chronic renal failure (CRF) is a common disease, which is a type of kidney disease characterized by a slow and progressive decline in renal function. The clinical practice indicates that WNA has a therapeutic effect on CRF. Therefore, we will provide a protocol to explore the effectiveness and safety of WNA for CRF. We will search the randomized controlled trials literatures of WNA for CRF in 5 English databases (PubMed, Web of Science, EMBASE, the Cochrane Central Register of Controlled Trials [Cochrane Library], and World Health Organization International Clinical Trials Registry Platform) and 4 Chinese databases (Chinese national knowledge infrastructure, Chinese Scientific Journal Database Information, Wanfang Database, and Chinese biomedical literature database). The renal function will be considered as the primary outcome and the secondary outcome will include curative effect, security, syndrome according to standards for assessing traditional Chinese medicine, adverse events caused by WNA, such as dizziness, nausea, vomiting, weariness, and so on. We will use EndnoteX7 software to perform the selection of the studies. And all analyses will be conducted by using RevMan software V5.3. This study will provide a rational synthesis of current evidences for WNM on CRF. The conclusion of this study will provide evidence to judge the effectiveness and safety of WNA on CRF.Registration: PROS-PERO CRD42019144530.","Huang, C.
 and Lin, Y.
 and Yang, Y.
 and Zeng, F.
 and Jiang, H.
 and Lin, T.
 and Zheng, L.","Huang, Lin, Yang, Zeng, Jiang, Lin, Zheng",https://dx.doi.org/10.1097/MD.0000000000018706,https://doi.org/10.1097/MD.0000000000018706,2021-08-03
939.0,,pubmed,Resective surgery for the treatment of furcation involvement - a systematic review,Resective surgery for the treatment of furcation involvement: A systematic review,"OBJECTIVE: Evaluation of the benefit of resective surgical periodontal therapy (root amputation or resection, root separation, tunnelling) in periodontitis patients exhibiting class II and III furcation involvement (FI) compared to non-surgical treatment (SRP) or open flap debridement (OFD). MATERIAL: Outcomes were tooth survival (primary), vertical probing attachment gain, and reduction of probing pocket depth (secondary) evidenced by randomized clinical trials, prospective and retrospective cohort studies and case series with >= 12 months follow-up. Search was performed on 3 electronic databases from January 1998 to December 2018. RESULTS: From a total of 683 articles 66 studies were identified for full-text analysis and 7 studies finally included. Six hundred and sixty-five patients contributed 2,021 teeth with class II or III FI. Data were very heterogeneous regarding follow-up and distribution of FI. A total of 1,515 teeth survived 4 to 30.8 years after therapy. Survival ranged from 38-94.4% (root amputation or resection, root separation), 62-67% (tunnelling), 63-85% (OFD), 68-80% (SRP). Over all, treatment provided better results for class II FI than class III. CONCLUSION: Within their limits the data indicate that in class II and III FI, SRP and OFD may result in similar survival rates as root amputation/resection, root separation or tunnelling.","To evaluate the benefit of resective surgical periodontal therapy (root amputation or resection, root separation, tunnelling) in periodontitis patients exhibiting class II and III furcation involvement (FI) compared with non-surgical treatment (SRP) or open flap debridement (OFD). Outcomes were tooth survival (primary), vertical probing attachment gain, and reduction in probing pocket depth (secondary) evidenced by randomized clinical trials, prospective and retrospective cohort studies and case series withÂ â‰¥Â 12Â months of follow-up. Search was performed on 3 electronic databases from January 1998 to December 2018. From a total of 683 articles, 66 studies were identified for full-text analysis and 7 studies finally included. Six hundred sixty-seven patients contributed 2,021 teeth with class II or III FI. Data were very heterogeneous regarding follow-up and distribution of FI. A total of 1,515 teeth survived 4 to 30.8Â years after therapy. Survival ranged from 38%-94.4% (root amputation or resection, root separation), 62%-67% (tunnelling), 63%-85% (OFD) and 68%-80% (SRP). Overall, treatment provided better results for class II FI than class III. Within their limits, the data indicate thatÂ in class II and III FI, SRP and OFD may result in similar survival rates as root amputation/resection, root separation or tunnelling.","Dommisch, H.
 and Walter, C.
 and Dannewitz, B.
 and Eickholz, P.","Dommisch, Walter, Dannewitz, Eickholz",https://dx.doi.org/10.1111/jcpe.13241,https://doi.org/10.1111/jcpe.13241,2021-08-03
3646.0,,pubmed,Applying Machine Learning in Liver Disease and Transplantation: A Comprehensive Review,Applying Machine Learning in Liver Disease and Transplantation: A Comprehensive Review,"Machine learning (ML) utilizes artificial intelligence to generate predictive models efficiently and more effectively than conventional methods through detection of hidden patterns within large data sets. With this in mind, there are several areas within hepatology where these methods can be applied. In this review, we examine the literature pertaining to machine learning in hepatology and liver transplant medicine. We provide an overview of the strengths and limitations of ML tools and their potential applications to both clinical and molecular data in hepatology. ML has been applied to various types of data in liver disease research, including clinical, demographic, molecular, radiological, and pathological data. We anticipate that use of ML tools to generate predictive algorithms will change the face of clinical practice in hepatology and transplantation. This review will provide readers with the opportunity to learn about the ML tools available and potential applications to questions of interest in hepatology.","Machine learning (ML) utilizes artificial intelligence to generate predictive models efficiently and more effectively than conventional methods through detection of hidden patterns within large data sets. With this in mind, there are several areas within hepatology where these methods can be applied. In this review, we examine the literature pertaining to machine learning in hepatology and liver transplant medicine. We provide an overview of the strengths and limitations of ML tools and their potential applications to both clinical and molecular data in hepatology. ML has been applied to various types of data in liver disease research, including clinical, demographic, molecular, radiological, and pathological data. We anticipate that use of ML tools to generate predictive algorithms will change the face of clinical practice in hepatology and transplantation. This review will provide readers with the opportunity to learn about the ML tools available and potential applications to questions of interest in hepatology.","Spann, A.
 and Yasodhara, A.
 and Kang, J.
 and Watt, K.
 and Wang, B.
 and Goldenberg, A.
 and Bhat, M.","Spann, Yasodhara, Kang, Watt, Wang, Goldenberg, Bhat",https://dx.doi.org/10.1002/hep.31103,https://doi.org/10.1002/hep.31103,2021-08-03
2309.0,,pubmed,Primary Care Opioid Taper Plans Are Associated with Sustained Opioid Dose Reduction,Primary Care Opioid Taper Plans Are Associated with Sustained Opioid Dose Reduction,"BACKGROUND: Primary care providers prescribe most long-term opioid therapy and are increasingly asked to taper the opioid doses of these patients to safer levels. A recent systematic review suggests that multiple interventions may facilitate opioid taper, but many of these are not feasible within the usual primary care practice. OBJECTIVE: To determine if opioid taper plans documented by primary care providers in the electronic health record are associated with significant and sustained opioid dose reductions among patients on long-term opioid therapy. DESIGN: A nested case-control design was used to compare cases (patients with a sustained opioid taper defined as average daily opioid dose of <= 30 mg morphine equivalent (MME) or a 50% reduction in MME) to controls (patients matched to cases on year and quarter of cohort entry, sex, and age group, who had not achieved a sustained taper). Each case was matched with four controls. PARTICIPANTS: Two thousand four hundred nine patients receiving a >= 60-day supply of opioids with an average daily dose of >= 50 MME during 2011-2015. MAIN MEASURES: Opioid taper plans documented in prescription instructions or clinical notes within the electronic health record identified through natural language processing; opioid dosing, patient characteristics, and taper plan components also abstracted from the electronic health record. KEY RESULTS: Primary care taper plans were associated with an increased likelihood of sustained opioid taper after adjusting for all patient covariates and near peak dose (OR = 3.63 [95% CI 2.96-4.46], p < 0.0001). Both taper plans in prescription instructions (OR = 4.03 [95% CI 3.19-5.09], p < 0.0001) and in clinical notes (OR = 2.82 [95% CI 2.00-3.99], p < 0.0001) were associated with sustained taper. CONCLUSIONS: These results suggest that planning for opioid taper during primary care visits may facilitate significant and sustained opioid dose reduction.","Primary care providers prescribe most long-term opioid therapy and are increasingly asked to taper the opioid doses of these patients to safer levels. A recent systematic review suggests that multiple interventions may facilitate opioid taper, but many of these are not feasible within the usual primary care practice. To determine if opioid taper plans documented by primary care providers in the electronic health record are associated with significant and sustained opioid dose reductions among patients on long-term opioid therapy. A nested case-control design was used to compare cases (patients with a sustained opioid taper defined as average daily opioid dose of â‰¤â€‰30 mg morphine equivalent (MME) or a 50% reduction in MME) to controls (patients matched to cases on year and quarter of cohort entry, sex, and age group, who had not achieved a sustained taper). Each case was matched with four controls. Two thousand four hundred nine patients receiving a â‰¥â€‰60-day supply of opioids with an average daily dose of â‰¥â€‰50 MME during 2011-2015. Opioid taper plans documented in prescription instructions or clinical notes within the electronic health record identified through natural language processing; opioid dosing, patient characteristics, and taper plan components also abstracted from the electronic health record. Primary care taper plans were associated with an increased likelihood of sustained opioid taper after adjusting for all patient covariates and near peak dose (OR = 3.63 [95% CI 2.96-4.46], p &lt; 0.0001). Both taper plans in prescription instructions (OR = 4.03 [95% CI 3.19-5.09], p &lt; 0.0001) and in clinical notes (OR = 2.82 [95% CI 2.00-3.99], p &lt; 0.0001) were associated with sustained taper. These results suggest that planning for opioid taper during primary care visits may facilitate significant and sustained opioid dose reduction.","M, D. Sullivan
 and Boudreau, D.
 and Ichikawa, L.
 and Cronkite, D.
 and Albertson-Junkans, L.
 and Salgado, G.
 and VonKorff, M.
 and Carrell, D. S.","D Sullivan, Boudreau, Ichikawa, Cronkite, Albertson-Junkans, Salgado, VonKorff, Carrell",https://dx.doi.org/10.1007/s11606-019-05445-1,https://doi.org/10.1007/s11606-019-05445-1,2021-08-03
3493.0,,pubmed,Brain Imaging Genomics: Integrated Analysis and Machine Learning,Brain Imaging Genomics: Integrated Analysis and Machine Learning,"Brain imaging genomics is an emerging data science field, where integrated analysis of brain imaging and genomics data, often combined with other biomarker, clinical and environmental data, is performed to gain new insights into the phenotypic, genetic and molecular characteristics of the brain as well as their impact on normal and disordered brain function and behavior. It has enormous potential to contribute significantly to biomedical discoveries in brain science. Given the increasingly important role of statistical and machine learning in biomedicine and rapidly growing literature in brain imaging genomics, we provide an up-to-date and comprehensive review of statistical and machine learning methods for brain imaging genomics, as well as a practical discussion on method selection for various biomedical applications.","Brain imaging genomics is an emerging data science field, where integrated analysis of brain imaging and genomics data, often combined with other biomarker, clinical and environmental data, is performed to gain new insights into the phenotypic, genetic and molecular characteristics of the brain as well as their impact on normal and disordered brain function and behavior. It has enormous potential to contribute significantly to biomedical discoveries in brain science. Given the increasingly important role of statistical and machine learning in biomedicine and rapidly growing literature in brain imaging genomics, we provide an up-to-date and comprehensive review of statistical and machine learning methods for brain imaging genomics, as well as a practical discussion on method selection for various biomedical applications.","Shen, L.
 and Thompson, P. M.","Shen, Thompson",https://dx.doi.org/10.1109/JPROC.2019.2947272,https://doi.org/10.1109/JPROC.2019.2947272,2021-08-03
2100.0,,pubmed,"PGxCorpus, a manually annotated corpus for pharmacogenomics","PGxCorpus, a manually annotated corpus for pharmacogenomics","Pharmacogenomics (PGx) studies how individual gene variations impact drug response phenotypes, which makes PGx-related knowledge a key component towards precision medicine. A significant part of the state-of-the-art knowledge in PGx is accumulated in scientific publications, where it is hardly reusable by humans or software. Natural language processing techniques have been developed to guide experts who curate this amount of knowledge. But existing works are limited by the absence of a high quality annotated corpus focusing on PGx domain. In particular, this absence restricts the use of supervised machine learning. This article introduces PGxCorpus, a manually annotated corpus, designed to fill this gap and to enable the automatic extraction of PGx relationships from text. It comprises 945 sentences from 911 PubMed abstracts, annotated with PGx entities of interest (mainly gene variations, genes, drugs and phenotypes), and relationships between those. In this article, we present the corpus itself, its construction and a baseline experiment that illustrates how it may be leveraged to synthesize and summarize PGx knowledge.","Pharmacogenomics (PGx) studies how individual gene variations impact drug response phenotypes, which makes PGx-related knowledge a key component towards precision medicine. A significant part of the state-of-the-art knowledge in PGx is accumulated in scientific publications, where it is hardly reusable by humans or software. Natural language processing techniques have been developed to guide experts who curate this amount of knowledge. But existing works are limited by the absence of a high quality annotated corpus focusing on PGx domain. In particular, this absence restricts the use of supervised machine learning. This article introduces PGxCorpus, a manually annotated corpus, designed to fill this gap and to enable the automatic extraction of PGx relationships from text. It comprises 945 sentences from 911 PubMed abstracts, annotated with PGx entities of interest (mainly gene variations, genes, drugs and phenotypes), and relationships between those. In this article, we present the corpus itself, its construction and a baseline experiment that illustrates how it may be leveraged to synthesize and summarize PGx knowledge.","Legrand, J.
 and Gogdemir, R.
 and Bousquet, C.
 and Dalleau, K.
 and Devignes, M. D.
 and Digan, W.
 and Lee, C. J.
 and Ndiaye, N. C.
 and Petitpain, N.
 and Ringot, P.
 and Smail-Tabbone, M.
 and Toussaint, Y.
 and Coulet, A.","Legrand, Gogdemir, Bousquet, Dalleau, Devignes, Digan, Lee, Ndiaye, Petitpain, Ringot, SmaÃ¯l-Tabbone, Toussaint, Coulet",https://dx.doi.org/10.1038/s41597-019-0342-9,https://doi.org/10.1038/s41597-019-0342-9,2021-08-03
977.0,,pubmed,Lead-I ECG for detecting atrial fibrillation in patients attending primary care with an irregular pulse using single-time point testing: A systematic review and economic evaluation,Lead-I ECG for detecting atrial fibrillation in patients attending primary care with an irregular pulse using single-time point testing: A systematic review and economic evaluation,"BACKGROUND: Atrial fibrillation (AF) is the most common type of cardiac arrhythmia and is associated with increased risk of stroke and congestive heart failure. Lead-I electrocardiogram (ECG) devices are handheld instruments that can detect AF at a single-time point. PURPOSE: To assess the diagnostic test accuracy, clinical impact and cost effectiveness of single-time point lead-I ECG devices compared with manual pulse palpation (MPP) followed by a 12-lead ECG for the detection of AF in symptomatic primary care patients with an irregular pulse. METHODS: Electronic databases (MEDLINE, MEDLINE Epub Ahead of Print and MEDLINE In-Process, EMBASE, PubMed and Cochrane Databases of Systematic Reviews, Cochrane Central Database of Controlled Trials, Database of Abstracts of Reviews of Effects, Health Technology Assessment Database) were searched to March 2018. Two reviewers screened the search results, extracted data and assessed study quality. Summary estimates of diagnostic accuracy were calculated using bivariate models. Cost-effectiveness was evaluated using an economic model consisting of a decision tree and two cohort Markov models. RESULTS: Diagnostic accuracy The diagnostic accuracy (13 publications reporting on nine studies) and clinical impact (24 publications reporting on 19 studies) results are derived from an asymptomatic population (used as a proxy for people with signs or symptoms of AF). The summary sensitivity of lead-I ECG devices was 93.9% (95% confidence interval [CI]: 86.2% to 97.4%) and summary specificity was 96.5% (95% CI: 90.4% to 98.8%). Cost effectiveness The de novo economic model yielded incremental cost effectiveness ratios (ICERs) per quality adjusted life year (QALY) gained. The results of the pairwise analysis show that all lead-I ECG devices generate ICERs per QALY gained below the 20,000-30,000 threshold. Kardia Mobile is the most cost effective option in a full incremental analysis. Lead-I ECG tests may identify more AF cases than the standard diagnostic pathway. This comes at a higher cost but with greater patient benefit in terms of mortality and quality of life. LIMITATIONS: No published data evaluating the diagnostic accuracy, clinical impact or cost effectiveness of lead-I ECG devices for the target population are available. CONCLUSIONS: The use of single-time point lead-I ECG devices in primary care for the detection of AF in people with signs or symptoms of AF and an irregular pulse appears to be a cost effective use of NHS resources compared with MPP followed by a 12-lead ECG, given the assumptions used in the base case model. REGISTRATION: The protocol for this review is registered on PROSPERO as CRD42018090375.","Atrial fibrillation (AF) is the most common type of cardiac arrhythmia and is associated with increased risk of stroke and congestive heart failure. Lead-I electrocardiogram (ECG) devices are handheld instruments that can detect AF at a single-time point. To assess the diagnostic test accuracy, clinical impact and cost effectiveness of single-time point lead-I ECG devices compared with manual pulse palpation (MPP) followed by a 12-lead ECG for the detection of AF in symptomatic primary care patients with an irregular pulse. Electronic databases (MEDLINE, MEDLINE Epub Ahead of Print and MEDLINE In-Process, EMBASE, PubMed and Cochrane Databases of Systematic Reviews, Cochrane Central Database of Controlled Trials, Database of Abstracts of Reviews of Effects, Health Technology Assessment Database) were searched to March 2018. Two reviewers screened the search results, extracted data and assessed study quality. Summary estimates of diagnostic accuracy were calculated using bivariate models. Cost-effectiveness was evaluated using an economic model consisting of a decision tree and two cohort Markov models. Diagnostic accuracy The diagnostic accuracy (13 publications reporting on nine studies) and clinical impact (24 publications reporting on 19 studies) results are derived from an asymptomatic population (used as a proxy for people with signs or symptoms of AF). The summary sensitivity of lead-I ECG devices was 93.9% (95% confidence interval [CI]: 86.2% to 97.4%) and summary specificity was 96.5% (95% CI: 90.4% to 98.8%). Cost effectiveness The de novo economic model yielded incremental cost effectiveness ratios (ICERs) per quality adjusted life year (QALY) gained. The results of the pairwise analysis show that all lead-I ECG devices generate ICERs per QALY gained below the Â£20,000-Â£30,000 threshold. Kardia Mobile is the most cost effective option in a full incremental analysis. Lead-I ECG tests may identify more AF cases than the standard diagnostic pathway. This comes at a higher cost but with greater patient benefit in terms of mortality and quality of life. No published data evaluating the diagnostic accuracy, clinical impact or cost effectiveness of lead-I ECG devices for the target population are available. The use of single-time point lead-I ECG devices in primary care for the detection of AF in people with signs or symptoms of AF and an irregular pulse appears to be a cost effective use of NHS resources compared with MPP followed by a 12-lead ECG, given the assumptions used in the base case model. The protocol for this review is registered on PROSPERO as CRD42018090375.","Duarte, R.
 and Stainthorpe, A.
 and Mahon, J.
 and Greenhalgh, J.
 and Richardson, M.
 and Nevitt, S.
 and Kotas, E.
 and Boland, A.
 and Thom, H.
 and Marshall, T.
 and Hall, M.
 and Takwoingi, Y.","Duarte, Stainthorpe, Mahon, Greenhalgh, Richardson, Nevitt, Kotas, Boland, Thom, Marshall, Hall, Takwoingi",https://dx.doi.org/10.1371/journal.pone.0226671,https://doi.org/10.1371/journal.pone.0226671,2021-08-03
3661.0,,pubmed,Characteristics Associated With US Outpatient Opioid Analgesic Prescribing and Gabapentinoid Co-Prescribing,Characteristics Associated With US Outpatient Opioid Analgesic Prescribing and Gabapentinoid Co-Prescribing,"INTRODUCTION: A considerable burden of prescription and illicit opioid-related mortality and morbidity in the U.S. is attributable to potentially unnecessary or excessive opioid prescribing, and co-prescribing gabapentinoids may increase risk of harm. Data are needed regarding physician and patient characteristics associated with opioid analgesic and opioid analgesic-gabapentinoid co-prescriptions to elucidate targets for reducing preventable harm. METHODS: Multiple logistic regression was utilized to examine patient and physician predictors of opioid analgesic prescriptions and opioid analgesic-gabapentinoid co-prescriptions in adult noncancer patients using the National Ambulatory Medical Care Survey 2015 public use data set. Potential predictors were selected based on literature review, clinical relevance, and random forest machine learning algorithms. RESULTS: Among the 11.8% (95% CI=9.8%, 13.9%) of medical encounters with an opioid prescription, 16.2% (95% CI=12.6%, 19.8%) had a gabapentinoid co-prescription. Among all gabapentinoid encounters, 40.7% (95% CI=32.6%, 48.7%) had an opioid co-prescription. Predictors of opioid prescription included arthritis (OR=1.87, 95% CI=1.30, 2.69). Predictors of new opioid prescription included physician status as an independent contractor (OR=3.67, 95% CI=1.38, 9.81) or part owner of the practice (OR=3.34, 95% CI=1.74, 6.42). Predictors of opioid-gabapentinoid co-prescription included patient age (peaking at age 55-64 years; OR=35.67, 95% CI=4.32, 294.43). CONCLUSIONS: Predictors of opioid analgesic prescriptions with and without gabapentinoid co-prescriptions were identified. These predictors can help inform and reinforce (e.g., educational) interventions seeking to reduce preventable harm, help identify populations for elucidating opioid-gabapentinoid risk-benefit profiles, and provide a baseline for evaluating subsequent public health measures.","A considerable burden of prescription and illicit opioid-related mortality and morbidity in the U.S. is attributable to potentially unnecessary or excessive opioid prescribing, and co-prescribing gabapentinoids may increase risk of harm. Data are needed regarding physician and patient characteristics associated with opioid analgesic and opioid analgesic-gabapentinoid co-prescriptions to elucidate targets for reducing preventable harm. Multiple logistic regression was utilized to examine patient and physician predictors of opioid analgesic prescriptions and opioid analgesic-gabapentinoid co-prescriptions in adult noncancer patients using the National Ambulatory Medical Care Survey 2015 public use data set. Potential predictors were selected based on literature review, clinical relevance, and random forest machine learning algorithms. Among the 11.8% (95% CI=9.8%, 13.9%) of medical encounters with an opioid prescription, 16.2% (95% CI=12.6%, 19.8%) had a gabapentinoid co-prescription. Among all gabapentinoid encounters, 40.7% (95% CI=32.6%, 48.7%) had an opioid co-prescription. Predictors of opioid prescription included arthritis (OR=1.87, 95% CI=1.30, 2.69). Predictors of new opioid prescription included physician status as an independent contractor (OR=3.67, 95% CI=1.38, 9.81) or part owner of the practice (OR=3.34, 95% CI=1.74, 6.42). Predictors of opioid-gabapentinoid co-prescription included patient age (peaking at age 55-64 years; OR=35.67, 95% CI=4.32, 294.43). Predictors of opioid analgesic prescriptions with and without gabapentinoid co-prescriptions were identified. These predictors can help inform and reinforce (e.g., educational) interventions seeking to reduce preventable harm, help identify populations for elucidating opioid-gabapentinoid risk-benefit profiles, and provide a baseline for evaluating subsequent public health measures.","St Clair, C. O.
 and Golub, N. I.
 and Ma, Y.
 and Song, J.
 and Winiecki, S. K.
 and Menschik, D. L.","St Clair, Golub, Ma, Song, Winiecki, Menschik",https://dx.doi.org/10.1016/j.amepre.2019.08.029,https://doi.org/10.1016/j.amepre.2019.08.029,2021-08-03
1034.0,,pubmed,Brain structural effects of treatments for depression and biomarkers of response: a systematic review of neuroimaging studies,Brain structural effects of treatments for depression and biomarkers of response: a systematic review of neuroimaging studies,"Antidepressive pharmacotherapy (AD), electroconvulsive therapy (ECT) and cognitive behavioural therapy (CBT) are effective treatments for major depressive disorder. With our review, we aim to provide a systematic overview of neuroimaging studies that investigate the effects of AD, ECT and CBT on brain grey matter volume (GMV) and biomarkers associated with response. After a systematic database research on PubMed, we included 50 studies using magnetic resonance imaging and investigating (1) changes in GMV, (2) pre-treatment GMV biomarkers associated with response, or (3) the accuracy of predictions of response to AD, ECT or CBT based on baseline GMV data. The strongest evidence for brain structural changes was found for ECT, showing volume increases within the temporal lobe and subcortical structures - such as the hippocampus-amygdala complex, anterior cingulate cortex (ACC) and striatum. For AD, the evidence is heterogeneous as only 4 of 11 studies reported significant changes in GMV. The results are not sufficient in order to draw conclusions about the structural brain effects of CBT. The findings show consistently that higher pre-treatment ACC volume is associated with response to AD, ECT and CBT. An association of higher pre-treatment hippocampal volume and response has only been reported for AD. Machine learning approaches based on pre-treatment whole brain patterns reach accuracies of 64-90% for predictions of AD or ECT response on the individual patient level. The findings underline the potential of brain biomarkers for the implementation in clinical practice as an additive feature within the process of treatment selection.","Antidepressive pharmacotherapy (AD), electroconvulsive therapy (ECT) and cognitive behavioural therapy (CBT) are effective treatments for major depressive disorder. With our review, we aim to provide a systematic overview of neuroimaging studies that investigate the effects of AD, ECT and CBT on brain grey matter volume (GMV) and biomarkers associated with response. After a systematic database research on PubMed, we included 50 studies using magnetic resonance imaging and investigating (1) changes in GMV, (2) pre-treatment GMV biomarkers associated with response, or (3) the accuracy of predictions of response to AD, ECT or CBT based on baseline GMV data. The strongest evidence for brain structural changes was found for ECT, showing volume increases within the temporal lobe and subcortical structures - such as the hippocampus-amygdala complex, anterior cingulate cortex (ACC) and striatum. For AD, the evidence is heterogeneous as only 4 of 11 studies reported significant changes in GMV. The results are not sufficient in order to draw conclusions about the structural brain effects of CBT. The findings show consistently that higher pre-treatment ACC volume is associated with response to AD, ECT and CBT. An association of higher pre-treatment hippocampal volume and response has only been reported for AD. Machine learning approaches based on pre-treatment whole brain patterns reach accuracies of 64-90% for predictions of AD or ECT response on the individual patient level. The findings underline the potential of brain biomarkers for the implementation in clinical practice as an additive feature within the process of treatment selection.","Enneking, V.
 and Leehr, E. J.
 and Dannlowski, U.
 and Redlich, R.","Enneking, Leehr, Dannlowski, Redlich",https://dx.doi.org/10.1017/S0033291719003660,https://doi.org/10.1017/S0033291719003660,2021-08-03
2039.0,,pubmed,Diagnostic accuracy of machine-learning-assisted detection for anterior cruciate ligament injury based on magnetic resonance imaging: Protocol for a systematic review and meta-analysis,Diagnostic accuracy of machine-learning-assisted detection for anterior cruciate ligament injury based on magnetic resonance imaging: Protocol for a systematic review and meta-analysis,"BACKGROUND: Although many machine learning algorithms have been developed to detect anterior cruciate ligament (ACL) injury based on magnetic resonance imaging (MRI), the performance of different algorithms required further investigation. The objectives of this current systematic review are to evaluate the diagnostic accuracy of machine-learning-assisted detection for ACL injury based on MRI and find the current best algorithm. METHOD: We will conduct a comprehensive database search for clinical diagnostic tests in PubMed, EMBASE, Cochrane Library, and Web of science without restrictions on publication status and language. The reference lists of the included articles will also be checked to identify additional studies for potential inclusion. Two reviewers will independently review all literature for inclusion and assess their methodological quality using Quality Assessment of Diagnostic Accuracy Studies version 2. Clinical diagnostic tests exploring the efficacy of machine-learning-assisted system for detecting ACL injury based on MRI will be considered for inclusion. Another 2 reviewers will independently extract data from eligible studies based on a pre-designed standardized form. Any disagreements will be resolved by consensus. RevMan 5.3 and Stata SE 12.0 software will be used for data synthesis. If appropriate, we will calculate the summary sensitivity, specificity, positive likelihood ratio, negative likelihood ratio, and diagnostic odds ratio of machine-learning-assisted diagnosis system for ACL injury detection. A hierarchical summary receiver operating characteristic (HSROC) curve will also be plotted, and the area under the ROC curve (AUC) is going to calculated using the bivariate model. If the pooling of results is considered inappropriate, we will present and describe our findings in diagrams and tables and describe them narratively. RESULT: This is the first systematic assessment of machine learning system for the detection of ACL injury based on MRI. We predict it will provide highquality synthesis of existing evidence for the diagnostic accuracy of machine-learning-assisted detection for ACL injury and a relatively comprehensive reference for clinical practice and development of interdisciplinary field of artificial intelligence and medicine. CONCLUSION: This protocol outlined the significance and methodologically details of a systematic review of machine-learning-assisted detection for ACL injury based on MRI. The ongoing systematic review will provide high-quality synthesis of current evidence of machine learning system for detecting ACL injury. REGISTRATION: The meta-analysis has been prospectively registered in PROSPERO (CRD42019136581).","Although many machine learning algorithms have been developed to detect anterior cruciate ligament (ACL) injury based on magnetic resonance imaging (MRI), the performance of different algorithms required further investigation. The objectives of this current systematic review are to evaluate the diagnostic accuracy of machine-learning-assisted detection for ACL injury based on MRI and find the current best algorithm. We will conduct a comprehensive database search for clinical diagnostic tests in PubMed, EMBASE, Cochrane Library, and Web of science without restrictions on publication status and language. The reference lists of the included articles will also be checked to identify additional studies for potential inclusion. Two reviewers will independently review all literature for inclusion and assess their methodological quality using Quality Assessment of Diagnostic Accuracy Studies version 2. Clinical diagnostic tests exploring the efficacy of machine-learning-assisted system for detecting ACL injury based on MRI will be considered for inclusion. Another 2 reviewers will independently extract data from eligible studies based on a pre-designed standardized form. Any disagreements will be resolved by consensus. RevMan 5.3 and Stata SE 12.0 software will be used for data synthesis. If appropriate, we will calculate the summary sensitivity, specificity, positive likelihood ratio, negative likelihood ratio, and diagnostic odds ratio of machine-learning-assisted diagnosis system for ACL injury detection. A hierarchical summary receiver operating characteristic (HSROC) curve will also be plotted, and the area under the ROC curve (AUC) is going to calculated using the bivariate model. If the pooling of results is considered inappropriate, we will present and describe our findings in diagrams and tables and describe them narratively. This is the first systematic assessment of machine learning system for the detection of ACL injury based on MRI. We predict it will provide highquality synthesis of existing evidence for the diagnostic accuracy of machine-learning-assisted detection for ACL injury and a relatively comprehensive reference for clinical practice and development of interdisciplinary field of artificial intelligence and medicine. This protocol outlined the significance and methodologically details of a systematic review of machine-learning-assisted detection for ACL injury based on MRI. The ongoing systematic review will provide high-quality synthesis of current evidence of machine learning system for detecting ACL injury. The meta-analysis has been prospectively registered in PROSPERO (CRD42019136581).","Lao, Y.
 and Jia, B.
 and Yan, P.
 and Pan, M.
 and Hui, X.
 and Li, J.
 and Luo, W.
 and Li, X.
 and Han, J.
 and Yao, L.","Lao, Jia, Yan, Pan, Hui, Li, Luo, Li, Han, Yan, Yao",https://dx.doi.org/10.1097/MD.0000000000018324,https://doi.org/10.1097/MD.0000000000018324,2021-08-03
2694.0,,pubmed,Do automated digital health behaviour change interventions have a positive effect on self-efficacy? A systematic review and meta-analysis,Do automated digital health behaviour change interventions have a positive effect on self-efficacy? A systematic review and meta-analysis,"Self-efficacy is an important determinant of health behaviour. Digital interventions are a potentially acceptable and cost-effective way of delivering programmes of health behaviour change at scale. Whether behaviour change interventions work to increase self-efficacy in this context is unknown. This systematic review and meta-analysis sought to identify whether automated digital interventions are associated with positive changes in self-efficacy amongst non-clinical populations for five major health behaviours, and which BCTs are associated with that change. A systematic literature search identified 20 studies (n = 5624) that assessed changes in self-efficacy and were included in a random-effects meta-analysis. Interventions targeted: healthy eating (k = 4), physical activity (k = 9), sexual behaviour (k = 3) and smoking (k = 4). No interventions targeting alcohol use were identified. Overall, interventions had a small, positive effect on self-efficacy [Formula: see text]. The effect of interventions on self-efficacy did not differ as a function of health behaviour type (Q-between = 7.3704, p = .061, df = 3). Inclusion of the BCT 'information about social and environmental consequences' had a small, negative effect on self-efficacy [Formula: see text]. Whilst this review indicates that digital interventions can be used to change self-efficacy, which techniques work best in this context is not clear.","Self-efficacy is an important determinant of health behaviour. Digital interventions are a potentially acceptable and cost-effective way of delivering programmes of health behaviour change at scale. Whether behaviour change interventions work to increase self-efficacy in this context is unknown. This systematic review and meta-analysis sought to identify whether automated digital interventions are associated with positive changes in self-efficacy amongst non-clinical populations for five major health behaviours, and which BCTs are associated with that change. A systematic literature search identified 20 studies (<i>n</i>â€‰=â€‰5624) that assessed changes in self-efficacy and were included in a random-effects meta-analysis. Interventions targeted: healthy eating (<i>k</i>â€‰=â€‰4), physical activity (<i>k</i>â€‰=â€‰9), sexual behaviour (<i>k</i>â€‰=â€‰3) and smoking (<i>k</i>â€‰=â€‰4). No interventions targeting alcohol use were identified. Overall, interventions had a small, positive effect on self-efficacy <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>g</mml:mi><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:mspace/><mml:mo>=</mml:mo><mml:mn>0.190</mml:mn><mml:mo>,</mml:mo><mml:mspace/><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mspace/><mml:mo>[</mml:mo><mml:mn>0.078</mml:mn><mml:mo>;</mml:mo><mml:mspace/><mml:mn>0.303</mml:mn><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:math>. The effect of interventions on self-efficacy did not differ as a function of health behaviour type (<i>Q</i>-betweenâ€‰=â€‰7.3704, <i>p</i>â€‰=â€‰.061, dfâ€‰=â€‰3). Inclusion of the BCT 'information about social and environmental consequences' had a small, negative effect on self-efficacy <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mo>(</mml:mo><mml:mi>Î”</mml:mi><mml:mrow><mml:mover><mml:mi>g</mml:mi><mml:mo>Â¯</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.297</mml:mn><mml:mo>,</mml:mo><mml:mspace/><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mn>7.072</mml:mn><mml:mo>,</mml:mo><mml:mspace/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>.008</mml:mn><mml:mo>)</mml:mo></mml:math>. Whilst this review indicates that digital interventions can be used to change self-efficacy, which techniques work best in this context is not clear.","Newby, K.
 and Teah, G.
 and Cooke, R.
 and Li, X.
 and Brown, K.
 and Salisbury-Finch, B.
 and Kwah, K.
 and Bartle, N.
 and Curtis, K.
 and Fulton, E.
 and Parsons, J.
 and Dusseldorp, E.
 and Williams, S. L.","Newby, Teah, Cooke, Li, Brown, Salisbury-Finch, Kwah, Bartle, Curtis, Fulton, Parsons, Dusseldorp, Williams",https://dx.doi.org/10.1080/17437199.2019.1705873,https://doi.org/10.1080/17437199.2019.1705873,2021-08-03
2874.0,,pubmed,Voice patterns in schizophrenia: A systematic review and Bayesian meta-analysis,Voice patterns in schizophrenia: A systematic review and Bayesian meta-analysis,"Voice atypicalities have been a characteristic feature of schizophrenia since its first definitions. They are often associated with core negative symptoms such as flat affect and alogia, and with the social impairments seen in the disorder. This suggests that voice atypicalities may represent a marker of clinical features and social functioning in schizophrenia. We systematically reviewed and meta-analyzed the evidence for distinctive acoustic patterns in schizophrenia, as well as their relation to clinical features. We identified 46 articles, including 55 studies with a total of 1254 patients with schizophrenia and 699 healthy controls. Summary effect sizes (Hedges'g and Pearson's r) estimates were calculated using multilevel Bayesian modeling. We identified weak atypicalities in pitch variability (g = -0.55) related to flat affect, and stronger atypicalities in proportion of spoken time, speech rate, and pauses (g's between -0.75 and -1.89) related to alogia and flat affect. However, the effects were mostly modest (with the important exception of pause duration) compared to perceptual and clinical judgments, and characterized by large heterogeneity between studies. Moderator analyses revealed that tasks with a more demanding cognitive and social component showed larger effects both in contrasting patients and controls and in assessing symptomatology. In conclusion, studies of acoustic patterns are a promising but, yet unsystematic avenue for establishing markers of schizophrenia. We outline recommendations towards more cumulative, open, and theory-driven research.","Voice atypicalities have been a characteristic feature of schizophrenia since its first definitions. They are often associated with core negative symptoms such as flat affect and alogia, and with the social impairments seen in the disorder. This suggests that voice atypicalities may represent a marker of clinical features and social functioning in schizophrenia. We systematically reviewed and meta-analyzed the evidence for distinctive acoustic patterns in schizophrenia, as well as their relation to clinical features. We identified 46 articles, including 55 studies with a total of 1254 patients with schizophrenia and 699 healthy controls. Summary effect sizes (Hedges'g and Pearson's r) estimates were calculated using multilevel Bayesian modeling. We identified weak atypicalities in pitch variability (gÂ =Â -0.55) related to flat affect, and stronger atypicalities in proportion of spoken time, speech rate, and pauses (g's between -0.75 and -1.89) related to alogia and flat affect. However, the effects were mostly modest (with the important exception of pause duration) compared to perceptual and clinical judgments, and characterized by large heterogeneity between studies. Moderator analyses revealed that tasks with a more demanding cognitive and social component showed larger effects both in contrasting patients and controls and in assessing symptomatology. In conclusion, studies of acoustic patterns are a promising but, yet unsystematic avenue for establishing markers of schizophrenia. We outline recommendations towards more cumulative, open, and theory-driven research.","Parola, A.
 and Simonsen, A.
 and Bliksted, V.
 and Fusaroli, R.","Parola, Simonsen, Bliksted, Fusaroli",https://dx.doi.org/10.1016/j.schres.2019.11.031,https://doi.org/10.1016/j.schres.2019.11.031,2021-08-03
1000.0,,pubmed,Deep learning to detect Alzheimer's disease from neuroimaging: A systematic literature review,Deep learning to detect Alzheimer's disease from neuroimaging: A systematic literature review,"Alzheimer's Disease (AD) is one of the leading causes of death in developed countries. From a research point of view, impressive results have been reported using computer-aided algorithms, but clinically no practical diagnostic method is available. In recent years, deep models have become popular, especially in dealing with images. Since 2013, deep learning has begun to gain considerable attention in AD detection research, with the number of published papers in this area increasing drastically since 2017. Deep models have been reported to be more accurate for AD detection compared to general machine learning techniques. Nevertheless, AD detection is still challenging, and for classification, it requires a highly discriminative feature representation to separate similar brain patterns. This paper reviews the current state of AD detection using deep learning. Through a systematic literature review of over 100 articles, we set out the most recent findings and trends. Specifically, we review useful biomarkers and features (personal information, genetic data, and brain scans), the necessary pre-processing steps, and different ways of dealing with neuroimaging data originating from single-modality and multi-modality studies. Deep models and their performance are described in detail. Although deep learning has achieved notable performance in detecting AD, there are several limitations, especially regarding the availability of datasets and training procedures.","Alzheimer's Disease (AD) is one of the leading causes of death in developed countries. From a research point of view, impressive results have been reported using computer-aided algorithms, but clinically no practical diagnostic method is available. In recent years, deep models have become popular, especially in dealing with images. Since 2013, deep learning has begun to gain considerable attention in AD detection research, with the number of published papers in this area increasing drastically since 2017. Deep models have been reported to be more accurate for AD detection compared to general machine learning techniques. Nevertheless, AD detection is still challenging, and for classification, it requires a highly discriminative feature representation to separate similar brain patterns. This paper reviews the current state of AD detection using deep learning. Through a systematic literature review of over 100 articles, we set out the most recent findings and trends. Specifically, we review useful biomarkers and features (personal information, genetic data, and brain scans), the necessary pre-processing steps, and different ways of dealing with neuroimaging data originating from single-modality and multi-modality studies. Deep models and their performance are described in detail. Although deep learning has achieved notable performance in detecting AD, there are several limitations, especially regarding the availability of datasets and training procedures.","Ebrahimighahnavieh, M. A.
 and Luo, S.
 and Chiong, R.","Ebrahimighahnavieh, Luo, Chiong",https://dx.doi.org/10.1016/j.cmpb.2019.105242,https://doi.org/10.1016/j.cmpb.2019.105242,2021-08-03
633.0,,pubmed,Current state of science in machine learning methods for automatic infant pain evaluation using facial expression information: study protocol of a systematic review and meta-analysis,Current state of science in machine learning methods for automatic infant pain evaluation using facial expression information: study protocol of a systematic review and meta-analysis,"INTRODUCTION: Infants can experience pain similar to adults, and improperly controlled pain stimuli could have a long-term adverse impact on their cognitive and neurological function development. The biggest challenge of achieving good infant pain control is obtaining objective pain assessment when direct communication is lacking. For years, computer scientists have developed many different facial expression-centred machine learning (ML) methods for automatic infant pain assessment. Many of these ML algorithms showed rather satisfactory performance and have demonstrated good potential to be further enhanced for implementation in real-world clinical settings. To date, there is no prior research that has systematically summarised and compared the performance of these ML algorithms. Our proposed meta-analysis will provide the first comprehensive evidence on this topic to guide further ML algorithm development and clinical implementation. METHODS AND ANALYSIS: We will search four major public electronic medical and computer science databases including Web of Science, PubMed, Embase and IEEE Xplore Digital Library from January 2008 to present. All the articles will be imported into the Covidence platform for study eligibility screening and inclusion. Study-level extracted data will be stored in the Systematic Review Data Repository online platform. The primary outcome will be the prediction accuracy of the ML model. The secondary outcomes will be model utility measures including generalisability, interpretability and computational efficiency. All extracted outcome data will be imported into RevMan V.5.2.1 software and R V3.3.2 for analysis. Risk of bias will be summarised using the latest Prediction Model Study Risk of Bias Assessment Tool. ETHICS AND DISSEMINATION: This systematic review and meta-analysis will only use study-level data from public databases, thus formal ethical approval is not required. The results will be disseminated in the form of an official publication in a peer-reviewed journal and/or presentation at relevant conferences. Prospero registration number: crd42019118784.","Infants can experience pain similar to adults, and improperly controlled pain stimuli could have a long-term adverse impact on their cognitive and neurological function development. The biggest challenge of achieving good infant pain control is obtaining objective pain assessment when direct communication is lacking. For years, computer scientists have developed many different facial expression-centred machine learning (ML) methods for automatic infant pain assessment. Many of these ML algorithms showed rather satisfactory performance and have demonstrated good potential to be further enhanced for implementation in real-world clinical settings. To date, there is no prior research that has systematically summarised and compared the performance of these ML algorithms. Our proposed meta-analysis will provide the first comprehensive evidence on this topic to guide further ML algorithm development and clinical implementation. We will search four major public electronic medical and computer science databases including Web of Science, PubMed, Embase and IEEE Xplore Digital Library from January 2008 to present. All the articles will be imported into the Covidence platform for study eligibility screening and inclusion. Study-level extracted data will be stored in the Systematic Review Data Repository online platform. The primary outcome will be the prediction accuracy of the ML model. The secondary outcomes will be model utility measures including generalisability, interpretability and computational efficiency. All extracted outcome data will be imported into RevMan V.5.2.1 software and R V3.3.2 for analysis. Risk of bias will be summarised using the latest Prediction Model Study Risk of Bias Assessment Tool. This systematic review and meta-analysis will only use study-level data from public databases, thus formal ethical approval is not required. The results will be disseminated in the form of an official publication in a peer-reviewed journal and/or presentation at relevant conferences. CRD42019118784.","Cheng, D.
 and Liu, D.
 and Philpotts, L. L.
 and Turner, D. P.
 and Houle, T. T.
 and Chen, L.
 and Zhang, M.
 and Yang, J.
 and Zhang, W.
 and Deng, H.","Cheng, Liu, Philpotts, Turner, Houle, Chen, Zhang, Yang, Zhang, Deng",https://dx.doi.org/10.1136/bmjopen-2019-030482,https://doi.org/10.1136/bmjopen-2019-030482,2021-08-03
3118.0,,pubmed,The use of machine learning techniques in trauma-related disorders: a systematic review,The use of machine learning techniques in trauma-related disorders: a systematic review,"Establishing the diagnosis of trauma-related disorders such as Acute Stress Disorder (ASD) and Posttraumatic Stress Disorder (PTSD) have always been a challenge in clinical practice and in academic research, due to clinical and biological heterogeneity. Machine learning (ML) techniques can be applied to improve classification of disorders, to predict outcomes or to determine person-specific treatment selection. We aim to review the existing literature on the use of machine learning techniques in the assessment of subjects with ASD or PTSD. We systematically searched PubMed, Embase and Web of Science for articles published in any language up to May 2019. We found 806 abstracts and included 49 studies in our review. Most of the included studies used multiple levels of biological data to predict risk factors or to identify early symptoms related to PTSD. Other studies used ML classification techniques to distinguish individuals with ASD or PTSD from other psychiatric disorder or from trauma-exposed and healthy controls. We also found studies that attempted to define outcome profiles using clustering techniques and studies that assessed the relationship among symptoms using network analysis. Finally, we proposed a quality assessment in this review, evaluating methodological and technical features on machine learning studies. We concluded that etiologic and clinical heterogeneity of ASD/PTSD patients is suitable to machine learning techniques and a major challenge for the future is to use it in clinical practice for the benefit of patients in an individual level.","Establishing the diagnosis of trauma-related disorders such as Acute Stress Disorder (ASD) and Posttraumatic Stress Disorder (PTSD) have always been a challenge in clinical practice and in academic research, due to clinical and biological heterogeneity. Machine learning (ML) techniques can be applied to improve classification of disorders, to predict outcomes or to determine person-specific treatment selection. We aim to review the existing literature on the use of machine learning techniques in the assessment of subjects with ASD or PTSD. We systematically searched PubMed, Embase and Web of Science for articles published in any language up to May 2019. We found 806 abstracts and included 49 studies in our review. Most of the included studies used multiple levels of biological data to predict risk factors or to identify early symptoms related to PTSD. Other studies used ML classification techniques to distinguish individuals with ASD or PTSD from other psychiatric disorder or from trauma-exposed and healthy controls. We also found studies that attempted to define outcome profiles using clustering techniques and studies that assessed the relationship among symptoms using network analysis. Finally, we proposed a quality assessment in this review, evaluating methodological and technical features on machine learning studies. We concluded that etiologic and clinical heterogeneity of ASD/PTSD patients is suitable to machine learning techniques and a major challenge for the future is to use it in clinical practice for the benefit of patients in an individual level.","Ramos-Lima, L. F.
 and Waikamp, V.
 and Antonelli-Salgado, T.
 and Passos, I. C.
 and Freitas, L. H. M.","Ramos-Lima, Waikamp, Antonelli-Salgado, Passos, Freitas",https://dx.doi.org/10.1016/j.jpsychires.2019.12.001,https://doi.org/10.1016/j.jpsychires.2019.12.001,2021-08-03
3017.0,,pubmed,Economic implications of localization strategies for cerebrospinal fluid rhinorrhea,Economic implications of localization strategies for cerebrospinal fluid rhinorrhea,"BACKGROUND: The direct costs associated with different diagnostic algorithms to localize cerebrospinal fluid (CSF) rhinorrhea have not been described. METHODS: A decision-tree analysis of imaging modalities used to localize CSF rhinorrhea was performed to compare associated direct costs. The primary outcome was cost, which was determined based on reimbursement data published by the Centers for Medicare and Medicaid Services in 2018. The model was parameterized after a literature review of published studies was performed from 1990 to 2018 to estimate the sensitivity CSF rhinorrhea localization of the following radiographic modalities: high-resolution computed tomography (HRCT), magnetic resonance cisternography (MRC), and CT cisternography (CTC). In addition to base case analysis, 1-way sensitivity analyses were also performed to evaluate the robustness of results to changes in model parameters. RESULTS: Among patients with a high suspicion for CSF rhinorrhea, use of HRCT followed by exploration in the operating room if preliminary HRCT was negative was found to be the optimal localization modality from a cost perspective ($172.25). The next least costly algorithm was HRCT followed by MRC ($294.10). Imaging algorithms beginning with CTC were the next least costly modality ($727.37). Sensitivity analyses generally supported HRCT to be the optimal initial radiographic strategy over a wide range of parameter values. CONCLUSION: This work advocates HRCT as first-line modality to localize CSF rhinorrhea from a cost perspective. Although algorithms beginning with MRC were on average $35 more expensive than those starting with CTC, associated risks of CTC were not modeled and may play a role in decision making.","The direct costs associated with different diagnostic algorithms to localize cerebrospinal fluid (CSF) rhinorrhea have not been described. A decision-tree analysis of imaging modalities used to localize CSF rhinorrhea was performed to compare associated direct costs. The primary outcome was cost, which was determined based on reimbursement data published by the Centers for Medicare and Medicaid Services in 2018. The model was parameterized after a literature review of published studies was performed from 1990 to 2018 to estimate the sensitivity CSF rhinorrhea localization of the following radiographic modalities: high-resolution computed tomography (HRCT), magnetic resonance cisternography (MRC), and CT cisternography (CTC). In addition to base case analysis, 1-way sensitivity analyses were also performed to evaluate the robustness of results to changes in model parameters. Among patients with a high suspicion for CSF rhinorrhea, use of HRCT followed by exploration in the operating room if preliminary HRCT was negative was found to be the optimal localization modality from a cost perspective ($172.25). The next least costly algorithm was HRCT followed by MRC ($294.10). Imaging algorithms beginning with CTC were the next least costly modality ($727.37). Sensitivity analyses generally supported HRCT to be the optimal initial radiographic strategy over a wide range of parameter values. This work advocates HRCT as first-line modality to localize CSF rhinorrhea from a cost perspective. Although algorithms beginning with MRC were on average $35 more expensive than those starting with CTC, associated risks of CTC were not modeled and may play a role in decision making.","Pool, C. D.
 and Patel, V. A.
 and Schilling, A.
 and Hollenbeak, C.
 and Goyal, N.","Pool, Patel, Schilling, Hollenbeak, Goyal",https://dx.doi.org/10.1002/alr.22501,https://doi.org/10.1002/alr.22501,2021-08-03
3752.0,,pubmed,Use of smartphones for detecting diabetic retinopathy: a protocol for a scoping review of diagnostic test accuracy studies,Use of smartphones for detecting diabetic retinopathy: a protocol for a scoping review of diagnostic test accuracy studies,"INTRODUCTION: Diabetic retinopathy (DR) is a common microvascular complication of diabetes mellitus and the leading cause of impaired vision in adults worldwide. Early detection and treatment for DR could improve patient outcomes. Traditional methods of detecting DR include the gold standard Early Treatment Diabetic Retinopathy Study seven standard fields fundus photography, ophthalmoscopy and slit-lamp biomicroscopy. These modalities can be expensive, difficult to access and require involvement of specialised healthcare professionals. With the development of mobile phone technology, there is a growing interest in their use for DR identification as this approach is potentially more affordable, accessible and easier to use. Smartphones can be employed in a variety of ways for ophthalmoscopy including the use of smartphone camera, various attachments and artificial intelligence for obtaining and grading of retinal images. The aim of this scoping review is to determine the diagnostic test accuracy of various smartphone ophthalmoscopy approaches for detecting DR in diabetic patients. METHODS AND ANALYSIS: We will perform an electronic search of MEDLINE, Embase and Cochrane Library for literature published from 2000 onwards. Two reviewers will independently analyse studies for eligibility and assess study quality using the QUADAS-2 tool. Data for a 22 contingency table will be extracted. If possible, we will pool sensitivity and specificity data using the random-effects model and construct a summary receiver operating characteristic curve. In case of high heterogeneity, we will present the findings narratively. Subgroup analysis and sensitivity analysis will be performed where appropriate. ETHICS AND DISSEMINATION: This scoping review aims to provide an overview of smartphone ophthalmoscopy in DR identification. It will present findings on the accuracy of smartphone ophthalmoscopy in detecting DR, identify gaps in the literature and provide recommendations for future research. This review does not require ethical approval as we will not collect primary data.","Diabetic retinopathy (DR) is a common microvascular complication of diabetes mellitus and the leading cause of impaired vision in adults worldwide. Early detection and treatment for DR could improve patient outcomes. Traditional methods of detecting DR include the gold standard Early Treatment Diabetic Retinopathy Study seven standard fields fundus photography, ophthalmoscopy and slit-lamp biomicroscopy. These modalities can be expensive, difficult to access and require involvement of specialised healthcare professionals. With the development of mobile phone technology, there is a growing interest in their use for DR identification as this approach is potentially more affordable, accessible and easier to use. Smartphones can be employed in a variety of ways for ophthalmoscopy including the use of smartphone camera, various attachments and artificial intelligence for obtaining and grading of retinal images. The aim of this scoping review is to determine the diagnostic test accuracy of various smartphone ophthalmoscopy approaches for detecting DR in diabetic patients. We will perform an electronic search of MEDLINE, Embase and Cochrane Library for literature published from 2000 onwards. Two reviewers will independently analyse studies for eligibility and assess study quality using the QUADAS-2 tool. Data for a 2â¨‰2 contingency table will be extracted. If possible, we will pool sensitivity and specificity data using the random-effects model and construct a summary receiver operating characteristic curve. In case of high heterogeneity, we will present the findings narratively. Subgroup analysis and sensitivity analysis will be performed where appropriate. This scoping review aims to provide an overview of smartphone ophthalmoscopy in DR identification. It will present findings on the accuracy of smartphone ophthalmoscopy in detecting DR, identify gaps in the literature and provide recommendations for future research. This review does not require ethical approval as we will not collect primary data.","Tan, C. H.
 and Quah, W. H.
 and Tan, C. S. H.
 and Smith, H.
 and Tudor Car, L.","Tan, Quah, Tan, Smith, Tudor Car",https://dx.doi.org/10.1136/bmjopen-2018-028811,https://doi.org/10.1136/bmjopen-2018-028811,2021-08-03
1151.0,,pubmed,Neer Type-II Distal Clavicle Fractures: A Cost-Effectiveness Analysis of Fixation Techniques,Neer Type-II Distal Clavicle Fractures: A Cost-Effectiveness Analysis of Fixation Techniques,"BACKGROUND: Neer type-II distal clavicle fractures are unstable and are generally appropriately managed with operative fixation. Fixation options include locking plates, hook plates, and suture button devices. No consensus on optimal technique exists. METHODS: A decision tree model was created describing fixation of Neer type-II fractures using hook plates, locking plates, or suture buttons. Outcomes included uneventful healing, symptomatic implant removal, deep infection requiring debridement, and nonunion requiring revision. Weighted averages derived from a systematic review were used for probabilities. Cost-effectiveness was evaluated by calculating incremental cost-effectiveness ratios (ICERs). The ICER is defined as the ratio of the difference in cost and difference in effectiveness of each strategy, and is measured in cost per quality-adjusted life year (QALY). The model was evaluated using thresholds of $50,000/QALY and $100,000/QALY. Sensitivity analysis was performed on all outcome probabilities for each fixation strategy to assess cost-effectiveness across a range of values. RESULTS: Forty-three papers met final inclusion criteria. Using suture buttons as the reference case in the health-care cost model, suture button repair was dominant (both less expensive and clinically superior). Hook plates cost substantially more ($5,360.52) compared with suture buttons and locking plates ($3,713.50 and $4,007.44, respectively). Suture buttons and locking plates yielded similar clinical outcomes (0.92 and 0.91 QALY, respectively). Suture button dominance persisted in the societal perspective model. Sensitivity analysis on outcome probabilities showed that locking plates became the most cost-effective strategy if the revision rate after their use was lowered to 2.2%, from the overall average in the sources of >19%. No other changes in outcome probabilities for any of the 3 techniques allowed suture buttons to be surpassed as the most cost-effective. CONCLUSIONS: The cost-effectiveness of suture buttons is driven by low revision rates and high uneventful healing rates. Similar QALY values for locking plate and suture button fixation were observed, which is consistent with existing literature that has failed to identify either as the clinically superior technique. Cost-effectiveness should fit prominently into the decision-making rubric for these injuries. LEVEL OF EVIDENCE: Economic Level IV. See Instructions for Authors for a complete description of levels of evidence.","Neer type-II distal clavicle fractures are unstable and are generally appropriately managed with operative fixation. Fixation options include locking plates, hook plates, and suture button devices. No consensus on optimal technique exists. A decision tree model was created describing fixation of Neer type-II fractures using hook plates, locking plates, or suture buttons. Outcomes included uneventful healing, symptomatic implant removal, deep infection requiring debridement, and nonunion requiring revision. Weighted averages derived from a systematic review were used for probabilities. Cost-effectiveness was evaluated by calculating incremental cost-effectiveness ratios (ICERs). The ICER is defined as the ratio of the difference in cost and difference in effectiveness of each strategy, and is measured in cost per quality-adjusted life year (QALY). The model was evaluated using thresholds of $50,000/QALY and $100,000/QALY. Sensitivity analysis was performed on all outcome probabilities for each fixation strategy to assess cost-effectiveness across a range of values. Forty-three papers met final inclusion criteria. Using suture buttons as the reference case in the health-care cost model, suture button repair was dominant (both less expensive and clinically superior). Hook plates cost substantially more ($5,360.52) compared with suture buttons and locking plates ($3,713.50 and $4,007.44, respectively). Suture buttons and locking plates yielded similar clinical outcomes (0.92 and 0.91 QALY, respectively). Suture button dominance persisted in the societal perspective model. Sensitivity analysis on outcome probabilities showed that locking plates became the most cost-effective strategy if the revision rate after their use was lowered to 2.2%, from the overall average in the sources of &gt;19%. No other changes in outcome probabilities for any of the 3 techniques allowed suture buttons to be surpassed as the most cost-effective. The cost-effectiveness of suture buttons is driven by low revision rates and high uneventful healing rates. Similar QALY values for locking plate and suture button fixation were observed, which is consistent with existing literature that has failed to identify either as the clinically superior technique. Cost-effectiveness should fit prominently into the decision-making rubric for these injuries. Economic Level IV. See Instructions for Authors for a complete description of levels of evidence.","Fox, H. M.
 and Ramsey, D. C.
 and Thompson, A. R.
 and Hoekstra, C. J.
 and Mirarchi, A. J.
 and Nazir, O. F.","Fox, Ramsey, Thompson, Hoekstra, Mirarchi, Nazir",https://dx.doi.org/10.2106/JBJS.19.00590,https://doi.org/10.2106/JBJS.19.00590,2021-08-03
2248.0,,pubmed,What is an Appropriate Etching Time For Sealant Application on Permanent Molars? Results from a Meta-Analysis,What is an Appropriate Etching Time For Sealant Application on Permanent Molars? Results from a Meta-Analysis,"PURPOSE: This meta-analysis investigated the influence of different acid etching times on the retention rate of pit-and-fissure sealants based on clinical trials with a minimum duration of two years. MATERIALS AND METHODS: A literature search was carried out in electronic databases along with hand searching to identify clinical trials that evaluated pit-and-fissure sealants in permanent molars. From 1280 identified abstracts, 195 studies were selected for full-text analysis, and 28 studies with 36 test groups were included in this meta-analysis. Test groups with etching times of 15 (n = 3), 20 (n = 2), 30 (n = 10), 40 (n = 1) and 60 s (n = 20) were found. Incidence rates of pit-and-fissure sealant losses were modelled using negative binomial regression. RESULTS: The regression analysis did not reveal a significant influence of etching time on the survival of pit-and-fissure sealants based on the identified and included clinical trials. CONCLUSIONS: Due to the limited number of clinical data for 15 and 20 s, conclusions regarding very short acid etching times were not possible. On the basis of regression analysis, a minimum of 30-s acid etching might be sufficient prior to fissure sealing.","This meta-analysis investigated the influence of different acid etching times on the retention rate of pit-and-fissure sealants based on clinical trials with a minimum duration of two years. A literature search was carried out in electronic databases along with hand searching to identify clinical trials that evaluated pit-and-fissure sealants in permanent molars. From 1280 identified abstracts, 195 studies were selected for full-text analysis, and 28 studies with 36 test groups were included in this meta-analysis. Test groups with etching times of 15 (n = 3), 20 (n = 2), 30 (n = 10), 40 (n = 1) and 60 s (n = 20) were found. Incidence rates of pit-and-fissure sealant losses were modelled using negative binomial regression. The regression analysis did not reveal a significant influence of etching time on the survival of pit-and-fissure sealants based on the identified and included clinical trials. Due to the limited number of clinical data for 15 and 20 s, conclusions regarding very short acid etching times were not possible. On the basis of regression analysis, a minimum of 30-s acid etching might be sufficient prior to fissure sealing.","Lo, Y. F.
 and Crispin, A.
 and Kessler, A.
 and Hickel, R.
 and Kuhnisch, J.","Lo, Crispin, Kessler, Hickel, KÃ¼hnisch",https://dx.doi.org/10.3290/j.jad.a43181,https://doi.org/10.3290/j.jad.a43181,2021-08-03
4207.0,,pubmed,Deep learning in clinical natural language processing: a methodical review,Deep learning in clinical natural language processing: a methodical review,"OBJECTIVE: This article methodically reviews the literature on deep learning (DL) for natural language processing (NLP) in the clinical domain, providing quantitative analysis to answer 3 research questions concerning methods, scope, and context of current research. MATERIALS AND METHODS: We searched MEDLINE, EMBASE, Scopus, the Association for Computing Machinery Digital Library, and the Association for Computational Linguistics Anthology for articles using DL-based approaches to NLP problems in electronic health records. After screening 1,737 articles, we collected data on 25 variables across 212 papers. RESULTS: DL in clinical NLP publications more than doubled each year, through 2018. Recurrent neural networks (60.8%) and word2vec embeddings (74.1%) were the most popular methods; the information extraction tasks of text classification, named entity recognition, and relation extraction were dominant (89.2%). However, there was a 'long tail' of other methods and specific tasks. Most contributions were methodological variants or applications, but 20.8% were new methods of some kind. The earliest adopters were in the NLP community, but the medical informatics community was the most prolific. DISCUSSION: Our analysis shows growing acceptance of deep learning as a baseline for NLP research, and of DL-based NLP in the medical community. A number of common associations were substantiated (eg, the preference of recurrent neural networks for sequence-labeling named entity recognition), while others were surprisingly nuanced (eg, the scarcity of French language clinical NLP with deep learning). CONCLUSION: Deep learning has not yet fully penetrated clinical NLP and is growing rapidly. This review highlighted both the popular and unique trends in this active field.","This article methodically reviews the literature on deep learning (DL) for natural language processing (NLP) in the clinical domain, providing quantitative analysis to answer 3 research questions concerning methods, scope, and context of current research. We searched MEDLINE, EMBASE, Scopus, the Association for Computing Machinery Digital Library, and the Association for Computational Linguistics Anthology for articles using DL-based approaches to NLP problems in electronic health records. After screening 1,737 articles, we collected data on 25 variables across 212 papers. DL in clinical NLP publications more than doubled each year, through 2018. Recurrent neural networks (60.8%) and word2vec embeddings (74.1%) were the most popular methods; the information extraction tasks of text classification, named entity recognition, and relation extraction were dominant (89.2%). However, there was a ""long tail"" of other methods and specific tasks. Most contributions were methodological variants or applications, but 20.8% were new methods of some kind. The earliest adopters were in the NLP community, but the medical informatics community was the most prolific. Our analysis shows growing acceptance of deep learning as a baseline for NLP research, and of DL-based NLP in the medical community. A number of common associations were substantiated (eg, the preference of recurrent neural networks for sequence-labeling named entity recognition), while others were surprisingly nuanced (eg, the scarcity of French language clinical NLP with deep learning). Deep learning has not yet fully penetrated clinical NLP and is growing rapidly. This review highlighted both the popular and unique trends in this active field.","Wu, S.
 and Roberts, K.
 and Datta, S.
 and Du, J.
 and Ji, Z.
 and Si, Y.
 and Soni, S.
 and Wang, Q.
 and Wei, Q.
 and Xiang, Y.
 and Zhao, B.
 and Xu, H.","Wu, Roberts, Datta, Du, Ji, Si, Soni, Wang, Wei, Xiang, Zhao, Xu",https://dx.doi.org/10.1093/jamia/ocz200,https://doi.org/10.1093/jamia/ocz200,2021-08-03
1365.0,,pubmed,Brain pathology identification using computer aided diagnostic tool: A systematic review,Brain pathology identification using computer aided diagnostic tool: A systematic review,"Computer aided diagnostic (CAD) has become a significant tool in expanding patient quality-of-life by reducing human errors in diagnosis. CAD can expedite decision-making on complex clinical data automatically. Since brain diseases can be fatal, rapid identification of brain pathology to prolong patient life is an important research topic. Many algorithms have been proposed for efficient brain pathology identification (BPI) over the past decade. Constant refinement of the various image processing algorithms must take place to expand performance of the automatic BPI task. In this paper, a systematic survey of contemporary BPI algorithms using brain magnetic resonance imaging (MRI) is presented. A summarization of recent literature provides investigators with a helpful synopsis of the domain. Furthermore, to enhance the performance of BPI, future research directions are indicated.","Computer aided diagnostic (CAD) has become a significant tool in expanding patient quality-of-life by reducing human errors in diagnosis. CAD can expedite decision-making on complex clinical data automatically. Since brain diseases can be fatal, rapid identification of brain pathology to prolong patient life is an important research topic. Many algorithms have been proposed for efficient brain pathology identification (BPI) over the past decade. Constant refinement of the various image processing algorithms must take place to expand performance of the automatic BPI task. In this paper, a systematic survey of contemporary BPI algorithms using brain magnetic resonance imaging (MRI) is presented. A summarization of recent literature provides investigators with a helpful synopsis of the domain. Furthermore, to enhance the performance of BPI, future research directions are indicated.","Gudigar, A.
 and Raghavendra, U.
 and Hegde, A.
 and Kalyani, M.
 and Ciaccio, E. J.
 and Rajendra Acharya, U.","Gudigar, Raghavendra, Hegde, Kalyani, Ciaccio, Rajendra Acharya",https://dx.doi.org/10.1016/j.cmpb.2019.105205,https://doi.org/10.1016/j.cmpb.2019.105205,2021-08-03
3070.0,,pubmed,"Vortioxetine treatment for generalised anxiety disorder: a meta-analysis of anxiety, quality of life and safety outcomes","Vortioxetine treatment for generalised anxiety disorder: a meta-analysis of anxiety, quality of life and safety outcomes","OBJECTIVES: The aim of this study was to investigate the efficacy, tolerability, safety, and impact on quality of life (QoL) and functional status of vortioxetine treatment for patients with generalised anxiety disorder (GAD) by performing a meta-analysis of randomised controlled trials (RCTs). DESIGN: Systematic review and meta-analysis. DATA SOURCES: Data mining was conducted in January 2019 across PubMed, EMBASE, PsycINFO, Cochrane Central Register of Controlled Trials Cochrane Library, Web of science and ClinicalTrials.gov. ELIGIBILITY CRITERIA FOR SELECTING STUDIES: All published RCTs, which assessed the effect of vortioxetine treatment for patients with GAD when compared with a placebo group, were included. DATA EXTRACTION AND SYNTHESIS: Relevant data were extracted and synthesised narratively. Results were expressed as standardised mean differences or ORs with 95% CIs. RESULTS: Our meta-analysis showed that multiple doses (2.5, 5 and 10 mg/day) of vortioxetine did not significantly improve the response rates, compared with placebo (OR 1.16, 95% CI 0.84 to 1.60, p=0.38; OR 1.41, 95% CI 0.82 to 2.41, p=0.21; and OR 1.05, 95% CI 0.76 to 1.46, p=0.75). Moreover, there was no statistically significant difference regarding the remission rates, discontinuation for any reason rates, discontinuation due to adverse events rates, Short-Form 36 Health Survey scores or Sheehan Disability Scale scores between administration of multiple doses (2.5, 5 and 10 mg/day) of vortioxetine and placebo. CONCLUSIONS: Although our results suggest that vortioxetine did not improve the GAD symptoms, QoL and functional status impairment of patients with GAD, it was safe and well tolerated. Clinicians should interpret and translate our data with caution, as the meta-analysis was based on a limited number of RCTs.","The aim of this study was to investigate the efficacy, tolerability, safety, and impact on quality of life (QoL) and functional status of vortioxetine treatment for patients with generalised anxiety disorder (GAD) by performing a meta-analysis of randomised controlled trials (RCTs). Systematic review and meta-analysis. Data mining was conducted in January 2019 across PubMed, EMBASE, PsycINFO, Cochrane Central Register of Controlled Trials Cochrane Library, Web of science and ClinicalTrials.gov. All published RCTs, which assessed the effect of vortioxetine treatment for patients with GAD when compared with a placebo group, were included. Relevant data were extracted and synthesised narratively. Results were expressed as standardised mean differences or ORs with 95% CIs. Our meta-analysis showed that multiple doses (2.5, 5 and 10â€‰mg/day) of vortioxetine did not significantly improve the response rates, compared with placebo (OR 1.16, 95% CI 0.84 to 1.60, p=0.38; OR 1.41, 95% CI 0.82 to 2.41, p=0.21; and OR 1.05, 95% CI 0.76 to 1.46, p=0.75). Moreover, there was no statistically significant difference regarding the remission rates, discontinuation for any reason rates, discontinuation due to adverse events rates, Short-Form 36 Health Survey scores or Sheehan Disability Scale scores between administration of multiple doses (2.5, 5 and 10â€‰mg/day) of vortioxetine and placebo. Although our results suggest that vortioxetine did not improve the GAD symptoms, QoL and functional status impairment of patients with GAD, it was safe and well tolerated. Clinicians should interpret and translate our data with caution, as the meta-analysis was based on a limited number of RCTs.","Qin, B.
 and Huang, G.
 and Yang, Q.
 and Zhao, M.
 and Chen, H.
 and Gao, W.
 and Yang, M.","Qin, Huang, Yang, Zhao, Chen, Gao, Yang",https://dx.doi.org/10.1136/bmjopen-2019-033161,https://doi.org/10.1136/bmjopen-2019-033161,2021-08-03
134.0,,pubmed,Artificial intelligence and robotics: a combination that is changing the operating room,Artificial intelligence and robotics: a combination that is changing the operating room,"PURPOSE: The aim of the current narrative review was to summarize the available evidence in the literature on artificial intelligence (AI) methods that have been applied during robotic surgery. METHODS: A narrative review of the literature was performed on MEDLINE/Pubmed and Scopus database on the topics of artificial intelligence, autonomous surgery, machine learning, robotic surgery, and surgical navigation, focusing on articles published between January 2015 and June 2019. All available evidences were analyzed and summarized herein after an interactive peer-review process of the panel. LITERATURE REVIEW: The preliminary results of the implementation of AI in clinical setting are encouraging. By providing a readout of the full telemetry and a sophisticated viewing console, robot-assisted surgery can be used to study and refine the application of AI in surgical practice. Machine learning approaches strengthen the feedback regarding surgical skills acquisition, efficiency of the surgical process, surgical guidance and prediction of postoperative outcomes. Tension-sensors on the robotic arms and the integration of augmented reality methods can help enhance the surgical experience and monitor organ movements. CONCLUSIONS: The use of AI in robotic surgery is expected to have a significant impact on future surgical training as well as enhance the surgical experience during a procedure. Both aim to realize precision surgery and thus to increase the quality of the surgical care. Implementation of AI in master-slave robotic surgery may allow for the careful, step-by-step consideration of autonomous robotic surgery.","The aim of the current narrative review was to summarize the available evidence in the literature on artificial intelligence (AI) methods that have been applied during robotic surgery. A narrative review of the literature was performed on MEDLINE/Pubmed and Scopus database on the topics of artificial intelligence, autonomous surgery, machine learning, robotic surgery, and surgical navigation, focusing on articles published between January 2015 and June 2019. All available evidences were analyzed and summarized herein after an interactive peer-review process of the panel. The preliminary results of the implementation of AI in clinical setting are encouraging. By providing a readout of the full telemetry and a sophisticated viewing console, robot-assisted surgery can be used to study and refine the application of AI in surgical practice. Machine learning approaches strengthen the feedback regarding surgical skills acquisition, efficiency of the surgical process, surgical guidance and prediction of postoperative outcomes. Tension-sensors on the robotic arms and the integration of augmented reality methods can help enhance the surgical experience and monitor organ movements. The use of AI in robotic surgery is expected to have a significant impact on future surgical training as well as enhance the surgical experience during a procedure. Both aim to realize precision surgery and thus to increase the quality of the surgical care. Implementation of AI in master-slave robotic surgery may allow for the careful, step-by-step consideration of autonomous robotic surgery.","Andras, I.
 and Mazzone, E.
 and van Leeuwen, F. W. B.
 and De Naeyer, G.
 and van Oosterom, M. N.
 and Beato, S.
 and Buckle, T.
 and O'Sullivan, S.
 and van Leeuwen, P. J.
 and Beulens, A.
 and Crisan, N.
 and D'Hondt, F.
 and Schatteman, P.
 and van Der Poel, H.
 and Dell'Oglio, P.
 and Mottrie, A.","Andras, Mazzone, van Leeuwen, De Naeyer, van Oosterom, Beato, Buckle, O'Sullivan, van Leeuwen, Beulens, Crisan, D'Hondt, Schatteman, van Der Poel, Dell'Oglio, Mottrie",https://dx.doi.org/10.1007/s00345-019-03037-6,https://doi.org/10.1007/s00345-019-03037-6,2021-08-03
564.0,,pubmed,Discovering novel disease comorbidities using electronic medical records,Discovering novel disease comorbidities using electronic medical records,"Increasing reliance on electronic medical records at large medical centers provides unique opportunities to perform population level analyses exploring disease progression and etiology. The massive accumulation of diagnostic, procedure, and laboratory codes in one place has enabled the exploration of co-occurring conditions, their risk factors, and potential prognostic factors. While most of the readily identifiable associations in medical records are (now) well known to the scientific community, there is no doubt many more relationships are still to be uncovered in EMR data. In this paper, we introduce a novel finding index to help with that task. This new index uses data mined from real-time PubMed abstracts to indicate the extent to which empirically discovered associations are already known (i.e., present in the scientific literature). Our methods leverage second-generation p-values, which better identify associations that are truly clinically meaningful. We illustrate our new method with three examples: Autism Spectrum Disorder, Alzheimer's Disease, and Optic Neuritis. Our results demonstrate wide utility for identifying new associations in EMR data that have the highest priority among the complex web of correlations and causalities. Data scientists and clinicians can work together more effectively to discover novel associations that are both empirically reliable and clinically understudied.","Increasing reliance on electronic medical records at large medical centers provides unique opportunities to perform population level analyses exploring disease progression and etiology. The massive accumulation of diagnostic, procedure, and laboratory codes in one place has enabled the exploration of co-occurring conditions, their risk factors, and potential prognostic factors. While most of the readily identifiable associations in medical records are (now) well known to the scientific community, there is no doubt many more relationships are still to be uncovered in EMR data. In this paper, we introduce a novel finding index to help with that task. This new index uses data mined from real-time PubMed abstracts to indicate the extent to which empirically discovered associations are already known (i.e., present in the scientific literature). Our methods leverage second-generation p-values, which better identify associations that are truly clinically meaningful. We illustrate our new method with three examples: Autism Spectrum Disorder, Alzheimer's Disease, and Optic Neuritis. Our results demonstrate wide utility for identifying new associations in EMR data that have the highest priority among the complex web of correlations and causalities. Data scientists and clinicians can work together more effectively to discover novel associations that are both empirically reliable and clinically understudied.","Chaganti, S.
 and Welty, V. F.
 and Taylor, W.
 and Albert, K.
 and Failla, M. D.
 and Cascio, C.
 and Smith, S.
 and Mawn, L.
 and Resnick, S. M.
 and Beason-Held, L. L.
 and Bagnato, F.
 and Lasko, T.
 and Blume, J. D.
 and Landman, B. A.","Chaganti, Welty, Taylor, Albert, Failla, Cascio, Smith, Mawn, Resnick, Beason-Held, Bagnato, Lasko, Blume, Landman",https://dx.doi.org/10.1371/journal.pone.0225495,https://doi.org/10.1371/journal.pone.0225495,2021-08-03
1836.0,,pubmed,Management of Teeth in the Line of Mandibular Angle Fractures Treated with Open Reduction and Internal Fixation: A Systematic Review and Meta-Analysis,Management of Teeth in the Line of Mandibular Angle Fractures Treated with Open Reduction and Internal Fixation: A Systematic Review and Meta-Analysis,"BACKGROUND: Mandibular angle fractures are common and frequently involve a tooth in the fracture line. Despite trends toward more conservative indications for tooth extraction during open repair, the literature remains heterogeneous. This review aims to ascertain the effect of tooth extraction/retention on patient outcomes following mandible open reduction and internal fixation and to evaluate the evidence surrounding indications for extraction. METHODS: PubMed, EMBASE, the Cochrane Library, Elsevier text mining tool database, and clinicaltrials.gov were queried through March of 2018 for English language publication on adults with traumatic mandibular fractures. The review protocol was not registered online. Quality of evidence was assigned using the Grading of Recommendations Assessment, Development and Evaluation methodology. Meta-analyses were performed when definitions of outcomes were deemed similar. RESULTS: Overall, 26 of 1212 identified studies met inclusion criteria. Indications for tooth extraction and rates of extraction varied considerably across studies. The quality of evidence was low or very low for all outcomes. Tooth retention was associated with lower overall complications (OR, 0.54; 95 percent CI, 0.37 to 0.79), major complications requiring readmission or reoperation (OR, 0.47; 95 percent CI, 0.24 to 0.92), and malocclusion (OR, 0.56; 95 percent CI, 0.32 to 0.97); there was no difference in wound issues or nonunion. Removal of asymptomatic teeth was associated with inferior alveolar nerve injury (39.4 percent versus 16.1 percent). CONCLUSIONS: The literature is limited by retrospective study deign and poor follow-up; however, when indicated, tooth extraction is not associated with an increased risk of infection or nonunion. Removal of asymptomatic teeth was associated with a risk of inferior alveolar nerve injury. Additional high-quality studies are needed to evaluate potentially expanded indications for tooth extraction.","Mandibular angle fractures are common and frequently involve a tooth in the fracture line. Despite trends toward more conservative indications for tooth extraction during open repair, the literature remains heterogeneous. This review aims to ascertain the effect of tooth extraction/retention on patient outcomes following mandible open reduction and internal fixation and to evaluate the evidence surrounding indications for extraction. PubMed, EMBASE, the Cochrane Library, Elsevier text mining tool database, and clinicaltrials.gov were queried through March of 2018 for English language publication on adults with traumatic mandibular fractures. The review protocol was not registered online. Quality of evidence was assigned using the Grading of Recommendations Assessment, Development and Evaluation methodology. Meta-analyses were performed when definitions of outcomes were deemed similar. Overall, 26 of 1212 identified studies met inclusion criteria. Indications for tooth extraction and rates of extraction varied considerably across studies. The quality of evidence was low or very low for all outcomes. Tooth retention was associated with lower overall complications (OR, 0.54; 95 percent CI, 0.37 to 0.79), major complications requiring readmission or reoperation (OR, 0.47; 95 percent CI, 0.24 to 0.92), and malocclusion (OR, 0.56; 95 percent CI, 0.32 to 0.97); there was no difference in wound issues or nonunion. Removal of asymptomatic teeth was associated with inferior alveolar nerve injury (39.4 percent versus 16.1 percent). The literature is limited by retrospective study deign and poor follow-up; however, when indicated, tooth extraction is not associated with an increased risk of infection or nonunion. Removal of asymptomatic teeth was associated with a risk of inferior alveolar nerve injury. Additional high-quality studies are needed to evaluate potentially expanded indications for tooth extraction.","Khavanin, N.
 and Jazayeri, H.
 and Xu, T.
 and Pedreira, R.
 and Lopez, J.
 and Reddy, S.
 and Shamliyan, T.
 and Peacock, Z. S.
 and Dorafshar, A. H.","Khavanin, Jazayeri, Xu, Pedreira, Lopez, Reddy, Shamliyan, Peacock, Dorafshar",https://dx.doi.org/10.1097/PRS.0000000000006255,https://doi.org/10.1097/PRS.0000000000006255,2021-08-03
1885.0,,pubmed,Management of incidental pulmonary nodules: current strategies and future perspectives,Management of incidental pulmonary nodules: current strategies and future perspectives,"<b>Introduction</b>: Detection and characterization of pulmonary nodules is an important issue, because the process is the first step in the management of lung cancers. <b>Areas covered</b>: Literature review was performed on May 15 2019 by using the PubMed, US National Library of Medicine National Institutes of Health, and the National Center for Biotechnology information. CT features helping identify the druggable mutations and predict the prognosis of malignant nodules were presented. Technical advancements in MRI and PET/CT were introduced for providing functional information about malignant nodules. Advances in various tissue biopsy techniques enabling molecular analysis and histologic diagnosis of indeterminate nodules were also presented. New techniques such as radiomics, deep learning (DL) technology, and artificial intelligence showing promise in differentiating between malignant and benign nodules were summarized. Recently, updated management guidelines for solid and subsolid nodules incidentally detected on CT were described. Risk stratification and prediction models for indeterminate nodules under active investigation were briefly summarized. <b>Expert opinion</b>: Advancement in CT knowledge has led to a better correlation between CT features and genomic alterations or tumor histology. Recent advances like PET/CT, MRI, radiomics, and DL-based approach have shown promising results in the characterization and prognostication of pulmonary nodules.","<b>Introduction</b>: Detection and characterization of pulmonary nodules is an important issue, because the process is the first step in the management of lung cancers.<b>Areas covered</b>: Literature review was performed on May 15 2019 by using the PubMed, US National Library of Medicine National Institutes of Health, and the National Center for Biotechnology information. CT features helping identify the druggable mutations and predict the prognosis of malignant nodules were presented. Technical advancements in MRI and PET/CT were introduced for providing functional information about malignant nodules. Advances in various tissue biopsy techniques enabling molecular analysis and histologic diagnosis of indeterminate nodules were also presented. New techniques such as radiomics, deep learning (DL) technology, and artificial intelligence showing promise in differentiating between malignant and benign nodules were summarized. Recently, updated management guidelines for solid and subsolid nodules incidentally detected on CT were described. Risk stratification and prediction models for indeterminate nodules under active investigation were briefly summarized.<b>Expert opinion</b>: Advancement in CT knowledge has led to a better correlation between CT features and genomic alterations or tumor histology. Recent advances like PET/CT, MRI, radiomics, and DL-based approach have shown promising results in the characterization and prognostication of pulmonary nodules.","Kim, T. J.
 and Kim, C. H.
 and Lee, H. Y.
 and Chung, M. J.
 and Shin, S. H.
 and Lee, K. J.
 and Lee, K. S.","Kim, Kim, Lee, Chung, Shin, Lee, Lee",https://dx.doi.org/10.1080/17476348.2020.1697853,https://doi.org/10.1080/17476348.2020.1697853,2021-08-03
237.0,,pubmed,Cost-Effectiveness of Therapeutic Use of Safety-Engineered Syringes in Healthcare Facilities in India,Cost-Effectiveness of Therapeutic Use of Safety-Engineered Syringes in Healthcare Facilities in India,"BACKGROUND: Globally, 16 billion injections are administered each year of which 95% are for curative care. India contributes 25-30% of the global injection load. Over 63% of these injections are reportedly unsafe or deemed unnecessary. OBJECTIVES: To assess the incremental cost per quality-adjusted life-year (QALY) gained with the introduction of safety-engineered syringes (SES) as compared to disposable syringes for therapeutic care in India. METHODS: A decision tree was used to compute the volume of needle-stick injuries (NSIs) and reuse episodes among healthcare professionals and the patient population. Subsequently, three separate Markov models were used to compute lifetime costs and QALYs for individuals infected with hepatitis B virus (HBV), hepatitis C virus (HCV) and human immunodeficiency virus (HIV). Three SES were evaluated-reuse prevention syringe (RUP), sharp injury prevention (SIP) syringe, and syringes with features of both RUP and SIP. A lifetime study horizon starting from a base year of 2017 was considered appropriate to cover all costs and consequences comprehensively. A systematic review was undertaken to assess the SES effects in terms of reduction in NSIs and reuse episodes. These were then modelled in terms of reduction in transmission of blood-borne infections, life-years and QALYs gained. Future costs and consequences were discounted at the rate of 3%. Incremental cost per QALY gained was computed to assess the cost-effectiveness. A probabilistic sensitivity analysis was undertaken to account for parameter uncertainties. RESULTS: The introduction of RUP, SIP and RUP + SIP syringes in India is estimated to incur an incremental cost of Indian National Rupee (INR) 61,028 (US$939), INR 7,768,215 (US$119,511) and INR 196,135 (US$3017) per QALY gained, respectively. A total of 96,296 HBV, 44,082 HCV and 5632 HIV deaths are estimated to be averted due to RUP in 20 years. RUP has an 84% probability to be cost-effective at a threshold of per capita gross domestic product (GDP). The RUP syringe can become cost saving at a unit price of INR 1.9. Similarly, SIP and RUP + SIP syringes can be cost-effective at a unit price of less than INR 1.2 and INR 5.9, respectively. CONCLUSION: RUP syringes are estimated to be cost-effective in the Indian context. SIP and RUP + SIP syringes are not cost-effective at the current unit prices. Efforts should be made to bring down the price of SES to improve its cost-effectiveness.","Globally, 16 billion injections are administered each year of which 95% are for curative care. India contributes 25-30% of the global injection load. Over 63% of these injections are reportedly unsafe or deemed unnecessary. To assess the incremental cost per quality-adjusted life-year (QALY) gained with the introduction of safety-engineered syringes (SES) as compared to disposable syringes for therapeutic care in India. A decision tree was used to compute the volume of needle-stick injuries (NSIs) and reuse episodes among healthcare professionals and the patient population. Subsequently, three separate Markov models were used to compute lifetime costs and QALYs for individuals infected with hepatitis B virus (HBV), hepatitis C virus (HCV) and human immunodeficiency virus (HIV). Three SES were evaluated-reuse prevention syringe (RUP), sharp injury prevention (SIP) syringe, and syringes with features of both RUP and SIP. A lifetime study horizon starting from a base year of 2017 was considered appropriate to cover all costs and consequences comprehensively. A systematic review was undertaken to assess the SES effects in terms of reduction in NSIs and reuse episodes. These were then modelled in terms of reduction in transmission of blood-borne infections, life-years and QALYs gained. Future costs and consequences were discounted at the rate of 3%. Incremental cost per QALY gained was computed to assess the cost-effectiveness. A probabilistic sensitivity analysis was undertaken to account for parameter uncertainties. The introduction of RUP, SIP and RUPâ€‰+â€‰SIP syringes in India is estimated to incur an incremental cost of Indian National Rupee (INR) 61,028 (US$939), INR 7,768,215 (US$119,511) and INR 196,135 (US$3017) per QALY gained, respectively. A total of 96,296 HBV, 44,082 HCV and 5632 HIV deaths are estimated to be averted due to RUP in 20Â years. RUP has an 84% probability to be cost-effective at a threshold of per capita gross domestic product (GDP). The RUP syringe can become cost saving at a unit price of INR 1.9. Similarly, SIP and RUPâ€‰+â€‰SIP syringes can be cost-effective at a unit price of less than INR 1.2 and INR 5.9, respectively. RUP syringes are estimated to be cost-effective in the Indian context. SIP and RUPâ€‰+â€‰SIP syringes are not cost-effective at the current unit prices. Efforts should be made to bring down the price of SES to improve its cost-effectiveness.","Bahuguna, P.
 and Prinja, S.
 and Lahariya, C.
 and Dhiman, R. K.
 and Kumar, M. P.
 and Sharma, V.
 and Aggarwal, A. K.
 and Bhaskar, R.
 and De Graeve, H.
 and Bekedam, H.","Bahuguna, Prinja, Lahariya, Dhiman, Kumar, Sharma, Aggarwal, Bhaskar, De Graeve, Bekedam",https://dx.doi.org/10.1007/s40258-019-00536-w,https://doi.org/10.1007/s40258-019-00536-w,2021-08-03
294.0,,pubmed,Anticoagulant Reversal Strategies in the Emergency Department Setting: Recommendations of a Multidisciplinary Expert Panel,Anticoagulant Reversal Strategies in the Emergency Department Setting: Recommendations of a Multidisciplinary Expert Panel,"Bleeding is the most common complication of anticoagulant use. The evaluation and management of the bleeding patient is a core competency of emergency medicine. As the prevalence of patients receiving anticoagulant agents and variety of anticoagulants with different mechanisms of action, pharmacokinetics, indications, and corresponding reversal agents increase, physicians and other clinicians working in the emergency department require a current and nuanced understanding of how best to assess, treat, and reverse anticoagulated patients. In this project, we convened an expert panel to create a consensus decision tree and framework for assessment of the bleeding patient receiving an anticoagulant, as well as use of anticoagulant reversal or coagulation factor replacement, and to address controversies and gaps relevant to this topic. To support decision tree interpretation, the panel also reached agreement on key definitions of life-threatening bleeding, bleeding at a critical site, and emergency surgery or urgent invasive procedure. To reach consensus recommendations, we used a structured literature review and a modified Delphi technique by an expert panel of academic and community physicians with training in emergency medicine, cardiology, hematology, internal medicine/thrombology, pharmacology, toxicology, transfusion medicine and hemostasis, neurology, and surgery, and by other key stakeholder groups.","Bleeding is the most common complication of anticoagulant use. The evaluation and management of the bleeding patient is a core competency of emergency medicine. As the prevalence of patients receiving anticoagulant agents and variety of anticoagulants with different mechanisms of action, pharmacokinetics, indications, and corresponding reversal agents increase, physicians and other clinicians working in the emergency department require a current and nuanced understanding of how best to assess, treat, and reverse anticoagulated patients. In this project, we convened an expert panel to create a consensus decision tree and framework for assessment of the bleeding patient receiving an anticoagulant, as well as use of anticoagulant reversal or coagulation factor replacement, and to address controversies and gaps relevant to this topic. To support decision tree interpretation, the panel also reached agreement on key definitions of life-threatening bleeding, bleeding at a critical site, and emergency surgery or urgent invasive procedure. To reach consensus recommendations, we used a structured literature review and a modified Delphi technique by an expert panel of academic and community physicians with training in emergency medicine, cardiology, hematology, internal medicine/thrombology, pharmacology, toxicology, transfusion medicine and hemostasis, neurology, and surgery, and by other key stakeholder groups.","Baugh, C. W.
 and Levine, M.
 and Cornutt, D.
 and Wilson, J. W.
 and Kwun, R.
 and Mahan, C. E.
 and Pollack, C. V., Jr.
 and Marcolini, E. G.
 and Milling, T. J., Jr.
 and Peacock, W. F.
 and Rosovsky, R. P.
 and Wu, F.
 and Sarode, R.
 and Spyropoulos, A. C.
 and Villines, T. C.
 and Woods, T. D.
 and McManus, J.
 and Williams, J.","Baugh, Levine, Cornutt, Wilson, Kwun, Mahan, Pollack, Marcolini, Milling, Peacock, Rosovsky, Wu, Sarode, Spyropoulos, Villines, Woods, McManus, Williams",https://dx.doi.org/10.1016/j.annemergmed.2019.09.001,https://doi.org/10.1016/j.annemergmed.2019.09.001,2021-08-03
2872.0,,pubmed,Measurement reliability of automated oscillometric blood pressure monitor in the elderly with atrial fibrillation: a systematic review and meta-analysis,Measurement reliability of automated oscillometric blood pressure monitor in the elderly with atrial fibrillation: a systematic review and meta-analysis,"OBJECTIVES: This study aimed to identify whether automated oscillometric blood pressure monitor (AOBPM) is a reliable blood pressure (BP) measurement tool in geriatric patients with atrial fibrillation (AF) with high variability in BP and to evaluate whether it can be applied in practice. METHODS: Electronic searches were performed in databases including MEDLINE, EMBASE, the Cochrane Library, and CINAHL by using the following keywords: 'atrial fibrillation,' 'atrial flutter, 'blood pressure monitor', 'sphygmomanometer.' The QUADAS-2 was applied to assess the internal validity of selected studies. Meta-analysis was performed using RevMan 5.3 program. DESIGN: Systematic review. RESULTS: We identified 10 studies, including 938 geriatric patients with AF. We compared with the previously used BP measurement method (mainly office) and AOBPM, and the patients with AF were divided into the AF-AF (atrial fibrillation rhythm continued) and AF-SR groups (sinus rhythm recovered). The difference in the systolic BP was -3.0 mmHg [95% confidence interval (CI): -6.58 to 0.59] and -1.62 (95% CI: -6.08 to 2.84) mmHg in the AF-AF and AF-SR groups, respectively. The difference in the diastolic BP was 0.17 (95% CI: -2.90 to 3.25) mmHg and -0.23 (95% CI: -5.11 to 4.65) mmHg, respectively. CONCLUSION: This review showed that the BP difference from AOBPM compared with the auscultatory BP method was less than 5 mmHg in the elderly with AF. This difference is acceptable in clinical practice. However, AOBPM compared with invasive arterial BP in the diastolic BP was a difference of 5 mmHg or more, and so its accuracy cannot be assured.","This study aimed to identify whether automated oscillometric blood pressure monitor (AOBPM) is a reliable blood pressure (BP) measurement tool in geriatric patients with atrial fibrillation (AF) with high variability in BP and to evaluate whether it can be applied in practice. Electronic searches were performed in databases including MEDLINE, EMBASE, the Cochrane Library, and CINAHL by using the following keywords: 'atrial fibrillation,' 'atrial flutter, 'blood pressure monitor', 'sphygmomanometer.' The QUADAS-2 was applied to assess the internal validity of selected studies. Meta-analysis was performed using RevMan 5.3 program. Systematic review. We identified 10 studies, including 938 geriatric patients with AF. We compared with the previously used BP measurement method (mainly office) and AOBPM, and the patients with AF were divided into the AF-AF (atrial fibrillation rhythm continued) and AF-SR groups (sinus rhythm recovered). The difference in the systolic BP was -3.0 mmHg [95% confidence interval (CI): -6.58 to 0.59] and -1.62 (95% CI: -6.08 to 2.84) mmHg in the AF-AF and AF-SR groups, respectively. The difference in the diastolic BP was 0.17 (95% CI: -2.90 to 3.25) mmHg and -0.23 (95% CI: -5.11 to 4.65) mmHg, respectively. This review showed that the BP difference from AOBPM compared with the auscultatory BP method was less than 5 mmHg in the elderly with AF. This difference is acceptable in clinical practice. However, AOBPM compared with invasive arterial BP in the diastolic BP was a difference of 5 mmHg or more, and so its accuracy cannot be assured.","Park, S. H.
 and Choi, Y. K.","Park, Choi",https://dx.doi.org/10.1097/MBP.0000000000000414,https://doi.org/10.1097/MBP.0000000000000414,2021-08-03
172.0,,pubmed,Exploring semantic deep learning for building reliable and reusable one health knowledge from PubMed systematic reviews and veterinary clinical notes,Exploring semantic deep learning for building reliable and reusable one health knowledge from PubMed systematic reviews and veterinary clinical notes,"BACKGROUND: Deep Learning opens up opportunities for routinely scanning large bodies of biomedical literature and clinical narratives to represent the meaning of biomedical and clinical terms. However, the validation and integration of this knowledge on a scale requires cross checking with ground truths (i.e. evidence-based resources) that are unavailable in an actionable or computable form. In this paper we explore how to turn information about diagnoses, prognoses, therapies and other clinical concepts into computable knowledge using free-text data about human and animal health. We used a Semantic Deep Learning approach that combines the Semantic Web technologies and Deep Learning to acquire and validate knowledge about 11 well-known medical conditions mined from two sets of unstructured free-text data: 300 K PubMed Systematic Review articles (the PMSB dataset) and 2.5 M veterinary clinical notes (the VetCN dataset). For each target condition we obtained 20 related clinical concepts using two deep learning methods applied separately on the two datasets, resulting in 880 term pairs (target term, candidate term). Each concept, represented by an n-gram, is mapped to UMLS using MetaMap; we also developed a bespoke method for mapping short forms (e.g. abbreviations and acronyms). Existing ontologies were used to formally represent associations. We also create ontological modules and illustrate how the extracted knowledge can be queried. The evaluation was performed using the content within BMJ Best Practice. RESULTS: MetaMap achieves an F measure of 88% (precision 85%, recall 91%) when applied directly to the total of 613 unique candidate terms for the 880 term pairs. When the processing of short forms is included, MetaMap achieves an F measure of 94% (precision 92%, recall 96%). Validation of the term pairs with BMJ Best Practice yields precision between 98 and 99%. CONCLUSIONS: The Semantic Deep Learning approach can transform neural embeddings built from unstructured free-text data into reliable and reusable One Health knowledge using ontologies and content from BMJ Best Practice.","Deep Learning opens up opportunities for routinely scanning large bodies of biomedical literature and clinical narratives to represent the meaning of biomedical and clinical terms. However, the validation and integration of this knowledge on a scale requires cross checking with ground truths (i.e. evidence-based resources) that are unavailable in an actionable or computable form. In this paper we explore how to turn information about diagnoses, prognoses, therapies and other clinical concepts into computable knowledge using free-text data about human and animal health. We used a Semantic Deep Learning approach that combines the Semantic Web technologies and Deep Learning to acquire and validate knowledge about 11 well-known medical conditions mined from two sets of unstructured free-text data: 300â€‰K PubMed Systematic Review articles (the PMSB dataset) and 2.5â€‰M veterinary clinical notes (the VetCN dataset). For each target condition we obtained 20 related clinical concepts using two deep learning methods applied separately on the two datasets, resulting in 880 term pairs (target term, candidate term). Each concept, represented by an n-gram, is mapped to UMLS using MetaMap; we also developed a bespoke method for mapping short forms (e.g. abbreviations and acronyms). Existing ontologies were used to formally represent associations. We also create ontological modules and illustrate how the extracted knowledge can be queried. The evaluation was performed using the content within BMJ Best Practice. MetaMap achieves an F measure of 88% (precision 85%, recall 91%) when applied directly to the total of 613 unique candidate terms for the 880 term pairs. When the processing of short forms is included, MetaMap achieves an F measure of 94% (precision 92%, recall 96%). Validation of the term pairs with BMJ Best Practice yields precision between 98 and 99%. The Semantic Deep Learning approach can transform neural embeddings built from unstructured free-text data into reliable and reusable One Health knowledge using ontologies and content from BMJ Best Practice.","Arguello-Casteleiro, M.
 and Stevens, R.
 and Des-Diz, J.
 and Wroe, C.
 and Fernandez-Prieto, M. J.
 and Maroto, N.
 and Maseda-Fernandez, D.
 and Demetriou, G.
 and Peters, S.
 and Noble, P. M.
 and Jones, P. H.
 and Dukes-McEwan, J.
 and Radford, A. D.
 and Keane, J.
 and Nenadic, G.","Arguello-Casteleiro, Stevens, Des-Diz, Wroe, Fernandez-Prieto, Maroto, Maseda-Fernandez, Demetriou, Peters, Noble, Jones, Dukes-McEwan, Radford, Keane, Nenadic",not available,https://doi.org/10.1186/s13326-019-0212-6,2021-08-03
1380.0,,pubmed,Association of Helicobacter pylori with oral potentially malignant disorders and oral squamous cell carcinoma-a systematic review and meta-analysis,Association of Helicobacter pylori with oral potentially malignant disorders and oral squamous cell carcinoma-a systematic review and meta-analysis,"OBJECTIVE: To assess any potential association between Helicobacter pylori and oral squamous cell carcinoma/oral potentially malignant disorders. MATERIALS AND METHODS: Data mining was done using PubMed, Cochrane Library, and SCOPUS databases. The search included articles published up to May 2019. Newcastle-Ottawa scale was used to score the quality of the included articles. Data including the type of study, the sample population, the type of oral lesion, and the resulting statistical data were extracted. RESULTS: Out of 131 screened articles, only 15 articles fulfilled the eligibility criteria. Among the 15 studies, 9 focused on oral squamous cell carcinoma and 6 focused on oral potentially malignant disorders. Eight out of the 9 oral squamous cell carcinoma studies were included in the meta-analysis. Forest plot was generated using the odds ratio and confidence intervals calculated for each of the included studies. Due to the lack of sufficient studies, the meta-analysis was not performed for oral potentially malignant disorders. CONCLUSION: Due to the contradictory results of the included studies, it was not possible to make any conclusive statement on the potential association of H. pylori with oral squamous cell carcinoma. The variations in the methodology, especially the differences in the sensitivity/specificity of the diagnostic modalities could be the cause for differential results. CLINICAL RELEVANCE: Although the association of H. pylori with oral squamous cell carcinoma could not be confirmed, it is vital to reduce the excess oral microbial load, especially in patients exhibiting oral mucosal changes with no history of associated risk factors.","To assess any potential association between Helicobacter pylori and oral squamous cell carcinoma/oral potentially malignant disorders. Data mining was done using PubMed, Cochrane Library, and SCOPUS databases. The search included articles published up to May 2019. Newcastle-Ottawa scale was used to score the quality of the included articles. Data including the type of study, the sample population, the type of oral lesion, and the resulting statistical data were extracted. Out of 131 screened articles, only 15 articles fulfilled the eligibility criteria. Among the 15 studies, 9 focused on oral squamous cell carcinoma and 6 focused on oral potentially malignant disorders. Eight out of the 9 oral squamous cell carcinoma studies were included in the meta-analysis. Forest plot was generated using the odds ratio and confidence intervals calculated for each of the included studies. Due to the lack of sufficient studies, the meta-analysis was not performed for oral potentially malignant disorders. Due to the contradictory results of the included studies, it was not possible to make any conclusive statement on the potential association of H. pylori with oral squamous cell carcinoma. The variations in the methodology, especially the differences in the sensitivity/specificity of the diagnostic modalities could be the cause for differential results. Although the association of H. pylori with oral squamous cell carcinoma could not be confirmed, it is vital to reduce the excess oral microbial load, especially in patients exhibiting oral mucosal changes with no history of associated risk factors.","Gupta, A. A.
 and Kheur, S.
 and Raj, A. T.
 and Mahajan, P.","Gupta, Kheur, Raj, Mahajan",https://dx.doi.org/10.1007/s00784-019-03125-2,https://doi.org/10.1007/s00784-019-03125-2,2021-08-03
1386.0,,pubmed,Computer-aided diagnosis in rheumatic diseases using ultrasound: an overview,Computer-aided diagnosis in rheumatic diseases using ultrasound: an overview,"Clinical evaluation of rheumatic and musculoskeletal diseases through images is a challenge for the beginner rheumatologist since image diagnosis is an expert task with a long learning curve. The aim of this work was to present a narrative review on the main ultrasound computer-aided diagnosis systems that may help clinicians thanks to the progress made in the application of artificial intelligence techniques. We performed a literature review searching for original articles in seven repositories, from 1970 to 2019, and identified 11 main methods currently used in ultrasound computer-aided diagnosis systems. Also, we found that rheumatoid arthritis, osteoarthritis, systemic lupus erythematosus, and idiopathic inflammatory myopathies are the four musculoskeletal and rheumatic diseases most studied that use these innovative systems, with an overall accuracy of > 75%.","Clinical evaluation of rheumatic and musculoskeletal diseases through images is a challenge for the beginner rheumatologist since image diagnosis is an expert task with a long learning curve. The aim of this work was to present a narrative review on the main ultrasound computer-aided diagnosis systems that may help clinicians thanks to the progress made in the application of artificial intelligence techniques. We performed a literature review searching for original articles in seven repositories, from 1970 to 2019, and identified 11 main methods currently used in ultrasound computer-aided diagnosis systems. Also, we found that rheumatoid arthritis, osteoarthritis, systemic lupus erythematosus, and idiopathic inflammatory myopathies are the four musculoskeletal and rheumatic diseases most studied that use these innovative systems, with an overall accuracy of &gt;â€‰75%.","Gutierrez-Martinez, J.
 and Pineda, C.
 and Sandoval, H.
 and Bernal-Gonzalez, A.","GutiÃ©rrez-MartÃ­nez, Pineda, Sandoval, Bernal-GonzÃ¡lez",https://dx.doi.org/10.1007/s10067-019-04791-z,https://doi.org/10.1007/s10067-019-04791-z,2021-08-03
2219.0,,pubmed,Data Mining of the Reviews from Online Private Doctors,Data Mining of the Reviews from Online Private Doctors,"<b>Background:</b> User-generated content shared in the online health communities (OHCs) is becoming a valuable resource for researchers to understand patients' decision-making behaviors in the management of their health. Many studies have focused on how to obtain useful information from online reviews in OHCs. <b>Introduction:</b> This study focuses on a telemedicine service called Online Private Doctor (OPD), which is offered by a leading Chinese physician review website (PRW). OPD reviews have not received much attention. By data mining the reviews, our goal is to determine what patients are talking about when they use the OPD service and whether they are satisfied with the service or not. <b>Materials and Methods:</b> We used a Python web crawler to collect 41,029 reviews and 84,510 short reviews (labels) of all 5,645 physicians who offered the OPD service on a PRW (haodf.com) in China. Mixed methods (i.e., a literature review, topic discovery, annotation, and a sentiment analysis) were used to determine the information that the OPD reviews are meant to express. <b>Results:</b> We discovered that the OPD reviews can be categorized into four subjects: competence (35.1%), communication (29.4%), treatment (26.0%), and convenience (9.5%). In terms of previously discovered topics, we found that competence, communication, and treatment have been discussed before, but convenience is an emerging subject. The sentiment analysis indicated that 93.67% of the reviews indicated positive emotions, and the area under the receiver operating characteristic (ROC) curve is 0.64. Furthermore, the labels indicated that only 0.72% (603/84,570) of reviews were negative toward the OPD service. The subjects of the labels were distributed according to competence (34.7%), communication (23.8%), treatment (33.5%), and convenience (8.0%). <b>Discussion:</b> The findings of our study suggest that patients who ever used OPD have been quite satisfied with the service. From their reviews, we discovered that OPD has its special characteristics and is convenient. However, it still has some shortcomings, for example, the quality of the phone connection. In terms of both the platform and the doctors, more efforts should be made to make the OPD better and more regulated. <b>Conclusion: </b> OPD is an emerging telemedicine service that still needs more time and space to evolve. For patients, it helps reduce problems such as scheduling and queuing. Therefore, it brings more convenience to people's daily lives. In the future, more attention should be paid to this service, as it is helpful in reducing the uneven distribution of medical resources.","<b>Background:</b> User-generated content shared in the online health communities (OHCs) is becoming a valuable resource for researchers to understand patients' decision-making behaviors in the management of their health. Many studies have focused on how to obtain useful information from online reviews in OHCs. <b>Introduction:</b> This study focuses on a telemedicine service called Online Private Doctor (OPD), which is offered by a leading Chinese physician review website (PRW). OPD reviews have not received much attention. By data mining the reviews, our goal is to determine what patients are talking about when they use the OPD service and whether they are satisfied with the service or not. <b>Materials and Methods:</b> We used a Python web crawler to collect 41,029 reviews and 84,510 short reviews (labels) of all 5,645 physicians who offered the OPD service on a PRW (haodf.com) in China. Mixed methods (i.e., a literature review, topic discovery, annotation, and a sentiment analysis) were used to determine the information that the OPD reviews are meant to express. <b>Results:</b> We discovered that the OPD reviews can be categorized into four subjects: competence (35.1%), communication (29.4%), treatment (26.0%), and convenience (9.5%). In terms of previously discovered topics, we found that competence, communication, and treatment have been discussed before, but convenience is an emerging subject. The sentiment analysis indicated that 93.67% of the reviews indicated positive emotions, and the area under the receiver operating characteristic (ROC) curve is 0.64. Furthermore, the labels indicated that only 0.72% (603/84,570) of reviews were negative toward the OPD service. The subjects of the labels were distributed according to competence (34.7%), communication (23.8%), treatment (33.5%), and convenience (8.0%). <b>Discussion:</b> The findings of our study suggest that patients who ever used OPD have been quite satisfied with the service. From their reviews, we discovered that OPD has its special characteristics and is convenient. However, it still has some shortcomings, for example, the quality of the phone connection. In terms of both the platform and the doctors, more efforts should be made to make the OPD better and more regulated. <b>Conclusion:</b> OPD is an emerging telemedicine service that still needs more time and space to evolve. For patients, it helps reduce problems such as scheduling and queuing. Therefore, it brings more convenience to people's daily lives. In the future, more attention should be paid to this service, as it is helpful in reducing the uneven distribution of medical resources.","Liu, J.
 and Zhang, W.
 and Jiang, X.
 and Zhou, Y.","Liu, Zhang, Jiang, Zhou",not available,https://doi.org/10.1089/tmj.2019.0159,2021-08-03
24.0,,pubmed,Empowering Young Persons with Congenital Heart Disease: Using Intervention Mapping to Develop a Transition Program - The STEPSTONES Project,Empowering Young Persons with Congenital Heart Disease: Using Intervention Mapping to Develop a Transition Program - The STEPSTONES Project,"PURPOSE: Describe the implementation of intervention mapping in the development of a transition program that aims to empower adolescents with congenital heart disease. DESIGN AND METHODS: To gain a better understanding of the problem, we conducted a literature review, focus group and individual interviews, and a cross-sectional survey. This information helped us decide on the scope of the intervention, relevant theories, determinants, formulate performance and change objectives and identify adequate evidence-based change methods. Once the transition program had been designed, effectiveness and process evaluation studies were planned. RESULTS: Young persons with congenital heart disease have insufficient disease-related knowledge, self-management skills and high parental involvement. The transition program involves three meetings with a trained transition coordinator over a two-and-a-half-year period and targets young persons with congenital heart disease and their parents. The transition coordinators use change techniques such as goal-setting, modeling and active learning in order to target three personal determinants (knowledge, self-efficacy and self-management). CONCLUSIONS: The use of intervention mapping may lead to designing interventions tailored to the needs of the targeted population. The transition program described in this paper is currently being evaluated in a hybrid experimental design with simultaneous undertaking of the process evaluation. PRACTICE IMPLICATIONS: This transition program can lead to the empowerment of young persons with congenital heart disease and help them in the process of becoming more responsible for their care. If proven effective, it can be implemented for other chronic conditions.","Describe the implementation of intervention mapping in the development of a transition program that aims to empower adolescents with congenital heart disease. To gain a better understanding of the problem, we conducted a literature review, focus group and individual interviews, and a cross-sectional survey. This information helped us decide on the scope of the intervention, relevant theories, determinants, formulate performance and change objectives and identify adequate evidence-based change methods. Once the transition program had been designed, effectiveness and process evaluation studies were planned. Young persons with congenital heart disease have insufficient disease-related knowledge, self-management skills and high parental involvement. The transition program involves three meetings with a trained transition coordinator over a two-and-a-half-year period and targets young persons with congenital heart disease and their parents. The transition coordinators use change techniques such as goal-setting, modeling and active learning in order to target three personal determinants (knowledge, self-efficacy and self-management). The use of intervention mapping may lead to designing interventions tailored to the needs of the targeted population. The transition program described in this paper is currently being evaluated in a hybrid experimental design with simultaneous undertaking of the process evaluation. This transition program can lead to the empowerment of young persons with congenital heart disease and help them in the process of becoming more responsible for their care. If proven effective, it can be implemented for other chronic conditions.","Acuna Mora, M.
 and Saarijarvi, M.
 and Sparud-Lundin, C.
 and Moons, P.
 and Bratt, E. L.","AcuÃ±a Mora, SaarijÃ¤rvi, Sparud-Lundin, Moons, Bratt",https://dx.doi.org/10.1016/j.pedn.2019.09.021,https://doi.org/10.1016/j.pedn.2019.09.021,2021-08-03
1480.0,,pubmed,Is there an association between cardiorespiratory fitness and stage of illness in psychotic disorders? A systematic review and meta-analysis,Is there an association between cardiorespiratory fitness and stage of illness in psychotic disorders? A systematic review and meta-analysis,"BACKGROUND: Clinical staging models describe where an individual exists on a continuum from asymptomatic at-risk states (Stage 0) through to established late-stage disease (Stage 4). We applied this framework to systematically assess evidence for any associations between objectively assessed cardiorespiratory fitness (CRF) and stage of psychosis. METHOD: Nine electronic databases were searched for relevant publications from inception until October 31, 2019. Pooled effect sizes (Hedges' g and 95% confidence intervals (95% CI)) were estimated for differences in CRF for studies that reported mean oxygen uptake (max, peak, or predicted VO<sub>2</sub> in ml/kg/min). RESULTS: Thirty-eight studies were eligible. Findings indicated that suboptimal CRF can be present at Stages 0 and 1. Meta-analyses of 22 studies demonstrated that CRF was significantly reduced in individuals classified between Stages 1 and 4 compared with matched or general population controls (g = -0.93; 95% CI -1.14, -0.71). Mean VO<sub>2</sub> was decreased by 28% in Stage 4 compared with Stage 1 (34.1 vs. 24.66 ml/kg/min); the largest effect size for CRF reduction was reported between Stages 2 and 3 (g = -1.16; 95% CI -1.31, -1.03). CONCLUSIONS: Although not identifying direct causal links between clinical stage and CRF, using this framework may enhance understanding of co-associations between mental and physical health markers across the entire spectrum of psychosis. Limitations include lack of research on CRF in Stages 0 and 1 alongside problems determining stage in some studies. However, impaired CRF is reported in emerging psychosis, supporting calls that early intervention programmes should address both mental and physical wellbeing.","Clinical staging models describe where an individual exists on a continuum from asymptomatic at-risk states (Stage 0) through to established late-stage disease (Stage 4). We applied this framework to systematically assess evidence for any associations between objectively assessed cardiorespiratory fitness (CRF) and stage of psychosis. Nine electronic databases were searched for relevant publications from inception until October 31, 2019. Pooled effect sizes (Hedges' g and 95% confidence intervals (95% CI)) were estimated for differences in CRF for studies that reported mean oxygen uptake (max, peak, or predicted VO<sub>2</sub> in ml/kg/min). Thirty-eight studies were eligible. Findings indicated that suboptimal CRF can be present at Stages 0 and 1. Meta-analyses of 22 studies demonstrated that CRF was significantly reduced in individuals classified between Stages 1 and 4 compared with matched or general population controls (gÂ =Â -0.93; 95% CI -1.14, -0.71). Mean VO<sub>2</sub> was decreased by 28% in Stage 4 compared with Stage 1 (34.1 vs. 24.66Â ml/kg/min); the largest effect size for CRF reduction was reported between Stages 2 and 3 (gÂ =Â -1.16; 95% CI -1.31, -1.03). Although not identifying direct causal links between clinical stage and CRF, using this framework may enhance understanding of co-associations between mental and physical health markers across the entire spectrum of psychosis. Limitations include lack of research on CRF in Stages 0 and 1 alongside problems determining stage in some studies. However, impaired CRF is reported in emerging psychosis, supporting calls that early intervention programmes should address both mental and physical wellbeing.","Heggelund, J.
 and Vancampfort, D.
 and Tacchi, M. J.
 and Morken, G.
 and Scott, J.","Heggelund, Vancampfort, Tacchi, Morken, Scott",https://dx.doi.org/10.1111/acps.13119,https://doi.org/10.1111/acps.13119,2021-08-03
767.0,,pubmed,Machine Learning and Cochlear Implantation-A Structured Review of Opportunities and Challenges,Machine Learning and Cochlear Implantation-A Structured Review of Opportunities and Challenges,"OBJECTIVE: The use of machine learning technology to automate intellectual processes and boost clinical process efficiency in medicine has exploded in the past 5 years. Machine learning excels in automating pattern recognition and in adapting learned representations to new settings. Moreover, machine learning techniques have the advantage of incorporating complexity and are free from many of the limitations of traditional deterministic approaches. Cochlear implants (CI) are a unique fit for machine learning techniques given the need for optimization of signal processing to fit complex environmental scenarios and individual patients' CI MAPping. However, there are many other opportunities where machine learning may assist in CI beyond signal processing. The objective of this review was to synthesize past applications of machine learning technologies for pediatric and adult CI and describe novel opportunities for research and development. DATA SOURCES: The PubMed/MEDLINE, EMBASE, Scopus, and ISI Web of Knowledge databases were mined using a directed search strategy to identify the nexus between CI and artificial intelligence/machine learning literature. STUDY SELECTION: Non-English language articles, articles without an available abstract or full-text, and nonrelevant articles were manually appraised and excluded. Included articles were evaluated for specific machine learning methodologies, content, and application success. DATA SYNTHESIS: The database search identified 298 articles. Two hundred fifty-nine articles (86.9%) were excluded based on the available abstract/full-text, language, and relevance. The remaining 39 articles were included in the review analysis. There was a marked increase in year-over-year publications from 2013 to 2018. Applications of machine learning technologies involved speech/signal processing optimization (17; 43.6% of articles), automated evoked potential measurement (6; 15.4%), postoperative performance/efficacy prediction (5; 12.8%), and surgical anatomy location prediction (3; 7.7%), and 2 (5.1%) in each of robotics, electrode placement performance, and biomaterials performance. CONCLUSION: The relationship between CI and artificial intelligence is strengthening with a recent increase in publications reporting successful applications. Considerable effort has been directed toward augmenting signal processing and automating postoperative MAPping using machine learning algorithms. Other promising applications include augmenting CI surgery mechanics and personalized medicine approaches for boosting CI patient performance. Future opportunities include addressing scalability and the research and clinical communities' acceptance of machine learning algorithms as effective techniques.","The use of machine learning technology to automate intellectual processes and boost clinical process efficiency in medicine has exploded in the past 5 years. Machine learning excels in automating pattern recognition and in adapting learned representations to new settings. Moreover, machine learning techniques have the advantage of incorporating complexity and are free from many of the limitations of traditional deterministic approaches. Cochlear implants (CI) are a unique fit for machine learning techniques given the need for optimization of signal processing to fit complex environmental scenarios and individual patients' CI MAPping. However, there are many other opportunities where machine learning may assist in CI beyond signal processing. The objective of this review was to synthesize past applications of machine learning technologies for pediatric and adult CI and describe novel opportunities for research and development. The PubMed/MEDLINE, EMBASE, Scopus, and ISI Web of Knowledge databases were mined using a directed search strategy to identify the nexus between CI and artificial intelligence/machine learning literature. Non-English language articles, articles without an available abstract or full-text, and nonrelevant articles were manually appraised and excluded. Included articles were evaluated for specific machine learning methodologies, content, and application success. The database search identified 298 articles. Two hundred fifty-nine articles (86.9%) were excluded based on the available abstract/full-text, language, and relevance. The remaining 39 articles were included in the review analysis. There was a marked increase in year-over-year publications from 2013 to 2018. Applications of machine learning technologies involved speech/signal processing optimization (17; 43.6% of articles), automated evoked potential measurement (6; 15.4%), postoperative performance/efficacy prediction (5; 12.8%), and surgical anatomy location prediction (3; 7.7%), and 2 (5.1%) in each of robotics, electrode placement performance, and biomaterials performance. The relationship between CI and artificial intelligence is strengthening with a recent increase in publications reporting successful applications. Considerable effort has been directed toward augmenting signal processing and automating postoperative MAPping using machine learning algorithms. Other promising applications include augmenting CI surgery mechanics and personalized medicine approaches for boosting CI patient performance. Future opportunities include addressing scalability and the research and clinical communities' acceptance of machine learning algorithms as effective techniques.","Crowson, M. G.
 and Lin, V.
 and Chen, J. M.
 and Chan, T. C. Y.","Crowson, Lin, Chen, Chan",https://dx.doi.org/10.1097/MAO.0000000000002440,https://doi.org/10.1097/MAO.0000000000002440,2021-08-03
848.0,,pubmed,Vocal and Laryngeal Symptoms and Associated Factors in Wind Instrumentalists: A Systematic Review,Vocal and Laryngeal Symptoms and Associated Factors in Wind Instrumentalists: A Systematic Review,"OBJECTIVE: This is a systematic literature review to identify vocal and laryngeal symptoms and associated factors in adult wind instrumentalists. METHOD: The authors performed a systematic review in the electronic databases Science Direct, Scopus, Web of Science, PubMed and LILACS, gray literature, and manual search. There were no language or publication time limitations, as recommended by the preferred reporting items for systematic reviews and meta-analyses. They also performed title and abstract analysis followed by full-text analysis, risk of bias assessment (Quality Assessment Tool for Observational Cohort and Cross-Sectional Studies), and result synthesis. Two researchers conducted the research independently. RESULTS: Although six articles met the eligibility criteria, none of them fulfilled all the criteria for the methodological quality analysis. According to the six studies selected for this review, the main vocal symptoms in wind instrumentalists are dysphonia, altered vocal quality, hoarseness, and voice failures; and laryngeal symptoms are dryness, sore throat, throat irritation, throat clearing, discomfort, and tension. The associated factors identified for those symptoms were shorter working time, intense use of the instrument, and individual vocal issues. CONCLUSION: The several vocal and laryngeal symptoms found in wind instrumentalists in the selected studies were associated with individual factors (prior vocal alteration, inappropriate vocal habits) and organizational factors (working time, intense use, and instrument type and technique). Although wind instrumentalists' vocal health is recognized in the literature, and vocal and laryngeal symptoms are identified and associated with playing a musical instrument, broadening studies with precise methodologies and analyses is necessary.","This is a systematic literature review to identify vocal and laryngeal symptoms and associated factors in adult wind instrumentalists. The authors performed a systematic review in the electronic databases Science Direct, Scopus, Web of Science, PubMed and LILACS, gray literature, and manual search. There were no language or publication time limitations, as recommended by the preferred reporting items for systematic reviews and meta-analyses. They also performed title and abstract analysis followed by full-text analysis, risk of bias assessment (Quality Assessment Tool for Observational Cohort and Cross-Sectional Studies), and result synthesis. Two researchers conducted the research independently. Although six articles met the eligibility criteria, none of them fulfilled all the criteria for the methodological quality analysis. According to the six studies selected for this review, the main vocal symptoms in wind instrumentalists are dysphonia, altered vocal quality, hoarseness, and voice failures; and laryngeal symptoms are dryness, sore throat, throat irritation, throat clearing, discomfort, and tension. The associated factors identified for those symptoms were shorter working time, intense use of the instrument, and individual vocal issues. The several vocal and laryngeal symptoms found in wind instrumentalists in the selected studies were associated with individual factors (prior vocal alteration, inappropriate vocal habits) and organizational factors (working time, intense use, and instrument type and technique). Although wind instrumentalists' vocal health is recognized in the literature, and vocal and laryngeal symptoms are identified and associated with playing a musical instrument, broadening studies with precise methodologies and analyses is necessary.","de Lima Silva, C. R.
 and da Silva Nunes, A. D.
 and de Souza, L. B. R.
 and Jerez-Roig, J.
 and Barbosa, I. R.","de Lima Silva, da Silva Nunes, de Souza, Jerez-Roig, Barbosa",https://dx.doi.org/10.1016/j.jvoice.2019.08.024,https://doi.org/10.1016/j.jvoice.2019.08.024,2021-08-03
3266.0,,pubmed,Deciphering Adverse Outcome Pathway Network Linked to Bisphenol F Using Text Mining and Systems Toxicology Approaches,Deciphering Adverse Outcome Pathway Network Linked to Bisphenol F Using Text Mining and Systems Toxicology Approaches,"Bisphenol F (BPF) is one of several Bisphenol A (BPA) substituents that is increasingly used in manufacturing industry leading to detectable human exposure. Whereas a large number of studies have been devoted to decipher BPA effects, much less is known about its substituents. To support decision making on BPF's safety, we have developed a new computational approach to rapidly explore the available data on its toxicological effects, combining text mining and integrative systems biology, and aiming at connecting BPF to adverse outcome pathways (AOPs). We first extracted from different databases BPF-protein associations that were expanded to protein complexes using protein-protein interaction datasets. Over-representation analysis of the protein complexes allowed to identify the most relevant biological pathways putatively targeted by BPF. Then, automatic screening of scientific abstracts from literature using the text mining tool, AOP-helpFinder, combined with data integration from various sources (AOP-wiki, CompTox, etc.) and manual curation allowed us to link BPF to AOP events. Finally, we combined all the information gathered through those analyses and built a comprehensive complex framework linking BPF to an AOP network including, as adverse outcomes, various types of cancers such as breast and thyroid malignancies. These results which integrate different types of data can support regulatory assessment of the BPA substituent, BPF, and trigger new epidemiological and experimental studies.","Bisphenol F (BPF) is one of several Bisphenol A (BPA) substituents that is increasingly used in manufacturing industry leading to detectable human exposure. Whereas a large number of studies have been devoted to decipher BPA effects, much less is known about its substituents. To support decision making on BPF's safety, we have developed a new computational approach to rapidly explore the available data on its toxicological effects, combining text mining and integrative systems biology, and aiming at connecting BPF to adverse outcome pathways (AOPs). We first extracted from different databases BPF-protein associations that were expanded to protein complexes using protein-protein interaction datasets. Over-representation analysis of the protein complexes allowed to identify the most relevant biological pathways putatively targeted by BPF. Then, automatic screening of scientific abstracts from literature using the text mining tool, AOP-helpFinder, combined with data integration from various sources (AOP-wiki, CompTox, etc.) and manual curation allowed us to link BPF to AOP events. Finally, we combined all the information gathered through those analyses and built a comprehensive complex framework linking BPF to an AOP network including, as adverse outcomes, various types of cancers such as breast and thyroid malignancies. These results which integrate different types of data can support regulatory assessment of the BPA substituent, BPF, and trigger new epidemiological and experimental studies.","Rugard, M.
 and Coumoul, X.
 and Carvaillo, J. C.
 and Barouki, R.
 and Audouze, K.","Rugard, Coumoul, Carvaillo, Barouki, Audouze",https://dx.doi.org/10.1093/toxsci/kfz214,https://doi.org/10.1093/toxsci/kfz214,2021-08-03
925.0,,pubmed,Using clinical reasoning ontologies to make smarter clinical decision support systems: a systematic review and data synthesis,Using clinical reasoning ontologies to make smarter clinical decision support systems: a systematic review and data synthesis,"OBJECTIVE: The study sought to describe the literature describing clinical reasoning ontology (CRO)-based clinical decision support systems (CDSSs) and identify and classify the medical knowledge and reasoning concepts and their properties within these ontologies to guide future research. METHODS: MEDLINE, Scopus, and Google Scholar were searched through January 30, 2019, for studies describing CRO-based CDSSs. Articles that explored the development or application of CROs or terminology were selected. Eligible articles were assessed for quality features of both CDSSs and CROs to determine the current practices. We then compiled concepts and properties used within the articles. RESULTS: We included 38 CRO-based CDSSs for the analysis. Diversity of the purpose and scope of their ontologies was seen, with a variety of knowledge sources were used for ontology development. We found 126 unique medical knowledge concepts, 38 unique reasoning concepts, and 240 unique properties (137 relationships and 103 attributes). Although there is a great diversity among the terms used across CROs, there is a significant overlap based on their descriptions. Only 5 studies described high quality assessment. CONCLUSION: We identified current practices used in CRO development and provided lists of medical knowledge concepts, reasoning concepts, and properties (relationships and attributes) used by CRO-based CDSSs. CRO developers reason that the inclusion of concepts used by clinicians' during medical decision making has the potential to improve CDSS performance. However, at present, few CROs have been used for CDSSs, and high-quality studies describing CROs are sparse. Further research is required in developing high-quality CDSSs based on CROs.","The study sought to describe the literature describing clinical reasoning ontology (CRO)-based clinical decision support systems (CDSSs) and identify and classify the medical knowledge and reasoning concepts and their properties within these ontologies to guide future research. MEDLINE, Scopus, and Google Scholar were searched through January 30, 2019, for studies describing CRO-based CDSSs. Articles that explored the development or application of CROs or terminology were selected. Eligible articles were assessed for quality features of both CDSSs and CROs to determine the current practices. We then compiled concepts and properties used within the articles. We included 38 CRO-based CDSSs for the analysis. Diversity of the purpose and scope of their ontologies was seen, with a variety of knowledge sources were used for ontology development. We found 126 unique medical knowledge concepts, 38 unique reasoning concepts, and 240 unique properties (137 relationships and 103 attributes). Although there is a great diversity among the terms used across CROs, there is a significant overlap based on their descriptions. Only 5 studies described high quality assessment. We identified current practices used in CRO development and provided lists of medical knowledge concepts, reasoning concepts, and properties (relationships and attributes) used by CRO-based CDSSs. CRO developers reason that the inclusion of concepts used by clinicians' during medical decision making has the potential to improve CDSS performance. However, at present, few CROs have been used for CDSSs, and high-quality studies describing CROs are sparse. Further research is required in developing high-quality CDSSs based on CROs.","Dissanayake, P. I.
 and Colicchio, T. K.
 and Cimino, J. J.","Dissanayake, Colicchio, Cimino",https://dx.doi.org/10.1093/jamia/ocz169,https://doi.org/10.1093/jamia/ocz169,2021-08-03
249.0,,pubmed,Effects of oral and oropharyngeal cancer on speech intelligibility using acoustic analysis: Systematic review,Effects of oral and oropharyngeal cancer on speech intelligibility using acoustic analysis: Systematic review,"BACKGROUND: The development of automatic tools based on acoustic analysis allows to overcome the limitations of perceptual assessment for patients with head and neck cancer. The aim of this study is to provide a systematic review of literature describing the effects of oral and oropharyngeal cancer on speech intelligibility using acoustic analysis. METHODS: Two databases (PubMed and Embase) were surveyed. The selection process, according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) statement, led to a final set of 22 articles. RESULTS: Nasalance is studied mainly in oropharyngeal patients. The vowels are mostly studied using formant analysis and vowel space area, the consonants by means of spectral moments with specific parameters according to their phonetic characteristic. Machine learning methods allow classifying 'intelligible' or 'unintelligible' speech for T3 or T4 tumors. CONCLUSIONS: The development of comprehensive models combining different acoustic measures would allow a better consideration of the functional impact of the speech disorder.","The development of automatic tools based on acoustic analysis allows to overcome the limitations of perceptual assessment for patients with head and neck cancer. The aim of this study is to provide a systematic review of literature describing the effects of oral and oropharyngeal cancer on speech intelligibility using acoustic analysis. Two databases (PubMed and Embase) were surveyed. The selection process, according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) statement, led to a final set of 22 articles. Nasalance is studied mainly in oropharyngeal patients. The vowels are mostly studied using formant analysis and vowel space area, the consonants by means of spectral moments with specific parameters according to their phonetic characteristic. Machine learning methods allow classifying ""intelligible"" or ""unintelligible"" speech for T3 or T4 tumors. The development of comprehensive models combining different acoustic measures would allow a better consideration of the functional impact of the speech disorder.","Balaguer, M.
 and Pommee, T.
 and Farinas, J.
 and Pinquier, J.
 and Woisard, V.
 and Speyer, R.","Balaguer, PommÃ©e, Farinas, Pinquier, Woisard, Speyer",https://dx.doi.org/10.1002/hed.25949,https://doi.org/10.1002/hed.25949,2021-08-03
4112.0,,pubmed,Recognizing software names in biomedical literature using machine learning,Recognizing software names in biomedical literature using machine learning,"Software tools now are essential to research and applications in the biomedical domain. However, existing software repositories are mainly built using manual curation, which is time-consuming and unscalable. This study took the initiative to manually annotate software names in 1,120 MEDLINE abstracts and titles and used this corpus to develop and evaluate machine learning-based named entity recognition systems for biomedical software. Specifically, two strategies were proposed for feature engineering: (1) domain knowledge features and (2) unsupervised word representation features of clustered and binarized word embeddings. Our best system achieved an F-measure of 91.79% for recognizing software from titles and an F-measure of 86.35% for recognizing software from both titles and abstracts using inexact matching criteria. We then created a biomedical software catalog with 19,557 entries using the developed system. This study demonstrates the feasibility of using natural language processing methods to automatically build a high-quality software index from biomedical literature.","Software tools now are essential to research and applications in the biomedical domain. However, existing software repositories are mainly built using manual curation, which is time-consuming and unscalable. This study took the initiative to manually annotate software names in 1,120 MEDLINE abstracts and titles and used this corpus to develop and evaluate machine learning-based named entity recognition systems for biomedical software. Specifically, two strategies were proposed for feature engineering: (1) domain knowledge features and (2) unsupervised word representation features of clustered and binarized word embeddings. Our best system achieved an F-measure of 91.79% for recognizing software from titles and an F-measure of 86.35% for recognizing software from both titles and abstracts using inexact matching criteria. We then created a biomedical software catalog with 19,557 entries using the developed system. This study demonstrates the feasibility of using natural language processing methods to automatically build a high-quality software index from biomedical literature.","Wei, Q.
 and Zhang, Y.
 and Amith, M.
 and Lin, R.
 and Lapeyrolerie, J.
 and Tao, C.
 and Xu, H.","Wei, Zhang, Amith, Lin, Lapeyrolerie, Tao, Xu",https://dx.doi.org/10.1177/1460458219869490,https://doi.org/10.1177/1460458219869490,2021-08-03
4080.0,,pubmed,Systematic Literature Review on the Spread of Health-related Misinformation on Social Media,Systematic Literature Review on the Spread of Health-related Misinformation on Social Media,"Contemporary commentators describe the current period as 'an era of fake news' in which misinformation, generated intentionally or unintentionally, spreads rapidly. Although affecting all areas of life, it poses particular problems in the health arena, where it can delay or prevent effective care, in some cases threatening the lives of individuals. While examples of the rapid spread of misinformation date back to the earliest days of scientific medicine, the internet, by allowing instantaneous communication and powerful amplification has brought about a quantum change. In democracies where ideas compete in the marketplace for attention, accurate scientific information, which may be difficult to comprehend and even dull, is easily crowded out by sensationalized news. In order to uncover the current evidence and better understand the mechanism of misinformation spread, we report a systematic review of the nature and potential drivers of health-related misinformation. We searched PubMed, Cochrane, Web of Science, Scopus and Google databases to identify relevant methodological and empirical articles published between 2012 and 2018. A total of 57 articles were included for full-text analysis. Overall, we observe an increasing trend in published articles on health-related misinformation and the role of social media in its propagation. The most extensively studied topics involving misinformation relate to vaccination, Ebola and Zika Virus, although others, such as nutrition, cancer, fluoridation of water and smoking also featured. Studies adopted theoretical frameworks from psychology and network science, while co-citation analysis revealed potential for greater collaboration across fields. Most studies employed content analysis, social network analysis or experiments, drawing on disparate disciplinary paradigms. Future research should examine susceptibility of different sociodemographic groups to misinformation and understand the role of belief systems on the intention to spread misinformation. Further interdisciplinary research is also warranted to identify effective and tailored interventions to counter the spread of health-related misinformation online.","Contemporary commentators describe the current period as ""an era of fake news"" in which misinformation, generated intentionally or unintentionally, spreads rapidly. Although affecting all areas of life, it poses particular problems in the health arena, where it can delay or prevent effective care, in some cases threatening the lives of individuals. While examples of the rapid spread of misinformation date back to the earliest days of scientific medicine, the internet, by allowing instantaneous communication and powerful amplification has brought about a quantum change. In democracies where ideas compete in the marketplace for attention, accurate scientific information, which may be difficult to comprehend and even dull, is easily crowded out by sensationalized news. In order to uncover the current evidence and better understand the mechanism of misinformation spread, we report a systematic review of the nature and potential drivers of health-related misinformation. We searched PubMed, Cochrane, Web of Science, Scopus and Google databases to identify relevant methodological and empirical articles published between 2012 and 2018. A total of 57 articles were included for full-text analysis. Overall, we observe an increasing trend in published articles on health-related misinformation and the role of social media in its propagation. The most extensively studied topics involving misinformation relate to vaccination, Ebola and Zika Virus, although others, such as nutrition, cancer, fluoridation of water and smoking also featured. Studies adopted theoretical frameworks from psychology and network science, while co-citation analysis revealed potential for greater collaboration across fields. Most studies employed content analysis, social network analysis or experiments, drawing on disparate disciplinary paradigms. Future research should examine susceptibility of different sociodemographic groups to misinformation and understand the role of belief systems on the intention to spread misinformation. Further interdisciplinary research is also warranted to identify effective and tailored interventions to counter the spread of health-related misinformation online.","Wang, Y.
 and McKee, M.
 and Torbica, A.
 and Stuckler, D.","Wang, McKee, Torbica, Stuckler",https://dx.doi.org/10.1016/j.socscimed.2019.112552,https://doi.org/10.1016/j.socscimed.2019.112552,2021-08-03
1502.0,,pubmed,Cost-Effectiveness Analysis of Vedolizumab Compared with Other Biologics in Anti-TNF-Naive Patients with Moderate-to-Severe Ulcerative Colitis in Japan,Cost-Effectiveness Analysis of Vedolizumab Compared with Other Biologics in Anti-TNF-NaÃ¯ve Patients with Moderate-to-Severe Ulcerative Colitis in Japan,"BACKGROUND: Vedolizumab (VDZ) was approved by the Japanese Ministry of Health, Labor and Welfare in 2018 for the treatment of patients with moderate-to-severe active ulcerative colitis (UC). The comparative cost-effectiveness of VDZ compared with other biologics is unknown in Japan. This information could be useful for decision makers at the time of repricing biologics for the treatment of patients with moderate-to-severe UC. OBJECTIVE: The aim was to assess the cost-effectiveness of VDZ versus other branded biologics for the treatment of patients with moderate-to-severe UC who were anti-tumor necrosis factor (TNF)-naive, from the Japanese public healthcare payer perspective. METHODS: A hybrid decision tree/Markov model was developed to predict the number of patients who achieved response and remission at the end of the induction phase and sustained it during the maintenance phase, translating this into quality-adjusted life-years (QALYs) and costs. Treatment-related adverse events, discontinuation and surgery, and their impact on QALYs and costs were also modeled. A systematic literature review and network meta-analysis were conducted to estimate the comparative efficacy of each treatment versus placebo. Rates of adverse events, surgery, surgery complications, and utilities were from the literature. Costs (2018 Japanese yen) were obtained from the Japanese National Health Insurance drug price list and medical fee table and local claims databases. Clinical and economic outcomes were projected over a lifetime and discounted at 2% annually. RESULTS: Over a lifetime, VDZ yielded greater QALYs and cost savings compared with golimumab and was cost-effective compared with adalimumab and infliximab (incremental cost-effectiveness ratios 4,821,940 and 4,687,692, respectively). Deterministic and probabilistic analyses supported the robustness of the findings in the base-case analysis, indicating that VDZ was either dominant or cost-effective in most scenarios and replications. The main limitations of this analysis include excluding tofacitinib and infliximab biosimilar as comparators, health-state utility estimates were obtained from population studies in the United Kingdom, and the impact of subsequent (i.e., second-line) biologic treatment was not evaluated. CONCLUSION: Our analysis suggests that VDZ is dominant or cost-effective compared with other branded biologics for the treatment of anti-TNF-naive patients with moderate-to-severe UC in Japan.","Vedolizumab (VDZ) was approved by the Japanese Ministry of Health, Labor and Welfare in 2018 for the treatment of patients with moderate-to-severe active ulcerative colitis (UC). The comparative cost-effectiveness of VDZ compared with other biologics is unknown in Japan. This information could be useful for decision makers at the time of repricing biologics for the treatment of patients with moderate-to-severe UC. The aim was to assess the cost-effectiveness of VDZ versus other branded biologics for the treatment of patients with moderate-to-severe UC who were anti-tumor necrosis factor (TNF)-naÃ¯ve, from the Japanese public healthcare payer perspective. A hybrid decision tree/Markov model was developed to predict the number of patients who achieved response and remission at the end of the induction phase and sustained it during the maintenance phase, translating this into quality-adjusted life-years (QALYs) and costs. Treatment-related adverse events, discontinuation and surgery, and their impact on QALYs and costs were also modeled. A systematic literature review and network meta-analysis were conducted to estimate the comparative efficacy of each treatment versus placebo. Rates of adverse events, surgery, surgery complications, and utilities were from the literature. Costs (2018 Japanese yen) were obtained from the Japanese National Health Insurance drug price list and medical fee table and local claims databases. Clinical and economic outcomes were projected over a lifetime and discounted at 2% annually. Over a lifetime, VDZ yielded greater QALYs and cost savings compared with golimumab and was cost-effectiveÂ compared with adalimumab and infliximab (incremental cost-effectiveness ratios Â¥4,821,940 and Â¥4,687,692, respectively). Deterministic and probabilistic analyses supported the robustness of the findings in the base-case analysis, indicating that VDZÂ was either dominant or cost-effective in most scenarios and replications. The main limitations of this analysis include excluding tofacitinib and infliximab biosimilar as comparators, health-state utility estimates were obtained from population studies in the United Kingdom, and the impact of subsequent (i.e., second-line) biologic treatment was not evaluated. Our analysis suggests that VDZ is dominant or cost-effective compared with other branded biologics for the treatment of anti-TNF-naÃ¯ve patients with moderate-to-severe UC in Japan.","Hernandez, L.
 and Kuwabara, H.
 and Shah, A.
 and Yamabe, K.
 and Burnett, H.
 and Fahrbach, K.
 and Koufopoulou, M.
 and Iwakiri, R.","Hernandez, Kuwabara, Shah, Yamabe, Burnett, Fahrbach, Koufopoulou, Iwakiri",https://dx.doi.org/10.1007/s40273-019-00841-1,https://doi.org/10.1007/s40273-019-00841-1,2021-08-03
3691.0,,pubmed,Validation of the PRESTO Score in Injured Children in a South-African Quaternary Trauma Center,Validation of the PRESTO score in injured children in a South-African quaternary trauma center,"INTRODUCTION: The Pediatric RESuscitation and Trauma Outcome (PRESTO) model was developed for standardized risk-adjustment in pediatric trauma and is adapted to low-resource settings. It includes easily-accessible demographic and physiologic variables that are available at point of care in virtually any setting. The purpose of this study was to evaluate the PRESTO model's ability to predict in-hospital mortality in a South African pediatric trauma unit by comparing it to the widely used Injury Severity Score (ISS). METHODS: Data prospectively collected between 2007 and 2017 in the Inkosi Albert Luthuli Central Hospital Trauma Registry were retrospectively reviewed. Injured children younger than 14years were included if they were admitted to hospital or died as a result of their injury. We excluded patients with minor injuries who were treated and discharged home and patients with incomplete hospital disposition data. Receiver-Operating Characteristic (ROC) curves were constructed for PRESTO and ISS, and the areas under the curve (AUCs) were compared using Delong's test. The sensitivity and specificity of PRESTO were calculated at different prognostic threshold values identified through literature review. RESULTS: We identified 419 patients; 67 died in hospital (16%). The AUCs for PRESTO and ISS were 0.82 (95% confidence interval CI [0.76-0.87]) and 0.75 (CI [0.68-0.81]), respectively. This difference trended towards statistical significance (p=0.07). Using the optimal threshold of 0.13 described in the original publication, PRESTO had a 97% sensitivity and 37% specificity, while a threshold of 0.50 yielded 90% sensitivity and 54% specificity. The mean predicted probability of in-hospital death among patients who died was 0.79. Using this value as a threshold yielded the 57% sensitivity and 85% specificity. CONCLUSION: This analysis has demonstrated the validity of the PRESTO model for in-hospital mortality prediction for pediatric trauma patients in the setting of a dedicated high-complexity trauma unit in a South African trauma referral center. LEVEL OF EVIDENCE: Level III: Case-control.","The Pediatric RESuscitation and Trauma Outcome (PRESTO) model was developed for standardized risk-adjustment in pediatric trauma and is adapted to low-resource settings. It includes easily-accessible demographic and physiologic variables that are available at point of care in virtually any setting. The purpose of this study was to evaluate the PRESTO model's ability to predict in-hospital mortality in a South African pediatric trauma unit by comparing it to the widely used Injury Severity Score (ISS). Data prospectively collected between 2007 and 2017 in the Inkosi Albert Luthuli Central Hospital Trauma Registry were retrospectively reviewed. Injured children younger than 14â€¯years were included if they were admitted to hospital or died as a result of their injury. We excluded patients with minor injuries who were treated and discharged home and patients with incomplete hospital disposition data. Receiver-Operating Characteristic (ROC) curves were constructed for PRESTO and ISS, and the areas under the curve (AUCs) were compared using Delong's test. The sensitivity and specificity of PRESTO were calculated at different prognostic threshold values identified through literature review. We identified 419 patients; 67 died in hospital (16%). The AUCs for PRESTO and ISS were 0.82 (95% confidence interval CI [0.76-0.87]) and 0.75 (CI [0.68-0.81]), respectively. This difference trended towards statistical significance (pâ€¯=â€¯0.07). Using the optimal threshold of 0.13 described in the original publication, PRESTO had a 97% sensitivity and 37% specificity, while a threshold of 0.50 yielded 90% sensitivity and 54% specificity. The mean predicted probability of in-hospital death among patients who died was 0.79. Using this value as a threshold yielded the 57% sensitivity and 85% specificity. This analysis has demonstrated the validity of the PRESTO model for in-hospital mortality prediction for pediatric trauma patients in the setting of a dedicated high-complexity trauma unit in a South African trauma referral center. Level III: Case-control.","St-Louis, E.
 and Hassamal, R.
 and Razek, T.
 and Baird, R.
 and Poenaru, D.
 and Hardcastle, T. C.","St-Louis, Hassamal, Razek, Baird, Poenaru, Hardcastle",https://dx.doi.org/10.1016/j.jpedsurg.2019.08.008,https://doi.org/10.1016/j.jpedsurg.2019.08.008,2021-08-03
482.0,,pubmed,Predictors of functional outcome in musculoskeletal healthcare: An umbrella review,Predictors of functional outcome in musculoskeletal healthcare: An umbrella review,"BACKGROUND: Multiple cohort and systematic review studies exist, reporting independent predictive factors associated with outcome in musculoskeletal populations. These studies have found evidence for a number of 'generic' factors that have been shown to predict outcome across musculoskeletal patient cohorts. This review provides a higher level review of the evidence with a focus on generic patient factors associated with functional musculoskeletal outcome with a view to informing predictive modelling. OBJECTIVES: (a) Identify patient factors found to have evidence to support their association with functional outcome, and (b) review these findings across body areas/conditions to identify generic predictive factors. DATABASES AND DATA TREATMENT: Electronic databases of MEDLINE, AMED, EMBASE, CINAHL and Cochrane were searched for eligible studies. Two reviewers independently extracted data and assessed quality using an established checklist for umbrella reviews. RESULTS: Twenty-one systematic reviews met inclusion criteria, all were of moderate/high quality. Six independent predictors were found to have strong evidence of association with worse musculoskeletal functional outcome across anatomical body sites (worse baseline function, higher symptom/pain severity, worse mental well-being, more comorbidities, older age and higher body mass index). Longer duration of symptoms, worse pain coping, presence of workers compensation, lower vitality and lower education were also found to have moderate evidence of association with worse functional outcome across body sites. CONCLUSIONS: This study identifies a number of factors associated with musculoskeletal functional outcome. The generic predictive factors identified should be considered for inclusion into musculoskeletal prognostic models, including models used for case-mix-adjustment of patient reported outcome measure data. SIGNIFICANCE: This article identifies 'generic' patient factors that predict functional outcome (measured using Patient Reported Outcome Measures (PROMs)) across musculoskeletal conditions. Findings provide support for the development and content of generic musculoskeletal prognostic models including models used to case-mix adjust PROM data for baseline complexity. Generic musculoskeletal models and functional PROMs would facilitate more feasible comparison and benchmarking of musculoskeletal services in order to identify variation and address health inequalities.","Multiple cohort and systematic review studies exist, reporting independent predictive factors associated with outcome in musculoskeletal populations. These studies have found evidence for a number of ""generic"" factors that have been shown to predict outcome across musculoskeletal patient cohorts. This review provides a higher level review of the evidence with a focus on generic patient factors associated with functional musculoskeletal outcome with a view to informing predictive modelling. (a) Identify patient factors found to have evidence to support their association with functional outcome, and (b) review these findings across body areas/conditions to identify generic predictive factors. Electronic databases of MEDLINE, AMED, EMBASE, CINAHL and Cochrane were searched for eligible studies. Two reviewers independently extracted data and assessed quality using an established checklist for umbrella reviews. Twenty-one systematic reviews met inclusion criteria, all were of moderate/high quality. Six independent predictors were found to have strong evidence of association with worse musculoskeletal functional outcome across anatomical body sites (worse baseline function, higher symptom/pain severity, worse mental well-being, more comorbidities, older age and higher body mass index). Longer duration of symptoms, worse pain coping, presence of workers compensation, lower vitality and lower education were also found to have moderate evidence of association with worse functional outcome across body sites. This study identifies a number of factors associated with musculoskeletal functional outcome. The generic predictive factors identified should be considered for inclusion into musculoskeletal prognostic models, including models used for case-mix-adjustment of patient reported outcome measure data. This article identifies ""generic"" patient factors that predict functional outcome (measured using Patient Reported Outcome Measures (PROMs)) across musculoskeletal conditions. Findings provide support for the development and content of generic musculoskeletal prognostic models including models used to case-mix adjust PROM data for baseline complexity. Generic musculoskeletal models and functional PROMs would facilitate more feasible comparison and benchmarking of musculoskeletal services in order to identify variation and address health inequalities.","Burgess, R.
 and Mansell, G.
 and Bishop, A.
 and Lewis, M.
 and Hill, J.","Burgess, Mansell, Bishop, Lewis, Hill",https://dx.doi.org/10.1002/ejp.1483,https://doi.org/10.1002/ejp.1483,2021-08-03
3048.0,,pubmed,Effects of Physical Exercise Training in the Workplace on Physical Fitness: A Systematic Review and Meta-analysis,Effects of Physical Exercise Training in the Workplace on Physical Fitness: A Systematic Review and Meta-analysis,"BACKGROUND: There is evidence that physical exercise training (PET) conducted at the workplace is effective in improving physical fitness and thus health. However, there is no current systematic review available that provides high-level evidence regarding the effects of PET on physical fitness in the workforce. OBJECTIVES: To quantify sex-, age-, and occupation type-specific effects of PET on physical fitness and to characterize dose-response relationships of PET modalities that could maximize gains in physical fitness in the working population. DATA SOURCES: A computerized systematic literature search was conducted in the databases PubMed and Cochrane Library (2000-2019) to identify articles related to PET in workers. STUDY ELIGIBILITY CRITERIA: Only randomized controlled trials with a passive control group were included if they investigated the effects of PET programs in workers and tested at least one fitness measure. STUDY APPRAISAL AND SYNTHESIS METHODS: Weighted mean standardised mean differences (SMD<sub>wm</sub>) were calculated using random effects models. A multivariate random effects meta-regression was computed to explain the influence of key training modalities (e.g., training frequency, session duration, intensity) on the effectiveness of PET on measures of physical fitness. Further, subgroup univariate analyses were computed for each training modality. Additionally, methodological quality of the included studies was rated with the help of the Physiotherapy Evidence Database (PEDro) Scale. RESULTS: Overall, 3423 workers aged 30-56 years participated in 17 studies (19 articles) that were eligible for inclusion. Methodological quality of the included studies was moderate with a median PEDro score of 6. Our analyses revealed significant, small-sized effects of PET on cardiorespiratory fitness (CRF), muscular endurance, and muscle power (0.29 <= SMD<sub>wm</sub> <= 0.48). Medium effects were found for CRF and muscular endurance in younger workers (<= 45 years) (SMD<sub>wm</sub> = 0.71) and white-collar workers (SMD<sub>wm</sub> = 0.60), respectively. Multivariate random effects meta-regression for CRF revealed that none of the examined training modalities predicted the effects of PET on CRF (R<sup>2</sup> = 0). Independently computed subgroup analyses showed significant PET effects on CRF when conducted for 9-12 weeks (SMD<sub>wm</sub> = 0.31) and for 17-20 weeks (SMD<sub>wm</sub> = 0.74). CONCLUSIONS: PET effects on physical fitness in healthy workers are moderated by age (CRF) and occupation type (muscular endurance). Further, independently computed subgroup analyses indicated that the training period of the PET programs may play an important role in improving CRF in workers.","There is evidence that physical exercise training (PET) conducted at the workplace is effective in improving physical fitness and thus health. However, there is no current systematic review available that provides high-level evidence regarding the effects of PET on physical fitness in the workforce. To quantify sex-, age-, and occupation type-specific effects of PET on physical fitness and to characterize dose-response relationships of PET modalities that could maximize gains in physical fitness in the working population. A computerized systematic literature search was conducted in the databases PubMed and Cochrane Library (2000-2019) to identify articles related to PET in workers. Only randomized controlled trials with a passive control group were included if they investigated the effects of PET programs in workers and tested at least one fitness measure. Weighted mean standardised mean differences (SMD<sub>wm</sub>) were calculated using random effects models. A multivariate random effects meta-regression was computed to explain the influence of key training modalities (e.g., training frequency, session duration, intensity) on the effectiveness of PET on measures of physical fitness. Further, subgroup univariate analyses were computed for each training modality. Additionally, methodological quality of the included studies was rated with the help of the Physiotherapy Evidence Database (PEDro) Scale. Overall, 3423 workers aged 30-56Â years participated in 17 studies (19 articles) that were eligible for inclusion. Methodological quality of the included studies was moderate with a median PEDro score of 6. Our analyses revealed significant, small-sized effects of PET on cardiorespiratory fitness (CRF), muscular endurance, and muscle power (0.29â€‰â‰¤â€‰SMD<sub>wm</sub>â€‰â‰¤â€‰0.48). Medium effects were found for CRF and muscular endurance in younger workers (â‰¤â€‰45Â years) (SMD<sub>wm</sub>â€‰=â€‰0.71) and white-collar workers (SMD<sub>wm</sub>â€‰=â€‰0.60), respectively. Multivariate random effects meta-regression for CRF revealed that none of the examined training modalities predicted the effects of PET on CRF (R<sup>2</sup>â€‰=â€‰0). Independently computed subgroup analyses showed significant PET effects on CRF when conducted for 9-12Â weeks (SMD<sub>wm</sub>â€‰=â€‰0.31) and for 17-20Â weeks (SMD<sub>wm</sub>â€‰=â€‰0.74). PET effects on physical fitness in healthy workers are moderated by age (CRF) and occupation type (muscular endurance). Further, independently computed subgroup analyses indicated that the training period of the PET programs may play an important role in improving CRF in workers.","Prieske, O.
 and Dalager, T.
 and Herz, M.
 and Hortobagyi, T.
 and Sjogaard, G.
 and Sogaard, K.
 and Granacher, U.","Prieske, Dalager, Herz, Hortobagyi, SjÃ¸gaard, SÃ¸gaard, Granacher",https://dx.doi.org/10.1007/s40279-019-01179-6,https://doi.org/10.1007/s40279-019-01179-6,2021-08-03
1709.0,,pubmed,Deep Learning in Alzheimer's Disease: Diagnostic Classification and Prognostic Prediction Using Neuroimaging Data,Deep Learning in Alzheimer's Disease: Diagnostic Classification and Prognostic Prediction Using Neuroimaging Data,"Deep learning, a state-of-the-art machine learning approach, has shown outstanding performance over traditional machine learning in identifying intricate structures in complex high-dimensional data, especially in the domain of computer vision. The application of deep learning to early detection and automated classification of Alzheimer's disease (AD) has recently gained considerable attention, as rapid progress in neuroimaging techniques has generated large-scale multimodal neuroimaging data. A systematic review of publications using deep learning approaches and neuroimaging data for diagnostic classification of AD was performed. A PubMed and Google Scholar search was used to identify deep learning papers on AD published between January 2013 and July 2018. These papers were reviewed, evaluated, and classified by algorithm and neuroimaging type, and the findings were summarized. Of 16 studies meeting full inclusion criteria, 4 used a combination of deep learning and traditional machine learning approaches, and 12 used only deep learning approaches. The combination of traditional machine learning for classification and stacked auto-encoder (SAE) for feature selection produced accuracies of up to 98.8% for AD classification and 83.7% for prediction of conversion from mild cognitive impairment (MCI), a prodromal stage of AD, to AD. Deep learning approaches, such as convolutional neural network (CNN) or recurrent neural network (RNN), that use neuroimaging data without pre-processing for feature selection have yielded accuracies of up to 96.0% for AD classification and 84.2% for MCI conversion prediction. The best classification performance was obtained when multimodal neuroimaging and fluid biomarkers were combined. Deep learning approaches continue to improve in performance and appear to hold promise for diagnostic classification of AD using multimodal neuroimaging data. AD research that uses deep learning is still evolving, improving performance by incorporating additional hybrid data types, such as-omics data, increasing transparency with explainable approaches that add knowledge of specific disease-related features and mechanisms.","Deep learning, a state-of-the-art machine learning approach, has shown outstanding performance over traditional machine learning in identifying intricate structures in complex high-dimensional data, especially in the domain of computer vision. The application of deep learning to early detection and automated classification of Alzheimer's disease (AD) has recently gained considerable attention, as rapid progress in neuroimaging techniques has generated large-scale multimodal neuroimaging data. A systematic review of publications using deep learning approaches and neuroimaging data for diagnostic classification of AD was performed. A PubMed and Google Scholar search was used to identify deep learning papers on AD published between January 2013 and July 2018. These papers were reviewed, evaluated, and classified by algorithm and neuroimaging type, and the findings were summarized. Of 16 studies meeting full inclusion criteria, 4 used a combination of deep learning and traditional machine learning approaches, and 12 used only deep learning approaches. The combination of traditional machine learning for classification and stacked auto-encoder (SAE) for feature selection produced accuracies of up to 98.8% for AD classification and 83.7% for prediction of conversion from mild cognitive impairment (MCI), a prodromal stage of AD, to AD. Deep learning approaches, such as convolutional neural network (CNN) or recurrent neural network (RNN), that use neuroimaging data without pre-processing for feature selection have yielded accuracies of up to 96.0% for AD classification and 84.2% for MCI conversion prediction. The best classification performance was obtained when multimodal neuroimaging and fluid biomarkers were combined. Deep learning approaches continue to improve in performance and appear to hold promise for diagnostic classification of AD using multimodal neuroimaging data. AD research that uses deep learning is still evolving, improving performance by incorporating additional hybrid data types, such as-omics data, increasing transparency with explainable approaches that add knowledge of specific disease-related features and mechanisms.","Jo, T.
 and Nho, K.
 and Saykin, A. J.","Jo, Nho, Saykin",https://dx.doi.org/10.3389/fnagi.2019.00220,https://doi.org/10.3389/fnagi.2019.00220,2021-08-03
1443.0,,pubmed,A systematic review of the diagnostic accuracy of artificial intelligence-based computer programs to analyze chest x-rays for pulmonary tuberculosis,A systematic review of the diagnostic accuracy of artificial intelligence-based computer programs to analyze chest x-rays for pulmonary tuberculosis,"We undertook a systematic review of the diagnostic accuracy of artificial intelligence-based software for identification of radiologic abnormalities (computer-aided detection, or CAD) compatible with pulmonary tuberculosis on chest x-rays (CXRs). We searched four databases for articles published between January 2005-February 2019. We summarized data on CAD type, study design, and diagnostic accuracy. We assessed risk of bias with QUADAS-2. We included 53 of the 4712 articles reviewed: 40 focused on CAD design methods ('Development' studies) and 13 focused on evaluation of CAD ('Clinical' studies). Meta-analyses were not performed due to methodological differences. Development studies were more likely to use CXR databases with greater potential for bias as compared to Clinical studies. Areas under the receiver operating characteristic curve (median AUC [IQR]) were significantly higher: in Development studies AUC: 0.88 [0.82-0.90]) versus Clinical studies (0.75 [0.66-0.87]; p-value 0.004); and with deep-learning (0.91 [0.88-0.99]) versus machine-learning (0.82 [0.75-0.89]; p = 0.001). We conclude that CAD programs are promising, but the majority of work thus far has been on development rather than clinical evaluation. We provide concrete suggestions on what study design elements should be improved.","We undertook a systematic review of the diagnostic accuracy of artificial intelligence-based software for identification of radiologic abnormalities (computer-aided detection, or CAD) compatible with pulmonary tuberculosis on chest x-rays (CXRs). We searched four databases for articles published between January 2005-February 2019. We summarized data on CAD type, study design, and diagnostic accuracy. We assessed risk of bias with QUADAS-2. We included 53 of the 4712 articles reviewed: 40 focused on CAD design methods (""Development"" studies) and 13 focused on evaluation of CAD (""Clinical"" studies). Meta-analyses were not performed due to methodological differences. Development studies were more likely to use CXR databases with greater potential for bias as compared to Clinical studies. Areas under the receiver operating characteristic curve (median AUC [IQR]) were significantly higher: in Development studies AUC: 0.88 [0.82-0.90]) versus Clinical studies (0.75 [0.66-0.87]; p-value 0.004); and with deep-learning (0.91 [0.88-0.99]) versus machine-learning (0.82 [0.75-0.89]; p = 0.001). We conclude that CAD programs are promising, but the majority of work thus far has been on development rather than clinical evaluation. We provide concrete suggestions on what study design elements should be improved.","Harris, M.
 and Qi, A.
 and Jeagal, L.
 and Torabi, N.
 and Menzies, D.
 and Korobitsyn, A.
 and Pai, M.
 and Nathavitharana, R. R.
 and Ahmad Khan, F.","Harris, Qi, Jeagal, Torabi, Menzies, Korobitsyn, Pai, Nathavitharana, Ahmad Khan",https://dx.doi.org/10.1371/journal.pone.0221339,https://doi.org/10.1371/journal.pone.0221339,2021-08-03
3243.0,,pubmed,Towards a characterization of apparent contradictions in the biomedical literature using context analysis,Towards a characterization of apparent contradictions in the biomedical literature using context analysis,"BACKGROUND: With the substantial growth in the biomedical research literature, a larger number of claims are published daily, some of which seemingly disagree with or contradict prior claims on the same topics. Resolving such contradictions is critical to advancing our understanding of human disease and developing effective treatments. Automated text analysis techniques can facilitate such analysis by extracting claims from the literature, flagging those that are potentially contradictory, and identifying any study characteristics that may explain such contradictions. METHODS: Using SemMedDB, our own PubMed-scale repository of semantic predications (subject-relation-object triples), we identified apparent contradictions in the biomedical research literature and developed a categorization of contextual characteristics that explain such contradictions. Clinically relevant semantic predications relating to 20 diseases and involving opposing predicate pairs (e.g., an intervention treats or causes a disease) were retrieved from SemMedDB. After addressing inference, uncertainty, generic concepts, and NLP errors through automatic and manual filtering steps, a set of apparent contradictions were identified and characterized. RESULTS: We retrieved 117,676 predication instances from 62,360 PubMed abstracts (Jan 1980-Dec 2016). From these instances, automatic filtering steps generated 2236 candidate contradictory pairs. Through manual analysis, we determined that 58 of these pairs (2.6%) were apparent contradictions. We identified five main categories of contextual characteristics that explain these contradictions: (a) internal to the patient, (b) external to the patient, (c) endogenous/exogenous, (d) known controversy, and (e) contradictions in literature. Categories (a) and (b) were subcategorized further (e.g., species, dosage) and accounted for the bulk of the contradictory information. CONCLUSIONS: Semantic predications, by accounting for lexical variability, and SemMedDB, owing to its literature scale, can support identification and elucidation of potentially contradictory claims across the biomedical domain. Further filtering and classification steps are needed to distinguish among them the true contradictory claims. The ability to detect contradictions automatically can facilitate important biomedical knowledge management tasks, such as tracking and verifying scientific claims, summarizing research on a given topic, identifying knowledge gaps, and assessing evidence for systematic reviews, with potential benefits to the scientific community. Future work will focus on automating these steps for fully automatic recognition of contradictions from the biomedical research literature.","With the substantial growth in the biomedical research literature, a larger number of claims are published daily, some of which seemingly disagree with or contradict prior claims on the same topics. Resolving such contradictions is critical to advancing our understanding of human disease and developing effective treatments. Automated text analysis techniques can facilitate such analysis by extracting claims from the literature, flagging those that are potentially contradictory, and identifying any study characteristics that may explain such contradictions. Using SemMedDB, our own PubMed-scale repository of semantic predications (subject-relation-object triples), we identified apparent contradictions in the biomedical research literature and developed a categorization of contextual characteristics that explain such contradictions. Clinically relevant semantic predications relating to 20 diseases and involving opposing predicate pairs (e.g., an intervention treats or causes a disease) were retrieved from SemMedDB. After addressing inference, uncertainty, generic concepts, and NLP errors through automatic and manual filtering steps, a set of apparent contradictions were identified and characterized. We retrieved 117,676 predication instances from 62,360 PubMed abstracts (Jan 1980-Dec 2016). From these instances, automatic filtering steps generated 2236 candidate contradictory pairs. Through manual analysis, we determined that 58 of these pairs (2.6%) were apparent contradictions. We identified five main categories of contextual characteristics that explain these contradictions: (a) internal to the patient, (b) external to the patient, (c) endogenous/exogenous, (d) known controversy, and (e) contradictions in literature. Categories (a) and (b) were subcategorized further (e.g., species, dosage) and accounted for the bulk of the contradictory information. Semantic predications, by accounting for lexical variability, and SemMedDB, owing to its literature scale, can support identification and elucidation of potentially contradictory claims across the biomedical domain. Further filtering and classification steps are needed to distinguish among them the true contradictory claims. The ability to detect contradictions automatically can facilitate important biomedical knowledge management tasks, such as tracking and verifying scientific claims, summarizing research on a given topic, identifying knowledge gaps, and assessing evidence for systematic reviews, with potential benefits to the scientific community. Future work will focus on automating these steps for fully automatic recognition of contradictions from the biomedical research literature.","Rosemblat, G.
 and Fiszman, M.
 and Shin, D.
 and Kilicoglu, H.","Rosemblat, Fiszman, Shin, Kilicoglu",https://dx.doi.org/10.1016/j.jbi.2019.103275,https://doi.org/10.1016/j.jbi.2019.103275,2021-08-03
2507.0,,pubmed,Clinical outcomes of automated anastomotic devices: A metanalysis,Clinical outcomes of automated anastomotic devices: A metanalysis,"BACKGROUND AND AIMS: We investigated neurological events, graft patency, major adverse cardiovascular events (MACEs), and mortality at 1 year following coronary artery bypass grafting (CABG) surgery using automated proximal anastomotic devices (APADs) and compared the overall rates with the current literature. METHODS: A systematic review of all available reports of APADs use in the literature was conducted. Cumulative incidence and 95% confidence interval (CI) were the main statistical indexes. Nine observational studies encompassing a total of 718 patients were included at the end of the selection process. RESULTS: The cumulative event rate of neurological complications was 4.8% (lower-upper limits: 2.8-8.0, P < .001; I<sup>2</sup> = 72.907%, P = .002; Egger's test: intercept = -2.47, P = 0.16; Begg and Mazumdar test: tau = -0.20, p = 0.57). Graft patency was 90.5% (80.4 to 95.7, P < .001; I<sup>2</sup> = 76.823%, P = .005; Egger's test: intercept = -3.04, P = .10; Begg and Mazumdar test: tau = -0.67, P = .17). Furthermore, the overall incidence of MACEs was 3.7% (1.3-10.4, P < .001; I<sup>2</sup> = 51.556%, P = .103; Egger's test: intercept = -1.98, P = < .11; Begg and Mazumdar test: tau = -0.67, P = .17). Finally, mortality within 1 year was 5% (3.5-7, P < .001; I<sup>2</sup> = 29.675%, P = .202; Egger's test: intercept = -0.91, P = .62; Begg and Mazumdar test: tau = -0.04, P = .88). CONCLUSIONS: APADs do not seem to be correlated with a reduction of either neurological events or mortality. By contrast, these tools showed satisfactory one-year graft patency and a low incidence of MACEs. Further research on this topic is warranted.","We investigated neurological events, graft patency, major adverse cardiovascular events (MACEs), and mortality at 1 year following coronary artery bypass grafting (CABG) surgery using automated proximal anastomotic devices (APADs) and compared the overall rates with the current literature. A systematic review of all available reports of APADs use in the literature was conducted. Cumulative incidence and 95% confidence interval (CI) were the main statistical indexes. Nine observational studies encompassing a total of 718 patients were included at the end of the selection process. The cumulative event rate of neurological complications was 4.8% (lower-upper limits: 2.8-8.0, Pâ€‰&lt;â€‰.001; I<sup>2</sup> â€‰=â€‰72.907%, Pâ€‰=â€‰.002; Egger's test: interceptâ€‰=â€‰-2.47, Pâ€‰=â€‰0.16; Begg and Mazumdar test: Ï„â€‰=â€‰-0.20, pâ€‰=â€‰0.57). Graft patency was 90.5% (80.4 to 95.7, Pâ€‰&lt;â€‰.001; I<sup>2</sup> â€‰=â€‰76.823%, Pâ€‰=â€‰.005; Egger's test: interceptâ€‰=â€‰-3.04, Pâ€‰=â€‰.10; Begg and Mazumdar test: Ï„â€‰=â€‰-0.67, Pâ€‰=â€‰.17). Furthermore, the overall incidence of MACEs was 3.7% (1.3-10.4, Pâ€‰&lt;â€‰.001; I<sup>2</sup> â€‰=â€‰51.556%, Pâ€‰=â€‰.103; Egger's test: interceptâ€‰=â€‰-1.98, Pâ€‰=â€‰â€‰&lt;â€‰.11; Begg and Mazumdar test: Ï„â€‰=â€‰-0.67, Pâ€‰=â€‰.17). Finally, mortality within 1 year was 5% (3.5-7, Pâ€‰&lt;â€‰.001; I<sup>2</sup> â€‰=â€‰29.675%, Pâ€‰=â€‰.202; Egger's test: interceptâ€‰=â€‰-0.91, Pâ€‰=â€‰.62; Begg and Mazumdar test: Ï„â€‰=â€‰-0.04, Pâ€‰=â€‰.88). APADs do not seem to be correlated with a reduction of either neurological events or mortality. By contrast, these tools showed satisfactory one-year graft patency and a low incidence of MACEs. Further research on this topic is warranted.","Micali, L. R.
 and Matteucci, F.
 and Parise, O.
 and Tetta, C.
 and Moula, A. I.
 and de Jong, M.
 and Londero, F.
 and Gelsomino, S.","Micali, Matteucci, Parise, Tetta, Moula, de Jong, Londero, Gelsomino",https://dx.doi.org/10.1111/jocs.14186,https://doi.org/10.1111/jocs.14186,2021-08-03
360.0,,pubmed,AVADA: toward automated pathogenic variant evidence retrieval directly from the full-text literature,AVADA: toward automated pathogenic variant evidence retrieval directly from the full-text literature,"PURPOSE: Both monogenic pathogenic variant cataloging and clinical patient diagnosis start with variant-level evidence retrieval followed by expert evidence integration in search of diagnostic variants and genes. Here, we try to accelerate pathogenic variant evidence retrieval by an automatic approach. METHODS: Automatic VAriant evidence DAtabase (AVADA) is a novel machine learning tool that uses natural language processing to automatically identify pathogenic genetic variant evidence in full-text primary literature about monogenic disease and convert it to genomic coordinates. RESULTS: AVADA automatically retrieved almost 60% of likely disease-causing variants deposited in the Human Gene Mutation Database (HGMD), a 4.4-fold improvement over the current best open source automated variant extractor. AVADA contains over 60,000 likely disease-causing variants that are in HGMD but not in ClinVar. AVADA also highlights the challenges of automated variant mapping and pathogenicity curation. However, when combined with manual validation, on 245 diagnosed patients, AVADA provides valuable evidence for an additional 18 diagnostic variants, on top of ClinVar's 21, versus only 2 using the best current automated approach. CONCLUSION: AVADA advances automated retrieval of pathogenic monogenic variant evidence from full-text literature. Far from perfect, but much faster than PubMed/Google Scholar search, careful curation of AVADA-retrieved evidence can aid both database curation and patient diagnosis.","Both monogenic pathogenic variant cataloging and clinical patient diagnosis start with variant-level evidence retrieval followed by expert evidence integration in search of diagnostic variants and genes. Here, we try to accelerate pathogenic variant evidence retrieval by an automatic approach. Automatic VAriant evidence DAtabase (AVADA) is a novel machine learning tool that uses natural language processing to automatically identify pathogenic genetic variant evidence in full-text primary literature about monogenic disease and convert it to genomic coordinates. AVADA automatically retrieved almost 60% of likely disease-causing variants deposited in the Human Gene Mutation Database (HGMD), a 4.4-fold improvement over the current best open source automated variant extractor. AVADA contains over 60,000 likely disease-causing variants that are in HGMD but not in ClinVar. AVADA also highlights the challenges of automated variant mapping and pathogenicity curation. However, when combined with manual validation, on 245 diagnosed patients, AVADA provides valuable evidence for an additional 18 diagnostic variants, on top of ClinVar's 21, versus only 2 using the best current automated approach. AVADA advances automated retrieval of pathogenic monogenic variant evidence from full-text literature. Far from perfect, but much faster than PubMed/Google Scholar search, careful curation of AVADA-retrieved evidence can aid both database curation and patient diagnosis.","Birgmeier, J.
 and Deisseroth, C. A.
 and Hayward, L. E.
 and Galhardo, L. M. T.
 and Tierno, A. P.
 and Jagadeesh, K. A.
 and Stenson, P. D.
 and Cooper, D. N.
 and Bernstein, J. A.
 and Haeussler, M.
 and Bejerano, G.","Birgmeier, Deisseroth, Hayward, Galhardo, Tierno, Jagadeesh, Stenson, Cooper, Bernstein, Haeussler, Bejerano",not available,https://doi.org/10.1038/s41436-019-0643-6,2021-08-03
444.0,,pubmed,"Developing a fully automated evidence synthesis tool for identifying, assessing and collating the evidence","Developing a fully automated evidence synthesis tool for identifying, assessing and collating the evidence","Evidence synthesis is a key element of evidence-based medicine. However, it is currently hampered by being labour intensive meaning that many trials are not incorporated into robust evidence syntheses and that many are out of date. To overcome this, a variety of techniques are being explored, including using automation technology. Here, we describe a fully automated evidence synthesis system for intervention studies, one that identifies all the relevant evidence, assesses the evidence for reliability and collates it to estimate the relative effectiveness of an intervention. Techniques used include machine learning, natural language processing and rule-based systems. Results are visualised using modern visualisation techniques. We believe this to be the first, publicly available, automated evidence synthesis system: an evidence mapping tool that synthesises evidence on the fly.","Evidence synthesis is a key element of evidence-based medicine. However, it is currently hampered by being labour intensive meaning that many trials are not incorporated into robust evidence syntheses and that many are out of date. To overcome this, a variety of techniques are being explored, including using automation technology. Here, we describe a fully automated evidence synthesis system for intervention studies, one that identifies all the relevant evidence, assesses the evidence for reliability and collates it to estimate the relative effectiveness of an intervention. Techniques used include machine learning, natural language processing and rule-based systems. Results are visualised using modern visualisation techniques. We believe this to be the first, publicly available, automated evidence synthesis system: an evidence mapping tool that synthesises evidence on the fly.","Brassey, J.
 and Price, C.
 and Edwards, J.
 and Zlabinger, M.
 and Bampoulidis, A.
 and Hanbury, A.","Brassey, Price, Edwards, Zlabinger, Bampoulidis, Hanbury",not available,https://doi.org/10.1136/bmjebm-2018-111126,2021-08-03
1270.0,,pubmed,Evaluation of text mining to reduce screening workload for injury-focused systematic reviews,Evaluation of text mining to reduce screening workload for injury-focused systematic reviews,"INTRODUCTION: Text mining to support screening in large-scale systematic reviews has been recommended; however, their suitability for reviews in injury research is not known. We examined the performance of text mining in supporting the second reviewer in a systematic review examining associations between fault attribution and health and work-related outcomes after transport injury. METHODS: Citations were independently screened in Abstrackr in full (reviewer 1; 10 559 citations), and until no more citations were predicted to be relevant (reviewer 2; 1809 citations, 17.1%). All potentially relevant full-text articles were assessed by reviewer 1 (555 articles). Reviewer 2 used text mining (Wordstat, QDA Miner) to reduce assessment to full-text articles containing >=1 fault-related exposure term (367 articles, 66.1%). RESULTS: Abstrackr offered excellent workload savings: 82.7% of citations did not require screening by reviewer 2, and total screening time was reduced by 36.6% compared with traditional dual screening of all citations. Abstrackr predictions had high specificity (83.7%), and low false negatives (0.3%), but overestimated citation relevance, probably due to the complexity of the review with multiple outcomes and high imbalance of relevant to irrelevant records, giving low sensitivity (29.7%) and precision (14.5%). Text mining of full-text articles reduced the number needing to be screened by 33.9%, and reduced total full-text screening time by 38.7% compared with traditional dual screening. CONCLUSIONS: Overall, text mining offered important benefits to systematic review workflow, but should not replace full screening by one reviewer, especially for complex reviews examining multiple health or injury outcomes. Trial registration number: crd42018084123.","Text mining to support screening in large-scale systematic reviews has been recommended; however, their suitability for reviews in injury research is not known. We examined the performance of text mining in supporting the second reviewer in a systematic review examining associations between fault attribution and health and work-related outcomes after transport injury. Citations were independently screened in Abstrackr in full (reviewer 1; 10â€‰559 citations), and until no more citations were predicted to be relevant (reviewer 2; 1809 citations, 17.1%). All potentially relevant full-text articles were assessed by reviewer 1 (555 articles). Reviewer 2 used text mining (Wordstat, QDA Miner) to reduce assessment to full-text articles containing â‰¥1 fault-related exposure term (367 articles, 66.1%). Abstrackr offered excellent workload savings: 82.7% of citations did not require screening by reviewer 2, and total screening time was reduced by 36.6% compared with traditional dual screening of all citations. Abstrackr predictions had high specificity (83.7%), and low false negatives (0.3%), but overestimated citation relevance, probably due to the complexity of the review with multiple outcomes and high imbalance of relevant to irrelevant records, giving low sensitivity (29.7%) and precision (14.5%). Text mining of full-text articles reduced the number needing to be screened by 33.9%, and reduced total full-text screening time by 38.7% compared with traditional dual screening. Overall, text mining offered important benefits to systematic review workflow, but should not replace full screening by one reviewer, especially for complex reviews examining multiple health or injury outcomes. CRD42018084123.","Giummarra, M. J.
 and Lau, G.
 and Gabbe, B. J.","Giummarra, Lau, Gabbe",not available,https://doi.org/10.1136/injuryprev-2019-043247,2021-08-03
1658.0,,pubmed,Closed and Open Reduction of Nasal Fractures,Closed and Open Reduction of Nasal Fractures,"AIM: The objective of this review was to determine the different types of surgical intervention in the management of nasal bone fractures, the outcomes, and complications associated with each intervention. METHODS: A search was conducted using the PubMed and Cochrane Database of Systematic Review databases from January 1, 1997 until September 9, 2017. The search strategy was constructed using the Population Intervention Comparison Outcome framework with keywords related to nasal fracture and its treatment. Two sets of independent researchers performed the analysis. Qualitative analysis was performed using the Methodological Index for Non-Randomized Studies and National Institute for Clinical Excellence methodology for randomized controlled trial checklists. RESULTS: The 4276 titles were obtained from PubMed database alone. Exclusion was made based on the title, abstract and full-text analysis. Finally, 23 papers were included and analyzed. Of the 23 papers, 13 (56.5%) were retrospective record review, 2 (8.7%) were randomized clinical trial or a randomized study and 8 case series (34.8%). 16 (69.6%) studies addressed closed reduction, 3 studies (13%) on open reduction and 4 studies (17.4%) addressed both open and closed reduction. The main focus in the outcome in all studies was accuracy of the anatomical reduction of the nasal bones. Three studies (13.0%) reported restoration of function such as breathing comfort or release in respiratory obstruction and another 3 (13.0%) addressed both cosmetic and breathing outcomes. Residual deformity was the most described complications in the studies (30.4%). In general, most of the studies were not of high quality as they lacked in some key elements in the Methodological Index for Non-Randomized Studies checklist. CONCLUSION: Both closed and open reduction provided good outcomes in cosmetic and breathing. Septoplasty is recommended to be performed simultaneously with fracture reduction.","The objective of this review was to determine the different types of surgical intervention in the management of nasal bone fractures, the outcomes, and complications associated with each intervention. A search was conducted using the PubMed and Cochrane Database of Systematic Review databases from January 1, 1997 until September 9, 2017. The search strategy was constructed using the Population Intervention Comparison Outcome framework with keywords related to nasal fracture and its treatment. Two sets of independent researchers performed the analysis. Qualitative analysis was performed using the Methodological Index for Non-Randomized Studies and National Institute for Clinical Excellence methodology for randomized controlled trial checklists. The 4276 titles were obtained from PubMed database alone. Exclusion was made based on the title, abstract and full-text analysis. Finally, 23 papers were included and analyzed. Of the 23 papers, 13 (56.5%) were retrospective record review, 2 (8.7%) were randomized clinical trial or a randomized study and 8 case series (34.8%). 16 (69.6%) studies addressed closed reduction, 3 studies (13%) on open reduction and 4 studies (17.4%) addressed both open and closed reduction. The main focus in the outcome in all studies was accuracy of the anatomical reduction of the nasal bones. Three studies (13.0%) reported restoration of function such as breathing comfort or release in respiratory obstruction and another 3 (13.0%) addressed both cosmetic and breathing outcomes. Residual deformity was the most described complications in the studies (30.4%). In general, most of the studies were not of high quality as they lacked in some key elements in the Methodological Index for Non-Randomized Studies checklist. Both closed and open reduction provided good outcomes in cosmetic and breathing. Septoplasty is recommended to be performed simultaneously with fracture reduction.","James, J. G.
 and Izam, A. S.
 and Nabil, S.
 and Rahman, N. A.
 and Ramli, R.","James, Izam, Nabil, Rahman, Ramli",https://dx.doi.org/10.1097/SCS.0000000000005812,https://doi.org/10.1097/SCS.0000000000005812,2021-08-03
3365.0,,pubmed,Automatic Neuroimage Processing and Analysis in Stroke-A Systematic Review,Automatic Neuroimage Processing and Analysis in Stroke-A Systematic Review,"This article presents a systematic review of the current computational technologies applied to medical images for the detection, segmentation, and classification of strokes. Besides, analyzing and evaluating the technological advances, the challenges to be overcome and the future trends are discussed. The principal approaches make use of artificial intelligence, digital image processing and analysis, and various other technologies to develop computer-aided diagnosis (CAD) systems to improve the accuracy in the diagnostic process, as well as the interpretation consistency of medical images. However, there are some points that require greater attention such as low sensitivity, optimization of the algorithm, a reduction of false positives, and improvement in the identification and segmentation processes of different sizes and shapes. Also, there is a need to improve the classification steps of different stroke types and subtypes. Furthermore, there is an additional need for further research to improve the current techniques and develop new algorithms to overcome disadvantages identified here. The main focus of this research is to analyze the applied technologies for the development of CAD systems and verify how effective they are for stroke detection, segmentation, and classification. The main contributions of this review are that it analyzes only up-to-date studies, mainly from 2015 to 2018, as well as organizing the various studies in the area according to the research proposal, i.e., detection, segmentation, and classification of the types of stroke and the respective techniques used. Thus, the review has great relevance for future research, since it presents an ample comparison of the most recent works in the area, clearly showing the existing difficulties and the models that have been proposed to overcome such difficulties.","This article presents a systematic review of the current computational technologies applied to medical images for the detection, segmentation, and classification of strokes. Besides, analyzing and evaluating the technological advances, the challenges to be overcome and the future trends are discussed. The principal approaches make use of artificial intelligence, digital image processing and analysis, and various other technologies to develop computer-aided diagnosis (CAD) systems to improve the accuracy in the diagnostic process, as well as the interpretation consistency of medical images. However, there are some points that require greater attention such as low sensitivity, optimization of the algorithm, a reduction of false positives, and improvement in the identification and segmentation processes of different sizes and shapes. Also, there is a need to improve the classification steps of different stroke types and subtypes. Furthermore, there is an additional need for further research to improve the current techniques and develop new algorithms to overcome disadvantages identified here. The main focus of this research is to analyze the applied technologies for the development of CAD systems and verify how effective they are for stroke detection, segmentation, and classification. The main contributions of this review are that it analyzes only up-to-date studies, mainly from 2015 to 2018, as well as organizing the various studies in the area according to the research proposal, i.e., detection, segmentation, and classification of the types of stroke and the respective techniques used. Thus, the review has great relevance for future research, since it presents an ample comparison of the most recent works in the area, clearly showing the existing difficulties and the models that have been proposed to overcome such difficulties.","Sarmento, R. M.
 and Vasconcelos, F. F. X.
 and Filho, P. P. R.
 and Wu, W.
 and de Albuquerque, V. H. C.","Sarmento, Vasconcelos, Filho, Wu, de Albuquerque",https://dx.doi.org/10.1109/RBME.2019.2934500,https://doi.org/10.1109/RBME.2019.2934500,2021-08-03
10837.0,,pubmed,Global Text Mining and Development of Pharmacogenomic Knowledge Resource for Precision Medicine,Global Text Mining and Development of Pharmacogenomic Knowledge Resource for Precision Medicine,"Understanding patients' genomic variations and their effect in protecting or predisposing them to drug response phenotypes is important for providing personalized healthcare. Several studies have manually curated such genotype-phenotype relationships into organized databases from clinical trial data or published literature. However, there are no text mining tools available to extract high-accuracy information from such existing knowledge. In this work, we used a semiautomated text mining approach to retrieve a complete pharmacogenomic (PGx) resource integrating disease-drug-gene-polymorphism relationships to derive a global perspective for ease in therapeutic approaches. We used an R package, pubmed. mineR, to automatically retrieve PGx-related literature. We identified 1,753 disease types, and 666 drugs, associated with 4,132 genes and 33,942 polymorphisms collated from 180,088 publications. With further manual curation, we obtained a total of 2,304 PGx relationships. We evaluated our approach by performance (precision = 0.806) with benchmark datasets like Pharmacogenomic Knowledgebase (PharmGKB) (0.904), Online Mendelian Inheritance in Man (OMIM) (0.600), and The Comparative Toxicogenomics Database (CTD) (0.729). We validated our study by comparing our results with 362 commercially used the US-Food and drug administration (FDA)-approved drug labeling biomarkers. Of the 2,304 PGx relationships identified, 127 belonged to the FDA list of 362 approved pharmacogenomic markers, indicating that our semiautomated text mining approach may reveal significant PGx information with markers for drug response prediction. In addition, it is a scalable and state-of-art approach in curation for PGx clinical utility.","Understanding patients' genomic variations and their effect in protecting or predisposing them to drug response phenotypes is important for providing personalized healthcare. Several studies have manually curated such genotype-phenotype relationships into organized databases from clinical trial data or published literature. However, there are no text mining tools available to extract high-accuracy information from such existing knowledge. In this work, we used a semiautomated text mining approach to retrieve a complete pharmacogenomic (PGx) resource integrating disease-drug-gene-polymorphism relationships to derive a global perspective for ease in therapeutic approaches. We used an R package, pubmed.mineR, to automatically retrieve PGx-related literature. We identified 1,753 disease types, and 666 drugs, associated with 4,132 genes and 33,942 polymorphisms collated from 180,088 publications. With further manual curation, we obtained a total of 2,304 PGx relationships. We evaluated our approach by performance (precision = 0.806) with benchmark datasets like Pharmacogenomic Knowledgebase (PharmGKB) (0.904), Online Mendelian Inheritance in Man (OMIM) (0.600), and The Comparative Toxicogenomics Database (CTD) (0.729). We validated our study by comparing our results with 362 commercially used the US- Food and drug administration (FDA)-approved drug labeling biomarkers. Of the 2,304 PGx relationships identified, 127 belonged to the FDA list of 362 approved pharmacogenomic markers, indicating that our semiautomated text mining approach may reveal significant PGx information with markers for drug response prediction. In addition, it is a scalable and state-of-art approach in curation for PGx clinical utility.","Guin, D. Rani, J. Singh, P. Grover, S. Bora, S. Talwar, P. Karthikeyan, M. Satyamoorthy, K. Adithan, C. Ramachandran, S. Saso, L. Hasija, Y. Kukreti, R.","Guin, Rani, Singh, Grover, Bora, Talwar, Karthikeyan, Satyamoorthy, Adithan, Ramachandran, Saso, Hasija, Kukreti",<Go to ISI>://WOS:000479139700001,https://doi.org/10.3389/fphar.2019.00839,2021-08-03
182.0,,pubmed,Infrequent use of clinical trials registries in published systematic reviews in urology,Infrequent use of clinical trials registries in published systematic reviews in urology,"OBJECTIVE: Validity of systematic reviews may be affected by non-publication of statistically non-significant or unfavorable clinical trial results. One function of clinical trial registries is to make these non-published studies available and thereby reduce potential publication bias. We aim to assess the use of clinical trial registries in published systematic reviews in urology. METHODS: We identified all systematic reviews published in the five highest-impact general urology journals that publish original research between January 1 and December 31, 2017. Full-text analysis was performed to determine if protocols included searching clinical trial registries meeting WHO Registry Network criteria. RESULTS: Of a total of 204 search results, 92 were included in the analysis as systematic reviews. The majority searched the MEDLINE (91, 98%), EMBASE (69, 75%), and Cochrane (49, 53%) databases. Based on published methods, only 16 (17%) systematic reviews searched any clinical trial registries: 14 (15%) ClinicalTrials.gov, 6 (6%) WHO International Clinical Trials Registry Platform, and 2 (2%) ISRCTN registry. Rates of searching clinical trial registries were low regardless of the journal: 8 of 34 (24%) in European Urology; 2 of 10 (20%) in BJU International; 3 of 17 (18%) in Urology; 2 of 18 (11%) in The Journal of Urology; and 1 of 13 (8%) in World Journal of Urology. CONCLUSION: The majority of recently published systematic reviews in urology do not routinely search clinical trial registries. Inclusion of these registries can help identify unpublished trial data, which may improve the quality of systematic reviews by reducing publication bias.","Validity of systematic reviews may be affected by non-publication of statistically non-significant or unfavorable clinical trial results. One function of clinical trial registries is to make these non-published studies available and thereby reduce potential publication bias. We aim to assess the use of clinical trial registries in published systematic reviews in urology. We identified all systematic reviews published in the five highest-impact general urology journals that publish original research between January 1 and December 31, 2017. Full-text analysis was performed to determine if protocols included searching clinical trial registries meeting WHO Registry Network criteria. Of a total of 204 search results, 92 were included in the analysis as systematic reviews. The majority searched the MEDLINE (91, 98%), EMBASE (69, 75%), and Cochrane (49, 53%) databases. Based on published methods, only 16 (17%) systematic reviews searched any clinical trial registries: 14 (15%) ClinicalTrials.gov, 6 (6%) WHO International Clinical Trials Registry Platform, and 2 (2%) ISRCTN registry. Rates of searching clinical trial registries were low regardless of the journal: 8 of 34 (24%) in European Urology; 2 of 10 (20%) in BJU International; 3 of 17 (18%) in Urology; 2 of 18 (11%) in The Journal of Urology; and 1 of 13 (8%) in World Journal of Urology. The majority of recently published systematic reviews in urology do not routinely search clinical trial registries. Inclusion of these registries can help identify unpublished trial data, which may improve the quality of systematic reviews by reducing publication bias.","Aro, T.
 and Koo, K.
 and Matlaga, B. R.","Aro, Koo, Matlaga",not available,https://doi.org/10.1007/s00345-019-02914-4,2021-08-03
2490.0,,pubmed,Immediate Reconstruction of Oncologic Spinal Wounds Is Cost-Effective Compared with Conventional Primary Wound Closure,Immediate Reconstruction of Oncologic Spinal Wounds Is Cost-Effective Compared with Conventional Primary Wound Closure,"BACKGROUND: Several studies have demonstrated a reduced wound complication rate when immediate soft-tissue reconstruction is performed after complex spine instrumentations in high-risk patients; however, the cost-effectiveness of this technique is not known. The authors hypothesized that immediate soft-tissue reconstruction of oncologic spine wounds would be a cost-effective strategy compared with the standard of care (i.e., oncologic spine surgery with conventional primary wound closure). METHODS: The authors used a decision tree model to evaluate the cost-utility, from the perspective of a hospital/insurer, of immediate reconstruction relative to the standard of care after oncologic spine surgery. A systematic review of the literature on oncologic spine surgery and immediate and delayed spinal wound reconstruction was performed to estimate health state probabilities. Overall expected cost and quality-adjusted life-years were assessed using a Monte Carlo simulation and sensitivity analyses. RESULTS: Immediate soft-tissue reconstruction after oncologic spine surgery had an expected cost of $81,458.90 and an expected average of 24.19 quality-adjusted life-years, whereas primary wound closure (no reconstruction) had an expected cost of $83,434.34 and an expected average of 24.17 quality-adjusted life-years, making immediate reconstruction the dominant, most cost-effective strategy. Monte Carlo sensitivity analysis demonstrated that immediate reconstruction was the preferred and most cost-effective option in the majority of simulations. Even when the willingness-to-pay threshold varied from $0 to $100,000 per quality-adjusted life-year, immediate reconstruction remained the dominant strategy across all iterations. CONCLUSION: This cost-utility analysis suggests that immediate soft-tissue reconstruction after oncologic spine surgery is more cost-effective than primary closure alone.","Several studies have demonstrated a reduced wound complication rate when immediate soft-tissue reconstruction is performed after complex spine instrumentations in high-risk patients; however, the cost-effectiveness of this technique is not known. The authors hypothesized that immediate soft-tissue reconstruction of oncologic spine wounds would be a cost-effective strategy compared with the standard of care (i.e., oncologic spine surgery with conventional primary wound closure). The authors used a decision tree model to evaluate the cost-utility, from the perspective of a hospital/insurer, of immediate reconstruction relative to the standard of care after oncologic spine surgery. A systematic review of the literature on oncologic spine surgery and immediate and delayed spinal wound reconstruction was performed to estimate health state probabilities. Overall expected cost and quality-adjusted life-years were assessed using a Monte Carlo simulation and sensitivity analyses. Immediate soft-tissue reconstruction after oncologic spine surgery had an expected cost of $81,458.90 and an expected average of 24.19 quality-adjusted life-years, whereas primary wound closure (no reconstruction) had an expected cost of $83,434.34 and an expected average of 24.17 quality-adjusted life-years, making immediate reconstruction the dominant, most cost-effective strategy. Monte Carlo sensitivity analysis demonstrated that immediate reconstruction was the preferred and most cost-effective option in the majority of simulations. Even when the willingness-to-pay threshold varied from $0 to $100,000 per quality-adjusted life-year, immediate reconstruction remained the dominant strategy across all iterations. This cost-utility analysis suggests that immediate soft-tissue reconstruction after oncologic spine surgery is more cost-effective than primary closure alone.","Mericli, A. F.
 and Rhines, L.
 and Bird, J.
 and Liu, J.
 and Selber, J. C.","Mericli, Rhines, Bird, Liu, Selber",https://dx.doi.org/10.1097/PRS.0000000000006170,https://doi.org/10.1097/PRS.0000000000006170,2021-08-03
3914.0,,pubmed,Deep Learning and Neurology: A Systematic Review,Deep Learning and Neurology: A Systematic Review,"Deciphering the massive volume of complex electronic data that has been compiled by hospital systems over the past decades has the potential to revolutionize modern medicine, as well as present significant challenges. Deep learning is uniquely suited to address these challenges, and recent advances in techniques and hardware have poised the field of medical machine learning for transformational growth. The clinical neurosciences are particularly well positioned to benefit from these advances given the subtle presentation of symptoms typical of neurologic disease. Here we review the various domains in which deep learning algorithms have already provided impetus for change-areas such as medical image analysis for the improved diagnosis of Alzheimer's disease and the early detection of acute neurologic events; medical image segmentation for quantitative evaluation of neuroanatomy and vasculature; connectome mapping for the diagnosis of Alzheimer's, autism spectrum disorder, and attention deficit hyperactivity disorder; and mining of microscopic electroencephalogram signals and granular genetic signatures. We additionally note important challenges in the integration of deep learning tools in the clinical setting and discuss the barriers to tackling the challenges that currently exist.","Deciphering the massive volume of complex electronic data that has been compiled by hospital systems over the past decades has the potential to revolutionize modern medicine, as well as present significant challenges. Deep learning is uniquely suited to address these challenges, and recent advances in techniques and hardware have poised the field of medical machine learning for transformational growth. The clinical neurosciences are particularly well positioned to benefit from these advances given the subtle presentation of symptoms typical of neurologic disease. Here we review the various domains in which deep learning algorithms have already provided impetus for change-areas such as medical image analysis for the improved diagnosis of Alzheimer's disease and the early detection of acute neurologic events; medical image segmentation for quantitative evaluation of neuroanatomy and vasculature; connectome mapping for the diagnosis of Alzheimer's, autism spectrum disorder, and attention deficit hyperactivity disorder; and mining of microscopic electroencephalogram signals and granular genetic signatures. We additionally note important challenges in the integration of deep learning tools in the clinical setting and discuss the barriers to tackling the challenges that currently exist.","Valliani, A. A.
 and Ranti, D.
 and Oermann, E. K.","Valliani, Ranti, Oermann",https://dx.doi.org/10.1007/s40120-019-00153-8,https://doi.org/10.1007/s40120-019-00153-8,2021-08-03
466.0,,pubmed,Machine learning applications to clinical decision support in neurosurgery: an artificial intelligence augmented systematic review,Machine learning applications to clinical decision support in neurosurgery: an artificial intelligence augmented systematic review,"Machine learning (ML) involves algorithms learning patterns in large, complex datasets to predict and classify. Algorithms include neural networks (NN), logistic regression (LR), and support vector machines (SVM). ML may generate substantial improvements in neurosurgery. This systematic review assessed the current state of neurosurgical ML applications and the performance of algorithms applied. Our systematic search strategy yielded 6866 results, 70 of which met inclusion criteria. Performance statistics analyzed included area under the receiver operating characteristics curve (AUC), accuracy, sensitivity, and specificity. Natural language processing (NLP) was used to model topics across the corpus and to identify keywords within surgical subspecialties. ML applications were heterogeneous. The densest cluster of studies focused on preoperative evaluation, planning, and outcome prediction in spine surgery. The main algorithms applied were NN, LR, and SVM. Input and output features varied widely and were listed to facilitate future research. The accuracy (F<sub>(2,19)</sub> = 6.56, p < 0.01) and specificity (F<sub>(2,16)</sub> = 5.57, p < 0.01) of NN, LR, and SVM differed significantly. NN algorithms demonstrated significantly higher accuracy than LR. SVM demonstrated significantly higher specificity than LR. We found no significant difference between NN, LR, and SVM AUC and sensitivity. NLP topic modeling reached maximum coherence at seven topics, which were defined by modeling approach, surgery type, and pathology themes. Keywords captured research foci within surgical domains. ML technology accurately predicts outcomes and facilitates clinical decision-making in neurosurgery. NNs frequently outperformed other algorithms on supervised learning tasks. This study identified gaps in the literature and opportunities for future neurosurgical ML research.","Machine learning (ML) involves algorithms learning patterns in large, complex datasets to predict and classify. Algorithms include neural networks (NN), logistic regression (LR), and support vector machines (SVM). ML may generate substantial improvements in neurosurgery. This systematic review assessed the current state of neurosurgical ML applications and the performance of algorithms applied. Our systematic search strategy yielded 6866 results, 70 of which met inclusion criteria. Performance statistics analyzed included area under the receiver operating characteristics curve (AUC), accuracy, sensitivity, and specificity. Natural language processing (NLP) was used to model topics across the corpus and to identify keywords within surgical subspecialties. ML applications were heterogeneous. The densest cluster of studies focused on preoperative evaluation, planning, and outcome prediction in spine surgery. The main algorithms applied were NN, LR, and SVM. Input and output features varied widely and were listed to facilitate future research. The accuracy (F<sub>(2,19)</sub>â€‰=â€‰6.56, pâ€‰&lt;â€‰0.01) and specificity (F<sub>(2,16)</sub>â€‰=â€‰5.57, pâ€‰&lt;â€‰0.01) of NN, LR, and SVM differed significantly. NN algorithms demonstrated significantly higher accuracy than LR. SVM demonstrated significantly higher specificity than LR. We found no significant difference between NN, LR, and SVM AUC and sensitivity. NLP topic modeling reached maximum coherence at seven topics, which were defined by modeling approach, surgery type, and pathology themes. Keywords captured research foci within surgical domains. ML technology accurately predicts outcomes and facilitates clinical decision-making in neurosurgery. NNs frequently outperformed other algorithms on supervised learning tasks. This study identified gaps in the literature and opportunities for future neurosurgical ML research.","Buchlak, Q. D.
 and Esmaili, N.
 and Leveque, J. C.
 and Farrokhi, F.
 and Bennett, C.
 and Piccardi, M.
 and Sethi, R. K.","Buchlak, Esmaili, Leveque, Farrokhi, Bennett, Piccardi, Sethi",https://dx.doi.org/10.1007/s10143-019-01163-8,https://doi.org/10.1007/s10143-019-01163-8,2021-08-03
3942.0,,pubmed,Physical fitness levels and moderators in people with epilepsy: A systematic review and meta-analysis,Physical fitness levels and moderators in people with epilepsy: A systematic review and meta-analysis,"Cardiorespiratory fitness (CRF) is a modifiable risk factor for mental and physical chronic conditions and premature mortality. Cardiorespiratory fitness levels and moderators among people living with epilepsy are unknown. The aim of the current meta-analysis was to (1) determine mean CRF in people living with epilepsy and compare levels with age- and gender-matched healthy controls (HCs), and (2) explore moderators of CRF. Major electronic databases were searched systematically for articles reporting CRF expressed as maximum or peak oxygen uptake (ml/min/kg). A random effects meta-analysis calculating the pooled mean CRF including subgroup- and meta-regression analyses was undertaken. Across 4 studies, the CRF level was 31.4ml/kg/min (95% confidence interval [CI]=27.3 to 35.5) (n=121; mean age=29-43years). Compared with age- and gender-matched controls (n=39), in people with epilepsy (n=39), CRF levels were 4.9ml/kg/min (95%CI=-5.9 to -3.9) lower (P<0.001). Cardiorespiratory fitness levels obtained via maximal tests were significantly (P<0.001) lower than obtained via submaximal tests. Future research should explore underlying mechanisms for the observed impairment in CRF in people with epilepsy.","Cardiorespiratory fitness (CRF) is a modifiable risk factor for mental and physical chronic conditions and premature mortality. Cardiorespiratory fitness levels and moderators among people living with epilepsy are unknown. The aim of the current meta-analysis was to (1) determine mean CRF in people living with epilepsy and compare levels with age- and gender-matched healthy controls (HCs), and (2) explore moderators of CRF. Major electronic databases were searched systematically for articles reporting CRF expressed as maximum or peak oxygen uptake (ml/min/kg). A random effects meta-analysis calculating the pooled mean CRF including subgroup- and meta-regression analyses was undertaken. Across 4 studies, the CRF level was 31.4â€¯ml/kg/min (95% confidence interval [CI]â€¯=â€¯27.3 to 35.5) (nâ€¯=â€¯121; mean ageâ€¯=â€¯29-43â€¯years). Compared with age- and gender-matched controls (nâ€¯=â€¯39), in people with epilepsy (nâ€¯=â€¯39), CRF levels were 4.9â€¯ml/kg/min (95%CIâ€¯=â€¯-5.9 to -3.9) lower (Pâ€¯&lt;â€¯0.001). Cardiorespiratory fitness levels obtained via maximal tests were significantly (Pâ€¯&lt;â€¯0.001) lower than obtained via submaximal tests. Future research should explore underlying mechanisms for the observed impairment in CRF in people with epilepsy.","Vancampfort, D.
 and Ward, P. B.
 and Stubbs, B.","Vancampfort, Ward, Stubbs",https://dx.doi.org/10.1016/j.yebeh.2019.106448,https://doi.org/10.1016/j.yebeh.2019.106448,2021-08-03
2109.0,,pubmed,Automated Segmentation of Tissues Using CT and MRI: A Systematic Review,Automated Segmentation of Tissues Using CT and MRI: A Systematic Review,"RATIONALE AND OBJECTIVES: The automated segmentation of organs and tissues throughout the body using computed tomography and magnetic resonance imaging has been rapidly increasing. Research into many medical conditions has benefited greatly from these approaches by allowing the development of more rapid and reproducible quantitative imaging markers. These markers have been used to help diagnose disease, determine prognosis, select patients for therapy, and follow responses to therapy. Because some of these tools are now transitioning from research environments to clinical practice, it is important for radiologists to become familiar with various methods used for automated segmentation. MATERIALS AND METHODS: The Radiology Research Alliance of the Association of University Radiologists convened an Automated Segmentation Task Force to conduct a systematic review of the peer-reviewed literature on this topic. RESULTS: The systematic review presented here includes 408 studies and discusses various approaches to automated segmentation using computed tomography and magnetic resonance imaging for neurologic, thoracic, abdominal, musculoskeletal, and breast imaging applications. CONCLUSION: These insights should help prepare radiologists to better evaluate automated segmentation tools and apply them not only to research, but eventually to clinical practice.","The automated segmentation of organs and tissues throughout the body using computed tomography and magnetic resonance imaging has been rapidly increasing. Research into many medical conditions has benefited greatly from these approaches by allowing the development of more rapid and reproducible quantitative imaging markers. These markers have been used to help diagnose disease, determine prognosis, select patients for therapy, and follow responses to therapy. Because some of these tools are now transitioning from research environments to clinical practice, it is important for radiologists to become familiar with various methods used for automated segmentation. The Radiology Research Alliance of the Association of University Radiologists convened an Automated Segmentation Task Force to conduct a systematic review of the peer-reviewed literature on this topic. The systematic review presented here includes 408 studies and discusses various approaches to automated segmentation using computed tomography and magnetic resonance imaging for neurologic, thoracic, abdominal, musculoskeletal, and breast imaging applications. These insights should help prepare radiologists to better evaluate automated segmentation tools and apply them not only to research, but eventually to clinical practice.","Lenchik, L.
 and Heacock, L.
 and Weaver, A. A.
 and Boutin, R. D.
 and Cook, T. S.
 and Itri, J.
 and Filippi, C. G.
 and Gullapalli, R. P.
 and Lee, J.
 and Zagurovskaya, M.
 and Retson, T.
 and Godwin, K.
 and Nicholson, J.
 and Narayana, P. A.","Lenchik, Heacock, Weaver, Boutin, Cook, Itri, Filippi, Gullapalli, Lee, Zagurovskaya, Retson, Godwin, Nicholson, Narayana",https://dx.doi.org/10.1016/j.acra.2019.07.006,https://doi.org/10.1016/j.acra.2019.07.006,2021-08-03
1767.0,,pubmed,The association between cardiorespiratory fitness and the incidence of common mental health disorders: A systematic review and meta-analysis,The association between cardiorespiratory fitness and the incidence of common mental health disorders: A systematic review and meta-analysis,"BACKGROUND: Physical activity is associated with a lower incidence of common mental health disorder, but less is known about the impact of cardiorespiratory fitness (CRF). METHODS: In this review, we systematically evaluated the relationship between CRF and the incidence of common mental health disorders in prospective cohort studies. We systematically searched six major electronic databases from inception to 23rd of May 2019. We assessed study quality using the Newcastle-Ottawa scale. RESULTS: We were able to pool the hazard ratios (HRs) and 95% confidence intervals (CIs) of four studies including at least 27,733,154 person-years of data. We found that low CRF (HR=1.47, [95% CI 1.23 - 1.76] p < 0.001 I<sup>2</sup>=85.1) and medium CRF (HR=1.23, [95% CI 1.09 - 1.38] p < 0.001 I<sup>2</sup>=87.20) CRF are associated with a 47% and 23% greater risk of a common mental health disorders respectively, compared with high CRF. We found evidence to suggest a dose-response relationship between CRF and the risk of common mental health disorders. LIMITATIONS: We were only able to identify a small number of eligible studies from our search and heterogeneity was substantial in the subsequent meta-analysis. CONCLUSIONS: Our findings indicate that there is a longitudinal association between CRF levels and the risk of a common mental health disorder. CRF levels could be useful for identifying and preventing common mental health disorders at a population-level.","Physical activity is associated with a lower incidence of common mental health disorder, but less is known about the impact of cardiorespiratory fitness (CRF). In this review, we systematically evaluated the relationship between CRF and the incidence of common mental health disorders in prospective cohort studies. We systematically searched six major electronic databases from inception to 23rd of May 2019. We assessed study quality using the Newcastle-Ottawa scale. We were able to pool the hazard ratios (HRs) and 95% confidence intervals (CIs) of four studies including at least 27,733,154 person-years of data. We found that low CRF (HRâ€¯=â€¯1.47, [95% CI 1.23 - 1.76] p &lt; 0.001 I<sup>2</sup>â€¯=â€¯85.1) and medium CRF (HRâ€¯=â€¯1.23, [95% CI 1.09 - 1.38] p &lt; 0.001 I<sup>2</sup>â€¯=â€¯87.20) CRF are associated with a 47% and 23% greater risk of a common mental health disorders respectively, compared with high CRF. We found evidence to suggest a dose-response relationship between CRF and the risk of common mental health disorders. We were only able to identify a small number of eligible studies from our search and heterogeneity was substantial in the subsequent meta-analysis. Our findings indicate that there is a longitudinal association between CRF levels and the risk of a common mental health disorder. CRF levels could be useful for identifying and preventing common mental health disorders at a population-level.","Kandola, A.
 and Ashdown-Franks, G.
 and Stubbs, B.
 and Osborn, D. P. J.
 and Hayes, J. F.","Kandola, Ashdown-Franks, Stubbs, Osborn, Hayes",https://dx.doi.org/10.1016/j.jad.2019.07.088,https://doi.org/10.1016/j.jad.2019.07.088,2021-08-03
4431.0,,pubmed,Effectiveness evaluation of computer-aided diagnosis system for the diagnosis of thyroid nodules on ultrasound: A systematic review and meta-analysis,Effectiveness evaluation of computer-aided diagnosis system for the diagnosis of thyroid nodules on ultrasound: A systematic review and meta-analysis,"BACKGROUND: More and more automated efficient ultrasound image analysis techniques, such as ultrasound-based computer-aided diagnosis system (CAD), were developed to obtain accurate, reproducible, and more objective diagnosis results for thyroid nodules. So far, whether the diagnostic performance of existing CAD systems can reach the diagnostic level of experienced radiologists is still controversial. The aim of the meta-analysis was to evaluate the accuracy of CAD for thyroid nodules' diagnosis by reviewing current literatures and summarizing the research status. METHODS: A detailed literature search on PubMed, Embase, and Cochrane Libraries for articles published until December 2018 was carried out. The diagnostic performances of CAD systems vs radiologist were evaluated by meta-analysis. We determined the sensitivity and the specificity across studies, calculated positive and negative likelihood ratios and constructed summary receiver-operating characteristic (SROC) curves. Meta-analysis of studies was performed using a mixed-effect, hierarchical logistic regression model. RESULTS: Five studies with 536 patients and 723 thyroid nodules were included in this meta-analysis. The pooled sensitivity, specificity, positive likelihood ratio, negative likelihood ratio, and diagnostic odds ratio (DOR) for CAD system were 0.87 (95% confidence interval [CI], 0.73-0.94), 0.79 (95% CI 0.63-0.89), 4.1 (95% CI 2.5-6.9), 0.17 (95% CI 0.09-0.32), and 25 (95% CI 15-42), respectively. The SROC curve indicated that the area under the curve was 0.90 (95% CI 0.87-0.92). The pooled sensitivity, specificity, positive likelihood ratio, negative likelihood ratio, and DOR for experienced radiologists were 0.82 (95% CI 0.69-0.91), 0.83 (95% CI 0.76-0.89), 4.9 (95% CI 3.4-7.0), 0.22 (95% CI 0.12-0.38), and 23 (95% CI 11-46), respectively. The SROC curve indicated that the area under the curve was 0.96 (95% CI 0.94-0.97). CONCLUSION: The sensitivity of the CAD system in the diagnosis of thyroid nodules was similar to that of experienced radiologists. However, the CAD system had lower specificity and DOR than experienced radiologists. The CAD system may play the potential role as a decision-making assistant alongside radiologists in the thyroid nodules' diagnosis. Future technical improvements would be helpful to increase the accuracy as well as diagnostic efficiency.","More and more automated efficient ultrasound image analysis techniques, such as ultrasound-based computer-aided diagnosis system (CAD), were developed to obtain accurate, reproducible, and more objective diagnosis results for thyroid nodules. So far, whether the diagnostic performance of existing CAD systems can reach the diagnostic level of experienced radiologists is still controversial. The aim of the meta-analysis was to evaluate the accuracy of CAD for thyroid nodules' diagnosis by reviewing current literatures and summarizing the research status. A detailed literature search on PubMed, Embase, and Cochrane Libraries for articles published until December 2018 was carried out. The diagnostic performances of CAD systems vs radiologist were evaluated by meta-analysis. We determined the sensitivity and the specificity across studies, calculated positive and negative likelihood ratios and constructed summary receiver-operating characteristic (SROC) curves. Meta-analysis of studies was performed using a mixed-effect, hierarchical logistic regression model. Five studies with 536 patients and 723 thyroid nodules were included in this meta-analysis. The pooled sensitivity, specificity, positive likelihood ratio, negative likelihood ratio, and diagnostic odds ratio (DOR) for CAD system were 0.87 (95% confidence interval [CI], 0.73-0.94), 0.79 (95% CI 0.63-0.89), 4.1 (95% CI 2.5-6.9), 0.17 (95% CI 0.09-0.32), and 25 (95% CI 15-42), respectively. The SROC curve indicated that the area under the curve was 0.90 (95% CI 0.87-0.92). The pooled sensitivity, specificity, positive likelihood ratio, negative likelihood ratio, and DOR for experienced radiologists were 0.82 (95% CI 0.69-0.91), 0.83 (95% CI 0.76-0.89), 4.9 (95% CI 3.4-7.0), 0.22 (95% CI 0.12-0.38), and 23 (95% CI 11-46), respectively. The SROC curve indicated that the area under the curve was 0.96 (95% CI 0.94-0.97). The sensitivity of the CAD system in the diagnosis of thyroid nodules was similar to that of experienced radiologists. However, the CAD system had lower specificity and DOR than experienced radiologists. The CAD system may play the potential role as a decision-making assistant alongside radiologists in the thyroid nodules' diagnosis. Future technical improvements would be helpful to increase the accuracy as well as diagnostic efficiency.","Zhao, W. J.
 and Fu, L. R.
 and Huang, Z. M.
 and Zhu, J. Q.
 and Ma, B. Y.","Zhao, Fu, Huang, Zhu, Ma",https://dx.doi.org/10.1097/MD.0000000000016379,https://doi.org/10.1097/MD.0000000000016379,2021-08-03
606.0,,pubmed,Medical knowledge infused convolutional neural networks for cohort selection in clinical trials,Medical knowledge infused convolutional neural networks for cohort selection in clinical trials,"OBJECTIVE: In this era of digitized health records, there has been a marked interest in using de-identified patient records for conducting various health related surveys. To assist in this research effort, we developed a novel clinical data representation model entitled medical knowledge-infused convolutional neural network (MKCNN), which is used for learning the clinical trial criteria eligibility status of patients to participate in cohort studies. MATERIALS AND METHODS: In this study, we propose a clinical text representation infused with medical knowledge (MK). First, we isolate the noise from the relevant data using a medically relevant description extractor; then we utilize log-likelihood ratio based weights from selected sentences to highlight 'met' and 'not-met' knowledge-infused representations in bichannel setting for each instance. The combined medical knowledge-infused representation (MK) from these modules helps identify significant clinical criteria semantics, which in turn renders effective learning when used with a convolutional neural network architecture. RESULTS: MKCNN outperforms other Medical Knowledge (MK) relevant learning architectures by approximately 3%; notably SVM and XGBoost implementations developed in this study. MKCNN scored 86.1% on F1metric, a gain of 6% above the average performance assessed from the submissions for n2c2 task. Although pattern/rule-based methods show a higher average performance for the n2c2 clinical data set, MKCNN significantly improves performance of machine learning implementations for clinical datasets. CONCLUSION: MKCNN scored 86.1% on the F1 score metric. In contrast to many of the rule-based systems introduced during the n2c2 challenge workshop, our system presents a model that heavily draws on machine-based learning. In addition, the MK representations add more value to clinical comprehension and interpretation of natural texts.","In this era of digitized health records, there has been a marked interest in using de-identified patient records for conducting various health related surveys. To assist in this research effort, we developed a novel clinical data representation model entitled medical knowledge-infused convolutional neural network (MKCNN), which is used for learning the clinical trial criteria eligibility status of patients to participate in cohort studies. In this study, we propose a clinical text representation infused with medical knowledge (MK). First, we isolate the noise from the relevant data using a medically relevant description extractor; then we utilize log-likelihood ratio based weights from selected sentences to highlight ""met"" and ""not-met"" knowledge-infused representations in bichannel setting for each instance. The combined medical knowledge-infused representation (MK) from these modules helps identify significant clinical criteria semantics, which in turn renders effective learning when used with a convolutional neural network architecture. MKCNN outperforms other Medical Knowledge (MK) relevant learning architectures by approximately 3%; notably SVM and XGBoost implementations developed in this study. MKCNN scored 86.1% on F1metric, a gain of 6% above the average performance assessed from the submissions for n2c2 task. Although pattern/rule-based methods show a higher average performance for the n2c2 clinical data set, MKCNN significantly improves performance of machine learning implementations for clinical datasets. MKCNN scored 86.1% on the F1 score metric. In contrast to many of the rule-based systems introduced during the n2c2 challenge workshop, our system presents a model that heavily draws on machine-based learning. In addition, the MK representations add more value to clinical comprehension and interpretation of natural texts.","Chen, C. J.
 and Warikoo, N.
 and Chang, Y. C.
 and Chen, J. H.
 and Hsu, W. L.","Chen, Warikoo, Chang, Chen, Hsu",https://dx.doi.org/10.1093/jamia/ocz128,https://doi.org/10.1093/jamia/ocz128,2021-08-03
1601.0,,pubmed,The use and performance of artificial intelligence applications in dental and maxillofacial radiology: A systematic review,The use and performance of artificial intelligence applications in dental and maxillofacial radiology: A systematic review,"OBJECTIVES: To investigate the current clinical applications and diagnostic performance of artificial intelligence (AI) in dental and maxillofacial radiology (DMFR). METHODS: Studies using applications related to DMFR to develop or implement AI models were sought by searching five electronic databases and four selected core journals in the field of DMFR. The customized assessment criteria based on QUADAS-2 were adapted for quality analysis of the studies included. RESULTS: The initial electronic search yielded 1862 titles, and 50 studies were eventually included. Most studies focused on AI applications for an automated localization of cephalometric landmarks, diagnosis of osteoporosis, classification/segmentation of maxillofacial cysts and/or tumors, and identification of periodontitis/periapical disease. The performance of AI models varies among different algorithms. CONCLUSION: The AI models proposed in the studies included exhibited wide clinical applications in DMFR. Nevertheless, it is still necessary to further verify the reliability and applicability of the AI models prior to transferring these models into clinical practice.","To investigate the current clinical applications and diagnostic performance of artificial intelligence (AI) in dental and maxillofacial radiology (DMFR). Studies using applications related to DMFR to develop or implement AI models were sought by searching five electronic databases and four selected core journals in the field of DMFR. The customized assessment criteria based on QUADAS-2 were adapted for quality analysis of the studies included. The initial electronic search yielded 1862 titles, and 50 studies were eventually included. Most studies focused on AI applications for an automated localization of cephalometric landmarks, diagnosis of osteoporosis, classification/segmentation of maxillofacial cysts and/or tumors, and identification of periodontitis/periapical disease. The performance of AI models varies among different algorithms. The AI models proposed in the studies included exhibited wide clinical applications in DMFR. Nevertheless, it is still necessary to further verify the reliability and applicability of the AI models prior to transferring these models into clinical practice.","Hung, K.
 and Montalvao, C.
 and Tanaka, R.
 and Kawai, T.
 and Bornstein, M. M.","Hung, Montalvao, Tanaka, Kawai, Bornstein",https://dx.doi.org/10.1259/dmfr.20190107,https://doi.org/10.1259/dmfr.20190107,2021-08-03
312.0,,pubmed,Artificial intelligence for assisting diagnostics and assessment of Parkinson's disease-A review,Artificial intelligence for assisting diagnostics and assessment of Parkinson's disease-A review,"Artificial intelligence, specifically machine learning, has found numerous applications in computer-aided diagnostics, monitoring and management of neurodegenerative movement disorders of parkinsonian type. These tasks are not trivial due to high inter-subject variability and similarity of clinical presentations of different neurodegenerative disorders in the early stages. This paper aims to give a comprehensive, high-level overview of applications of artificial intelligence through machine learning algorithms in kinematic analysis of movement disorders, specifically Parkinson's disease (PD). We surveyed papers published between January 2007 and January 2019, within online databases, including PubMed and Science Direct, with a focus on the most recently published studies. The search encompassed papers dealing with the implementation of machine learning algorithms for diagnosis and assessment of PD using data describing motion of upper and lower extremities. This systematic review presents an overview of 48 relevant studies published in the abovementioned period, which investigate the use of artificial intelligence for diagnostics, therapy assessment and progress prediction in PD based on body kinematics. Different machine learning algorithms showed promising results, particularly for early PD diagnostics. The investigated publications demonstrated the potentials of collecting data from affordable and globally available devices. However, to fully exploit artificial intelligence technologies in the future, more widespread collaboration is advised among medical institutions, clinicians and researchers, to facilitate aligning of data collection protocols, sharing and merging of data sets.","Artificial intelligence, specifically machine learning, has found numerous applications in computer-aided diagnostics, monitoring and management of neurodegenerative movement disorders of parkinsonian type. These tasks are not trivial due to high inter-subject variability and similarity of clinical presentations of different neurodegenerative disorders in the early stages. This paper aims to give a comprehensive, high-level overview of applications of artificial intelligence through machine learning algorithms in kinematic analysis of movement disorders, specifically Parkinson's disease (PD). We surveyed papers published between January 2007 and January 2019, within online databases, including PubMed and Science Direct, with a focus on the most recently published studies. The search encompassed papers dealing with the implementation of machine learning algorithms for diagnosis and assessment of PD using data describing motion of upper and lower extremities. This systematic review presents an overview of 48 relevant studies published in the abovementioned period, which investigate the use of artificial intelligence for diagnostics, therapy assessment and progress prediction in PD based on body kinematics. Different machine learning algorithms showed promising results, particularly for early PD diagnostics. The investigated publications demonstrated the potentials of collecting data from affordable and globally available devices. However, to fully exploit artificial intelligence technologies in the future, more widespread collaboration is advised among medical institutions, clinicians and researchers, to facilitate aligning of data collection protocols, sharing and merging of data sets.","Belic, M.
 and Bobic, V.
 and Badza, M.
 and Solaja, N.
 and Duric-Jovicic, M.
 and Kostic, V. S.","BeliÄ‡, BobiÄ‡, BadÅ¾a, Å olaja, ÄuriÄ‡-JoviÄiÄ‡, KostiÄ‡",https://dx.doi.org/10.1016/j.clineuro.2019.105442,https://doi.org/10.1016/j.clineuro.2019.105442,2021-08-03
4184.0,,pubmed,From pattern classification to stratification: towards conceptualizing the heterogeneity of Autism Spectrum Disorder,From pattern classification to stratification: towards conceptualizing the heterogeneity of Autism Spectrum Disorder,"Pattern classification and stratification approaches have increasingly been used in research on Autism Spectrum Disorder (ASD) over the last ten years with the goal of translation towards clinical applicability. Here, we present an extensive scoping literature review on those two approaches. We screened a total of 635 studies, of which 57 pattern classification and 19 stratification studies were included. We observed large variance across pattern classification studies in terms of predictive performance from about 60% to 98% accuracy, which is among other factors likely linked to sampling bias, different validation procedures across studies, the heterogeneity of ASD and differences in data quality. Stratification studies were less prevalent with only two studies reporting replications and just a few showing external validation. While some identified strata based on cognition and intelligence reappear across studies, biology as a stratification marker is clearly underexplored. In summary, mapping biological differences at the level of the individual with ASD is a major challenge for the field now. Conceptualizing those mappings and individual trajectories that lead to the diagnosis of ASD, will become a major challenge in the near future.","Pattern classification and stratification approaches have increasingly been used in research on Autism Spectrum Disorder (ASD) over the last ten years with the goal of translation towards clinical applicability. Here, we present an extensive scoping literature review on those two approaches. We screened a total of 635 studies, of which 57 pattern classification and 19 stratification studies were included. We observed large variance across pattern classification studies in terms of predictive performance from about 60% to 98% accuracy, which is among other factors likely linked to sampling bias, different validation procedures across studies, the heterogeneity of ASD and differences in data quality. Stratification studies were less prevalent with only two studies reporting replications and just a few showing external validation. While some identified strata based on cognition and intelligence reappear across studies, biology as a stratification marker is clearly underexplored. In summary, mapping biological differences at the level of the individual with ASD is a major challenge for the field now. Conceptualizing those mappings and individual trajectories that lead to the diagnosis of ASD, will become a major challenge in the near future.","Wolfers, T.
 and Floris, D. L.
 and Dinga, R.
 and van Rooij, D.
 and Isakoglou, C.
 and Kia, S. M.
 and Zabihi, M.
 and Llera, A.
 and Chowdanayaka, R.
 and Kumar, V. J.
 and Peng, H.
 and Laidi, C.
 and Batalle, D.
 and Dimitrova, R.
 and Charman, T.
 and Loth, E.
 and Lai, M. C.
 and Jones, E.
 and Baumeister, S.
 and Moessnang, C.
 and Banaschewski, T.
 and Ecker, C.
 and Dumas, G.
 and O'Muircheartaigh, J.
 and Murphy, D.
 and Buitelaar, J. K.
 and Marquand, A. F.
 and Beckmann, C. F.","Wolfers, Floris, Dinga, van Rooij, Isakoglou, Kia, Zabihi, Llera, Chowdanayaka, Kumar, Peng, Laidi, Batalle, Dimitrova, Charman, Loth, Lai, Jones, Baumeister, Moessnang, Banaschewski, Ecker, Dumas, O'Muircheartaigh, Murphy, Buitelaar, Marquand, Beckmann",https://dx.doi.org/10.1016/j.neubiorev.2019.07.010,https://doi.org/10.1016/j.neubiorev.2019.07.010,2021-08-03
1737.0,,pubmed,The efficacy of placebo for the treatment of cancer-related fatigue: a systematic review and meta-analysis,The efficacy of placebo for the treatment of cancer-related fatigue: a systematic review and meta-analysis,"PURPOSE: Cancer-related fatigue (CRF) is a common symptom among patients with cancer. The efficacy of placebo, however, was never the main objective of any meta-analysis. Predicting the efficacy of placebo may facilitate researchers in designing future clinical trials for the treatment of CRF. METHODS: We performed a systematic review searching for prospective clinical trials comparing any treatment versus placebo for the treatment of CRF. We included studies that enrolled patients with any primary site of neoplasia and any stage of cancer. We excluded all studies that assessed fatigue related to any treatment. The primary endpoint of this study is the mean effect of placebo on fatigue according to the Functional Assessment of Chronic Illness (FACIT-F) and Brief Fatigue Inventory (BFI) scales. The secondary endpoint was the proportion of patients who reported improvement in fatigue (response rate). RESULTS: We found 520 studies, and 29 studies with 3758 participants were included in the meta-analysis. Placebo had a mean effect of + 4.88 (95%CI + 2.45 to + 7.29) using the FACIT-F scale, although it was statistically worse than the interventions studied (p = 0.005). Using the BFI scale, placebo had an average effect of + 0.64 (95%CI + 0.02 to + 1.30), although it was also worse than the other interventions studied (p = 0.002). In terms of the response rate, 29% (95%CI 25-32%) of patients taking a placebo reported a significant improvement in CRF compared with 36% of patients treated with other interventions (p = 0.030). CONCLUSIONS: Placebo treatments had a significant effect on CRF, and predicting these effects may help design future studies for CRF.","Cancer-related fatigue (CRF) is a common symptom among patients with cancer. The efficacy of placebo, however, was never the main objective of any meta-analysis. Predicting the efficacy of placebo may facilitate researchers in designing future clinical trials for the treatment of CRF. We performed a systematic review searching for prospective clinical trials comparing any treatment versus placebo for the treatment of CRF. We included studies that enrolled patients with any primary site of neoplasia and any stage of cancer. We excluded all studies that assessed fatigue related to any treatment. The primary endpoint of this study is the mean effect of placebo on fatigue according to the Functional Assessment of Chronic Illness (FACIT-F) and Brief Fatigue Inventory (BFI) scales. The secondary endpoint was the proportion of patients who reported improvement in fatigue (response rate). We found 520 studies, and 29 studies with 3758 participants were included in the meta-analysis. Placebo had a mean effect of +â€‰4.88 (95%CI +â€‰2.45 to +â€‰7.29) using the FACIT-F scale, although it was statistically worse than the interventions studied (pâ€‰=â€‰0.005). Using the BFI scale, placebo had an average effect of +â€‰0.64 (95%CI +â€‰0.02 to +â€‰1.30), although it was also worse than the other interventions studied (pâ€‰=â€‰0.002). In terms of the response rate, 29% (95%CI 25-32%) of patients taking a placebo reported a significant improvement in CRF compared with 36% of patients treated with other interventions (pâ€‰=â€‰0.030). Placebo treatments had a significant effect on CRF, and predicting these effects may help design future studies for CRF.","Junior, P. N. A.
 and Barreto, C. M. N.
 and de Iracema Gomes Cubero, D.
 and Del Giglio, A.","Junior, Barreto, de Iracema Gomes Cubero, Del Giglio",https://dx.doi.org/10.1007/s00520-019-04977-w,https://doi.org/10.1007/s00520-019-04977-w,2021-08-03
1966.0,,pubmed,Iterative processes: a review of semi-supervised machine learning in rehabilitation science,Iterative processes: a review of semi-supervised machine learning in rehabilitation science,"<b>Purpose:</b> To define semi-supervised machine learning (SSML) and explore current and potential applications of this analytic strategy in rehabilitation research. <b>Method:</b> We conducted a scoping review using PubMed, GoogleScholar and Medline. Studies were included if they: (1) described a semi-supervised approach to apply machine learning algorithms during data analysis and (2) examined constructs encompassed by the International Classification of Functioning, Disability and Health (ICF). The first two authors reviewed identified articles and recorded study and participant characteristics. The ICF domain used in each study was also identified.","<b>Purpose:</b> To define semi-supervised machine learning (SSML) and explore current and potential applications of this analytic strategy in rehabilitation research.<b>Method:</b> We conducted a scoping review using PubMed, GoogleScholar and Medline. Studies were included if they: (1) described a semi-supervised approach to apply machine learning algorithms during data analysis and (2) examined constructs encompassed by the International Classification of Functioning, Disability and Health (ICF). The first two authors reviewed identified articles and recorded study and participant characteristics. The ICF domain used in each study was also identified.<b>Results:</b> After combining information from the eight studies, we established that SSML was a feasible approach for analysis of complex data in rehabilitation research. We also determined that semi-supervised approaches may be more accurate than supervised machine learning approaches.<b>Conclusions:</b> A semi-supervised approach to machine learning has potential to enhance our understanding of complex data sets in rehabilitation science. SSML mirrors the iterative process of rehabilitation, making this approach ideal for calibrating devices, classifying activities or identifying just-in-time interventions. Rehabilitation scientists who are interested in conducting SSML should collaborate with data scientists to advance the application of this approach within our field.Implications for rehabilitationSemi-supervised machine learning applications may be a feasible approach for analyses of complex data sets in rehabilitation research.Semi-supervised machine learning approaches uses a combination of labelled and unlabelled data to produce accurate predictive models, thereby requiring less user-input data than other machine learning approaches (i.e., supervised, unsupervised), reducing resource cost and user-burden.Semi-supervised machine learning is an iterative process that, when applied to rehabilitation assessment and outcomes, could produce accurate personalized models for treatment.Rehabilitation researchers and data scientists should collaborate to implement semi-supervised machine learning approaches in rehabilitation research, optimizing the power of large datasets that are becoming more readily available within the field (e.g., EEG signals, sensors, smarthomes).","Kringle, E. A.
 and Knutson, E. C.
 and Engstrom, C.
 and Terhorst, L.","Kringle, Knutson, Engstrom, Terhorst",https://dx.doi.org/10.1080/17483107.2019.1604831,https://doi.org/10.1080/17483107.2019.1604831,2021-08-03
1785.0,,pubmed,A Systematic Review of Techniques Employed for Determining Mental Health Using Social Media in Psychological Surveillance During Disasters,A Systematic Review of Techniques Employed for Determining Mental Health Using Social Media in Psychological Surveillance During Disasters,"ABSTRACTDuring disasters, people share their thoughts and emotions on social media and also provide information about the event. Mining the social media messages and updates can be helpful in understanding the emotional state of people during such unforeseen events as they are real-time data. The objective of this review is to explore the feasibility of using social media data for mental health surveillance as well as the techniques used for determining mental health using social media data during disasters. PubMed, PsycINFO, and PsycARTICLES databases were searched from 2009 to November 2018 for primary research studies. After screening and analyzing the records, 18 studies were included in this review. Twitter was the widely researched social media platform for understanding the mental health of people during a disaster. Psychological surveillance was done by identifying the sentiments expressed by people or the emotions they displayed in their social media posts. Classification of sentiments and emotions were done using lexicon-based or machine learning methods. It is not possible to conclude that a particular technique is the best performing one, because the performance of any method depends upon factors such as the disaster size, the volume of data, disaster setting, and the disaster web environment.","During disasters, people share their thoughts and emotions on social media and also provide information about the event. Mining the social media messages and updates can be helpful in understanding the emotional state of people during such unforeseen events as they are real-time data. The objective of this review is to explore the feasibility of using social media data for mental health surveillance as well as the techniques used for determining mental health using social media data during disasters. PubMed, PsycINFO, and PsycARTICLES databases were searched from 2009 to November 2018 for primary research studies. After screening and analyzing the records, 18 studies were included in this review. Twitter was the widely researched social media platform for understanding the mental health of people during a disaster. Psychological surveillance was done by identifying the sentiments expressed by people or the emotions they displayed in their social media posts. Classification of sentiments and emotions were done using lexicon-based or machine learning methods. It is not possible to conclude that a particular technique is the best performing one, because the performance of any method depends upon factors such as the disaster size, the volume of data, disaster setting, and the disaster web environment.","Karmegam, D.
 and Ramamoorthy, T.
 and Mappillairajan, B.","Karmegam, Ramamoorthy, Mappillairajan",https://dx.doi.org/10.1017/dmp.2019.40,https://doi.org/10.1017/dmp.2019.40,2021-08-03
1062.0,,pubmed,Diagnostic Clinical Trials in Breast Cancer Brain Metastases: Barriers and Innovations,Diagnostic Clinical Trials in Breast Cancer Brain Metastases: Barriers and Innovations,"Optimal treatment of breast cancer brain metastases (BCBM) is often hampered by limitations in diagnostic abilities. Developing innovative tools for BCBM diagnosis is vital for early detection and effective treatment. In this study we explored the advances in trial for the diagnosis of BCBM, with review of the literature. On May 8, 2019, we searched ClinicalTrials.gov for interventional and diagnostic clinical trials involving BCBM, without limiting for date or location. Information on trial characteristics, experimental interventions, results, and publications were collected and analyzed. In addition, a systematic review of the literature was conducted to explore published studies related to BCBM diagnosis. Only 9 diagnostic trials explored BCBM. Of these, 1 trial was withdrawn because of low accrual numbers. Three trials were completed; however, none had published results. Modalities in trial for BCBM diagnosis entailed magnetic resonance imaging (MRI), computed tomography (CT), positron emission tomography (PET), PET-CT, nanobodies, and circulating tumor cells (CTCs), along with a collection of novel tracers and imaging biomarkers. MRI continues to be the diagnostic modality of choice, whereas CT is best suited for acute settings. Advances in PET and PET-CT allow the collection of metabolic and functional information related to BCBM. CTC characterization can help reflect on the molecular foundations of BCBM, whereas cell-free DNA offers new genetic material for further exploration in trials. The integration of machine learning in BCBM diagnosis seems inevitable as we continue to aim for rapid and accurate detection and better patient outcomes.","Optimal treatment of breast cancer brain metastases (BCBM) is often hampered by limitations in diagnostic abilities. Developing innovative tools for BCBM diagnosis is vital for early detection and effective treatment. In this study we explored the advances in trial for the diagnosis of BCBM, with review of the literature. On May 8, 2019, we searched ClinicalTrials.gov for interventional and diagnostic clinical trials involving BCBM, without limiting for date or location. Information on trial characteristics, experimental interventions, results, and publications were collected and analyzed. In addition, a systematic review of the literature was conducted to explore published studies related to BCBM diagnosis. Only 9 diagnostic trials explored BCBM. Of these, 1 trial was withdrawn because of low accrual numbers. Three trials were completed; however, none had published results. Modalities in trial for BCBM diagnosis entailed magnetic resonance imaging (MRI), computed tomography (CT), positron emission tomography (PET), PET-CT, nanobodies, and circulating tumor cells (CTCs), along with a collection of novel tracers and imaging biomarkers. MRI continues to be the diagnostic modality of choice, whereas CT is best suited for acute settings. Advances in PET and PET-CT allow the collection of metabolic and functional information related to BCBM. CTC characterization can help reflect on the molecular foundations of BCBM, whereas cell-free DNA offers new genetic material for further exploration in trials. The integration of machine learning in BCBM diagnosis seems inevitable as we continue to aim for rapid and accurate detection and better patient outcomes.","Fares, J.
 and Kanojia, D.
 and Rashidi, A.
 and Ahmed, A. U.
 and Balyasnikova, I. V.
 and Lesniak, M. S.","Fares, Kanojia, Rashidi, Ahmed, Balyasnikova, Lesniak",https://dx.doi.org/10.1016/j.clbc.2019.05.018,https://doi.org/10.1016/j.clbc.2019.05.018,2021-08-03
4141.0,,pubmed,How accurate are suicide risk prediction models? Asking the right questions for clinical practice,How accurate are suicide risk prediction models? Asking the right questions for clinical practice,"Prediction models assist in stratifying and quantifying an individual's risk of developing a particular adverse outcome, and are widely used in cardiovascular and cancer medicine. Whether these approaches are accurate in predicting self-harm and suicide has been questioned. We searched for systematic reviews in the suicide risk assessment field, and identified three recent reviews that have examined current tools and models derived using machine learning approaches. In this clinical review, we present a critical appraisal of these reviews, and highlight three major limitations that are shared between them. First, structured tools are not compared with unstructured assessments routine in clinical practice. Second, they do not sufficiently consider a range of performance measures, including negative predictive value and calibration. Third, the potential role of these models as clinical adjuncts is not taken into consideration. We conclude by presenting the view that the current role of prediction models for self-harm and suicide is currently not known, and discuss some methodological issues and implications of some machine learning and other analytic techniques for clinical utility.","Prediction models assist in stratifying and quantifying an individual's risk of developing a particular adverse outcome, and are widely used in cardiovascular and cancer medicine. Whether these approaches are accurate in predicting self-harm and suicide has been questioned. We searched for systematic reviews in the suicide risk assessment field, and identified three recent reviews that have examined current tools and models derived using machine learning approaches. In this clinical review, we present a critical appraisal of these reviews, and highlight three major limitations that are shared between them. First, structured tools are not compared with unstructured assessments routine in clinical practice. Second, they do not sufficiently consider a range of performance measures, including negative predictive value and calibration. Third, the potential role of these models as clinical adjuncts is not taken into consideration. We conclude by presenting the view that the current role of prediction models for self-harm and suicide is currently not known, and discuss some methodological issues and implications of some machine learning and other analytic techniques for clinical utility.","Whiting, D.
 and Fazel, S.","Whiting, Fazel",https://dx.doi.org/10.1136/ebmental-2019-300102,https://doi.org/10.1136/ebmental-2019-300102,2021-08-03
853.0,,pubmed,"New Insights in Computational Methods for Pharmacovigilance: E-Synthesis, a Bayesian Framework for Causal Assessment","New Insights in Computational Methods for Pharmacovigilance: <i>E-Synthesis</i>, a Bayesian Framework for Causal Assessment","Today's surge of big data coming from multiple sources is raising the stakes that pharmacovigilance has to win, making evidence synthesis a more and more robust approach in the field. In this scenario, many scholars believe that new computational methods derived from data mining will effectively enhance the detection of early warning signals for adverse drug reactions, solving the gauntlets that post-marketing surveillance requires. This article highlights the need for a philosophical approach in order to fully realize a pharmacovigilance 2.0 revolution. A state of the art on evidence synthesis is presented, followed by the illustration of E-Synthesis, a Bayesian framework for causal assessment. Computational results regarding dose-response evidence are shown at the end of this article.","Today's surge of big data coming from multiple sources is raising the stakes that pharmacovigilance has to win, making evidence synthesis a more and more robust approach in the field. In this scenario, many scholars believe that new computational methods derived from data mining will effectively enhance the detection of early warning signals for adverse drug reactions, solving the gauntlets that post-marketing surveillance requires. This article highlights the need for a philosophical approach in order to fully realize a pharmacovigilance 2.0 revolution. A state of the art on evidence synthesis is presented, followed by the illustration of <i>E-Synthesis</i>, a Bayesian framework for causal assessment. Computational results regarding dose-response evidence are shown at the end of this article.","De Pretis, F.
 and Osimani, B.","De Pretis, Osimani",https://dx.doi.org/10.3390/ijerph16122221,https://doi.org/10.3390/ijerph16122221,2021-08-03
2704.0,,pubmed,The effects of Tai Chi on quality of life of cancer survivors: a systematic review and meta-analysis,The effects of Tai Chi on quality of life of cancer survivors: a systematic review and meta-analysis,"PURPOSES: To assess the effects of Tai Chi on quality of life (QOL) of cancer survivors. METHODS: The following databases were searched: PubMed, Cochrane CENTRAL, EBSCO (including MEDLINE, CINAHL, and other databases), ScienceDirect, CNKI, Wangfang Data, and CQVIP until April 25, 2018. Randomized controlled trials (RCTs) published in English or Chinese examining the effects of Tai Chi intervention for cancer survivors were included. The primary outcome was QOL; the secondary outcomes were limb function/muscular strength, immune function indicators, cancer-related fatigue (CRF), and sleep disturbance. Methodological quality was assessed using the Cochrane Risk of Bias tool. Results of RCTs were pooled with mean difference (MD) or standardized mean difference (SMD) with 95% confidence intervals (CI). Quality of evidence for each outcome was assessed with the GRADE system. RESULTS: Twenty-two RCTs were included in this review. Tai Chi improved the physical (SMD 0.34, 95%CI 0.09, 0.59) and mental health (SMD 0.60, 95%CI 0.12, 1.08) domains of quality of life. The intervention improved the limb/muscular function of breast cancer survivors (SMD 1.19, 95%CI 0.63, 1.75) and in mixed samples of cancer survivors reduced the levels of cortisol (MD - 0.09, 95%CI - 0.16, - 0.02), alleviated CRF (SMD - 0.37, 95%CI - 0.70, - 0.04), and promoted sleep (SMD - 0.37, 95%CI - 0.72, - 0.02). CONCLUSION: There is low-level evidence suggesting that Tai Chi improves physical and mental dimensions of QOL and sleep. There is moderate-level evidence suggesting Tai Chi reduces levels of cortisol and CRF and improves limb function. Additional studies with larger sample sizes and with higher-quality RCT designs comparing different regimens of Tai Chi are warranted.","To assess the effects of Tai Chi on quality of life (QOL) of cancer survivors. The following databases were searched: PubMed, Cochrane CENTRAL, EBSCO (including MEDLINE, CINAHL, and other databases), ScienceDirect, CNKI, Wangfang Data, and CQVIP until April 25, 2018. Randomized controlled trials (RCTs) published in English or Chinese examining the effects of Tai Chi intervention for cancer survivors were included. The primary outcome was QOL; the secondary outcomes were limb function/muscular strength, immune function indicators, cancer-related fatigue (CRF), and sleep disturbance. Methodological quality was assessed using the Cochrane Risk of Bias tool. Results of RCTs were pooled with mean difference (MD) or standardized mean difference (SMD) with 95% confidence intervals (CI). Quality of evidence for each outcome was assessed with the GRADE system. Twenty-two RCTs were included in this review. Tai Chi improved the physical (SMD 0.34, 95%CI 0.09, 0.59) and mental health (SMD 0.60, 95%CI 0.12, 1.08) domains of quality of life. The intervention improved the limb/muscular function of breast cancer survivors (SMD 1.19, 95%CI 0.63, 1.75) and in mixed samples of cancer survivors reduced the levels of cortisol (MD -â€‰0.09, 95%CI -â€‰0.16, -â€‰0.02), alleviated CRF (SMD -â€‰0.37, 95%CI -â€‰0.70, -â€‰0.04), and promoted sleep (SMD -â€‰0.37, 95%CI -â€‰0.72, -â€‰0.02). There is low-level evidence suggesting that Tai Chi improves physical and mental dimensions of QOL and sleep. There is moderate-level evidence suggesting Tai Chi reduces levels of cortisol and CRF and improves limb function. Additional studies with larger sample sizes and with higher-quality RCT designs comparing different regimens of Tai Chi are warranted.","Ni, X.
 and Chan, R. J.
 and Yates, P.
 and Hu, W.
 and Huang, X.
 and Lou, Y.","Ni, Chan, Yates, Hu, Huang, Lou",https://dx.doi.org/10.1007/s00520-019-04911-0,https://doi.org/10.1007/s00520-019-04911-0,2021-08-03
967.0,,pubmed,ML-Net: multi-label classification of biomedical texts with deep neural networks,ML-Net: multi-label classification of biomedical texts with deep neural networks,"OBJECTIVE: In multi-label text classification, each textual document is assigned 1 or more labels. As an important task that has broad applications in biomedicine, a number of different computational methods have been proposed. Many of these methods, however, have only modest accuracy or efficiency and limited success in practical use. We propose ML-Net, a novel end-to-end deep learning framework, for multi-label classification of biomedical texts. MATERIALS AND METHODS: ML-Net combines a label prediction network with an automated label count prediction mechanism to provide an optimal set of labels. This is accomplished by leveraging both the predicted confidence score of each label and the deep contextual information (modeled by ELMo) in the target document. We evaluate ML-Net on 3 independent corpora in 2 text genres: biomedical literature and clinical notes. For evaluation, we use example-based measures, such as precision, recall, and the F measure. We also compare ML-Net with several competitive machine learning and deep learning baseline models. RESULTS: Our benchmarking results show that ML-Net compares favorably to state-of-the-art methods in multi-label classification of biomedical text. ML-Net is also shown to be robust when evaluated on different text genres in biomedicine. CONCLUSION: ML-Net is able to accuractely represent biomedical document context and dynamically estimate the label count in a more systematic and accurate manner. Unlike traditional machine learning methods, ML-Net does not require human effort for feature engineering and is a highly efficient and scalable approach to tasks with a large set of labels, so there is no need to build individual classifiers for each separate label.","In multi-label text classification, each textual document is assigned 1 or more labels. As an important task that has broad applications in biomedicine, a number of different computational methods have been proposed. Many of these methods, however, have only modest accuracy or efficiency and limited success in practical use. We propose ML-Net, a novel end-to-end deep learning framework, for multi-label classification of biomedical texts. ML-Net combines a label prediction network with an automated label count prediction mechanism to provide an optimal set of labels. This is accomplished by leveraging both the predicted confidence score of each label and the deep contextual information (modeled by ELMo) in the target document. We evaluate ML-Net on 3 independent corpora in 2 text genres: biomedical literature and clinical notes. For evaluation, we use example-based measures, such as precision, recall, and the F measure. We also compare ML-Net with several competitive machine learning and deep learning baseline models. Our benchmarking results show that ML-Net compares favorably to state-of-the-art methods in multi-label classification of biomedical text. ML-Net is also shown to be robust when evaluated on different text genres in biomedicine. ML-Net is able to accuractely represent biomedical document context and dynamically estimate the label count in a more systematic and accurate manner. Unlike traditional machine learning methods, ML-Net does not require human effort for feature engineering and is a highly efficient and scalable approach to tasks with a large set of labels, so there is no need to build individual classifiers for each separate label.","Du, J.
 and Chen, Q.
 and Peng, Y.
 and Xiang, Y.
 and Tao, C.
 and Lu, Z.","Du, Chen, Peng, Xiang, Tao, Lu",https://dx.doi.org/10.1093/jamia/ocz085,https://doi.org/10.1093/jamia/ocz085,2021-08-03
3275.0,,pubmed,Machine learning with the hierarchy-of-hypotheses (HoH) approach discovers novel pattern in studies on biological invasions,Machine learning with the hierarchy-of-hypotheses (HoH) approach discovers novel pattern in studies on biological invasions,"Research synthesis on simple yet general hypotheses and ideas is challenging in scientific disciplines studying highly context-dependent systems such as medical, social, and biological sciences. This study shows that machine learning, equation-free statistical modeling of artificial intelligence, is a promising synthesis tool for discovering novel patterns and the source of controversy in a general hypothesis. We apply a decision tree algorithm, assuming that evidence from various contexts can be adequately integrated in a hierarchically nested structure. As a case study, we analyzed 163 articles that studied a prominent hypothesis in invasion biology, the enemy release hypothesis. We explored if any of the nine attributes that classify each study can differentiate conclusions as classification problem. Results corroborated that machine learning can be useful for research synthesis, as the algorithm could detect patterns that had been already focused in previous narrative reviews. Compared with the previous synthesis study that assessed the same evidence collection based on experts' judgement, the algorithm has newly proposed that the studies focusing on Asian regions mostly supported the hypothesis, suggesting that more detailed investigations in these regions can enhance our understanding of the hypothesis. We suggest that machine learning algorithms can be a promising synthesis tool especially where studies (a) reformulate a general hypothesis from different perspectives, (b) use different methods or variables, or (c) report insufficient information for conducting meta-analyses.","Research synthesis on simple yet general hypotheses and ideas is challenging in scientific disciplines studying highly context-dependent systems such as medical, social, and biological sciences. This study shows that machine learning, equation-free statistical modeling of artificial intelligence, is a promising synthesis tool for discovering novel patterns and the source of controversy in a general hypothesis. We apply a decision tree algorithm, assuming that evidence from various contexts can be adequately integrated in a hierarchically nested structure. As a case study, we analyzed 163 articles that studied a prominent hypothesis in invasion biology, the enemy release hypothesis. We explored if any of the nine attributes that classify each study can differentiate conclusions as classification problem. Results corroborated that machine learning can be useful for research synthesis, as the algorithm could detect patterns that had been already focused in previous narrative reviews. Compared with the previous synthesis study that assessed the same evidence collection based on experts' judgement, the algorithm has newly proposed that the studies focusing on Asian regions mostly supported the hypothesis, suggesting that more detailed investigations in these regions can enhance our understanding of the hypothesis. We suggest that machine learning algorithms can be a promising synthesis tool especially where studies (a) reformulate a general hypothesis from different perspectives, (b) use different methods or variables, or (c) report insufficient information for conducting meta-analyses.","Ryo, M.
 and Jeschke, J. M.
 and Rillig, M. C.
 and Heger, T.","Ryo, Jeschke, Rillig, Heger",https://dx.doi.org/10.1002/jrsm.1363,https://doi.org/10.1002/jrsm.1363,2021-08-03
2756.0,,pubmed,A question of trust: can we build an evidence base to gain trust in systematic review automation technologies?,A question of trust: can we build an evidence base to gain trust in systematic review automation technologies?,"BACKGROUND: Although many aspects of systematic reviews use computational tools, systematic reviewers have been reluctant to adopt machine learning tools. DISCUSSION: We discuss that the potential reason for the slow adoption of machine learning tools into systematic reviews is multifactorial. We focus on the current absence of trust in automation and set-up challenges as major barriers to adoption. It is important that reviews produced using automation tools are considered non-inferior or superior to current practice. However, this standard will likely not be sufficient to lead to widespread adoption. As with many technologies, it is important that reviewers see 'others' in the review community using automation tools. Adoption will also be slow if the automation tools are not compatible with workflows and tasks currently used to produce reviews. Many automation tools being developed for systematic reviews mimic classification problems. Therefore, the evidence that these automation tools are non-inferior or superior can be presented using methods similar to diagnostic test evaluations, i.e., precision and recall compared to a human reviewer. However, the assessment of automation tools does present unique challenges for investigators and systematic reviewers, including the need to clarify which metrics are of interest to the systematic review community and the unique documentation challenges for reproducible software experiments. CONCLUSION: We discuss adoption barriers with the goal of providing tool developers with guidance as to how to design and report such evaluations and for end users to assess their validity. Further, we discuss approaches to formatting and announcing publicly available datasets suitable for assessment of automation technologies and tools. Making these resources available will increase trust that tools are non-inferior or superior to current practice. Finally, we identify that, even with evidence that automation tools are non-inferior or superior to current practice, substantial set-up challenges remain for main stream integration of automation into the systematic review process.","Although many aspects of systematic reviews use computational tools, systematic reviewers have been reluctant to adopt machine learning tools. We discuss that the potential reason for the slow adoption of machine learning tools into systematic reviews is multifactorial. We focus on the current absence of trust in automation and set-up challenges as major barriers to adoption. It is important that reviews produced using automation tools are considered non-inferior or superior to current practice. However, this standard will likely not be sufficient to lead to widespread adoption. As with many technologies, it is important that reviewers see ""others"" in the review community using automation tools. Adoption will also be slow if the automation tools are not compatible with workflows and tasks currently used to produce reviews. Many automation tools being developed for systematic reviews mimic classification problems. Therefore, the evidence that these automation tools are non-inferior or superior can be presented using methods similar to diagnostic test evaluations, i.e., precision and recall compared to a human reviewer. However, the assessment of automation tools does present unique challenges for investigators and systematic reviewers, including the need to clarify which metrics are of interest to the systematic review community and the unique documentation challenges for reproducible software experiments. We discuss adoption barriers with the goal of providing tool developers with guidance as to how to design and report such evaluations and for end users to assess their validity. Further, we discuss approaches to formatting and announcing publicly available datasets suitable for assessment of automation technologies and tools. Making these resources available will increase trust that tools are non-inferior or superior to current practice. Finally, we identify that, even with evidence that automation tools are non-inferior or superior to current practice, substantial set-up challenges remain for main stream integration of automation into the systematic review process.","O'Connor, A. M.
 and Tsafnat, G.
 and Thomas, J.
 and Glasziou, P.
 and Gilbert, S. B.
 and Hutton, B.","O'Connor, Tsafnat, Thomas, Glasziou, Gilbert, Hutton",not available,https://doi.org/10.1186/s13643-019-1062-0,2021-08-03
2609.0,,pubmed,A Systematic Review of Bystander Interventions for the Prevention of Sexual Violence,A Systematic Review of Bystander Interventions for the Prevention of Sexual Violence,"INTRODUCTION: Bystander interventions have been successful in changing bystander attitudes and behaviors to prevent sexual violence. This systematic review was performed to summarize and categorize the characteristics of sexual violence bystander intervention programs and analyze bystander intervention training approaches for the primary prevention of sexual violence and assault. METHOD: From June to July 2017, the authors searched both published and unpublished American and Canadian studies from 2007 to 2017. The published sources included six major electronic databases and the unpublished sources were Google Scholar and the 40 program websites. From the 706 studies that resulted from this initial search, a total of 44 studies (that included a single bystander intervention program and assessments at both pretest and at least one posttest) were included. RESULTS: Thirty-two percent of studies analyzed bystander behavior postintervention, and most found significant beneficial outcomes. The most frequently used training methods were presentation, discussion, and active learning exercises. Bringing in the Bystander and The Men's Program had the most replicated empirical support for effectiveness. DISCUSSION: There has been a substantive increase in quasi-experimental and randomized controlled trial approaches to assessing the effectiveness of this type of intervention since 2014. The training methods shared between these efficacious programs may translate to bystander interventions for other victimization types, such as child abuse. CONCLUSION: The use of in-person bystander training can make positive changes in attitudes and behaviors by increasing awareness of a problem and responsibility to solve it.","Bystander interventions have been successful in changing bystander attitudes and behaviors to prevent sexual violence. This systematic review was performed to summarize and categorize the characteristics of sexual violence bystander intervention programs and analyze bystander intervention training approaches for the primary prevention of sexual violence and assault. From June to July 2017, the authors searched both published and unpublished American and Canadian studies from 2007 to 2017. The published sources included six major electronic databases and the unpublished sources were Google Scholar and the 40 program websites. From the 706 studies that resulted from this initial search, a total of 44 studies (that included a single bystander intervention program and assessments at both pretest and at least one posttest) were included. Thirty-two percent of studies analyzed bystander behavior postintervention, and most found significant beneficial outcomes. The most frequently used training methods were presentation, discussion, and active learning exercises. Bringing in the Bystander and The Men's Program had the most replicated empirical support for effectiveness. There has been a substantive increase in quasi-experimental and randomized controlled trial approaches to assessing the effectiveness of this type of intervention since 2014. The training methods shared between these efficacious programs may translate to bystander interventions for other victimization types, such as child abuse. The use of in-person bystander training can make positive changes in attitudes and behaviors by increasing awareness of a problem and responsibility to solve it.","Mujal, G. N.
 and Taylor, M. E.
 and Fry, J. L.
 and Gochez-Kerr, T. H.
 and Weaver, N. L.","Mujal, Taylor, Fry, Gochez-Kerr, Weaver",https://dx.doi.org/10.1177/1524838019849587,https://doi.org/10.1177/1524838019849587,2021-08-03
2315.0,,pubmed,Secondary use of standardized nursing care data for advancing nursing science and practice: a systematic review,Secondary use of standardized nursing care data for advancing nursing science and practice: a systematic review,"OBJECTIVE: The study sought to present the findings of a systematic review of studies involving secondary analyses of data coded with standardized nursing terminologies (SNTs) retrieved from electronic health records (EHRs). MATERIALS AND METHODS: We identified studies that performed secondary analysis of SNT-coded nursing EHR data from PubMed, CINAHL, and Google Scholar. We screened 2570 unique records and identified 44 articles of interest. We extracted research questions, nursing terminologies, sample characteristics, variables, and statistical techniques used from these articles. An adapted STROBE (Strengthening The Reporting of OBservational Studies in Epidemiology) Statement checklist for observational studies was used for reproducibility assessment. RESULTS: Forty-four articles were identified. Their study foci were grouped into 3 categories: (1) potential uses of SNT-coded nursing data or challenges associated with this type of data (feasibility of standardizing nursing data), (2) analysis of SNT-coded nursing data to describe the characteristics of nursing care (characterization of nursing care), and (3) analysis of SNT-coded nursing data to understand the impact or effectiveness of nursing care (impact of nursing care). The analytical techniques varied including bivariate analysis, data mining, and predictive modeling. DISCUSSION: SNT-coded nursing data extracted from EHRs is useful in characterizing nursing practice and offers the potential for demonstrating its impact on patient outcomes. CONCLUSIONS: Our study provides evidence of the value of SNT-coded nursing data in EHRs. Future studies are needed to identify additional useful methods of analyzing SNT-coded nursing data and to combine nursing data with other data elements in EHRs to fully characterize the patient's health care experience.","The study sought to present the findings of a systematic review of studies involving secondary analyses of data coded with standardized nursing terminologies (SNTs) retrieved from electronic health records (EHRs). We identified studies that performed secondary analysis of SNT-coded nursing EHR data from PubMed, CINAHL, and Google Scholar. We screened 2570 unique records and identified 44 articles of interest. We extracted research questions, nursing terminologies, sample characteristics, variables, and statistical techniques used from these articles. An adapted STROBE (Strengthening The Reporting of OBservational Studies in Epidemiology) Statement checklist for observational studies was used for reproducibility assessment. Forty-four articles were identified. Their study foci were grouped into 3 categories: (1) potential uses of SNT-coded nursing data or challenges associated with this type of data (feasibility of standardizing nursing data), (2) analysis of SNT-coded nursing data to describe the characteristics of nursing care (characterization of nursing care), and (3) analysis of SNT-coded nursing data to understand the impact or effectiveness of nursing care (impact of nursing care). The analytical techniques varied including bivariate analysis, data mining, and predictive modeling. SNT-coded nursing data extracted from EHRs is useful in characterizing nursing practice and offers the potential for demonstrating its impact on patient outcomes. Our study provides evidence of the value of SNT-coded nursing data in EHRs. Future studies are needed to identify additional useful methods of analyzing SNT-coded nursing data and to combine nursing data with other data elements in EHRs to fully characterize the patient's health care experience.","Macieira, T. G. R.
 and Chianca, T. C. M.
 and Smith, M. B.
 and Yao, Y.
 and Bian, J.
 and Wilkie, D. J.
 and Dunn Lopez, K.
 and Keenan, G. M.","Macieira, Chianca, Smith, Yao, Bian, Wilkie, Dunn Lopez, Keenan",https://dx.doi.org/10.1093/jamia/ocz086,https://doi.org/10.1093/jamia/ocz086,2021-08-03
1286.0,,pubmed,The development of search filters for adverse effects of medical devices in medline and embase,The development of search filters for adverse effects of medical devices in medline and embase,"BACKGROUND: Objective ly derived search filters for adverse drug effects and complications in surgery have been developed but not for medical device adverse effects. OBJECTIVE: To develop and validate search filters to retrieve evidence on medical device adverse effects from ovid medline and embase. METHODS: We identified systematic reviews from Epistemonikos and the Health Technology Assessment (hta) database. Included studies within these reviews that reported on medical device adverse effects were randomly divided into three test sets and one validation set of records. Using word frequency analysis from one test set, we constructed a sensitivity maximising search strategy. This strategy was refined using two other test sets, then validated. RESULTS: From 186 systematic reviews which met our inclusion criteria, 1984 unique included studies were available from medline and 1986 from embase. Generic adverse effects searches in medline and embase achieved 84% and 83% sensitivity. Recall was improved to over 90%, however, when specific adverse effects terms were added. CONCLUSION: We have derived and validated novel search filters that retrieve over 80% of records with medical device adverse effects data in medline and embase. The addition of specific adverse effects terms is required to achieve higher levels of sensitivity.","Objectively derived search filters for adverse drug effects and complications in surgery have been developed but not for medical device adverse effects. To develop and validate search filters to retrieve evidence on medical device adverse effects from ovid medline and embase. We identified systematic reviews from Epistemonikos and the Health Technology Assessment (hta) database. Included studies within these reviews that reported on medical device adverse effects were randomly divided into three test sets and one validation set of records. Using word frequency analysis from one test set, we constructed a sensitivity maximising search strategy. This strategy was refined using two other test sets, then validated. From 186 systematic reviews which met our inclusion criteria, 1984 unique included studies were available from medline and 1986 from embase. Generic adverse effects searches in medline and embase achieved 84% and 83% sensitivity. Recall was improved to over 90%, however, when specific adverse effects terms were added. We have derived and validated novel search filters that retrieve over 80% of records with medical device adverse effects data in medline and embase. The addition of specific adverse effects terms is required to achieve higher levels of sensitivity.","Golder, S.
 and Farrah, K.
 and Mierzwinski-Urban, M.
 and Wright, K.
 and Loke, Y. K.","Golder, Farrah, Mierzwinski-Urban, Wright, Loke",https://dx.doi.org/10.1111/hir.12260,https://doi.org/10.1111/hir.12260,2021-08-03
2177.0,,pubmed,Effect of acupuncture at 3 anti-fatigue acupoints in the treatment of cancer-related fatigue in patients with cancer: Protocol for a systematic review and meta-analysis of randomized controlled trials,Effect of acupuncture at 3 anti-fatigue acupoints in the treatment of cancer-related fatigue in patients with cancer: Protocol for a systematic review and meta-analysis of randomized controlled trials,"BACKGROUND: Cancer-related fatigue (CRF), is a common distressing symptom of cancer. What's more, 'Three anti-fatigue acupoints' is one of the most important components of 'Jin's 3-needle therapy' created by Rui Jin, a professor of Guangzhou University of Chinese Medicine, which can be used in the treatment of CRF. In this article, researchers will assess the safety and effect of acupuncture at 3 anti-fatigue acupoints on CRF in patients with cancer. METHODS: Literature search for relevant articles up to October 2018 will be carried out in 9 databases: Cochrane Library, Embase, PubMed, VIP, CBM, CNKI, Wanfang Database, CiNii, and OASIS. The included literatures will be randomized controlled trials of acupuncture at 3 anti-fatigue acupoints on CRF in patients with cancer. The certain common scales, which reflect the patients' fatigue degree or life quality will be the primary outcome measures. The secondary outcome measures will be defined with the blood index. After collecting the data, we will utilize Stata V.13.0. to perform data synthesis, subgroup analysis, partial sequence analysis, sensitivity analysis, and so on. A funnel plot will be used to assess reporting biases. And the funnel plot will be evaluated by the Egger and Begg tests. The quality of evidence will be judged by the grading of recommendations assessment, development, and evaluation. RESULTS: The results of this systematic review and meta-analysis will be published in a peer-reviewed journal. CONCLUSION: Our study will provide the evidence for the clinical efficacy and safety of acupuncture at 3 anti-fatigue acupoints in the treatment of CRF.","Cancer-related fatigue (CRF), is a common distressing symptom of cancer. What's more, ""Three anti-fatigue acupoints"" is one of the most important components of ""Jin's 3-needle therapy"" created by Rui Jin, a professor of Guangzhou University of Chinese Medicine, which can be used in the treatment of CRF. In this article, researchers will assess the safety and effect of acupuncture at 3 anti-fatigue acupoints on CRF in patients with cancer. Literature search for relevant articles up to October 2018 will be carried out in 9 databases: Cochrane Library, Embase, PubMed, VIP, CBM, CNKI, Wanfang Database, CiNii, and OASIS. The included literatures will be randomized controlled trials of acupuncture at 3 anti-fatigue acupoints on CRF in patients with cancer. The certain common scales, which reflect the patients' fatigue degree or life quality will be the primary outcome measures. The secondary outcome measures will be defined with the blood index. After collecting the data, we will utilize Stata V.13.0. to perform data synthesis, subgroup analysis, partial sequence analysis, sensitivity analysis, and so on. A funnel plot will be used to assess reporting biases. And the funnel plot will be evaluated by the Egger and Begg tests. The quality of evidence will be judged by the grading of recommendations assessment, development, and evaluation. The results of this systematic review and meta-analysis will be published in a peer-reviewed journal. Our study will provide the evidence for the clinical efficacy and safety of acupuncture at 3 anti-fatigue acupoints in the treatment of CRF.","Liao, M.
 and Xie, Y.
 and Yan, J.
 and Lin, T.
 and Ji, S.
 and Li, Z.
 and Zhao, W.
 and Yang, Y.
 and Lin, L.
 and Lin, J.","Liao, Xie, Yan, Lin, Ji, Li, Zhao, Yang, Lin, Lin",not available,https://doi.org/10.1097/MD.0000000000015919,2021-08-03
0.0,,pubmed,Barriers and facilitators to clinical information seeking: a systematic review,Barriers and facilitators to clinical information seeking: a systematic review,"OBJECTIVE: The study sought to identify barriers to and facilitators of point-of-care information seeking and use of knowledge resources. MATERIALS AND METHODS: We searched MEDLINE, Embase, PsycINFO, and Cochrane Library from 1991 to February 2017. We included qualitative studies in any language exploring barriers to and facilitators of point-of-care information seeking or use of electronic knowledge resources. Two authors independently extracted data on users, study design, and study quality. We inductively identified specific barriers or facilitators and from these synthesized a model of key determinants of information-seeking behaviors. RESULTS: Forty-five qualitative studies were included, reporting data derived from interviews (n = 26), focus groups (n = 21), ethnographies (n = 6), logs (n = 4), and usability studies (n = 2). Most studies were performed within the context of general medicine (n = 28) or medical specialties (n = 13). We inductively identified 58 specific barriers and facilitators and then created a model reflecting 5 key determinants of information-seeking behaviors: time includes subthemes of time availability, efficiency of information seeking, and urgency of information need; accessibility includes subthemes of hardware access, hardware speed, hardware portability, information restriction, and cost of resources; personal skills and attitudes includes subthemes of computer literacy, information-seeking skills, and contextual attitudes about information seeking; institutional attitudes, cultures, and policies includes subthemes describing external individual and institutional information-seeking influences; and knowledge resource features includes subthemes describing information-seeking efficiency, information content, information organization, resource familiarity, information credibility, information currency, workflow integration, compatibility of recommendations with local processes, and patient educational support. CONCLUSIONS: Addressing these determinants of information-seeking behaviors may facilitate clinicians' question answering to improve patient care.","The study sought to identify barriers to and facilitators of point-of-care information seeking and use of knowledge resources. We searched MEDLINE, Embase, PsycINFO, and Cochrane Library from 1991 to February 2017. We included qualitative studies in any language exploring barriers to and facilitators of point-of-care information seeking or use of electronic knowledge resources. Two authors independently extracted data on users, study design, and study quality. We inductively identified specific barriers or facilitators and from these synthesized a model of key determinants of information-seeking behaviors. Forty-five qualitative studies were included, reporting data derived from interviews (nâ€‰=â€‰26), focus groups (nâ€‰=â€‰21), ethnographies (nâ€‰=â€‰6), logs (nâ€‰=â€‰4), and usability studies (nâ€‰=â€‰2). Most studies were performed within the context of general medicine (nâ€‰=â€‰28) or medical specialties (nâ€‰=â€‰13). We inductively identified 58 specific barriers and facilitators and then created a model reflecting 5 key determinants of information-seeking behaviors: time includes subthemes of time availability, efficiency of information seeking, and urgency of information need; accessibility includes subthemes of hardware access, hardware speed, hardware portability, information restriction, and cost of resources; personal skills and attitudes includes subthemes of computer literacy, information-seeking skills, and contextual attitudes about information seeking; institutional attitudes, cultures, and policies includes subthemes describing external individual and institutional information-seeking influences; and knowledge resource features includes subthemes describing information-seeking efficiency, information content, information organization, resource familiarity, information credibility, information currency, workflow integration, compatibility of recommendations with local processes, and patient educational support. Addressing these determinants of information-seeking behaviors may facilitate clinicians' question answering to improve patient care.","Aakre, C. A.
 and Maggio, L. A.
 and Fiol, G. D.
 and Cook, D. A.","Aakre, Maggio, Fiol, Cook",https://dx.doi.org/10.1093/jamia/ocz065,https://doi.org/10.1093/jamia/ocz065,2021-08-03
779.0,,pubmed,Animal models of chemotherapy-induced peripheral neuropathy: A machine-assisted systematic review and meta-analysis,Animal models of chemotherapy-induced peripheral neuropathy: A machine-assisted systematic review and meta-analysis,"We report a systematic review and meta-analysis of research using animal models of chemotherapy-induced peripheral neuropathy (CIPN). We systematically searched 5 online databases in September 2012 and updated the search in November 2015 using machine learning and text mining to reduce the screening for inclusion workload and improve accuracy. For each comparison, we calculated a standardised mean difference (SMD) effect size, and then combined effects in a random-effects meta-analysis. We assessed the impact of study design factors and reporting of measures to reduce risks of bias. We present power analyses for the most frequently reported behavioural tests; 337 publications were included. Most studies (84%) used male animals only. The most frequently reported outcome measure was evoked limb withdrawal in response to mechanical monofilaments. There was modest reporting of measures to reduce risks of bias. The number of animals required to obtain 80% power with a significance level of 0.05 varied substantially across behavioural tests. In this comprehensive summary of the use of animal models of CIPN, we have identified areas in which the value of preclinical CIPN studies might be increased. Using both sexes of animals in the modelling of CIPN, ensuring that outcome measures align with those most relevant in the clinic, and the animal's pain contextualised ethology will likely improve external validity. Measures to reduce risk of bias should be employed to increase the internal validity of studies. Different outcome measures have different statistical power, and this can refine our approaches in the modelling of CIPN.","We report a systematic review and meta-analysis of research using animal models of chemotherapy-induced peripheral neuropathy (CIPN). We systematically searched 5 online databases in September 2012 and updated the search in November 2015 using machine learning and text mining to reduce the screening for inclusion workload and improve accuracy. For each comparison, we calculated a standardised mean difference (SMD) effect size, and then combined effects in a random-effects meta-analysis. We assessed the impact of study design factors and reporting of measures to reduce risks of bias. We present power analyses for the most frequently reported behavioural tests; 337 publications were included. Most studies (84%) used male animals only. The most frequently reported outcome measure was evoked limb withdrawal in response to mechanical monofilaments. There was modest reporting of measures to reduce risks of bias. The number of animals required to obtain 80% power with a significance level of 0.05 varied substantially across behavioural tests. In this comprehensive summary of the use of animal models of CIPN, we have identified areas in which the value of preclinical CIPN studies might be increased. Using both sexes of animals in the modelling of CIPN, ensuring that outcome measures align with those most relevant in the clinic, and the animal's pain contextualised ethology will likely improve external validity. Measures to reduce risk of bias should be employed to increase the internal validity of studies. Different outcome measures have different statistical power, and this can refine our approaches in the modelling of CIPN.","Currie, G. L.
 and Angel-Scott, H. N.
 and Colvin, L.
 and Cramond, F.
 and Hair, K.
 and Khandoker, L.
 and Liao, J.
 and Macleod, M.
 and McCann, S. K.
 and Morland, R.
 and Sherratt, N.
 and Stewart, R.
 and Tanriver-Ayder, E.
 and Thomas, J.
 and Wang, Q.
 and Wodarski, R.
 and Xiong, R.
 and Rice, A. S. C.
 and Sena, E. S.","Currie, Angel-Scott, Colvin, Cramond, Hair, Khandoker, Liao, Macleod, McCann, Morland, Sherratt, Stewart, Tanriver-Ayder, Thomas, Wang, Wodarski, Xiong, Rice, Sena",https://dx.doi.org/10.1371/journal.pbio.3000243,https://doi.org/10.1371/journal.pbio.3000243,2021-08-03
2860.0,,pubmed,Deep learning and radiomics in precision medicine,Deep learning and radiomics in precision medicine,"Introduction: The radiological reading room is undergoing a paradigm shift to a symbiosis of computer science and radiology using artificial intelligence integrated with machine and deep learning with radiomics to better define tissue characteristics. The goal is to use integrated deep learning and radiomics with radiological parameters to produce a personalized diagnosis for a patient. Areas covered: This review provides an overview of historical and current deep learning and radiomics methods in the context of precision medicine in radiology. A literature search for 'Deep Learning', 'Radiomics', 'Machine learning', 'Artificial Intelligence', 'Convolutional Neural Network', 'Generative Adversarial Network', 'Autoencoders', Deep Belief Networks', Reinforcement Learning', and 'Multiparametric MRI' was performed in PubMed, ArXiv, Scopus, CVPR, SPIE, IEEE Xplore, and NIPS to identify articles of interest. Expert opinion: In conclusion, both deep learning and radiomics are two rapidly advancing technologies that will unite in the future to produce a single unified framework for clinical decision support with a potential to completely revolutionize the field of precision medicine.","The radiological reading room is undergoing a paradigm shift to a symbiosis of computer science and radiology using artificial intelligence integrated with machine and deep learning with radiomics to better define tissue characteristics. The goal is to use integrated deep learning and radiomics with radiological parameters to produce a personalized diagnosis for a patient. This review provides an overview of historical and current deep learning and radiomics methods in the context of precision medicine in radiology. A literature search for 'Deep Learning', 'Radiomics', 'Machine learning', 'Artificial Intelligence', 'Convolutional Neural Network', 'Generative Adversarial Network', 'Autoencoders', Deep Belief Networks"", Reinforcement Learning"", and 'Multiparametric MRI' was performed in PubMed, ArXiv, Scopus, CVPR, SPIE, IEEE Xplore, and NIPS to identify articles of interest. In conclusion, both deep learning and radiomics are two rapidly advancing technologies that will unite in the future to produce a single unified framework for clinical decision support with a potential to completely revolutionize the field of precision medicine.","Parekh, V. S.
 and Jacobs, M. A.","Parekh, Jacobs",https://dx.doi.org/10.1080/23808993.2019.1585805,https://doi.org/10.1080/23808993.2019.1585805,2021-08-03
790.0,,pubmed,Proximal Row Carpectomy versus Four-Corner Arthrodesis for the Treatment of Scapholunate Advanced Collapse/Scaphoid Nonunion Advanced Collapse Wrist: A Cost-Utility Analysis,Proximal Row Carpectomy versus Four-Corner Arthrodesis for the Treatment of Scapholunate Advanced Collapse/Scaphoid Nonunion Advanced Collapse Wrist: A Cost-Utility Analysis,"BACKGROUND: Two mainstay surgical options for salvage in scapholunate advanced collapse and scaphoid nonunion advanced collapse are proximal row carpectomy and four-corner arthrodesis. This study evaluates the cost-utility of proximal row carpectomy versus three methods of four-corner arthrodesis for the treatment of scapholunate advanced collapse/scaphoid nonunion advanced collapse wrist. METHODS: A cost-utility analysis was performed in accordance with the Second Panel on Cost-Effectiveness in Health and Medicine. A comprehensive literature review was performed to obtain the probability of potential complications. Costs were derived using both societal and health care sector perspectives. A visual analogue scale survey of expert hand surgeons estimated utilities. Overall cost, probabilities, and quality-adjusted life-years were used to complete a decision tree analysis. Both deterministic and probabilistic sensitivity analyses were performed. RESULTS: Forty studies yielding 1730 scapholunate advanced collapse/scaphoid nonunion advanced collapse wrists were identified. Decision tree analysis determined that both four-corner arthrodesis with screw fixation and proximal row carpectomy were cost-effective options, but four-corner arthrodesis with screw was the optimal treatment strategy. Four-corner arthrodesis with Kirschner-wire fixation and four-corner arthrodesis with plate fixation were dominated (inferior) strategies and therefore not cost-effective. One-way sensitivity analysis demonstrated that when the quality-adjusted life-years for a successful four-corner arthrodesis with screw fixation are lower than 26.36, proximal row carpectomy becomes the optimal strategy. However, multivariate probabilistic sensitivity analysis confirmed the results of our model. CONCLUSIONS: Four-corner arthrodesis with screw fixation and proximal row carpectomy are both cost-effective treatment options for scapholunate advanced collapse/scaphoid nonunion advanced collapse wrist because of their lower complication profile and high efficacy, with four-corner arthrodesis with screw as the most cost-effective treatment. Four-corner arthrodesis with plate and Kirschner-wire fixation should be avoided from a cost-effectiveness standpoint.","Two mainstay surgical options for salvage in scapholunate advanced collapse and scaphoid nonunion advanced collapse are proximal row carpectomy and four-corner arthrodesis. This study evaluates the cost-utility of proximal row carpectomy versus three methods of four-corner arthrodesis for the treatment of scapholunate advanced collapse/scaphoid nonunion advanced collapse wrist. A cost-utility analysis was performed in accordance with the Second Panel on Cost-Effectiveness in Health and Medicine. A comprehensive literature review was performed to obtain the probability of potential complications. Costs were derived using both societal and health care sector perspectives. A visual analogue scale survey of expert hand surgeons estimated utilities. Overall cost, probabilities, and quality-adjusted life-years were used to complete a decision tree analysis. Both deterministic and probabilistic sensitivity analyses were performed. Forty studies yielding 1730 scapholunate advanced collapse/scaphoid nonunion advanced collapse wrists were identified. Decision tree analysis determined that both four-corner arthrodesis with screw fixation and proximal row carpectomy were cost-effective options, but four-corner arthrodesis with screw was the optimal treatment strategy. Four-corner arthrodesis with Kirschner-wire fixation and four-corner arthrodesis with plate fixation were dominated (inferior) strategies and therefore not cost-effective. One-way sensitivity analysis demonstrated that when the quality-adjusted life-years for a successful four-corner arthrodesis with screw fixation are lower than 26.36, proximal row carpectomy becomes the optimal strategy. However, multivariate probabilistic sensitivity analysis confirmed the results of our model. Four-corner arthrodesis with screw fixation and proximal row carpectomy are both cost-effective treatment options for scapholunate advanced collapse/scaphoid nonunion advanced collapse wrist because of their lower complication profile and high efficacy, with four-corner arthrodesis with screw as the most cost-effective treatment. Four-corner arthrodesis with plate and Kirschner-wire fixation should be avoided from a cost-effectiveness standpoint.","Daar, D. A.
 and Shah, A.
 and Mirrer, J. T.
 and Thanik, V.
 and Hacquebord, J.","Daar, Shah, Mirrer, Thanik, Hacquebord",https://dx.doi.org/10.1097/PRS.0000000000005558,https://doi.org/10.1097/PRS.0000000000005558,2021-08-03
3483.0,,pubmed,A Clinical Decision Support System for Predicting the Early Complications of One-Anastomosis Gastric Bypass Surgery,A Clinical Decision Support System for Predicting the Early Complications of One-Anastomosis Gastric Bypass Surgery,"BACKGROUND/OBJECTIVE: One of the most effective treatments for patients with obesity, albeit with some complications, is obesity surgery. The aim of this study was to develop a clinical decision support system (CDSS) to predict the early complications of one-anastomosis gastric bypass (OAGB) surgery. SUBJECTS/METHODS: This study was conducted in Tehran, Iran on patients who underwent OAGB surgery in 2011-2014 in five hospitals. Initially, variables affecting the OAGB early complications were identified using the literature review. Patients' data were extracted from an existing database of obesity surgery. Then, different artificial neural networks (ANNs) (multilayer perceptron (MLP) network) were developed and evaluated for prediction of 10-day, 1-month, and 3-month complications. RESULTS: Factors including age, BMI, smoking status, intra-operative complications, comorbidities, laboratory tests, sonography results, and endoscopy results were considered important factors for predicting early complications of OAGB. A CDSS was developed with these variables. The accuracy, specificity, and sensitivity of the 10-day prediction system in the test data were 98.4%, 98.6%, and 98.3%, respectively. These figures for 1-month system were 96%, 93%, and 98.4% and for the 3-month system were 89.3%, 86.6%, and 91.5%, respectively. CONCLUSIONS: Using the CDSS designed, we could accurately predict the early complications of OAGB surgery.","One of the most effective treatments for patients with obesity, albeit with some complications, is obesity surgery. The aim of this study was to develop a clinical decision support system (CDSS) to predict the early complications of one-anastomosis gastric bypass (OAGB) surgery. This study was conducted in Tehran, Iran on patients who underwent OAGB surgery in 2011-2014 in five hospitals. Initially, variables affecting the OAGB early complications were identified using the literature review. Patients' data were extracted from an existing database of obesity surgery. Then, different artificial neural networks (ANNs) (multilayer perceptron (MLP) network) were developed and evaluated for prediction of 10-day, 1-month, and 3-month complications. Factors including age, BMI, smoking status, intra-operative complications, comorbidities, laboratory tests, sonography results, and endoscopy results were considered important factors for predicting early complications of OAGB. A CDSS was developed with these variables. The accuracy, specificity, and sensitivity of the 10-day prediction system in the test data were 98.4%, 98.6%, and 98.3%, respectively. These figures for 1-month system were 96%, 93%, and 98.4% and for the 3-month system were 89.3%, 86.6%, and 91.5%, respectively. Using the CDSS designed, we could accurately predict the early complications of OAGB surgery.","Sheikhtaheri, A.
 and Orooji, A.
 and Pazouki, A.
 and Beitollahi, M.","Sheikhtaheri, Orooji, Pazouki, Beitollahi",https://dx.doi.org/10.1007/s11695-019-03849-w,https://doi.org/10.1007/s11695-019-03849-w,2021-08-03
2344.0,,pubmed,Drug prioritization using the semantic properties of a knowledge graph,Drug prioritization using the semantic properties of a knowledge graph,"Compounds that are candidates for drug repurposing can be ranked by leveraging knowledge available in the biomedical literature and databases. This knowledge, spread across a variety of sources, can be integrated within a knowledge graph, which thereby comprehensively describes known relationships between biomedical concepts, such as drugs, diseases, genes, etc. Our work uses the semantic information between drug and disease concepts as features, which are extracted from an existing knowledge graph that integrates 200 different biological knowledge sources. RepoDB, a standard drug repurposing database which describes drug-disease combinations that were approved or that failed in clinical trials, is used to train a random forest classifier. The 10-times repeated 10-fold cross-validation performance of the classifier achieves a mean area under the receiver operating characteristic curve (AUC) of 92.2%. We apply the classifier to prioritize 21 preclinical drug repurposing candidates that have been suggested for Autosomal Dominant Polycystic Kidney Disease (ADPKD). Mozavaptan, a vasopressin V2 receptor antagonist is predicted to be the drug most likely to be approved after a clinical trial, and belongs to the same drug class as tolvaptan, the only treatment for ADPKD that is currently approved. We conclude that semantic properties of concepts in a knowledge graph can be exploited to prioritize drug repurposing candidates for testing in clinical trials.","Compounds that are candidates for drug repurposing can be ranked by leveraging knowledge available in the biomedical literature and databases. This knowledge, spread across a variety of sources, can be integrated within a knowledge graph, which thereby comprehensively describes known relationships between biomedical concepts, such as drugs, diseases, genes, etc. Our work uses the semantic information between drug and disease concepts as features, which are extracted from an existing knowledge graph that integrates 200 different biological knowledge sources. RepoDB, a standard drug repurposing database which describes drug-disease combinations that were approved or that failed in clinical trials, is used to train a random forest classifier. The 10-times repeated 10-fold cross-validation performance of the classifier achieves a mean area under the receiver operating characteristic curve (AUC) of 92.2%. We apply the classifier to prioritize 21 preclinical drug repurposing candidates that have been suggested for Autosomal Dominant Polycystic Kidney Disease (ADPKD). Mozavaptan, a vasopressin V2 receptor antagonist is predicted to be the drug most likely to be approved after a clinical trial, and belongs to the same drug class as tolvaptan, the only treatment for ADPKD that is currently approved. We conclude that semantic properties of concepts in a knowledge graph can be exploited to prioritize drug repurposing candidates for testing in clinical trials.","Malas, T. B.
 and Vlietstra, W. J.
 and Kudrin, R.
 and Starikov, S.
 and Charrout, M.
 and Roos, M.
 and Peters, D. J. M.
 and Kors, J. A.
 and Vos, R.
 and t Hoen, P. A. C.
 and van Mulligen, E. M.
 and Hettne, K. M.","Malas, Vlietstra, Kudrin, Starikov, Charrout, Roos, Peters, Kors, Vos, 't Hoen, van Mulligen, Hettne",https://dx.doi.org/10.1038/s41598-019-42806-6,https://doi.org/10.1038/s41598-019-42806-6,2021-08-03
2566.0,,pubmed,"PGxO and PGxLOD: a reconciliation of pharmacogenomic knowledge of various provenances, enabling further comparison","PGxO and PGxLOD: a reconciliation of pharmacogenomic knowledge of various provenances, enabling further comparison","BACKGROUND: Pharmacogenomics (PGx) studies how genomic variations impact variations in drug response phenotypes. Knowledge in pharmacogenomics is typically composed of units that have the form of ternary relationships gene variant - drug - adverse event. Such a relationship states that an adverse event may occur for patients having the specified gene variant and being exposed to the specified drug. State-of-the-art knowledge in PGx is mainly available in reference databases such as PharmGKB and reported in scientific biomedical literature. But, PGx knowledge can also be discovered from clinical data, such as Electronic Health Records (EHRs), and in this case, may either correspond to new knowledge or confirm state-of-the-art knowledge that lacks 'clinical counterpart' or validation. For this reason, there is a need for automatic comparison of knowledge units from distinct sources. RESULTS: In this article, we propose an approach, based on Semantic Web technologies, to represent and compare PGx knowledge units. To this end, we developed PGxO, a simple ontology that represents PGx knowledge units and their components. Combined with PROV-O, an ontology developed by the W3C to represent provenance information, PGxO enables encoding and associating provenance information to PGx relationships. Additionally, we introduce a set of rules to reconcile PGx knowledge, i.e. to identify when two relationships, potentially expressed using different vocabularies and levels of granularity, refer to the same, or to different knowledge units. We evaluated our ontology and rules by populating PGxO with knowledge units extracted from PharmGKB (2701), the literature (65,720) and from discoveries reported in EHR analysis studies (only 10, manually extracted); and by testing their similarity. We called PGxLOD (PGx Linked Open Data) the resulting knowledge base that represents and reconciles knowledge units of those various origins. CONCLUSIONS: The proposed ontology and reconciliation rules constitute a first step toward a more complete framework for knowledge comparison in PGx. In this direction, the experimental instantiation of PGxO, named PGxLOD, illustrates the ability and difficulties of reconciling various existing knowledge sources.","Pharmacogenomics (PGx) studies how genomic variations impact variations in drug response phenotypes. Knowledge in pharmacogenomics is typically composed of units that have the form of ternary relationships gene variant - drug - adverse event. Such a relationship states that an adverse event may occur for patients having the specified gene variant and being exposed to the specified drug. State-of-the-art knowledge in PGx is mainly available in reference databases such as PharmGKB and reported in scientific biomedical literature. But, PGx knowledge can also be discovered from clinical data, such as Electronic Health Records (EHRs), and in this case, may either correspond to new knowledge or confirm state-of-the-art knowledge that lacks ""clinical counterpart"" or validation. For this reason, there is a need for automatic comparison of knowledge units from distinct sources. In this article, we propose an approach, based on Semantic Web technologies, to represent and compare PGx knowledge units. To this end, we developed PGxO, a simple ontology that represents PGx knowledge units and their components. Combined with PROV-O, an ontology developed by the W3C to represent provenance information, PGxO enables encoding and associating provenance information to PGx relationships. Additionally, we introduce a set of rules to reconcile PGx knowledge, i.e. to identify when two relationships, potentially expressed using different vocabularies and levels of granularity, refer to the same, or to different knowledge units. We evaluated our ontology and rules by populating PGxO with knowledge units extracted from PharmGKB (2701), the literature (65,720) and from discoveries reported in EHR analysis studies (only 10, manually extracted); and by testing their similarity. We called PGxLOD (PGx Linked Open Data) the resulting knowledge base that represents and reconciles knowledge units of those various origins. The proposed ontology and reconciliation rules constitute a first step toward a more complete framework for knowledge comparison in PGx. In this direction, the experimental instantiation of PGxO, named PGxLOD, illustrates the ability and difficulties of reconciling various existing knowledge sources.","Monnin, P.
 and Legrand, J.
 and Husson, G.
 and Ringot, P.
 and Tchechmedjiev, A.
 and Jonquet, C.
 and Napoli, A.
 and Coulet, A.","Monnin, Legrand, Husson, Ringot, Tchechmedjiev, Jonquet, Napoli, Coulet",https://dx.doi.org/10.1186/s12859-019-2693-9,https://doi.org/10.1186/s12859-019-2693-9,2021-08-03
928.0,,pubmed,"Strategies for tackling Taenia solium taeniosis/cysticercosis: A systematic review and comparison of transmission models, including an assessment of the wider Taeniidae family transmission models","Strategies for tackling Taenia solium taeniosis/cysticercosis: A systematic review and comparison of transmission models, including an assessment of the wider Taeniidae family transmission models","BACKGROUND: The cestode Taenia solium causes the neglected (zoonotic) tropical disease cysticercosis, a leading cause of preventable epilepsy in endemic low and middle-income countries. Transmission models can inform current scaling-up of control efforts by helping to identify, validate and optimise control and elimination strategies as proposed by the World Health Organization (WHO). METHODOLOGY/PRINCIPAL FINDINGS: A systematic literature search was conducted using the PRISMA approach to identify and compare existing T. solium transmission models, and related Taeniidae infection transmission models. In total, 28 modelling papers were identified, of which four modelled T. solium exclusively. Different modelling approaches for T. solium included deterministic, Reed-Frost, individual-based, decision-tree, and conceptual frameworks. Simulated interventions across models agreed on the importance of coverage for impactful effectiveness to be achieved. Other Taeniidae infection transmission models comprised force-of-infection (FoI), population-based (mainly Echinococcus granulosus) and individual-based (mainly E. multilocularis) modelling approaches. Spatial structure has also been incorporated (E. multilocularis and Taenia ovis) in recognition of spatial aggregation of parasite eggs in the environment and movement of wild animal host populations. CONCLUSIONS/SIGNIFICANCE: Gaps identified from examining the wider Taeniidae family models highlighted the potential role of FoI modelling to inform model parameterisation, as well as the need for spatial modelling and suitable structuring of interventions as key areas for future T. solium model development. We conclude that working with field partners to address data gaps and conducting cross-model validation with baseline and longitudinal data will be critical to building consensus-led and epidemiological setting-appropriate intervention strategies to help fulfil the WHO targets.","The cestode Taenia solium causes the neglected (zoonotic) tropical disease cysticercosis, a leading cause of preventable epilepsy in endemic low and middle-income countries. Transmission models can inform current scaling-up of control efforts by helping to identify, validate and optimise control and elimination strategies as proposed by the World Health Organization (WHO). A systematic literature search was conducted using the PRISMA approach to identify and compare existing T. solium transmission models, and related Taeniidae infection transmission models. In total, 28 modelling papers were identified, of which four modelled T. solium exclusively. Different modelling approaches for T. solium included deterministic, Reed-Frost, individual-based, decision-tree, and conceptual frameworks. Simulated interventions across models agreed on the importance of coverage for impactful effectiveness to be achieved. Other Taeniidae infection transmission models comprised force-of-infection (FoI), population-based (mainly Echinococcus granulosus) and individual-based (mainly E. multilocularis) modelling approaches. Spatial structure has also been incorporated (E. multilocularis and Taenia ovis) in recognition of spatial aggregation of parasite eggs in the environment and movement of wild animal host populations. Gaps identified from examining the wider Taeniidae family models highlighted the potential role of FoI modelling to inform model parameterisation, as well as the need for spatial modelling and suitable structuring of interventions as key areas for future T. solium model development. We conclude that working with field partners to address data gaps and conducting cross-model validation with baseline and longitudinal data will be critical to building consensus-led and epidemiological setting-appropriate intervention strategies to help fulfil the WHO targets.","Dixon, M. A.
 and Braae, U. C.
 and Winskill, P.
 and Walker, M.
 and Devleesschauwer, B.
 and Gabriel, S.
 and Basanez, M. G.","Dixon, Braae, Winskill, Walker, Devleesschauwer, GabriÃ«l, BasÃ¡Ã±ez",https://dx.doi.org/10.1371/journal.pntd.0007301,https://doi.org/10.1371/journal.pntd.0007301,2021-08-03
2738.0,,pubmed,A scoping review of ontologies related to human behaviour change,A scoping review of ontologies related to human behaviour change,"Ontologies are classification systems specifying entities, definitions and inter-relationships for a given domain, with the potential to advance knowledge about human behaviour change. A scoping review was conducted to: (1) identify what ontologies exist related to human behaviour change, (2) describe the methods used to develop these ontologies and (3) assess the quality of identified ontologies. Using a systematic search, 2,303 papers were identified. Fifteen ontologies met the eligibility criteria for inclusion, developed in areas such as cognition, mental disease and emotions. Methods used for developing the ontologies were expert consultation, data-driven techniques and reuse of terms from existing taxonomies, terminologies and ontologies. Best practices used in ontology development and maintenance were documented. The review did not identify any ontologies representing the breadth and detail of human behaviour change. This suggests that advancing behavioural science would benefit from the development of a behaviour change intervention ontology.","Ontologies are classification systems specifying entities, definitions and inter-relationships for a given domain, with the potential to advance knowledge about human behaviour change. A scoping review was conducted to: (1) identify what ontologies exist related to human behaviour change, (2) describe the methods used to develop these ontologies and (3) assess the quality of identified ontologies. Using a systematic search, 2,303 papers were identified. Fifteen ontologies met the eligibility criteria for inclusion, developed in areas such as cognition, mental disease and emotions. Methods used for developing the ontologies were expert consultation, data-driven techniques and reuse of terms from existing taxonomies, terminologies and ontologies. Best practices used in ontology development and maintenance were documented. The review did not identify any ontologies representing the breadth and detail of human behaviour change. This suggests that advancing behavioural science would benefit from the development of a behaviour change intervention ontology.","Norris, E.
 and Finnerty, A. N.
 and Hastings, J.
 and Stokes, G.
 and Michie, S.","Norris, Finnerty, Hastings, Stokes, Michie",https://dx.doi.org/10.1038/s41562-018-0511-4,https://doi.org/10.1038/s41562-018-0511-4,2021-08-03
4314.0,,pubmed,A systematic literature review of machine learning in online personal health data,A systematic literature review of machine learning in online personal health data,"OBJECTIVE: User-generated content (UGC) in online environments provides opportunities to learn an individual's health status outside of clinical settings. However, the nature of UGC brings challenges in both data collecting and processing. The purpose of this study is to systematically review the effectiveness of applying machine learning (ML) methodologies to UGC for personal health investigations. MATERIALS AND METHODS: We searched PubMed, Web of Science, IEEE Library, ACM library, AAAI library, and the ACL anthology. We focused on research articles that were published in English and in peer-reviewed journals or conference proceedings between 2010 and 2018. Publications that applied ML to UGC with a focus on personal health were identified for further systematic review. RESULTS: We identified 103 eligible studies which we summarized with respect to 5 research categories, 3 data collection strategies, 3 gold standard dataset creation methods, and 4 types of features applied in ML models. Popular off-the-shelf ML models were logistic regression (n = 22), support vector machines (n = 18), naive Bayes (n = 17), ensemble learning (n = 12), and deep learning (n = 11). The most investigated problems were mental health (n = 39) and cancer (n = 15). Common health-related aspects extracted from UGC were treatment experience, sentiments and emotions, coping strategies, and social support. CONCLUSIONS: The systematic review indicated that ML can be effectively applied to UGC in facilitating the description and inference of personal health. Future research needs to focus on mitigating bias introduced when building study cohorts, creating features from free text, improving clinical creditability of UGC, and model interpretability.","User-generated content (UGC) in online environments provides opportunities to learn an individual's health status outside of clinical settings. However, the nature of UGC brings challenges in both data collecting and processing. The purpose of this study is to systematically review the effectiveness of applying machine learning (ML) methodologies to UGC for personal health investigations. We searched PubMed, Web of Science, IEEE Library, ACM library, AAAI library, and the ACL anthology. We focused on research articles that were published in English and in peer-reviewed journals or conference proceedings between 2010 and 2018. Publications that applied ML to UGC with a focus on personal health were identified for further systematic review. We identified 103 eligible studies which we summarized with respect to 5 research categories, 3 data collection strategies, 3 gold standard dataset creation methods, and 4 types of features applied in ML models. Popular off-the-shelf ML models were logistic regression (nâ€‰=â€‰22), support vector machines (nâ€‰=â€‰18), naive Bayes (nâ€‰=â€‰17), ensemble learning (nâ€‰=â€‰12), and deep learning (nâ€‰=â€‰11). The most investigated problems were mental health (nâ€‰=â€‰39) and cancer (nâ€‰=â€‰15). Common health-related aspects extracted from UGC were treatment experience, sentiments and emotions, coping strategies, and social support. The systematic review indicated that ML can be effectively applied to UGC in facilitating the description and inference of personal health. Future research needs to focus on mitigating bias introduced when building study cohorts, creating features from free text, improving clinical creditability of UGC, and model interpretability.","Yin, Z.
 and Sulieman, L. M.
 and Malin, B. A.","Yin, Sulieman, Malin",https://dx.doi.org/10.1093/jamia/ocz009,https://doi.org/10.1093/jamia/ocz009,2021-08-03
752.0,,pubmed,The development and evaluation of an online application to assist in the extraction of data from graphs for use in systematic reviews,The development and evaluation of an online application to assist in the extraction of data from graphs for use in systematic reviews,"<b>Background:</b> The extraction of data from the reports of primary studies, on which the results of systematic reviews depend, needs to be carried out accurately. To aid reliability, it is recommended that two researchers carry out data extraction independently. The extraction of statistical data from graphs in PDF files is particularly challenging, as the process is usually completely manual, and reviewers need sometimes to revert to holding a ruler against the page to read off values: an inherently time-consuming and error-prone process.","<b>Background:</b> The extraction of data from the reports of primary studies, on which the results of systematic reviews depend, needs to be carried out accurately. To aid reliability, it is recommended that two researchers carry out data extraction independently. The extraction of statistical data from graphs in PDF files is particularly challenging, as the process is usually completely manual, and reviewers need sometimes to revert to holding a ruler against the page to read off values: an inherently time-consuming and error-prone process. <b>Methods:</b> To mitigate some of the above problems we integrated and customised two existing JavaScript libraries to create a new web-based graphical data extraction tool to assist reviewers in extracting data from graphs. This tool aims to facilitate more accurate and timely data extraction through a user interface which can be used to extract data through mouse clicks. We carried out a non-inferiority evaluation to examine its performance in comparison with participants' standard practice for extracting data from graphs in PDF documents. <b>Results:</b> We found that the customised graphical data extraction tool is not inferior to users' (N=10) prior standard practice. Our study was not designed to show superiority, but suggests that, on average, participants saved around 6 minutes per graph using the new tool, accompanied by a substantial increase in accuracy. <b><i>Conclusions:</i></b> Our study suggests that the incorporation of this type of tool in online systematic review software would be beneficial in facilitating the production of accurate and timely evidence synthesis to improve decision-making.","Cramond, F.
 and O'Mara-Eves, A.
 and Doran-Constant, L.
 and Rice, A. S.
 and Macleod, M.
 and Thomas, J.","Cramond, O'Mara-Eves, Doran-Constant, Rice, Macleod, Thomas",not available,https://doi.org/10.12688/wellcomeopenres.14738.3,2021-08-03
1362.0,,pubmed,Effect of advanced care on psychological condition in patients with chronic renal failure undergoing hemodialysis: A protocol of a systematic review,Effect of advanced care on psychological condition in patients with chronic renal failure undergoing hemodialysis: A protocol of a systematic review,"BACKGROUND: The protocol of this systematic review will be proposed for assessing the effects of advanced care (AC) on psychological condition in patients with chronic renal failure (CRF) undergoing hemodialysis. METHODS: We will search the following electronic databases: MEDLINE, EMBASE, Cochrane Central Register of Controlled Trials (CENTRAL), Web of Science, Chinese Biomedical Literature Database, and China National Knowledge Infrastructure from inception to the January 30, 2019. Any randomized controlled trials (RCTs) for assessing the effects of AC on psychological condition in patients with CRF undergoing hemodialysis will be fully considered. The methodological quality will be assessed by using Cochrane risk of bias tool. Two independent reviewers will perform the study selection, data extraction, and methodological quality assessment. A third reviewer will be invited to judge the disagreements between 2 reviewers by discussion. RESULTS: The protocol of this proposed systematic review will compare the effects of AC on psychological condition in patients with CRF undergoing hemodialysis. The outcomes will comprise of depression. The secondary outcome includes anxiety, health related quality of life, and any adverse events. CONCLUSION: The findings of this systematic review will summarize the latest evidence of AC on psychological condition in patients with CRF undergoing hemodialysis. ETHICS AND DISSEMINATION: All data used in this systematic review will be collected from previous published clinical studies. Thus, no ethic approval is required for this study. The findings of this study will be published at a peer-reviewed journal. Prospero registration number: prospero crd42019122275.","The protocol of this systematic review will be proposed for assessing the effects of advanced care (AC) on psychological condition in patients with chronic renal failure (CRF) undergoing hemodialysis. We will search the following electronic databases: MEDLINE, EMBASE, Cochrane Central Register of Controlled Trials (CENTRAL), Web of Science, Chinese Biomedical Literature Database, and China National Knowledge Infrastructure from inception to the January 30, 2019. Any randomized controlled trials (RCTs) for assessing the effects of AC on psychological condition in patients with CRF undergoing hemodialysis will be fully considered. The methodological quality will be assessed by using Cochrane risk of bias tool. Two independent reviewers will perform the study selection, data extraction, and methodological quality assessment. A third reviewer will be invited to judge the disagreements between 2 reviewers by discussion. The protocol of this proposed systematic review will compare the effects of AC on psychological condition in patients with CRF undergoing hemodialysis. The outcomes will comprise of depression. The secondary outcome includes anxiety, health related quality of life, and any adverse events. The findings of this systematic review will summarize the latest evidence of AC on psychological condition in patients with CRF undergoing hemodialysis. All data used in this systematic review will be collected from previous published clinical studies. Thus, no ethic approval is required for this study. The findings of this study will be published at a peer-reviewed journal. PROSPERO CRD42019122275.","Guan, Y.
 and He, Y. X.","Guan, He",https://dx.doi.org/10.1097/MD.0000000000014738,https://doi.org/10.1097/MD.0000000000014738,2021-08-03
4216.0,,pubmed,Mining Disease-Symptom Relation from Massive Biomedical Literature and Its Application in Severe Disease Diagnosis,Mining Disease-Symptom Relation from Massive Biomedical Literature and Its Application in Severe Disease Diagnosis,"Disease-symptom relation is an important biomedical relation that can be used for clinical decision support including building medical diagnostic systems. Here we present a study on mining disease-symptom relation from massive biomedical literature and constructing biomedical knowledge graph from the relation. From 15,970,134 MEDLINE/PubMed citation records, occurrences of 8,514 disease concepts from the Human Disease Ontology and 842 symptom concepts from the Symptom Ontology and their relation were analyzed and characterized. We improve previous disease-symptom relation mining work by: (1) leveraging the hierarchy information of concepts in medical entity association discovery; and (2) including more exquisite relationship with weights between entities for knowledge graph construction. A medical diagnostic system for severe disease diagnosis was implemented based on the constructed knowledge graph and achieved the best performance compared to all other methods.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Mining Disease-Symptom Relation from Massive Biomedical Literature and Its Application in Severe Disease Diagnosis.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs http://knowledge.amia.org/67852-amia-1.4259402/t004-1.4263758/t004-1.4263759/2976467-1.4263778/2976026-1.4263775; Years 2018; Authors Eryu Xia; Wen Sun; Jing Mei; Enliang Xu; Ke Wang; Yong Qin; Deduplication_Notes ; X Mining Disease-Symptom Relation from Massive Biomedical Literature and Its Application in Severe Disease Diagnosis.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]","Disease-symptom relation is an important biomedical relation that can be used for clinical decision support including building medical diagnostic systems. Here we present a study on mining disease-symptom relation from massive biomedical literature and constructing biomedical knowledge graph from the relation. From 15,970,134 MEDLINE/PubMed citation records, occurrences of 8,514 disease concepts from the Human Disease Ontology and 842 symptom concepts from the Symptom Ontology and their relation were analyzed and characterized. We improve previous disease-symptom relation mining work by: (1) leveraging the hierarchy information of concepts in medical entity association discovery; and (2) including more exquisite relationship with weights between entities for knowledge graph construction. A medical diagnostic system for severe disease diagnosis was implemented based on the constructed knowledge graph and achieved the best performance compared to all other methods.","Xia, E.
 and Sun, W.
 and Mei, J.
 and Xu, E.
 and Wang, K.
 and Qin, Y.","Xia, Sun, Mei, Xu, Wang, Qin",not available,https://www.google.com/search?q=Mining+Disease-Symptom+Relation+from+Massive+Biomedical+Literature+and+Its+Application+in+Severe+Disease+Diagnosis.,2021-08-03
2257.0,,pubmed,Mining social networks to improve suicide prevention: A scoping review,Mining social networks to improve suicide prevention: A scoping review,"Attention about the risks of online social networks (SNs) has been called upon reports describing their use to express emotional distress and suicidal ideation or plans. On the Internet, cyberbullying, suicide pacts, Internet addiction, and 'extreme' communities seem to increase suicidal behavior (SB). In this study, the scientific literature about SBs and SNs was narratively reviewed. Some authors focus on detecting at-risk populations through data mining, identification of risks factors, and web activity patterns. Others describe prevention practices on the Internet, such as websites, screening, and applications. Targeted interventions through SNs are also contemplated when suicidal ideation is present. Multiple predictive models should be defined, implemented, tested, and combined in order to deal with the risk of SB through an effective decision support system. This endeavor might require a reorganization of care for SNs users presenting suicidal ideation.","Attention about the risks of online social networks (SNs) has been called upon reports describing their use to express emotional distress and suicidal ideation or plans. On the Internet, cyberbullying, suicide pacts, Internet addiction, and ""extreme"" communities seem to increase suicidal behavior (SB). In this study, the scientific literature about SBs and SNs was narratively reviewed. Some authors focus on detecting at-risk populations through data mining, identification of risks factors, and web activity patterns. Others describe prevention practices on the Internet, such as websites, screening, and applications. Targeted interventions through SNs are also contemplated when suicidal ideation is present. Multiple predictive models should be defined, implemented, tested, and combined in order to deal with the risk of SB through an effective decision support system. This endeavor might require a reorganization of care for SNs users presenting suicidal ideation.","Lopez-Castroman, J.
 and Moulahi, B.
 and Aze, J.
 and Bringay, S.
 and Deninotti, J.
 and Guillaume, S.
 and Baca-Garcia, E.","Lopez-Castroman, Moulahi, AzÃ©, Bringay, Deninotti, Guillaume, Baca-Garcia",https://dx.doi.org/10.1002/jnr.24404,https://doi.org/10.1002/jnr.24404,2021-08-03
2754.0,,pubmed,Still moving toward automation of the systematic review process: a summary of discussions at the third meeting of the International Collaboration for Automation of Systematic Reviews (ICASR),Still moving toward automation of the systematic review process: a summary of discussions at the third meeting of the International Collaboration for Automation of Systematic Reviews (ICASR),"The third meeting of the International Collaboration for Automation of Systematic Reviews (ICASR) was held 17-18 October 2017 in London, England. ICASR is an interdisciplinary group whose goal is to maximize the use of technology for conducting rapid, accurate, and efficient systematic reviews of scientific evidence. The group seeks to facilitate the development and widespread acceptance of automated techniques for systematic reviews. The meeting's conclusion was that the most pressing needs at present are to develop approaches for validating currently available tools and to provide increased access to curated corpora that can be used for validation. To that end, ICASR's short-term goals in 2018-2019 are to propose and publish protocols for key tasks in systematic reviews and to develop an approach for sharing curated corpora for validating the automation of the key tasks.","The third meeting of the International Collaboration for Automation of Systematic Reviews (ICASR) was held 17-18 October 2017 in London, England. ICASR is an interdisciplinary group whose goal is to maximize the use of technology for conducting rapid, accurate, and efficient systematic reviews of scientific evidence. The group seeks to facilitate the development and widespread acceptance of automated techniques for systematic reviews. The meeting's conclusion was that the most pressing needs at present are to develop approaches for validating currently available tools and to provide increased access to curated corpora that can be used for validation. To that end, ICASR's short-term goals in 2018-2019 are to propose and publish protocols for key tasks in systematic reviews and to develop an approach for sharing curated corpora for validating the automation of the key tasks.","O'Connor, A. M.
 and Tsafnat, G.
 and Gilbert, S. B.
 and Thayer, K. A.
 and Shemilt, I.
 and Thomas, J.
 and Glasziou, P.
 and Wolfe, M. S.","O'Connor, Tsafnat, Gilbert, Thayer, Shemilt, Thomas, Glasziou, Wolfe",not available,https://doi.org/10.1186/s13643-019-0975-y,2021-08-03
2192.0,,pubmed,Acupuncture for fecal incontinence: Protocol for a systematic review and data mining,Acupuncture for fecal incontinence: Protocol for a systematic review and data mining,"BACKGROUND: Fecal incontinence is a socially and emotionally destructive condition that has a negative impact on personal image, self-confidence, and quality of life. Acupuncture is commonly used to treat chronic conditions, including fecal incontinence. However, no relevant systematic review or meta-analysis has been designed to evaluate the effects of acupuncture on fecal incontinence. METHODS: We will identify relevant randomized controlled trials (RCTs) from the Cochrane Library, Medline, Embase, PubMed, Springer, Web of Science, China National Knowledge Infrastructure, VIP Chinese Science and Technology Journals Database, Wanfang database, and clinical trial registration center from their inception to February 28, 2019. The primary outcome measures will be clinical effective rate, functional outcomes, and quality of life. Data that meets the inclusion criteria will be extracted and analyzed using RevMan V.5.3 software. Two reviewers will evaluate the studies using the Cochrane Collaboration risk of bias tool. Publication bias will be assessed by funnel plots, Egger test, and Begg test using the Stata software. Acupoints characteristics will be analyzed by Traditional Chinese Medicine inheritance support system. RESULTS: This study will analyze the clinical effective rate, functional outcomes, quality of life, daily average number of fecal incontinence, and effective prescriptions of acupuncture for patients with fecal incontinence. CONCLUSION: Our findings will provide evidence for the effectiveness and potential treatment prescriptions of acupuncture for patients with fecal incontinence. Prospero registration number: prospero crd42019119680.","Fecal incontinence is a socially and emotionally destructive condition that has a negative impact on personal image, self-confidence, and quality of life. Acupuncture is commonly used to treat chronic conditions, including fecal incontinence. However, no relevant systematic review or meta-analysis has been designed to evaluate the effects of acupuncture on fecal incontinence. We will identify relevant randomized controlled trials (RCTs) from the Cochrane Library, Medline, Embase, PubMed, Springer, Web of Science, China National Knowledge Infrastructure, VIP Chinese Science and Technology Journals Database, Wanfang database, and clinical trial registration center from their inception to February 28, 2019. The primary outcome measures will be clinical effective rate, functional outcomes, and quality of life. Data that meets the inclusion criteria will be extracted and analyzed using RevMan V.5.3 software. Two reviewers will evaluate the studies using the Cochrane Collaboration risk of bias tool. Publication bias will be assessed by funnel plots, Egger test, and Begg test using the Stata software. Acupoints characteristics will be analyzed by Traditional Chinese Medicine inheritance support system. This study will analyze the clinical effective rate, functional outcomes, quality of life, daily average number of fecal incontinence, and effective prescriptions of acupuncture for patients with fecal incontinence. Our findings will provide evidence for the effectiveness and potential treatment prescriptions of acupuncture for patients with fecal incontinence. PROSPERO CRD42019119680.","Lin, H.
 and Zhang, Z.
 and Hu, G.
 and Wang, X.
 and Lin, C.
 and Chen, Y.","Lin, Zhang, Hu, Wang, Lin, Chen",https://dx.doi.org/10.1097/MD.0000000000014482,https://doi.org/10.1097/MD.0000000000014482,2021-08-03
366.0,,pubmed,Speech recognition for clinical documentation from 1990 to 2018: a systematic review,Speech recognition for clinical documentation from 1990 to 2018: a systematic review,"OBJECTIVE: The study sought to review recent literature regarding use of speech recognition (SR) technology for clinical documentation and to understand the impact of SR on document accuracy, provider efficiency, institutional cost, and more. MATERIALS AND METHODS: We searched 10 scientific and medical literature databases to find articles about clinician use of SR for documentation published between January 1, 1990, and October 15, 2018. We annotated included articles with their research topic(s), medical domain(s), and SR system(s) evaluated and analyzed the results. RESULTS: One hundred twenty-two articles were included. Forty-eight (39.3%) involved the radiology department exclusively and 10 (8.2%) involved emergency medicine; 10 (8.2%) mentioned multiple departments. Forty-eight (39.3%) articles studied productivity; 20 (16.4%) studied the effect of SR on documentation time, with mixed findings. Decreased turnaround time was reported in all 19 (15.6%) studies in which it was evaluated. Twenty-nine (23.8%) studies conducted error analyses, though various evaluation metrics were used. Reported percentage of documents with errors ranged from 4.8% to 71%; reported word error rates ranged from 7.4% to 38.7%. Seven (5.7%) studies assessed documentation-associated costs; 5 reported decreases and 2 reported increases. Many studies (44.3%) used products by Nuance Communications. Other vendors included IBM (9.0%) and Philips (6.6%); 7 (5.7%) used self-developed systems. CONCLUSION: Despite widespread use of SR for clinical documentation, research on this topic remains largely heterogeneous, often using different evaluation metrics with mixed findings. Further, that SR-assisted documentation has become increasingly common in clinical settings beyond radiology warrants further investigation of its use and effectiveness in these settings.","The study sought to review recent literature regarding use of speech recognition (SR) technology for clinical documentation and to understand the impact of SR on document accuracy, provider efficiency, institutional cost, and more. We searched 10 scientific and medical literature databases to find articles about clinician use of SR for documentation published between January 1, 1990, and October 15, 2018. We annotated included articles with their research topic(s), medical domain(s), and SR system(s) evaluated and analyzed the results. One hundred twenty-two articles were included. Forty-eight (39.3%) involved the radiology department exclusively and 10 (8.2%) involved emergency medicine; 10 (8.2%) mentioned multiple departments. Forty-eight (39.3%) articles studied productivity; 20 (16.4%) studied the effect of SR on documentation time, with mixed findings. Decreased turnaround time was reported in all 19 (15.6%) studies in which it was evaluated. Twenty-nine (23.8%) studies conducted error analyses, though various evaluation metrics were used. Reported percentage of documents with errors ranged from 4.8% to 71%; reported word error rates ranged from 7.4% to 38.7%. Seven (5.7%) studies assessed documentation-associated costs; 5 reported decreases and 2 reported increases. Many studies (44.3%) used products by Nuance Communications. Other vendors included IBM (9.0%) and Philips (6.6%); 7 (5.7%) used self-developed systems. Despite widespread use of SR for clinical documentation, research on this topic remains largely heterogeneous, often using different evaluation metrics with mixed findings. Further, that SR-assisted documentation has become increasingly common in clinical settings beyond radiology warrants further investigation of its use and effectiveness in these settings.","Blackley, S. V.
 and Huynh, J.
 and Wang, L.
 and Korach, Z.
 and Zhou, L.","Blackley, Huynh, Wang, Korach, Zhou",https://dx.doi.org/10.1093/jamia/ocy179,https://doi.org/10.1093/jamia/ocy179,2021-08-03
1923.0,,pubmed,Natural language processing of symptoms documented in free-text narratives of electronic health records: a systematic review,Natural language processing of symptoms documented in free-text narratives of electronic health records: a systematic review,"OBJECTIVE: Natural language processing (NLP) of symptoms from electronic health records (EHRs) could contribute to the advancement of symptom science. We aim to synthesize the literature on the use of NLP to process or analyze symptom information documented in EHR free-text narratives. MATERIALS AND METHODS: Our search of 1964 records from PubMed and EMBASE was narrowed to 27 eligible articles. Data related to the purpose, free-text corpus, patients, symptoms, NLP methodology, evaluation metrics, and quality indicators were extracted for each study. RESULTS: Symptom-related information was presented as a primary outcome in 14 studies. EHR narratives represented various inpatient and outpatient clinical specialties, with general, cardiology, and mental health occurring most frequently. Studies encompassed a wide variety of symptoms, including shortness of breath, pain, nausea, dizziness, disturbed sleep, constipation, and depressed mood. NLP approaches included previously developed NLP tools, classification methods, and manually curated rule-based processing. Only one-third (n = 9) of studies reported patient demographic characteristics. DISCUSSION: NLP is used to extract information from EHR free-text narratives written by a variety of healthcare providers on an expansive range of symptoms across diverse clinical specialties. The current focus of this field is on the development of methods to extract symptom information and the use of symptom information for disease classification tasks rather than the examination of symptoms themselves. CONCLUSION: Future NLP studies should concentrate on the investigation of symptoms and symptom documentation in EHR free-text narratives. Efforts should be undertaken to examine patient characteristics and make symptom-related NLP algorithms or pipelines and vocabularies openly available.","Natural language processing (NLP) of symptoms from electronic health records (EHRs) could contribute to the advancement of symptom science. We aim to synthesize the literature on the use of NLP to process or analyze symptom information documented in EHR free-text narratives. Our search of 1964 records from PubMed and EMBASE was narrowed to 27 eligible articles. Data related to the purpose, free-text corpus, patients, symptoms, NLP methodology, evaluation metrics, and quality indicators were extracted for each study. Symptom-related information was presented as a primary outcome in 14 studies. EHR narratives represented various inpatient and outpatient clinical specialties, with general, cardiology, and mental health occurring most frequently. Studies encompassed a wide variety of symptoms, including shortness of breath, pain, nausea, dizziness, disturbed sleep, constipation, and depressed mood. NLP approaches included previously developed NLP tools, classification methods, and manually curated rule-based processing. Only one-third (nâ€‰=â€‰9) of studies reported patient demographic characteristics. NLP is used to extract information from EHR free-text narratives written by a variety of healthcare providers on an expansive range of symptoms across diverse clinical specialties. The current focus of this field is on the development of methods to extract symptom information and the use of symptom information for disease classification tasks rather than the examination of symptoms themselves. Future NLP studies should concentrate on the investigation of symptoms and symptom documentation in EHR free-text narratives. Efforts should be undertaken to examine patient characteristics and make symptom-related NLP algorithms or pipelines and vocabularies openly available.","Koleck, T. A.
 and Dreisbach, C.
 and Bourne, P. E.
 and Bakken, S.","Koleck, Dreisbach, Bourne, Bakken",not available,https://doi.org/10.1093/jamia/ocy173,2021-08-03
478.0,,pubmed,Artificial Intelligence for the Otolaryngologist: A State of the Art Review,Artificial Intelligence for the Otolaryngologist: A State of the Art Review,"OBJECTIVE: To provide a state of the art review of artificial intelligence (AI), including its subfields of machine learning and natural language processing, as it applies to otolaryngology and to discuss current applications, future impact, and limitations of these technologies. DATA SOURCES: PubMed and Medline search engines. REVIEW METHODS: A structured search of the current literature was performed (up to and including September 2018). Search terms related to topics of AI in otolaryngology were identified and queried to identify relevant articles. CONCLUSIONS: AI is at the forefront of conversation in academic research and popular culture. In recent years, it has been touted for its potential to revolutionize health care delivery. Yet, to date, it has made few contributions to actual medical practice or patient care. Future adoption of AI technologies in otolaryngology practice may be hindered by misconceptions of what AI is and a fear that machine errors may compromise patient care. However, with potential clinical and economic benefits, it is vital for otolaryngologists to understand the principles and scope of AI. IMPLICATIONS FOR PRACTICE: In the coming years, AI is likely to have a major impact on biomedical research and the practice of medicine. Otolaryngologists are key stakeholders in the development and clinical integration of meaningful AI technologies that will improve patient care. High-quality data collection is essential for the development of AI technologies, and otolaryngologists should seek opportunities to collaborate with data scientists to guide them toward the most impactful clinical questions.","To provide a state of the art review of artificial intelligence (AI), including its subfields of machine learning and natural language processing, as it applies to otolaryngology and to discuss current applications, future impact, and limitations of these technologies. PubMed and Medline search engines. A structured search of the current literature was performed (up to and including September 2018). Search terms related to topics of AI in otolaryngology were identified and queried to identify relevant articles. AI is at the forefront of conversation in academic research and popular culture. In recent years, it has been touted for its potential to revolutionize health care delivery. Yet, to date, it has made few contributions to actual medical practice or patient care. Future adoption of AI technologies in otolaryngology practice may be hindered by misconceptions of what AI is and a fear that machine errors may compromise patient care. However, with potential clinical and economic benefits, it is vital for otolaryngologists to understand the principles and scope of AI. In the coming years, AI is likely to have a major impact on biomedical research and the practice of medicine. Otolaryngologists are key stakeholders in the development and clinical integration of meaningful AI technologies that will improve patient care. High-quality data collection is essential for the development of AI technologies, and otolaryngologists should seek opportunities to collaborate with data scientists to guide them toward the most impactful clinical questions.","Bur, A. M.
 and Shew, M.
 and New, J.","Bur, Shew, New",https://dx.doi.org/10.1177/0194599819827507,https://doi.org/10.1177/0194599819827507,2021-08-03
3634.0,,pubmed,"Neuroanatomy, the Achille's Heel of Medical Students A Systematic Analysis of Educational Strategies for the Teaching of Neuroanatomy","Neuroanatomy, the Achille's Heel of Medical Students A Systematic Analysis of Educational Strategies for the Teaching of Neuroanatomy","Neuroanatomy has been deemed crucial for clinical neurosciences. It has been one of the most challenging parts of the anatomical curriculum and is one of the causes of 'neurophobia,' whose main implication is a negative influence on the choice of neurology in the near future. In the last decades, several educational strategies have been identified to improve the skills of students and to promote a deep learning. The aim of this study was to systematically review the literature to identify the most effective method/s to teach human neuroanatomy. The search was restricted to publications written in English language and to articles describing teaching tools in undergraduate medical courses from January 2006 through December 2017. The primary outcome was the observation of improvement of anatomical knowledge in undergraduate medical students. Secondary outcomes were the amelioration of long-term retention knowledge and the grade of satisfaction of students. Among 18 selected studies, 44.4% have used three-dimensional (3D) teaching tools, 16.6% near peer teaching tool, 5.55% flipped classroom tool, 5.55% applied neuroanatomy elective course, 5.55% equivalence-based instruction-rote learning, 5.55% mobile augmented reality, 5.55% inquiry-based clinical case, 5.55% cadaver dissection, and 5.55% Twitter. The high in-between study heterogeneity was the main issue to identify the most helpful teaching tool to improve neuroanatomical knowledge among medical students. Data from this study suggest that a combination of multiple pedagogical resources seems to be the more advantageous for teaching neuroanatomy.","Neuroanatomy has been deemed crucial for clinical neurosciences. It has been one of the most challenging parts of the anatomical curriculum and is one of the causes of ""neurophobia,"" whose main implication is a negative influence on the choice of neurology in the near future. In the last decades, several educational strategies have been identified to improve the skills of students and to promote a deep learning. The aim of this study was to systematically review the literature to identify the most effective method/s to teach human neuroanatomy. The search was restricted to publications written in English language and to articles describing teaching tools in undergraduate medical courses from January 2006 through December 2017. The primary outcome was the observation of improvement of anatomical knowledge in undergraduate medical students. Secondary outcomes were the amelioration of long-term retention knowledge and the grade of satisfaction of students. Among 18 selected studies, 44.4% have used three-dimensional (3D) teaching tools, 16.6% near peer teaching tool, 5.55% flipped classroom tool, 5.55% applied neuroanatomy elective course, 5.55% equivalence-based instruction-rote learning, 5.55% mobile augmented reality, 5.55% inquiry-based clinical case, 5.55% cadaver dissection, and 5.55% Twitter. The high in-between study heterogeneity was the main issue to identify the most helpful teaching tool to improve neuroanatomical knowledge among medical students. Data from this study suggest that a combination of multiple pedagogical resources seems to be the more advantageous for teaching neuroanatomy.","Sotgiu, M. A.
 and Mazzarello, V.
 and Bandiera, P.
 and Madeddu, R.
 and Montella, A.
 and Moxham, B.","Sotgiu, Mazzarello, Bandiera, Madeddu, Montella, Moxham",https://dx.doi.org/10.1002/ase.1866,https://doi.org/10.1002/ase.1866,2021-08-03
1053.0,,pubmed,Predictive Markers for Postsurgical Medical Management of Acromegaly: A Systematic Review and Consensus Treatment Guideline,PREDICTIVE MARKERS FOR POSTSURGICAL MEDICAL MANAGEMENT OF ACROMEGALY: A SYSTEMATIC REVIEW AND CONSENSUS TREATMENT GUIDELINE,"<b>Objective:</b> To clarify the selection of medical therapy following transsphenoidal surgery in patients with acromegaly, based on growth hormone (GH)/insulin-like growth factor 1 (IGF-1) response and glucometabolic control. <b>Methods:</b> We carried out a systematic literature review on three of the best studied and most practical predictive markers of the response to somatostatin analogues (SSAs): somatostatin receptor (SSTR) expression, tumor morphologic classification, and T2-weighted magnetic resonance imaging (MRI) signal intensity. Additional analyses focused on glucose metabolism in treated patients. <b>Results:</b> The literature survey confirmed significant associations of all three factors with SSA responsiveness. SSTR expression appears necessary for the SSA response; however, it is not sufficient, as approximately half of SSTR2-positive tumors failed to respond clinically to first-generation SSAs. MRI findings (T2-hypo-intensity) and a densely granulated phenotype also correlate with SSA efficacy, and are advantageous as predictive markers relative to SSTR expression alone. Glucometabolic control declines with SSA monotherapy, whereas GH receptor antagonist (GHRA) monotherapy may restore normoglycemia. <b>Conclusion: </b> We propose a decision tree to guide selection among SSAs, dopamine agonists (DAs), and GHRA for medical treatment of acromegaly in the postsurgical setting. This decision tree employs three validated predictive markers and other clinical considerations, to determine whether SSAs are appropriate first-line medical therapy in the postsurgical setting. DA treatment is favored in patients with modest IGF-1 elevation. GHRA treatment should be considered for patients with T2-hyperintense tumors with a sparsely granulated phenotype and/or low SSTR2 staining, and may also be favored for individuals with diabetes. Prospective analyses are required to test the utility of this therapeutic paradigm. <b>Abbreviations: DA</b> = dopamine agonist; <b>DG</b> = densely granulated; <b>GH</b> = growth hormone; <b>GHRA</b> = growth hormone receptor antagonist; <b>HbA1c</b> = glycated hemoglobin; <b>IGF-1</b> = insulin-like growth factor-1; <b>MRI</b> = magnetic resonance imaging; <b>SG</b> = sparsely granulated; <b>SSA</b> = somatostatin analogue; <b>SSTR</b> = somatostatin receptor.","<b><i>Objective:</i></b> To clarify the selection of medical therapy following transsphenoidal surgery in patients with acromegaly, based on growth hormone (GH)/insulin-like growth factor 1 (IGF-1) response and glucometabolic control. <b><i>Methods:</i></b> We carried out a systematic literature review on three of the best studied and most practical predictive markers of the response to somatostatin analogues (SSAs): somatostatin receptor (SSTR) expression, tumor morphologic classification, and T2-weighted magnetic resonance imaging (MRI) signal intensity. Additional analyses focused on glucose metabolism in treated patients. <b><i>Results:</i></b> The literature survey confirmed significant associations of all three factors with SSA responsiveness. SSTR expression appears necessary for the SSA response; however, it is not sufficient, as approximately half of SSTR2-positive tumors failed to respond clinically to first-generation SSAs. MRI findings (T2-hypo-intensity) and a densely granulated phenotype also correlate with SSA efficacy, and are advantageous as predictive markers relative to SSTR expression alone. Glucometabolic control declines with SSA monotherapy, whereas GH receptor antagonist (GHRA) monotherapy may restore normoglycemia. <b><i>Conclusion:</i></b> We propose a decision tree to guide selection among SSAs, dopamine agonists (DAs), and GHRA for medical treatment of acromegaly in the postsurgical setting. This decision tree employs three validated predictive markers and other clinical considerations, to determine whether SSAs are appropriate first-line medical therapy in the postsurgical setting. DA treatment is favored in patients with modest IGF-1 elevation. GHRA treatment should be considered for patients with T2-hyperintense tumors with a sparsely granulated phenotype and/or low SSTR2 staining, and may also be favored for individuals with diabetes. Prospective analyses are required to test the utility of this therapeutic paradigm. <b>Abbreviations: DA</b> = dopamine agonist; <b>DG</b> = densely granulated; <b>GH</b> = growth hormone; <b>GHRA</b> = growth hormone receptor antagonist; <b>HbA1c</b> = glycated hemoglobin; <b>IGF-1</b> = insulin-like growth factor-1; <b>MRI</b> = magnetic resonance imaging; <b>SG</b> = sparsely granulated; <b>SSA</b> = somatostatin analogue; <b>SSTR</b> = somatostatin receptor.","Ezzat, S.
 and Caspar-Bell, G. M.
 and Chik, C. L.
 and Denis, M. C.
 and Domingue, M. E.
 and Imran, S. A.
 and Johnson, M. D.
 and Lochnan, H. A.
 and Gregoire Nyomba, B. L.
 and Prebtani, A.
 and Ridout, R.
 and Ramirez, J. A. R.
 and Van Uum, S.","Ezzat, Caspar-Bell, Chik, Denis, Domingue, Imran, Johnson, Lochnan, GrÃ©goire Nyomba, Prebtani, Ridout, Ramirez, Van Uum",https://dx.doi.org/10.4158/EP-2018-0500,https://doi.org/10.4158/EP-2018-0500,2021-08-03
258.0,,pubmed,Machine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error,Machine learning algorithms for systematic review: reducing workload in a preclinical review of animal studies and reducing human screening error,"BACKGROUND: Here, we outline a method of applying existing machine learning (ML) approaches to aid citation screening in an on-going broad and shallow systematic review of preclinical animal studies. The aim is to achieve a high-performing algorithm comparable to human screening that can reduce human resources required for carrying out this step of a systematic review. METHODS: We applied ML approaches to a broad systematic review of animal models of depression at the citation screening stage. We tested two independently developed ML approaches which used different classification models and feature sets. We recorded the performance of the ML approaches on an unseen validation set of papers using sensitivity, specificity and accuracy. We aimed to achieve 95% sensitivity and to maximise specificity. The classification model providing the most accurate predictions was applied to the remaining unseen records in the dataset and will be used in the next stage of the preclinical biomedical sciences systematic review. We used a cross-validation technique to assign ML inclusion likelihood scores to the human screened records, to identify potential errors made during the human screening process (error analysis). RESULTS: ML approaches reached 98.7% sensitivity based on learning from a training set of 5749 records, with an inclusion prevalence of 13.2%. The highest level of specificity reached was 86%. Performance was assessed on an independent validation dataset. Human errors in the training and validation sets were successfully identified using the assigned inclusion likelihood from the ML model to highlight discrepancies. Training the ML algorithm on the corrected dataset improved the specificity of the algorithm without compromising sensitivity. Error analysis correction leads to a 3% improvement in sensitivity and specificity, which increases precision and accuracy of the ML algorithm. CONCLUSIONS: This work has confirmed the performance and application of ML algorithms for screening in systematic reviews of preclinical animal studies. It has highlighted the novel use of ML algorithms to identify human error. This needs to be confirmed in other reviews with different inclusion prevalence levels, but represents a promising approach to integrating human decisions and automation in systematic review methodology.","Here, we outline a method of applying existing machine learning (ML) approaches to aid citation screening in an on-going broad and shallow systematic review of preclinical animal studies. The aim is to achieve a high-performing algorithm comparable to human screening that can reduce human resources required for carrying out this step of a systematic review. We applied ML approaches to a broad systematic review of animal models of depression at the citation screening stage. We tested two independently developed ML approaches which used different classification models and feature sets. We recorded the performance of the ML approaches on an unseen validation set of papers using sensitivity, specificity and accuracy. We aimed to achieve 95% sensitivity and to maximise specificity. The classification model providing the most accurate predictions was applied to the remaining unseen records in the dataset and will be used in the next stage of the preclinical biomedical sciences systematic review. We used a cross-validation technique to assign ML inclusion likelihood scores to the human screened records, to identify potential errors made during the human screening process (error analysis). ML approaches reached 98.7% sensitivity based on learning from a training set of 5749 records, with an inclusion prevalence of 13.2%. The highest level of specificity reached was 86%. Performance was assessed on an independent validation dataset. Human errors in the training and validation sets were successfully identified using the assigned inclusion likelihood from the ML model to highlight discrepancies. Training the ML algorithm on the corrected dataset improved the specificity of the algorithm without compromising sensitivity. Error analysis correction leads to a 3% improvement in sensitivity and specificity, which increases precision and accuracy of the ML algorithm. This work has confirmed the performance and application of ML algorithms for screening in systematic reviews of preclinical animal studies. It has highlighted the novel use of ML algorithms to identify human error. This needs to be confirmed in other reviews with different inclusion prevalence levels, but represents a promising approach to integrating human decisions and automation in systematic review methodology.","Bannach-Brown, A.
 and Przybyla, P.
 and Thomas, J.
 and Rice, A. S. C.
 and Ananiadou, S.
 and Liao, J.
 and Macleod, M. R.","Bannach-Brown, PrzybyÅ‚a, Thomas, Rice, Ananiadou, Liao, Macleod",not available,https://doi.org/10.1186/s13643-019-0942-7,2021-08-03
681.0,,pubmed,Accuracy of automated blood pressure measurements in the presence of atrial fibrillation: systematic review and meta-analysis,Accuracy of automated blood pressure measurements in the presence of atrial fibrillation: systematic review and meta-analysis,"Atrial fibrillation (AF) affects ~3% of the general population and is twice as common with hypertension. Validation protocols for automated sphygmomanometers exclude people with AF, raising concerns over accuracy of hypertension diagnosis or management, using out-of-office blood pressure (BP) monitoring, in the presence of AF. Some devices include algorithms to detect AF; a feature open to misinterpretation as offering accurate BP measurement with AF. We undertook this review to explore accuracy of automated devices, with or without AF detection, for measuring BP. We searched Medline and Embase to October 2018 for studies comparing automated BP measurement devices to a standard mercury sphygmomanometer contemporaneously. Data were extracted by two reviewers. Mean BP differences between devices and mercury were calculated, where not reported and compared; meta-analyses were undertaken where possible. We included 13 studies reporting 14 devices. Mean systolic and diastolic BP differences from mercury ranged from -3.1 to + 6.1/-4.6 to +9.0 mmHg. Considerable heterogeneity existed between devices (I<sup>2</sup>: 80 to 90%). Devices with AF detection algorithms appeared no more accurate for BP measurement with AF than other devices. A previous review concluded that oscillometric devices are accurate for systolic but not diastolic BP measurement in AF. The present findings do not support that conclusion. Due to heterogeneity between devices, they should be evaluated on individual performance. We found no evidence that devices with AF detection measure BP more accurately in AF than other devices. More home or ambulatory automated BP monitors require validation in populations with AF.","Atrial fibrillation (AF) affects ~3% of the general population and is twice as common with hypertension. Validation protocols for automated sphygmomanometers exclude people with AF, raising concerns over accuracy of hypertension diagnosis or management, using out-of-office blood pressure (BP) monitoring, in the presence of AF. Some devices include algorithms to detect AF; a feature open to misinterpretation as offering accurate BP measurement with AF. We undertook this review to explore accuracy of automated devices, with or without AF detection, for measuring BP. We searched Medline and Embase to October 2018 for studies comparing automated BP measurement devices to a standard mercury sphygmomanometer contemporaneously. Data were extracted by two reviewers. Mean BP differences between devices and mercury were calculated, where not reported and compared; meta-analyses were undertaken where possible. We included 13 studies reporting 14 devices. Mean systolic and diastolic BP differences from mercury ranged from -3.1 to +â€‰6.1/-4.6 to +9.0â€‰mmHg. Considerable heterogeneity existed between devices (I<sup>2</sup>: 80 to 90%). Devices with AF detection algorithms appeared no more accurate for BP measurement with AF than other devices. A previous review concluded that oscillometric devices are accurate for systolic but not diastolic BP measurement in AF. The present findings do not support that conclusion. Due to heterogeneity between devices, they should be evaluated on individual performance. We found no evidence that devices with AF detection measure BP more accurately in AF than other devices. More home or ambulatory automated BP monitors require validation in populations with AF.","Clark, C. E.
 and McDonagh, S. T. J.
 and McManus, R. J.","Clark, McDonagh, McManus",https://dx.doi.org/10.1038/s41371-018-0153-z,https://doi.org/10.1038/s41371-018-0153-z,2021-08-03
392.0,,pubmed,Can Endodontic Irrigating Solutions Influence the Bond Strength of Adhesives to Coronal Dental Substrates? A Systematic Review and Meta-Analysis of In Vitro Studies,Can Endodontic Irrigating Solutions Influence the Bond Strength of Adhesives to Coronal Dental Substrates? A Systematic Review and Meta-Analysis of In Vitro Studies,"PURPOSE: To systematically review the literature to analyze the influence of endodontic irrigating solutions on the bond strength of adhesives to coronal enamel or dentin. MATERIALS AND METHODS: The PubMed/MEDLINE, Web of Science and Scopus electronic databases were used to select laboratory studies related to the research question, without publication year or language limits. From 2461 potentially eligible studies, 2451 were selected for full-text analysis, and 97 were included in the systematic review. Two authors independently selected the studies, extracted the data, and assessed the risk of bias. Pooling bond strength data were calculated using RevMan5.1 with random effects model (alpha = 0.05), comparing control (no endodontic irrigating solution) and experimental groups (one or more endodontic solutions). RESULTS: No significant difference was found between the control and experimental groups (p = 0.12) in the overall meta-analysis and in the meta-analysis excluding chlorhexidine (p = 0.06). High heterogeneity was found in the meta-analyses. Most included studies in the systematic review were scored as having a high risk of bias. CONCLUSION: The different endodontic irrigating solutions evaluated showed no negative influence on the bond strength of dental adhesives to coronal dental substrates.","To systematically review the literature to analyze the influence of endodontic irrigating solutions on the bond strength of adhesives to coronal enamel or dentin. The PubMed/MEDLINE, Web of Science and Scopus electronic databases were used to select laboratory studies related to the research question, without publication year or language limits. From 2461 potentially eligible studies, 2451 were selected for full-text analysis, and 97 were included in the systematic review. Two authors independently selected the studies, extracted the data, and assessed the risk of bias. Pooling bond strength data were calculated using RevMan5.1 with random effects model (Î± = 0.05), comparing control (no endodontic irrigating solution) and experimental groups (one or more endodontic solutions). No significant difference was found between the control and experimental groups (p = 0.12) in the overall meta-analysis and in the meta-analysis excluding chlorhexidine (p = 0.06). High heterogeneity was found in the meta-analyses. Most included studies in the systematic review were scored as having a high risk of bias. The different endodontic irrigating solutions evaluated showed no negative influence on the bond strength of dental adhesives to coronal dental substrates.","Bohrer, T. C.
 and Fontana, P. E.
 and Lenzi, T. L.
 and Soares, F. Z. M.
 and Rocha, R. O.","Bohrer, Fontana, Lenzi, Soares, Rocha",https://dx.doi.org/10.3290/j.jad.a41633,https://doi.org/10.3290/j.jad.a41633,2021-08-03
836.0,,pubmed,Viability of Bovine Teeth as a Substrate in Bond Strength Tests: A Systematic Review and Meta-analysis,Viability of Bovine Teeth as a Substrate in Bond Strength Tests: A Systematic Review and Meta-analysis,"PURPOSE: To assess whether bovine teeth can be used as viable alternatives for human teeth in tensile and shear bond strength testing. MATERIALS AND METHODS: Articles were selected from Web of Science, PubMed, Scopus, LILACS-Bireme, and BBO electronic databases using keywords obtained from Medical Subject Headings (MeSH). Of 1540 potentially eligible studies, 157 were selected for full text analysis. Five independent reviewers (Kappa = 0.89) selected the studies, abstracted information, and assessed quality based on standardized scales. After the analysis, 78 studies comparing bovine teeth to human teeth were found. Only 18 studies comparing bovine and human substrates in bond strength tests were included in the systematic review and 13 in the meta-analysis. Two authors independently selected the studies, extracted the data and assessed the risk of bias. Mean differences were obtained by comparing tensile and shear bond strengths between human and bovine teeth (permanent and deciduous) and considering enamel and dentin separately (subgroup analysis). Statistical analysis was performed using RevMan5.1, with a random-effect model, at a 5% significance level. RESULTS: No significant difference was found between human and bovine teeth in tensile tests (p = 0.41) for dentin (p = 0.86), but there was a difference for enamel (p = 0.01). Regarding shear bond strength, no significant difference was found between human and bovine teeth (p = 0.16) either for enamel (p = 0.07) or dentin (p = 0.68). Regarding shear bond strength on deciduous teeth, no significant difference was found between human and bovine substrates (p = 0.54), either for enamel (p = 0.42) or dentin (p = 0.05). Most studies were at high (low or unclear) risk of bias. CONCLUSIONS: In shear bond strength testing, bovine teeth can be a suitable alternative for permanent and deciduous human teeth, for both enamel and dentin substrates. However, they may not be suitable for enamel tensile bond strength testing. The findings are based on low quality studies (considerable heterogeneity) and should be interpreted with caution.","To assess whether bovine teeth can be used as viable alternatives for human teeth in tensile and shear bond strength testing. Articles were selected from Web of Science, PubMed, Scopus, LILACS-Bireme, and BBO electronic databases using keywords obtained from Medical Subject Headings (MeSH). Of 1540 potentially eligible studies, 157 were selected for full text analysis. Five independent reviewers (Kappa = 0.89) selected the studies, abstracted information, and assessed quality based on standardized scales. After the analysis, 78 studies comparing bovine teeth to human teeth were found. Only 18 studies comparing bovine and human substrates in bond strength tests were included in the systematic review and 13 in the meta-analysis. Two authors independently selected the studies, extracted the data and assessed the risk of bias. Mean differences were obtained by comparing tensile and shear bond strengths between human and bovine teeth (permanent and deciduous) and considering enamel and dentin separately (subgroup analysis). Statistical analysis was performed using RevMan5.1, with a random-effect model, at a 5% significance level. No significant difference was found between human and bovine teeth in tensile tests (p = 0.41) for dentin (p = 0.86), but there was a difference for enamel (p = 0.01). Regarding shear bond strength, no significant difference was found between human and bovine teeth (p = 0.16) either for enamel (p = 0.07) or dentin (p = 0.68). Regarding shear bond strength on deciduous teeth, no significant difference was found between human and bovine substrates (p = 0.54), either for enamel (p = 0.42) or dentin (p = 0.05). Most studies were at high (low or unclear) risk of bias. In shear bond strength testing, bovine teeth can be a suitable alternative for permanent and deciduous human teeth, for both enamel and dentin substrates. However, they may not be suitable for enamel tensile bond strength testing. The findings are based on low quality studies (considerable heterogeneity) and should be interpreted with caution.","de Carvalho, M. F. F.
 and Leijoto-Lannes, A. C. N.
 and Rodrigues, M. C. N.
 and Nogueira, L. C.
 and Ferraz, N. K. L.
 and Moreira, A. N.
 and Yamauti, M.
 and Zina, L. G.
 and Magalhaes, C. S.","de Carvalho, LeijÃ´to-Lannes, Rodrigues, Nogueira, Ferraz, Moreira, Yamauti, Zina, MagalhÃ£es",https://dx.doi.org/10.3290/j.jad.a41636,https://doi.org/10.3290/j.jad.a41636,2021-08-03
4053.0,,pubmed,Clinical massage therapy for patients with cancer-related fatigue protocol of a systematic review,Clinical massage therapy for patients with cancer-related fatigue protocol of a systematic review,"BACKGROUND: Cancer-related fatigue (CRF) is a prevalent and debilitating symptom experienced by cancer survivors, one that severely compromises functional independence and quality of life. Clinical massage therapy (CMT), as an important part of complementary and alternative medicine, is widely employed among massage therapists, physical therapists, nurses, and physicians when managing CRF. Clinical research indicates that CMT produced relief of CRF. In this systematic review, we aim to evaluate the effectiveness and safety of CMT for patients with CRF. METHODS: We will search the following electronic databases for randomized controlled trials to evaluate the effectiveness and safety of CMT for CRF in cancer patients: CENTRAL, Embase, MEDILINE, CINAHL and China National Knowledge Infrastructure. Each database will be searched from inception to October 2018. The entire process will include study selection, data extraction, risk of bias assessment and meta-analyses. RESULTS: This proposed study will evaluate the effectiveness and safety of CMT for CRF. The outcomes will include change in quality of life, fatigue relief and adverse effect. CONCLUSIONS: This proposed systematic review will evaluate the existing evidence on the effectiveness and safety of CMT for patients with CRF. DISSEMINATION AND ETHICS: The results of this review will be disseminated through peer-reviewed publication. Because all of the data used in this systematic review and meta-analysis has been published, this review does not require ethical approval. Furthermore, all data will be analyzed anonymously during the review process.","Cancer-related fatigue (CRF) is a prevalent and debilitating symptom experienced by cancer survivors, one that severely compromises functional independence and quality of life. Clinical massage therapy (CMT), as an important part of complementary and alternative medicine, is widely employed among massage therapists, physical therapists, nurses, and physicians when managing CRF. Clinical research indicates that CMT produced relief of CRF. In this systematic review, we aim to evaluate the effectiveness and safety of CMT for patients with CRF. We will search the following electronic databases for randomized controlled trials to evaluate the effectiveness and safety of CMT for CRF in cancer patients: CENTRAL, Embase, MEDILINE, CINAHL and China National Knowledge Infrastructure. Each database will be searched from inception to October 2018. The entire process will include study selection, data extraction, risk of bias assessment and meta-analyses. This proposed study will evaluate the effectiveness and safety of CMT for CRF. The outcomes will include change in quality of life, fatigue relief and adverse effect. This proposed systematic review will evaluate the existing evidence on the effectiveness and safety of CMT for patients with CRF. The results of this review will be disseminated through peer-reviewed publication. Because all of the data used in this systematic review and meta-analysis has been published, this review does not require ethical approval. Furthermore, all data will be analyzed anonymously during the review process.","Wang, K.
 and Qi, S.
 and Lai, H.
 and Zhu, X.
 and Fu, G.","Wang, Qi, Lai, Zhu, Fu",not available,https://doi.org/10.1097/MD.0000000000013440,2021-08-03
2212.0,,pubmed,Machine learning methods for automatic pain assessment using facial expression information: Protocol for a systematic review and meta-analysis,Machine learning methods for automatic pain assessment using facial expression information: Protocol for a systematic review and meta-analysis,"INTRODUCTION: Prediction of pain using machine learning algorithms is an emerging field in both computer science and clinical medicine. Several machine algorithms were developed and validated in recent years. However, the majority of studies in this topic was published on bioinformatics or computer science journals instead of medical journals. This tendency and preference led to a gap of knowledge and acknowledgment between computer scientists who invent the algorithm and medical researchers who may use the algorithms in practice. As a consequence, some of these prediction papers did not discuss the clinical utility aspects and were causally reported without following related professional guidelines (e.g., TRIPOD statement). The aim of this protocol is to systematically summarize the current evidences about performance and utility of different machine learning methods used for automatic pain assessments based on human facial expression. In addition, this study is aimed to demonstrate and fill the knowledge gap to promote interdisciplinary collaboration. METHODS AND ANALYSIS: We will search all English language literature in the following electronic databases: PubMed, Web of Science and IEEE Xplore. A systematic review and meta-analysis summarizing the accuracy, interpretability, generalizability, and computational efficiency of machine learning methods will be conducted. Subgroup analyses by machine learning method types will be conducted. TIMELINE: The formal meta-analysis will start on Jan 15, 2019 and expected to finish by April 15, 2019. ETHICS AND DISSEMINATION: Ethical approval will be exempted or will not be required because the data collected and analyzed in this meta-analysis will not be on an individual level. The results will be disseminated in the form of an official publication in a peer-reviewed journal and/or presentation at relevant conferences. Registration: prospero crd42018103059.","Prediction of pain using machine learning algorithms is an emerging field in both computer science and clinical medicine. Several machine algorithms were developed and validated in recent years. However, the majority of studies in this topic was published on bioinformatics or computer science journals instead of medical journals. This tendency and preference led to a gap of knowledge and acknowledgment between computer scientists who invent the algorithm and medical researchers who may use the algorithms in practice. As a consequence, some of these prediction papers did not discuss the clinical utility aspects and were causally reported without following related professional guidelines (e.g., TRIPOD statement). The aim of this protocol is to systematically summarize the current evidences about performance and utility of different machine learning methods used for automatic pain assessments based on human facial expression. In addition, this study is aimed to demonstrate and fill the knowledge gap to promote interdisciplinary collaboration. We will search all English language literature in the following electronic databases: PubMed, Web of Science and IEEE Xplore. A systematic review and meta-analysis summarizing the accuracy, interpretability, generalizability, and computational efficiency of machine learning methods will be conducted. Subgroup analyses by machine learning method types will be conducted. The formal meta-analysis will start on Jan 15, 2019 and expected to finish by April 15, 2019. Ethical approval will be exempted or will not be required because the data collected and analyzed in this meta-analysis will not be on an individual level. The results will be disseminated in the form of an official publication in a peer-reviewed journal and/or presentation at relevant conferences. PROSPERO CRD42018103059.","Liu, D.
 and Cheng, D.
 and Houle, T. T.
 and Chen, L.
 and Zhang, W.
 and Deng, H.","Liu, Cheng, Houle, Chen, Zhang, Deng",https://dx.doi.org/10.1097/MD.0000000000013421,https://doi.org/10.1097/MD.0000000000013421,2021-08-03
1098.0,,pubmed,Computer-assisted diagnosis techniques (dermoscopy and spectroscopy-based) for diagnosing skin cancer in adults,Computer-assisted diagnosis techniques (dermoscopy and spectroscopy-based) for diagnosing skin cancer in adults,"BACKGROUND: Early accurate detection of all skin cancer types is essential to guide appropriate management and to improve morbidity and survival. Melanoma and cutaneous squamous cell carcinoma (cSCC) are high-risk skin cancers which have the potential to metastasise and ultimately lead to death, whereas basal cell carcinoma (BCC) is usually localised with potential to infiltrate and damage surrounding tissue. Anxiety around missing early curable cases needs to be balanced against inappropriate referral and unnecessary excision of benign lesions. Computer-assisted diagnosis (CAD) systems use artificial intelligence to analyse lesion data and arrive at a diagnosis of skin cancer. When used in unreferred settings ('primary care'), CAD may assist general practitioners (GPs) or other clinicians to more appropriately triage high-risk lesions to secondary care. Used alongside clinical and dermoscopic suspicion of malignancy, CAD may reduce unnecessary excisions without missing melanoma cases. OBJECTIVES: To determine the accuracy of CAD systems for diagnosing cutaneous invasive melanoma and atypical intraepidermal melanocytic variants, BCC or cSCC in adults, and to compare its accuracy with that of dermoscopy. SEARCH METHODS: We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials (CENTRAL); MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. SELECTION CRITERIA: Studies of any design that evaluated CAD alone, or in comparison with dermoscopy, in adults with lesions suspicious for melanoma or BCC or cSCC, and compared with a reference standard of either histological confirmation or clinical follow-up. DATA COLLECTION AND ANALYSIS: Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS-2). We contacted authors of included studies where information related to the target condition or diagnostic threshold were missing. We estimated summary sensitivities and specificities separately by type of CAD system, using the bivariate hierarchical model. We compared CAD with dermoscopy using (a) all available CAD data (indirect comparisons), and (b) studies providing paired data for both tests (direct comparisons). We tested the contribution of human decision-making to the accuracy of CAD diagnoses in a sensitivity analysis by removing studies that gave CAD results to clinicians to guide diagnostic decision-making. MAIN RESULTS: We included 42 studies, 24 evaluating digital dermoscopy-based CAD systems (Derm-CAD) in 23 study cohorts with 9602 lesions (1220 melanomas, at least 83 BCCs, 9 cSCCs), providing 32 datasets for Derm-CAD and seven for dermoscopy. Eighteen studies evaluated spectroscopy-based CAD (Spectro-CAD) in 16 study cohorts with 6336 lesions (934 melanomas, 163 BCC, 49 cSCCs), providing 32 datasets for Spectro-CAD and six for dermoscopy. These consisted of 15 studies using multispectral imaging (MSI), two studies using electrical impedance spectroscopy (EIS) and one study using diffuse-reflectance spectroscopy. Studies were incompletely reported and at unclear to high risk of bias across all domains. Included studies inadequately address the review question, due to an abundance of low-quality studies, poor reporting, and recruitment of highly selected groups of participants.Across all CAD systems, we found considerable variation in the hardware and software technologies used, the types of classification algorithm employed, methods used to train the algorithms, and which lesion morphological features were extracted and analysed across all CAD systems, and even between studies evaluating CAD systems. Meta-analysis found CAD systems had high sensitivity for correct identification of cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in highly selected populations, but with low and very variable specificity, particularly for Spectro-CAD systems. Pooled data from 22 studies estimated the sensitivity of Derm-CAD for the detection of melanoma as 90.1% (95% confidence interval (CI) 84.0% to 94.0%) and specificity as 74.3% (95% CI 63.6% to 82.7%). Pooled data from eight studies estimated the sensitivity of multispectral imaging CAD (MSI-CAD) as 92.9% (95% CI 83.7% to 97.1%) and specificity as 43.6% (95% CI 24.8% to 64.5%). When applied to a hypothetical population of 1000 lesions at the mean observed melanoma prevalence of 20%, Derm-CAD would miss 20 melanomas and would lead to 206 false-positive results for melanoma. MSI-CAD would miss 14 melanomas and would lead to 451 false diagnoses for melanoma. Preliminary findings suggest CAD systems are at least as sensitive as assessment of dermoscopic images for the diagnosis of invasive melanoma and atypical intraepidermal melanocytic variants. We are unable to make summary statements about the use of CAD in unreferred populations, or its accuracy in detecting keratinocyte cancers, or its use in any setting as a diagnostic aid, because of the paucity of studies. AUTHORS' CONCLUSIONS: In highly selected patient populations all CAD types demonstrate high sensitivity, and could prove useful as a back-up for specialist diagnosis to assist in minimising the risk of missing melanomas. However, the evidence base is currently too poor to understand whether CAD system outputs translate to different clinical decision-making in practice. Insufficient data are available on the use of CAD in community settings, or for the detection of keratinocyte cancers. The evidence base for individual systems is too limited to draw conclusions on which might be preferred for practice. Prospective comparative studies are required that evaluate the use of already evaluated CAD systems as diagnostic aids, by comparison to face-to-face dermoscopy, and in participant populations that are representative of those in which the test would be used in practice.","Early accurate detection of all skin cancer types is essential to guide appropriate management and to improve morbidity and survival. Melanoma and cutaneous squamous cell carcinoma (cSCC) are high-risk skin cancers which have the potential to metastasise and ultimately lead to death, whereas basal cell carcinoma (BCC) is usually localised with potential to infiltrate and damage surrounding tissue. Anxiety around missing early curable cases needs to be balanced against inappropriate referral and unnecessary excision of benign lesions. Computer-assisted diagnosis (CAD) systems use artificial intelligence to analyse lesion data and arrive at a diagnosis of skin cancer. When used in unreferred settings ('primary care'), CAD may assist general practitioners (GPs) or other clinicians to more appropriately triage high-risk lesions to secondary care. Used alongside clinical and dermoscopic suspicion of malignancy, CAD may reduce unnecessary excisions without missing melanoma cases. To determine the accuracy of CAD systems for diagnosing cutaneous invasive melanoma and atypical intraepidermal melanocytic variants, BCC or cSCC in adults, and to compare its accuracy with that of dermoscopy. We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials (CENTRAL); MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Studies of any design that evaluated CAD alone, or in comparison with dermoscopy, in adults with lesions suspicious for melanoma or BCC or cSCC, and compared with a reference standard of either histological confirmation or clinical follow-up. Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS-2). We contacted authors of included studies where information related to the target condition or diagnostic threshold were missing. We estimated summary sensitivities and specificities separately by type of CAD system, using the bivariate hierarchical model. We compared CAD with dermoscopy using (a) all available CAD data (indirect comparisons), and (b) studies providing paired data for both tests (direct comparisons). We tested the contribution of human decision-making to the accuracy of CAD diagnoses in a sensitivity analysis by removing studies that gave CAD results to clinicians to guide diagnostic decision-making. We included 42 studies, 24 evaluating digital dermoscopy-based CAD systems (Derm-CAD) in 23 study cohorts with 9602 lesions (1220 melanomas, at least 83 BCCs, 9 cSCCs), providing 32 datasets for Derm-CAD and seven for dermoscopy. Eighteen studies evaluated spectroscopy-based CAD (Spectro-CAD) in 16 study cohorts with 6336 lesions (934 melanomas, 163 BCC, 49 cSCCs), providing 32 datasets for Spectro-CAD and six for dermoscopy. These consisted of 15 studies using multispectral imaging (MSI), two studies using electrical impedance spectroscopy (EIS) and one study using diffuse-reflectance spectroscopy. Studies were incompletely reported and at unclear to high risk of bias across all domains. Included studies inadequately address the review question, due to an abundance of low-quality studies, poor reporting, and recruitment of highly selected groups of participants.Across all CAD systems, we found considerable variation in the hardware and software technologies used, the types of classification algorithm employed, methods used to train the algorithms, and which lesion morphological features were extracted and analysed across all CAD systems, and even between studies evaluating CAD systems. Meta-analysis found CAD systems had high sensitivity for correct identification of cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in highly selected populations, but with low and very variable specificity, particularly for Spectro-CAD systems. Pooled data from 22 studies estimated the sensitivity of Derm-CAD for the detection of melanoma as 90.1% (95% confidence interval (CI) 84.0% to 94.0%) and specificity as 74.3% (95% CI 63.6% to 82.7%). Pooled data from eight studies estimated the sensitivity of multispectral imaging CAD (MSI-CAD) as 92.9% (95% CI 83.7% to 97.1%) and specificity as 43.6% (95% CI 24.8% to 64.5%). When applied to a hypothetical population of 1000 lesions at the mean observed melanoma prevalence of 20%, Derm-CAD would miss 20 melanomas and would lead to 206 false-positive results for melanoma. MSI-CAD would miss 14 melanomas and would lead to 451 false diagnoses for melanoma. Preliminary findings suggest CAD systems are at least as sensitive as assessment of dermoscopic images for the diagnosis of invasive melanoma and atypical intraepidermal melanocytic variants. We are unable to make summary statements about the use of CAD in unreferred populations, or its accuracy in detecting keratinocyte cancers, or its use in any setting as a diagnostic aid, because of the paucity of studies. In highly selected patient populations all CAD types demonstrate high sensitivity, and could prove useful as a back-up for specialist diagnosis to assist in minimising the risk of missing melanomas. However, the evidence base is currently too poor to understand whether CAD system outputs translate to different clinical decision-making in practice. Insufficient data are available on the use of CAD in community settings, or for the detection of keratinocyte cancers. The evidence base for individual systems is too limited to draw conclusions on which might be preferred for practice. Prospective comparative studies are required that evaluate the use of already evaluated CAD systems as diagnostic aids, by comparison to face-to-face dermoscopy, and in participant populations that are representative of those in which the test would be used in practice.","Ferrante di Ruffano, L.
 and Takwoingi, Y.
 and Dinnes, J.
 and Chuchu, N.
 and Bayliss, S. E.
 and Davenport, C.
 and Matin, R. N.
 and Godfrey, K.
 and O'Sullivan, C.
 and Gulati, A.
 and Chan, S. A.
 and Durack, A.
 and O'Connell, S.
 and Gardiner, M. D.
 and Bamber, J.
 and Deeks, J. J.
 and Williams, H. C.
 and Cochrane Skin Cancer Diagnostic Test Accuracy, Group","Ferrante di Ruffano, Takwoingi, Dinnes, Chuchu, Bayliss, Davenport, Matin, Godfrey, O'Sullivan, Gulati, Chan, Durack, O'Connell, Gardiner, Bamber, Deeks, Williams",https://dx.doi.org/10.1002/14651858.CD013186,https://doi.org/10.1002/14651858.CD013186,2021-08-03
665.0,,pubmed,Smartphone applications for triaging adults with skin lesions that are suspicious for melanoma,Smartphone applications for triaging adults with skin lesions that are suspicious for melanoma,"BACKGROUND: Melanoma accounts for a small proportion of all skin cancer cases but is responsible for most skin cancer-related deaths. Early detection and treatment can improve survival. Smartphone applications are readily accessible and potentially offer an instant risk assessment of the likelihood of malignancy so that the right people seek further medical attention from a clinician for more detailed assessment of the lesion. There is, however, a risk that melanomas will be missed and treatment delayed if the application reassures the user that their lesion is low risk. OBJECTIVES: To assess the diagnostic accuracy of smartphone applications to rule out cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in adults with concerns about suspicious skin lesions. SEARCH METHODS: We undertook a comprehensive search of the following databases from inception to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. SELECTION CRITERIA: Studies of any design evaluating smartphone applications intended for use by individuals in a community setting who have lesions that might be suspicious for melanoma or atypical intraepidermal melanocytic variants versus a reference standard of histological confirmation or clinical follow-up and expert opinion. DATA COLLECTION AND ANALYSIS: Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS-2). Due to scarcity of data and poor quality of studies, we did not perform a meta-analysis for this review. For illustrative purposes, we plotted estimates of sensitivity and specificity on coupled forest plots for each application under consideration. MAIN RESULTS: This review reports on two cohorts of lesions published in two studies. Both studies were at high risk of bias from selective participant recruitment and high rates of non-evaluable images. Concerns about applicability of findings were high due to inclusion only of lesions already selected for excision in a dermatology clinic setting, and image acquisition by clinicians rather than by smartphone app users. We report data for five mobile phone applications and 332 suspicious skin lesions with 86 melanomas across the two studies. Across the four artificial intelligence-based applications that classified lesion images (photographs) as melanomas (one application) or as high risk or 'problematic' lesions (three applications) using a pre-programmed algorithm, sensitivities ranged from 7% (95% CI 2% to 16%) to 73% (95% CI 52% to 88%) and specificities from 37% (95% CI 29% to 46%) to 94% (95% CI 87% to 97%). The single application using store-and-forward review of lesion images by a dermatologist had a sensitivity of 98% (95% CI 90% to 100%) and specificity of 30% (95% CI 22% to 40%). The number of test failures (lesion images analysed by the applications but classed as 'unevaluable' and excluded by the study authors) ranged from 3 to 31 (or 2% to 18% of lesions analysed). The store-and-forward application had one of the highest rates of test failure (15%). At least one melanoma was classed as unevaluable in three of the four application evaluations. AUTHORS' CONCLUSIONS: Smartphone applications using artificial intelligence-based analysis have not yet demonstrated sufficient promise in terms of accuracy, and they are associated with a high likelihood of missing melanomas. Applications based on store-and-forward images could have a potential role in the timely presentation of people with potentially malignant lesions by facilitating active self-management health practices and early engagement of those with suspicious skin lesions; however, they may incur a significant increase in resource and workload. Given the paucity of evidence and low methodological quality of existing studies, it is not possible to draw any implications for practice. Nevertheless, this is a rapidly advancing field, and new and better applications with robust reporting of studies could change these conclusions substantially.","Melanoma accounts for a small proportion of all skin cancer cases but is responsible for most skin cancer-related deaths. Early detection and treatment can improve survival. Smartphone applications are readily accessible and potentially offer an instant risk assessment of the likelihood of malignancy so that the right people seek further medical attention from a clinician for more detailed assessment of the lesion. There is, however, a risk that melanomas will be missed and treatment delayed if the application reassures the user that their lesion is low risk. To assess the diagnostic accuracy of smartphone applications to rule out cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in adults with concerns about suspicious skin lesions. We undertook a comprehensive search of the following databases from inception to August 2016: Cochrane Central Register of Controlled Trials; MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Studies of any design evaluating smartphone applications intended for use by individuals in a community setting who have lesions that might be suspicious for melanoma or atypical intraepidermal melanocytic variants versus a reference standard of histological confirmation or clinical follow-up and expert opinion. Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS-2). Due to scarcity of data and poor quality of studies, we did not perform a meta-analysis for this review. For illustrative purposes, we plotted estimates of sensitivity and specificity on coupled forest plots for each application under consideration. This review reports on two cohorts of lesions published in two studies. Both studies were at high risk of bias from selective participant recruitment and high rates of non-evaluable images. Concerns about applicability of findings were high due to inclusion only of lesions already selected for excision in a dermatology clinic setting, and image acquisition by clinicians rather than by smartphone app users.We report data for five mobile phone applications and 332 suspicious skin lesions with 86 melanomas across the two studies. Across the four artificial intelligence-based applications that classified lesion images (photographs) as melanomas (one application) or as high risk or 'problematic' lesions (three applications) using a pre-programmed algorithm, sensitivities ranged from 7% (95% CI 2% to 16%) to 73% (95% CI 52% to 88%) and specificities from 37% (95% CI 29% to 46%) to 94% (95% CI 87% to 97%). The single application using store-and-forward review of lesion images by a dermatologist had a sensitivity of 98% (95% CI 90% to 100%) and specificity of 30% (95% CI 22% to 40%).The number of test failures (lesion images analysed by the applications but classed as 'unevaluable' and excluded by the study authors) ranged from 3 to 31 (or 2% to 18% of lesions analysed). The store-and-forward application had one of the highest rates of test failure (15%). At least one melanoma was classed as unevaluable in three of the four application evaluations. Smartphone applications using artificial intelligence-based analysis have not yet demonstrated sufficient promise in terms of accuracy, and they are associated with a high likelihood of missing melanomas. Applications based on store-and-forward images could have a potential role in the timely presentation of people with potentially malignant lesions by facilitating active self-management health practices and early engagement of those with suspicious skin lesions; however, they may incur a significant increase in resource and workload. Given the paucity of evidence and low methodological quality of existing studies, it is not possible to draw any implications for practice. Nevertheless, this is a rapidly advancing field, and new and better applications with robust reporting of studies could change these conclusions substantially.","Chuchu, N.
 and Takwoingi, Y.
 and Dinnes, J.
 and Matin, R. N.
 and Bassett, O.
 and Moreau, J. F.
 and Bayliss, S. E.
 and Davenport, C.
 and Godfrey, K.
 and O'Connell, S.
 and Jain, A.
 and Walter, F. M.
 and Deeks, J. J.
 and Williams, H. C.
 and Cochrane Skin Cancer Diagnostic Test Accuracy, Group","Chuchu, Takwoingi, Dinnes, Matin, Bassett, Moreau, Bayliss, Davenport, Godfrey, O'Connell, Jain, Walter, Deeks, Williams",https://dx.doi.org/10.1002/14651858.CD013192,https://doi.org/10.1002/14651858.CD013192,2021-08-03
2021.0,,pubmed,"Temporal Trends in the Cardiorespiratory Fitness of 2,525,827 Adults Between 1967 and 2016: A Systematic Review","Temporal Trends in the Cardiorespiratory Fitness of 2,525,827 Adults Between 1967 and 2016: A Systematic Review","OBJECTIVE: To estimate international and national temporal trends in the cardiorespiratory fitness (CRF) of adults, and to examine relationships between trends in CRF and trends in health-related, socioeconomic, and environmental indicators. METHODS: Data were obtained from a systematic search of studies that explicitly reported temporal trends in the CRF of apparently healthy adults aged 18-59 years. Sample-weighted temporal trends were estimated using best-fitting regression models relating the year of testing to mean CRF. Post-stratified population-weighted mean changes in percent and standardized CRF were estimated. Pearson's correlations were used to describe associations between linear trends in CRF and linear trends in health-related, socioeconomic, and environmental indicators. RESULTS: 2,525,827 adults representing eight high- and upper-middle-income countries between 1967 and 2016 collectively showed a moderate decline of 7.7% (95% CI - 8.4 to - 7.0) or 1.6% per decade (95% CI - 1.7 to - 1.5). Internationally, CRF improved in the 1960s and 1970s, and progressively declined at an increasing rate thereafter. Declines were larger for men than for women, and for young adults (< 40 years) than for middle-aged adults (>= 40 years). All countries experienced declines in CRF with a very strong negative correlation between CRF trends and obesity trends. CONCLUSIONS: There has been a meaningful decline in the CRF of adults since 1980, which has progressively increased in magnitude over time, suggestive of a corresponding decline in population health. Continuous national and international surveillance systems are needed in order to monitor health and fitness trends, especially among low- and middle-income countries for which data do not currently exist. PROSPERO registration number: CRD42013003678.","To estimate international and national temporal trends in the cardiorespiratory fitness (CRF) of adults, and toÂ examine relationships between trends in CRF and trends in health-related, socioeconomic, and environmental indicators. Data were obtained from a systematic search of studies that explicitly reported temporal trends in the CRF of apparently healthy adults aged 18-59Â years. Sample-weighted temporal trends were estimated using best-fitting regression models relating the year of testing to mean CRF. Post-stratified population-weighted mean changes in percent and standardized CRF were estimated. Pearson's correlations were used to describe associations between linear trends in CRF and linear trends in health-related, socioeconomic, and environmental indicators. 2,525,827 adults representing eight high- and upper-middle-income countries between 1967 and 2016 collectively showed a moderate decline of 7.7% (95% CI -â€‰8.4 to -â€‰7.0) or 1.6% per decade (95% CI -â€‰1.7 to -â€‰1.5). Internationally, CRF improved in the 1960s and 1970s, and progressively declined at an increasing rate thereafter. Declines were larger for men than for women, and for young adults (&lt;â€‰40Â years) than for middle-aged adults (â‰¥â€‰40Â years). All countries experienced declines in CRF with a very strong negative correlation between CRF trends and obesity trends. There has been a meaningful decline in the CRF of adults since 1980, which has progressively increased in magnitude over time, suggestive of a corresponding decline in population health. Continuous national and international surveillance systems are needed in order to monitor health and fitness trends, especially among low- and middle-income countries for which data do not currently exist. PROSPERO registration number: CRD42013003678.","Lamoureux, N. R.
 and Fitzgerald, J. S.
 and Norton, K. I.
 and Sabato, T.
 and Tremblay, M. S.
 and Tomkinson, G. R.","Lamoureux, Fitzgerald, Norton, Sabato, Tremblay, Tomkinson",https://dx.doi.org/10.1007/s40279-018-1017-y,https://doi.org/10.1007/s40279-018-1017-y,2021-08-03
395.0,,pubmed,Development and validation of the PEPPER framework (Prenatal Exposure PubMed ParsER) with applications to food additives,Development and validation of the PEPPER framework (Prenatal Exposure PubMed ParsER) with applications to food additives,"Background: Globally, 36% of deaths among children can be attributed to environmental factors. However, no comprehensive list of environmental exposures exists. We seek to address this gap by developing a literature-mining algorithm to catalog prenatal environmental exposures. Methods: We designed a framework called. PEPPER: Prenatal Exposure PubMed ParsER to a) catalog prenatal exposures studied in the literature and b) identify study type. Using PubMed Central, PEPPER classifies article type (methodology, systematic review) and catalogs prenatal exposures. We coupled PEPPER with the FDA's food additive database to form a master set of exposures. Results: We found that of 31 764 prenatal exposure studies only 53.0% were methodology studies. PEPPER consists of 219 prenatal exposures, including a common set of 43 exposures. PEPPER captured prenatal exposures from 56.4% of methodology studies (9492/16 832 studies). Two raters independently reviewed 50 randomly selected articles and annotated presence of exposures and study methodology type. Error rates for PEPPER's exposure assignment ranged from 0.56% to 1.30% depending on the rater. Evaluation of the study type assignment showed agreement ranging from 96% to 100% (kappa = 0.909, p < .001). Using a gold-standard set of relevant prenatal exposure studies, PEPPER achieved a recall of 94.4%. Conclusions: Using curated exposures and food additives; PEPPER provides the first comprehensive list of 219 prenatal exposures studied in methodology papers. On average, 1.45 exposures were investigated per study. PEPPER successfully distinguished article type for all prenatal studies allowing literature gaps to be easily identified.","Globally, 36% of deaths among children can be attributed to environmental factors. However, no comprehensive list of environmental exposures exists. We seek to address this gap by developing a literature-mining algorithm to catalog prenatal environmental exposures. We designed a framework called. Prenatal Exposure PubMed ParsER to a) catalog prenatal exposures studied in the literature and b) identify study type. Using PubMed Central, PEPPER classifies article type (methodology, systematic review) and catalogs prenatal exposures. We coupled PEPPER with the FDA's food additive database to form a master set of exposures. We found that of 31 764 prenatal exposure studies only 53.0% were methodology studies. PEPPER consists of 219 prenatal exposures, including a common set of 43 exposures. PEPPER captured prenatal exposures from 56.4% of methodology studies (9492/16 832 studies). Two raters independently reviewed 50 randomly selected articles and annotated presence of exposures and study methodology type. Error rates for PEPPER's exposure assignment ranged from 0.56% to 1.30% depending on the rater. Evaluation of the study type assignment showed agreement ranging from 96% to 100% (kappaâ€‰=â€‰0.909, pâ€‰&lt;â€‰.001). Using a gold-standard set of relevant prenatal exposure studies, PEPPER achieved a recall of 94.4%. Using curated exposures and food additives; PEPPER provides the first comprehensive list of 219 prenatal exposures studied in methodology papers. On average, 1.45 exposures were investigated per study. PEPPER successfully distinguished article type for all prenatal studies allowing literature gaps to be easily identified.","Boland, M. R.
 and Kashyap, A.
 and Xiong, J.
 and Holmes, J.
 and Lorch, S.","Boland, Kashyap, Xiong, Holmes, Lorch",https://dx.doi.org/10.1093/jamia/ocy119,https://doi.org/10.1093/jamia/ocy119,2021-08-03
2914.0,,pubmed,Machine learning of neuroimaging for assisted diagnosis of cognitive impairment and dementia: A systematic review,Machine learning of neuroimaging for assisted diagnosis of cognitive impairment and dementia: A systematic review,"Introduction: Advanced machine learning methods might help to identify dementia risk from neuroimaging, but their accuracy to date is unclear. Methods: We systematically reviewed the literature, 2006 to late 2016, for machine learning studies differentiating healthy aging from dementia of various types, assessing study quality, and comparing accuracy at different disease boundaries. Results: Of 111 relevant studies, most assessed Alzheimer's disease versus healthy controls, using AD Neuroimaging Initiative data, support vector machines, and only T1-weighted sequences. Accuracy was highest for differentiating Alzheimer's disease from healthy controls and poor for differentiating healthy controls versus mild cognitive impairment versus Alzheimer's disease or mild cognitive impairment converters versus nonconverters. Accuracy increased using combined data types, but not by data source, sample size, or machine learning method. Discussion: Machine learning does not differentiate clinically relevant disease categories yet. More diverse data sets, combinations of different types of data, and close clinical integration of machine learning would help to advance the field.","Advanced machine learning methods might help to identify dementia risk from neuroimaging, but their accuracy to date is unclear. We systematically reviewed the literature, 2006 to late 2016, for machine learning studies differentiating healthy aging from dementia of various types, assessing study quality, and comparing accuracy at different disease boundaries. Of 111 relevant studies, most assessed Alzheimer's disease versus healthy controls, using AD Neuroimaging Initiative data, support vector machines, and only T1-weighted sequences. Accuracy was highest for differentiating Alzheimer's disease from healthy controls and poor for differentiating healthy controls versus mild cognitive impairment versus Alzheimer's disease or mild cognitive impairment converters versus nonconverters. Accuracy increased using combined data types, but not by data source, sample size, or machine learning method. Machine learning does not differentiate clinically relevant disease categories yet. More diverse data sets, combinations of different types of data, and close clinical integration of machine learning would help to advance the field.","Pellegrini, E.
 and Ballerini, L.
 and Hernandez, Mdcv
 and Chappell, F. M.
 and Gonzalez-Castro, V.
 and Anblagan, D.
 and Danso, S.
 and Munoz-Maniega, S.
 and Job, D.
 and Pernet, C.
 and Mair, G.
 and MacGillivray, T. J.
 and Trucco, E.
 and Wardlaw, J. M.","Pellegrini, Ballerini, Hernandez, Chappell, GonzÃ¡lez-Castro, Anblagan, Danso, MuÃ±oz-Maniega, Job, Pernet, Mair, MacGillivray, Trucco, Wardlaw",https://dx.doi.org/10.1016/j.dadm.2018.07.004,https://doi.org/10.1016/j.dadm.2018.07.004,2021-08-03
451.0,,pubmed,Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review,Skin Cancer Classification Using Convolutional Neural Networks: Systematic Review,"BACKGROUND: State-of-the-art classifiers based on convolutional neural networks (CNNs) were shown to classify images of skin cancer on par with dermatologists and could enable lifesaving and fast diagnoses, even outside the hospital via installation of apps on mobile devices. To our knowledge, at present there is no review of the current work in this research area. OBJECTIVE: This study presents the first systematic review of the state-of-the-art research on classifying skin lesions with CNNs. We limit our review to skin lesion classifiers. In particular, methods that apply a CNN only for segmentation or for the classification of dermoscopic patterns are not considered here. Furthermore, this study discusses why the comparability of the presented procedures is very difficult and which challenges must be addressed in the future. METHODS: We searched the Google Scholar, PubMed, Medline, ScienceDirect, and Web of Science databases for systematic reviews and original research articles published in English. Only papers that reported sufficient scientific proceedings are included in this review. RESULTS: We found 13 papers that classified skin lesions using CNNs. In principle, classification methods can be differentiated according to three principles. Approaches that use a CNN already trained by means of another large dataset and then optimize its parameters to the classification of skin lesions are the most common ones used and they display the best performance with the currently available limited datasets. CONCLUSIONS: CNNs display a high performance as state-of-the-art skin lesion classifiers. Unfortunately, it is difficult to compare different classification methods because some approaches use nonpublic datasets for training and/or testing, thereby making reproducibility difficult. Future publications should use publicly available benchmarks and fully disclose methods used for training to allow comparability.","State-of-the-art classifiers based on convolutional neural networks (CNNs) were shown to classify images of skin cancer on par with dermatologists and could enable lifesaving and fast diagnoses, even outside the hospital via installation of apps on mobile devices. To our knowledge, at present there is no review of the current work in this research area. This study presents the first systematic review of the state-of-the-art research on classifying skin lesions with CNNs. We limit our review to skin lesion classifiers. In particular, methods that apply a CNN only for segmentation or for the classification of dermoscopic patterns are not considered here. Furthermore, this study discusses why the comparability of the presented procedures is very difficult and which challenges must be addressed in the future. We searched the Google Scholar, PubMed, Medline, ScienceDirect, and Web of Science databases for systematic reviews and original research articles published in English. Only papers that reported sufficient scientific proceedings are included in this review. We found 13 papers that classified skin lesions using CNNs. In principle, classification methods can be differentiated according to three principles. Approaches that use a CNN already trained by means of another large dataset and then optimize its parameters to the classification of skin lesions are the most common ones used and they display the best performance with the currently available limited datasets. CNNs display a high performance as state-of-the-art skin lesion classifiers. Unfortunately, it is difficult to compare different classification methods because some approaches use nonpublic datasets for training and/or testing, thereby making reproducibility difficult. Future publications should use publicly available benchmarks and fully disclose methods used for training to allow comparability.","Brinker, T. J.
 and Hekler, A.
 and Utikal, J. S.
 and Grabe, N.
 and Schadendorf, D.
 and Klode, J.
 and Berking, C.
 and Steeb, T.
 and Enk, A. H.
 and von Kalle, C.","Brinker, Hekler, Utikal, Grabe, Schadendorf, Klode, Berking, Steeb, Enk, von Kalle",https://dx.doi.org/10.2196/11936,https://doi.org/10.2196/11936,2021-08-03
2748.0,,pubmed,"A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature","A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature","We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the 'PICO' elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.","We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the 'PICO' elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.","Nye, B.
 and Jessy Li, J.
 and Patel, R.
 and Yang, Y.
 and Marshall, I. J.
 and Nenkova, A.
 and Wallace, B. C.","Nye, Jessy Li, Patel, Yang, Marshall, Nenkova, Wallace",not available,"https://www.google.com/search?q=A+Corpus+with+Multi-Level+Annotations+of+Patients,+Interventions+and+Outcomes+to+Support+Language+Processing+for+Medical+Literature.",2021-08-03
2438.0,,pubmed,Morphometric MRI as a diagnostic biomarker of frontotemporal dementia: A systematic review to determine clinical applicability,Morphometric MRI as a diagnostic biomarker of frontotemporal dementia: A systematic review to determine clinical applicability,"Frontotemporal dementia (FTD) is difficult to diagnose, due to its heterogeneous nature and overlap in symptoms with primary psychiatric disorders. Brain MRI for atrophy is a key biomarker but lacks sensitivity in the early stage. Morphometric MRI-based measures and machine learning techniques are a promising tool to improve diagnostic accuracy. Our aim was to review the current state of the literature using morphometric MRI to classify FTD and assess its applicability for clinical practice. A search was completed using Pubmed and PsychInfo of studies which conducted a classification of subjects with FTD from non-FTD (controls or another disorder) using morphometric MRI metrics on an individual level, using single or combined approaches. 28 relevant articles were included and systematically reviewed following PRISMA guidelines. The studies were categorized based on the type of FTD subjects included and the group(s) against which they were classified. Studies varied considerably in subject selection, MRI methodology, and classification approach, and results are highly heterogeneous. Overall many studies indicate good diagnostic accuracy, with higher performance when differentiating FTD from controls (highest result was accuracy of 100%) than other dementias (highest result was AUC of 0.874). Very few machine learning algorithms have been tested in prospective replication. In conclusion, morphometric MRI with machine learning shows potential as an early diagnostic biomarker of FTD, however studies which use rigorous methodology and validate findings in an independent real-life cohort are necessary before this method can be recommended for use clinically.","Frontotemporal dementia (FTD) is difficult to diagnose, due to its heterogeneous nature and overlap in symptoms with primary psychiatric disorders. Brain MRI for atrophy is a key biomarker but lacks sensitivity in the early stage. Morphometric MRI-based measures and machine learning techniques are a promising tool to improve diagnostic accuracy. Our aim was to review the current state of the literature using morphometric MRI to classify FTD and assess its applicability for clinical practice. A search was completed using Pubmed and PsychInfo of studies which conducted a classification of subjects with FTD from non-FTD (controls or another disorder) using morphometric MRI metrics on an individual level, using single or combined approaches. 28 relevant articles were included and systematically reviewed following PRISMA guidelines. The studies were categorized based on the type of FTD subjects included and the group(s) against which they were classified. Studies varied considerably in subject selection, MRI methodology, and classification approach, and results are highly heterogeneous. Overall many studies indicate good diagnostic accuracy, with higher performance when differentiating FTD from controls (highest result was accuracy of 100%) than other dementias (highest result was AUC of 0.874). Very few machine learning algorithms have been tested in prospective replication. In conclusion, morphometric MRI with machine learning shows potential as an early diagnostic biomarker of FTD, however studies which use rigorous methodology and validate findings in an independent real-life cohort are necessary before this method can be recommended for use clinically.","McCarthy, J.
 and Collins, D. L.
 and Ducharme, S.","McCarthy, Collins, Ducharme",https://dx.doi.org/10.1016/j.nicl.2018.08.028,https://doi.org/10.1016/j.nicl.2018.08.028,2021-08-03
2488.0,,pubmed,"HLBS-PopOmics: an online knowledge base to accelerate dissemination and implementation of research advances in population genomics to reduce the burden of heart, lung, blood, and sleep disorders","HLBS-PopOmics: an online knowledge base to accelerate dissemination and implementation of research advances in population genomics to reduce the burden of heart, lung, blood, and sleep disorders","Recent dramatic advances in multiomics research coupled with exponentially increasing volume, complexity, and interdisciplinary nature of publications are making it challenging for scientists to stay up-to-date on the literature. Strategies to address this challenge include the creation of online databases and warehouses to support timely and targeted dissemination of research findings. Although most of the early examples have been in cancer genomics and pharmacogenomics, the approaches used can be adapted to support investigators in heart, lung, blood, and sleep (HLBS) disorders research. In this article, we describe the creation of an HLBS population genomics (HLBS-PopOmics) knowledge base as an online, continuously updated, searchable database to support the dissemination and implementation of studies and resources that are relevant to clinical and public health practice. In addition to targeted searches based on the HLBS disease categories, cross-cutting themes reflecting the ethical, legal, and social implications of genomics research; systematic evidence reviews; and clinical practice guidelines supporting screening, detection, evaluation, and treatment are also emphasized in HLBS-PopOmics. Future updates of the knowledge base will include additional emphasis on transcriptomics, proteomics, metabolomics, and other omics research; explore opportunities for leveraging data sets designed to support scientific discovery; and incorporate advanced machine learning bioinformatics capabilities.","Recent dramatic advances in multiomics research coupled with exponentially increasing volume, complexity, and interdisciplinary nature of publications are making it challenging for scientists to stay up-to-date on the literature. Strategies to address this challenge include the creation of online databases and warehouses to support timely and targeted dissemination of research findings. Although most of the early examples have been in cancer genomics and pharmacogenomics, the approaches used can be adapted to support investigators in heart, lung, blood, and sleep (HLBS) disorders research. In this article, we describe the creation of an HLBS population genomics (HLBS-PopOmics) knowledge base as an online, continuously updated, searchable database to support the dissemination and implementation of studies and resources that are relevant to clinical and public health practice. In addition to targeted searches based on the HLBS disease categories, cross-cutting themes reflecting the ethical, legal, and social implications of genomics research; systematic evidence reviews; and clinical practice guidelines supporting screening, detection, evaluation, and treatment are also emphasized in HLBS-PopOmics. Future updates of the knowledge base will include additional emphasis on transcriptomics, proteomics, metabolomics, and other omics research; explore opportunities for leveraging data sets designed to support scientific discovery; and incorporate advanced machine learning bioinformatics capabilities.","Mensah, G. A.
 and Yu, W.
 and Barfield, W. L.
 and Clyne, M.
 and Engelgau, M. M.
 and Khoury, M. J.","Mensah, Yu, Barfield, Clyne, Engelgau, Khoury",https://dx.doi.org/10.1038/s41436-018-0118-1,https://doi.org/10.1038/s41436-018-0118-1,2021-08-03
4132.0,,pubmed,Risk scores to guide referral decisions for people with suspected ovarian cancer in secondary care: a systematic review and cost-effectiveness analysis,Risk scores to guide referral decisions for people with suspected ovarian cancer in secondary care: a systematic review and cost-effectiveness analysis,"BACKGROUND: Ovarian cancer is the sixth most common cancer in UK women and can be difficult to diagnose, particularly in the early stages. Risk-scoring can help to guide referral to specialist centres. OBJECTIVES: To assess the clinical and cost-effectiveness of risk scores to guide referral decisions for women with suspected ovarian cancer in secondary care. METHODS: Twenty-one databases, including MEDLINE and EMBASE, were searched from inception to November 2016. Review methods followed published guidelines. The meta-analysis using weighted averages and random-effects modelling was used to estimate summary sensitivity and specificity with 95% confidence intervals (CIs). The cost-effectiveness analysis considered the long-term costs and quality-adjusted life-years (QALYs) associated with different risk-scoring methods, and subsequent care pathways. Modelling comprised a decision tree and a Markov model. The decision tree was used to model short-term outcomes and the Markov model was used to estimate the long-term costs and QALYs associated with treatment and progression. RESULTS: Fifty-one diagnostic cohort studies were included in the systematic review. The Risk of Ovarian Malignancy Algorithm (ROMA) score did not offer any advantage over the Risk of Malignancy Index 1 (RMI 1). Patients with borderline tumours or non-ovarian primaries appeared to account for disproportionately high numbers of false-negative, low-risk ROMA scores. (Confidential information has been removed.) To achieve similar levels of sensitivity to the Assessment of Different NEoplasias in the adneXa (ADNEX) model and the International Ovarian Tumour Analysis (IOTA) group's simple ultrasound rules, a very low RMI 1 decision threshold (25) would be needed; the summary sensitivity and specificity estimates for the RMI 1 at this threshold were 94.9% (95% CI 91.5% to 97.2%) and 51.1% (95% CI 47.0% to 55.2%), respectively. In the base-case analysis, RMI 1 (threshold of 250) was the least effective [16.926 life-years (LYs), 13.820 QALYs] and the second cheapest (5669). The IOTA group's simple ultrasound rules (inconclusive, assumed to be malignant) were the cheapest (5667) and the second most effective [16.954 LYs, 13.841 QALYs], dominating RMI 1. The ADNEX model (threshold of 10%), costing 5699, was the most effective (16.957 LYs, 13.843 QALYs), and compared with the IOTA group's simple ultrasound rules, resulted in an incremental cost-effectiveness ratio of 15,304 per QALY gained. At thresholds of up to 15,304 per QALY gained, the IOTA group's simple ultrasound rules are cost-effective; the ADNEX model (threshold of 10%) is cost-effective for higher thresholds. LIMITATIONS: Information on the downstream clinical consequences of risk-scoring was limited. CONCLUSIONS: Both the ADNEX model and the IOTA group's simple ultrasound rules may offer increased sensitivity relative to current practice (RMI 1); that is, more women with malignant tumours would be referred to a specialist multidisciplinary team, although more women with benign tumours would also be referred. The cost-effectiveness model supports prioritisation of sensitivity over specificity. Further research is needed on the clinical consequences of risk-scoring. STUDY REGISTRATION: This study is registered as PROSPERO CRD42016053326. FUNDING DETAILS: The National Institute for Health Research Health Technology Assessment programme.","Ovarian cancer is the sixth most common cancer in UK women and can be difficult to diagnose, particularly in the early stages. Risk-scoring can help to guide referral to specialist centres. To assess the clinical and cost-effectiveness of risk scores to guide referral decisions for women with suspected ovarian cancer in secondary care. Twenty-one databases, including MEDLINE and EMBASE, were searched from inception to November 2016. Review methods followed published guidelines. The meta-analysis using weighted averages and random-effects modelling was used to estimate summary sensitivity and specificity with 95% confidence intervals (CIs). The cost-effectiveness analysis considered the long-term costs and quality-adjusted life-years (QALYs) associated with different risk-scoring methods, and subsequent care pathways. Modelling comprised a decision tree and a Markov model. The decision tree was used to model short-term outcomes and the Markov model was used to estimate the long-term costs and QALYs associated with treatment and progression. Fifty-one diagnostic cohort studies were included in the systematic review. The Risk of Ovarian Malignancy Algorithm (ROMA) score did not offer any advantage over the Risk of Malignancy Index 1 (RMI 1). Patients with borderline tumours or non-ovarian primaries appeared to account for disproportionately high numbers of false-negative, low-risk ROMA scores. (Confidential information has been removed.) To achieve similar levels of sensitivity to the Assessment of Different NEoplasias in the adneXa (ADNEX) model and the International Ovarian Tumour Analysis (IOTA) group's simple ultrasound rules, a very low RMI 1 decision threshold (25) would be needed; the summary sensitivity and specificity estimates for the RMI 1 at this threshold were 94.9% (95% CI 91.5% to 97.2%) and 51.1% (95% CI 47.0% to 55.2%), respectively. In the base-case analysis, RMI 1 (threshold of 250) was the least effective [16.926 life-years (LYs), 13.820 QALYs] and the second cheapest (Â£5669). The IOTA group's simple ultrasound rules (inconclusive, assumed to be malignant) were the cheapest (Â£5667) and the second most effective [16.954 LYs, 13.841 QALYs], dominating RMI 1. The ADNEX model (threshold of 10%), costing Â£5699, was the most effective (16.957 LYs, 13.843 QALYs), and compared with the IOTA group's simple ultrasound rules, resulted in an incremental cost-effectiveness ratio of Â£15,304 per QALY gained. At thresholds of up to Â£15,304 per QALY gained, the IOTA group's simple ultrasound rules are cost-effective; the ADNEX model (threshold of 10%) is cost-effective for higher thresholds. Information on the downstream clinical consequences of risk-scoring was limited. Both the ADNEX model and the IOTA group's simple ultrasound rules may offer increased sensitivity relative to current practice (RMI 1); that is, more women with malignant tumours would be referred to a specialist multidisciplinary team, although more women with benign tumours would also be referred. The cost-effectiveness model supports prioritisation of sensitivity over specificity. Further research is needed on the clinical consequences of risk-scoring. This study is registered as PROSPERO CRD42016053326. The National Institute for Health Research Health Technology Assessment programme.","Westwood, M.
 and Ramaekers, B.
 and Lang, S.
 and Grimm, S.
 and Deshpande, S.
 and de Kock, S.
 and Armstrong, N.
 and Joore, M.
 and Kleijnen, J.","Westwood, Ramaekers, Lang, Grimm, Deshpande, de Kock, Armstrong, Joore, Kleijnen",https://dx.doi.org/10.3310/hta22440,https://doi.org/10.3310/hta22440,2021-08-03
4006.0,,pubmed,A systematic review of frameworks for the interrelationships of mental health evidence and policy in low- and middle-income countries,A systematic review of frameworks for the interrelationships of mental health evidence and policy in low- and middle-income countries,"BACKGROUND: The interrelationships between research evidence and policy-making are complex. Different theoretical frameworks exist to explain general evidence-policy interactions. One largely unexplored element of these interrelationships is how evidence interrelates with, and influences, policy/political agenda-setting. This review aims to identify the elements and processes of theories, frameworks and models on interrelationships of research evidence and health policy-making, with a focus on actionability and agenda-setting in the context of mental health in low- and middle-income countries (LMICs). METHODS: A systematic review of theories was conducted based on the BeHeMOTh search method, using a tested and refined search strategy. Nine electronic databases and other relevant sources were searched for peer-reviewed and grey literature. Two reviewers screened the abstracts, reviewed full-text articles, extracted data and performed quality assessments. Analysis was based on a thematic analysis. The included papers had to present an actionable theoretical framework/model on evidence and policy interrelationships, such as knowledge translation or evidence-based policy, specifically target the agenda-setting process, focus on mental health, be from LMICs and published in English. RESULTS: From 236 publications included in the full text analysis, no studies fully complied with our inclusion criteria. Widening the focus by leaving out 'agenda-setting', we included ten studies, four of which had unique conceptual frameworks focusing on mental health and LMICs but not agenda-setting. The four analysed frameworks confirmed research gaps from LMICs and mental health, and a lack of focus on agenda-setting. Frameworks and models from other health and policy areas provide interesting conceptual approaches and lessons with regards to agenda-setting. CONCLUSION: Our systematic review identified frameworks on evidence and policy interrelations that differ in their elements and processes. No framework fulfilled all inclusion criteria. Four actionable frameworks are applicable to mental health and LMICs, but none specifically target agenda-setting. We have identified agenda-setting as a research theory gap in the context of mental health knowledge translation in LMICs. Frameworks from other health/policy areas could offer lessons on agenda-setting and new approaches for creating policy impact for mental health and to tackle the translational gap in LMICs.","The interrelationships between research evidence and policy-making are complex. Different theoretical frameworks exist to explain general evidence-policy interactions. One largely unexplored element of these interrelationships is how evidence interrelates with, and influences, policy/political agenda-setting. This review aims to identify the elements and processes of theories, frameworks and models on interrelationships of research evidence and health policy-making, with a focus on actionability and agenda-setting in the context of mental health in low- and middle-income countries (LMICs). A systematic review of theories was conducted based on the BeHeMOTh search method, using a tested and refined search strategy. Nine electronic databases and other relevant sources were searched for peer-reviewed and grey literature. Two reviewers screened the abstracts, reviewed full-text articles, extracted data and performed quality assessments. Analysis was based on a thematic analysis. The included papers had to present an actionable theoretical framework/model on evidence and policy interrelationships, such as knowledge translation or evidence-based policy, specifically target the agenda-setting process, focus on mental health, be from LMICs and published in English. From 236 publications included in the full text analysis, no studies fully complied with our inclusion criteria. Widening the focus by leaving out 'agenda-setting', we included ten studies, four of which had unique conceptual frameworks focusing on mental health and LMICs but not agenda-setting. The four analysed frameworks confirmed research gaps from LMICs and mental health, and a lack of focus on agenda-setting. Frameworks and models from other health and policy areas provide interesting conceptual approaches and lessons with regards to agenda-setting. Our systematic review identified frameworks on evidence and policy interrelations that differ in their elements and processes. No framework fulfilled all inclusion criteria. Four actionable frameworks are applicable to mental health and LMICs, but none specifically target agenda-setting. We have identified agenda-setting as a research theory gap in the context of mental health knowledge translation in LMICs. Frameworks from other health/policy areas could offer lessons on agenda-setting and new approaches for creating policy impact for mental health and to tackle the translational gap in LMICs.","Votruba, N.
 and Ziemann, A.
 and Grant, J.
 and Thornicroft, G.","Votruba, Ziemann, Grant, Thornicroft",not available,https://doi.org/10.1186/s12961-018-0357-2,2021-08-03
1648.0,,pubmed,CBCT vs other imaging modalities to assess peri-implant bone and diagnose complications: a systematic review,CBCT vs other imaging modalities to assess peri-implant bone and diagnose complications: a systematic review,"AIM: The objective of this systematic review was to evaluate the diagnostic value of CBCT compared with 2D imaging and clinical gold standard techniques in peri-implant bone defect detection and measurement. MATERIALS AND METHODS: Literature search was performed using MEDLINE, Embase and Web of Science databases up to July 2017. Clinical, ex vivo, in vitro and animal studies that assessed and measured peri-implant bone defects using different imaging modalities were included in this review. Two reviewers performed data extraction and qualitative analysis. The methodological quality of each study was reviewed using the QUADAS-2 tool. RESULTS: The initial search revealed 2849 unique papers. Full-text analysis was performed on 60 articles. For the present review, nine studies were considered eligible to be included for qualitative analysis. CBCT performed similar to intraoral radiography in mesiodistal defect detection and measurements. Additional buccolingual visualisation and volumetric and morphological assessment of peri-implant bone defects are major advantages of 3D visualisation with CBCT. Nevertheless, one must be aware of metal artefacts masking osseointegration, shallow bony defects and other peri-implant radiolucencies, thus impeding early diagnosis of intrabony lesions. CONCLUSIONS: The present review did not provide evidence to support the use of CBCT as standard postoperative procedure to evaluate peri-implant bone. Up to date, we are clinically forced to remain with intraoral radiography, notwithstanding the inherent limitations related to restricted field of view and two-dimensional overlap. A 3D imaging approach for postoperative implant evaluation is crucial, making further development of an optimised and artefact-free CBCT protocol indispensable.","The objective of this systematic review was to evaluate the diagnostic value of CBCT compared with 2D imaging and clinical gold standard techniques in peri-implant bone defect detection and measurement. Literature search was performed using MEDLINE, Embase and Web of Science databases up to July 2017. Clinical, ex vivo, in vitro and animal studies that assessed and measured peri-implant bone defects using different imaging modalities were included in this review. Two reviewers performed data extraction and qualitative analysis. The methodological quality of each study was reviewed using the QUADAS-2 tool. The initial search revealed 2849 unique papers. Full-text analysis was performed on 60 articles. For the present review, nine studies were considered eligible to be included for qualitative analysis. CBCT performed similar to intraoral radiography in mesiodistal defect detection and measurements. Additional buccolingual visualisation and volumetric and morphological assessment of peri-implant bone defects are major advantages of 3D visualisation with CBCT. Nevertheless, one must be aware of metal artefacts masking osseointegration, shallow bony defects and other peri-implant radiolucencies, thus impeding early diagnosis of intrabony lesions. The present review did not provide evidence to support the use of CBCT as standard postoperative procedure to evaluate peri-implant bone. Up to date, we are clinically forced to remain with intraoral radiography, notwithstanding the inherent limitations related to restricted field of view and two-dimensional overlap. A 3D imaging approach for postoperative implant evaluation is crucial, making further development of an optimised and artefact-free CBCT protocol indispensable.","Jacobs, R.
 and Vranckx, M.
 and Vanderstuyft, T.
 and Quirynen, M.
 and Salmon, B.","Jacobs, Vranckx, Vanderstuyft, Quirynen, Salmon",not available,https://www.google.com/search?q=CBCT+vs+other+imaging+modalities+to+assess+peri-implant+bone+and+diagnose+complications:+a+systematic+review.,2021-08-03
2036.0,,pubmed,Discriminating between empirical studies and nonempirical works using automated text classification,Discriminating between empirical studies and nonempirical works using automated text classification,"OBJECTIVE: Identify the most performant automated text classification method (eg, algorithm) for differentiating empirical studies from nonempirical works in order to facilitate systematic mixed studies reviews. METHODS: The algorithms were trained and validated with 8050 database records, which had previously been manually categorized as empirical or nonempirical. A Boolean mixed filter developed for filtering MEDLINE records (title, abstract, keywords, and full texts) was used as a baseline. The set of features (eg, characteristics from the data) included observable terms and concepts extracted from a metathesaurus. The efficiency of the approaches was measured using sensitivity, precision, specificity, and accuracy. RESULTS: The decision trees algorithm demonstrated the highest performance, surpassing the accuracy of the Boolean mixed filter by 30%. The use of full texts did not result in significant gains compared with title, abstract, keywords, and records. Results also showed that mixing concepts with observable terms can improve the classification. SIGNIFICANCE: Screening of records, identified in bibliographic databases, for relevant studies to include in systematic reviews can be accelerated with automated text classification.","Identify the most performant automated text classification method (eg, algorithm) for differentiating empirical studies from nonempirical works in order to facilitate systematic mixed studies reviews. The algorithms were trained and validated with 8050 database records, which had previously been manually categorized as empirical or nonempirical. A Boolean mixed filter developed for filtering MEDLINE records (title, abstract, keywords, and full texts) was used as a baseline. The set of features (eg, characteristics from the data) included observable terms and concepts extracted from a metathesaurus. The efficiency of the approaches was measured using sensitivity, precision, specificity, and accuracy. The decision trees algorithm demonstrated the highest performance, surpassing the accuracy of the Boolean mixed filter by 30%. The use of full texts did not result in significant gains compared with title, abstract, keywords, and records. Results also showed that mixing concepts with observable terms can improve the classification. Screening of records, identified in bibliographic databases, for relevant studies to include in systematic reviews can be accelerated with automated text classification.","Langlois, A.
 and Nie, J. Y.
 and Thomas, J.
 and Hong, Q. N.
 and Pluye, P.","Langlois, Nie, Thomas, Hong, Pluye",not available,https://doi.org/10.1002/jrsm.1317,2021-08-03
4202.0,,pubmed,DrugMetab: An Integrated Machine Learning and Lexicon Mapping Named Entity Recognition Method for Drug Metabolite,DrugMetab: An Integrated Machine Learning and Lexicon Mapping Named Entity Recognition Method for Drug Metabolite,"Drug metabolites (DMs) are critical in pharmacology research areas, such as drug metabolism pathways and drug-drug interactions. However, there is no terminology dictionary containing comprehensive drug metabolite names, and there is no named entity recognition (NER) algorithm focusing on drug metabolite identification. In this article, we developed a novel NER system, DrugMetab, to identify DMs from the PubMed abstracts. DrugMetab utilizes the features characterized from the Part-of-Speech, drug index, and pre/suffix, and determines DMs within context. To evaluate the performance, a gold-standard corpus was manually constructed. In this task, DrugMetab with sequential minimal optimization (SMO) classifier achieves 0.89 precision, 0.77 recall, and 0.83 F-measure in the internal testing set; and 0.86 precision, 0.85 recall, and 0.86 F-measure in the external validation set. We further compared the performance between DrugMetab and whatizitChemical, which was designed for identifying small molecules or chemical entities. DrugMetab outperformed whatizitChemical, which had a lower recall rate of 0.65.","Drug metabolites (DMs) are critical in pharmacology research areas, such as drug metabolism pathways and drug-drug interactions. However, there is no terminology dictionary containing comprehensive drug metabolite names, and there is no named entity recognition (NER) algorithm focusing on drug metabolite identification. In this article, we developed a novel NER system, DrugMetab, to identify DMs from the PubMed abstracts. DrugMetab utilizes the features characterized from the Part-of-Speech, drug index, and pre/suffix, and determines DMs within context. To evaluate the performance, a gold-standard corpus was manually constructed. In this task, DrugMetab with sequential minimal optimization (SMO) classifier achieves 0.89 precision, 0.77 recall, and 0.83Â F-measure in the internal testing set; and 0.86 precision, 0.85 recall, and 0.86Â F-measure in the external validation set. We further compared the performance between DrugMetab and whatizitChemical, which was designed for identifying small molecules or chemical entities. DrugMetab outperformed whatizitChemical, which had a lower recall rate of 0.65.","Wu, H. Y.
 and Lu, D.
 and Hyder, M.
 and Zhang, S.
 and Quinney, S. K.
 and Desta, Z.
 and Li, L.","Wu, Lu, Hyder, Zhang, Quinney, Desta, Li",https://dx.doi.org/10.1002/psp4.12340,https://doi.org/10.1002/psp4.12340,2021-08-03
2040.0,,pubmed,Conversational agents in healthcare: a systematic review,Conversational agents in healthcare: a systematic review,"Objective: Our objective was to review the characteristics, current applications, and evaluation measures of conversational agents with unconstrained natural language input capabilities used for health-related purposes. Methods: We searched PubMed, Embase, CINAHL, PsycInfo, and ACM Digital using a predefined search strategy. Studies were included if they focused on consumers or healthcare professionals; involved a conversational agent using any unconstrained natural language input; and reported evaluation measures resulting from user interaction with the system. Studies were screened by independent reviewers and Cohen's kappa measured inter-coder agreement. Results: The database search retrieved 1513 citations; 17 articles (14 different conversational agents) met the inclusion criteria. Dialogue management strategies were mostly finite-state and frame-based (6 and 7 conversational agents, respectively); agent-based strategies were present in one type of system. Two studies were randomized controlled trials (RCTs), 1 was cross-sectional, and the remaining were quasi-experimental. Half of the conversational agents supported consumers with health tasks such as self-care. The only RCT evaluating the efficacy of a conversational agent found a significant effect in reducing depression symptoms (effect size d = 0.44, p = .04). Patient safety was rarely evaluated in the included studies. Conclusions: The use of conversational agents with unconstrained natural language input capabilities for health-related purposes is an emerging field of research, where the few published studies were mainly quasi-experimental, and rarely evaluated efficacy or safety. Future studies would benefit from more robust experimental designs and standardized reporting. Protocol Registration: The protocol for this systematic review is registered at PROSPERO with the number CRD42017065917.","Our objective was to review the characteristics, current applications, and evaluation measures of conversational agents with unconstrained natural language input capabilities used for health-related purposes. We searched PubMed, Embase, CINAHL, PsycInfo, and ACM Digital using a predefined search strategy. Studies were included if they focused on consumers or healthcare professionals; involved a conversational agent using any unconstrained natural language input; and reported evaluation measures resulting from user interaction with the system. Studies were screened by independent reviewers and Cohen's kappa measured inter-coder agreement. The database search retrieved 1513 citations; 17 articles (14 different conversational agents) met the inclusion criteria. Dialogue management strategies were mostly finite-state and frame-based (6 and 7 conversational agents, respectively); agent-based strategies were present in one type of system. Two studies were randomized controlled trials (RCTs), 1 was cross-sectional, and the remaining were quasi-experimental. Half of the conversational agents supported consumers with health tasks such as self-care. The only RCT evaluating the efficacy of a conversational agent found a significant effect in reducing depression symptoms (effect size dâ€‰=â€‰0.44, pâ€‰=â€‰.04). Patient safety was rarely evaluated in the included studies. The use of conversational agents with unconstrained natural language input capabilities for health-related purposes is an emerging field of research, where the few published studies were mainly quasi-experimental, and rarely evaluated efficacy or safety. Future studies would benefit from more robust experimental designs and standardized reporting. The protocol for this systematic review is registered at PROSPERO with the number CRD42017065917.","Laranjo, L.
 and Dunn, A. G.
 and Tong, H. L.
 and Kocaballi, A. B.
 and Chen, J.
 and Bashir, R.
 and Surian, D.
 and Gallego, B.
 and Magrabi, F.
 and Lau, A. Y. S.
 and Coiera, E.","Laranjo, Dunn, Tong, Kocaballi, Chen, Bashir, Surian, Gallego, Magrabi, Lau, Coiera",https://dx.doi.org/10.1093/jamia/ocy072,https://doi.org/10.1093/jamia/ocy072,2021-08-03
2018.0,,pubmed,Cost-effectiveness of three different strategies for the treatment of first recurrent Clostridium difficile infection diagnosed in a community setting,Cost-effectiveness of three different strategies for the treatment of first recurrent Clostridium difficile infection diagnosed in a community setting,"OBJECTIVE: A significant portion of patients with Clostridium difficile infections (CDI) experience recurrence, and there is little consensus on its treatment. With the availability of newer agents for CDI and the added burdens of recurrent disease, a cost-effectiveness analysis may provide insight on the most efficient use of resources. DESIGN: A decision-tree analysis was created to compare the cost-effectiveness of 3 possible treatments for patients with first CDI recurrence: oral vancomycin, fidaxomicin, or bezlotoxumab plus vancomycin. The model was performed from a payer's perspective with direct cost inputs and a timeline of 1 year. A systematic review of literature was performed to identify clinical, utility, and cost data. Quality-adjusted life years (QALY) and incremental cost-effectiveness ratios were calculated. The willingness-to-pay (WTP) threshold was set at $100,000 per QALY gained. The robustness of the model was tested using one-way sensitivity analyses and probabilistic sensitivity analysis. RESULTS: Vancomycin had the lowest cost ($15,692) and was associated with a QALY gain of 0.8019 years. Bezlotoxumab plus vancomycin was a dominated strategy. Fidaxomicin led to a higher QALY compared to vancomycin, at an incremental cost of $500,975 per QALY gained. Based on our WTP threshold, vancomycin alone was the most cost-effective regimen for treating the first recurrence of CDI. Sensitivity analyses demonstrated the model's robustness. CONCLUSIONS: Vancomycin alone appears to be the most cost-effective regimen for the treatment of first recurrence of CDI. Fidaxomicin alone led to the highest QALY gained, but at a cost beyond what is considered cost-effective.","A significant portion of patients with Clostridium difficile infections (CDI) experience recurrence, and there is little consensus on its treatment. With the availability of newer agents for CDI and the added burdens of recurrent disease, a cost-effectiveness analysis may provide insight on the most efficient use of resources. A decision-tree analysis was created to compare the cost-effectiveness of 3 possible treatments for patients with first CDI recurrence: oral vancomycin, fidaxomicin, or bezlotoxumab plus vancomycin. The model was performed from a payer's perspective with direct cost inputs and a timeline of 1 year. A systematic review of literature was performed to identify clinical, utility, and cost data. Quality-adjusted life years (QALY) and incremental cost-effectiveness ratios were calculated. The willingness-to-pay (WTP) threshold was set at $100,000 per QALY gained. The robustness of the model was tested using one-way sensitivity analyses and probabilistic sensitivity analysis. Vancomycin had the lowest cost ($15,692) and was associated with a QALY gain of 0.8019 years. Bezlotoxumab plus vancomycin was a dominated strategy. Fidaxomicin led to a higher QALY compared to vancomycin, at an incremental cost of $500,975 per QALY gained. Based on our WTP threshold, vancomycin alone was the most cost-effective regimen for treating the first recurrence of CDI. Sensitivity analyses demonstrated the model's robustness. Vancomycin alone appears to be the most cost-effective regimen for the treatment of first recurrence of CDI. Fidaxomicin alone led to the highest QALY gained, but at a cost beyond what is considered cost-effective.","Lam, S. W.
 and Neuner, E. A.
 and Fraser, T. G.
 and Delgado, D.
 and Chalfin, D. B.","Lam, Neuner, Fraser, Delgado, Chalfin",https://dx.doi.org/10.1017/ice.2018.139,https://doi.org/10.1017/ice.2018.139,2021-08-03
3053.0,,pubmed,Prioritising references for systematic reviews with RobotAnalyst: A user study,Prioritising references for systematic reviews with RobotAnalyst: A user study,"Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.","Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43Â 610 abstract-level decisions. The number of references that needed to be screened to identify 95% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.","Przybyla, P.
 and Brockmeier, A. J.
 and Kontonatsios, G.
 and Le Pogam, M. A.
 and McNaught, J.
 and von Elm, E.
 and Nolan, K.
 and Ananiadou, S.","PrzybyÅ‚a, Brockmeier, Kontonatsios, Le Pogam, McNaught, von Elm, Nolan, Ananiadou",not available,https://doi.org/10.1002/jrsm.1311,2021-08-03
4093.0,,pubmed,Automated monitoring compared to standard care for the early detection of sepsis in critically ill patients,Automated monitoring compared to standard care for the early detection of sepsis in critically ill patients,"BACKGROUND: Sepsis is a life-threatening condition that is usually diagnosed when a patient has a suspected or documented infection, and meets two or more criteria for systemic inflammatory response syndrome (SIRS). The incidence of sepsis is higher among people admitted to critical care settings such as the intensive care unit (ICU) than among people in other settings. If left untreated sepsis can quickly worsen; severe sepsis has a mortality rate of 40% or higher, depending on definition. Recognition of sepsis can be challenging as it usually requires patient data to be combined from multiple unconnected sources, and interpreted correctly, which can be complex and time consuming to do. Electronic systems that are designed to connect information sources together, and automatically collate, analyse, and continuously monitor the information, as well as alerting healthcare staff when pre-determined diagnostic thresholds are met, may offer benefits by facilitating earlier recognition of sepsis and faster initiation of treatment, such as antimicrobial therapy, fluid resuscitation, inotropes, and vasopressors if appropriate. However, there is the possibility that electronic, automated systems do not offer benefits, or even cause harm. This might happen if the systems are unable to correctly detect sepsis (meaning that treatment is not started when it should be, or it is started when it shouldn't be), or healthcare staff may not respond to alerts quickly enough, or get 'alarm fatigue' especially if the alarms go off frequently or give too many false alarms. OBJECTIVES: To evaluate whether automated systems for the early detection of sepsis can reduce the time to appropriate treatment (such as initiation of antibiotics, fluids, inotropes, and vasopressors) and improve clinical outcomes in critically ill patients in the ICU. SEARCH METHODS: We searched CENTRAL; MEDLINE; Embase; CINAHL; ISI Web of science; and LILACS, clinicaltrials.gov, and the World Health Organization trials portal. We searched all databases from their date of inception to 18 September 2017, with no restriction on country or language of publication. SELECTION CRITERIA: We included randomized controlled trials (RCTs) that compared automated sepsis-monitoring systems to standard care (such as paper-based systems) in participants of any age admitted to intensive or critical care units for critical illness. We defined an automated system as any process capable of screening patient records or data (one or more systems) automatically at intervals for markers or characteristics that are indicative of sepsis. We defined critical illness as including, but not limited to postsurgery, trauma, stroke, myocardial infarction, arrhythmia, burns, and hypovolaemic or haemorrhagic shock. We excluded non-randomized studies, quasi-randomized studies, and cross-over studies . We also excluded studies including people already diagnosed with sepsis. DATA COLLECTION AND ANALYSIS: We used the standard methodological procedures expected by Cochrane. Our primary outcomes were: time to initiation of antimicrobial therapy; time to initiation of fluid resuscitation; and 30-day mortality. Secondary outcomes included: length of stay in ICU; failed detection of sepsis; and quality of life. We used GRADE to assess the quality of evidence for each outcome. MAIN RESULTS: We included three RCTs in this review. It was unclear if the RCTs were three separate studies involving 1199 participants in total, or if they were reports from the same study involving fewer participants. We decided to treat the studies separately, as we were unable to make contact with the study authors to clarify.All three RCTs are of very low study quality because of issues with unclear randomization methods, allocation concealment and uncertainty of effect size. Some of the studies were reported as abstracts only and contained limited data, which prevented meaningful analysis and assessment of potential biases. The studies included participants who all received automated electronic monitoring during their hospital stay. Participants were randomized to an intervention group (automated alerts sent from the system) or to usual care (no automated alerts sent from the system). Evidence from all three studies reported 'Time to initiation of antimicrobial therapy'. We were unable to pool the data, but the largest study involving 680 participants reported median time to initiation of antimicrobial therapy in the intervention group of 5.6 hours (interquartile range (IQR) 2.3 to 19.7) in the intervention group (n = not stated) and 7.8 hours (IQR 2.5 to 33.1) in the control group (n = not stated). No studies reported 'Time to initiation of fluid resuscitation' or the adverse event 'Mortality at 30 days'. However very low-quality evidence was available where mortality was reported at other time points. One study involving 77 participants reported 14-day mortality of 20% in the intervention group and 21% in the control group (numerator and denominator not stated). One study involving 442 participants reported mortality at 28 days, or discharge was 14% in the intervention group and 10% in the control group (numerator and denominator not reported). Sample sizes were not reported adequately for these outcomes and so we could not estimate confidence intervals.Very low-quality evidence from one study involving 442 participants reported 'Length of stay in ICU'. Median length of stay was 3.0 days in the intervention group (IQR = 2.0 to 5.0), and 3.0 days (IQR 2.0 to 4.0 in the control). Very low-quality evidence from one study involving at least 442 participants reported the adverse effect 'Failed detection of sepsis'. Data were only reported for failed detection of sepsis in two participants and it wasn't clear which group(s) this outcome occurred in.No studies reported 'Quality of life'. AUTHORS' CONCLUSIONS: It is unclear what effect automated systems for monitoring sepsis have on any of the outcomes included in this review. Very low-quality evidence is only available on automated alerts, which is only one component of automated monitoring systems. It is uncertain whether such systems can replace regular, careful review of the patient's condition by experienced healthcare staff.","Sepsis is a life-threatening condition that is usually diagnosed when a patient has a suspected or documented infection, and meets two or more criteria for systemic inflammatory response syndrome (SIRS). The incidence of sepsis is higher among people admitted to critical care settings such as the intensive care unit (ICU) than among people in other settings. If left untreated sepsis can quickly worsen; severe sepsis has a mortality rate of 40% or higher, depending on definition. Recognition of sepsis can be challenging as it usually requires patient data to be combined from multiple unconnected sources, and interpreted correctly, which can be complex and time consuming to do. Electronic systems that are designed to connect information sources together, and automatically collate, analyse, and continuously monitor the information, as well as alerting healthcare staff when pre-determined diagnostic thresholds are met, may offer benefits by facilitating earlier recognition of sepsis and faster initiation of treatment, such as antimicrobial therapy, fluid resuscitation, inotropes, and vasopressors if appropriate. However, there is the possibility that electronic, automated systems do not offer benefits, or even cause harm. This might happen if the systems are unable to correctly detect sepsis (meaning that treatment is not started when it should be, or it is started when it shouldn't be), or healthcare staff may not respond to alerts quickly enough, or get 'alarm fatigue' especially if the alarms go off frequently or give too many false alarms. To evaluate whether automated systems for the early detection of sepsis can reduce the time to appropriate treatment (such as initiation of antibiotics, fluids, inotropes, and vasopressors) and improve clinical outcomes in critically ill patients in the ICU. We searched CENTRAL; MEDLINE; Embase; CINAHL; ISI Web of science; and LILACS, clinicaltrials.gov, and the World Health Organization trials portal. We searched all databases from their date of inception to 18 September 2017, with no restriction on country or language of publication. We included randomized controlled trials (RCTs) that compared automated sepsis-monitoring systems to standard care (such as paper-based systems) in participants of any age admitted to intensive or critical care units for critical illness. We defined an automated system as any process capable of screening patient records or data (one or more systems) automatically at intervals for markers or characteristics that are indicative of sepsis. We defined critical illness as including, but not limited to postsurgery, trauma, stroke, myocardial infarction, arrhythmia, burns, and hypovolaemic or haemorrhagic shock. We excluded non-randomized studies, quasi-randomized studies, and cross-over studies . We also excluded studies including people already diagnosed with sepsis. We used the standard methodological procedures expected by Cochrane. Our primary outcomes were: time to initiation of antimicrobial therapy; time to initiation of fluid resuscitation; and 30-day mortality. Secondary outcomes included: length of stay in ICU; failed detection of sepsis; and quality of life. We used GRADE to assess the quality of evidence for each outcome. We included three RCTs in this review. It was unclear if the RCTs were three separate studies involving 1199 participants in total, or if they were reports from the same study involving fewer participants. We decided to treat the studies separately, as we were unable to make contact with the study authors to clarify.All three RCTs are of very low study quality because of issues with unclear randomization methods, allocation concealment and uncertainty of effect size. Some of the studies were reported as abstracts only and contained limited data, which prevented meaningful analysis and assessment of potential biases.The studies included participants who all received automated electronic monitoring during their hospital stay. Participants were randomized to an intervention group (automated alerts sent from the system) or to usual care (no automated alerts sent from the system).Evidence from all three studies reported 'Time to initiation of antimicrobial therapy'. We were unable to pool the data, but the largest study involving 680 participants reported median time to initiation of antimicrobial therapy in the intervention group of 5.6 hours (interquartile range (IQR) 2.3 to 19.7) in the intervention group (n = not stated) and 7.8 hours (IQR 2.5 to 33.1) in the control group (n = not stated).No studies reported 'Time to initiation of fluid resuscitation' or the adverse event 'Mortality at 30 days'. However very low-quality evidence was available where mortality was reported at other time points. One study involving 77 participants reported 14-day mortality of 20% in the intervention group and 21% in the control group (numerator and denominator not stated). One study involving 442 participants reported mortality at 28 days, or discharge was 14% in the intervention group and 10% in the control group (numerator and denominator not reported). Sample sizes were not reported adequately for these outcomes and so we could not estimate confidence intervals.Very low-quality evidence from one study involving 442 participants reported 'Length of stay in ICU'. Median length of stay was 3.0 days in the intervention group (IQR = 2.0 to 5.0), and 3.0 days (IQR 2.0 to 4.0 in the control).Very low-quality evidence from one study involving at least 442 participants reported the adverse effect 'Failed detection of sepsis'. Data were only reported for failed detection of sepsis in two participants and it wasn't clear which group(s) this outcome occurred in.No studies reported 'Quality of life'. It is unclear what effect automated systems for monitoring sepsis have on any of the outcomes included in this review. Very low-quality evidence is only available on automated alerts, which is only one component of automated monitoring systems. It is uncertain whether such systems can replace regular, careful review of the patient's condition by experienced healthcare staff.","Warttig, S.
 and Alderson, P.
 and Evans, D. J.
 and Lewis, S. R.
 and Kourbeti, I. S.
 and Smith, A. F.","Warttig, Alderson, Evans, Lewis, Kourbeti, Smith",https://dx.doi.org/10.1002/14651858.CD012404.pub2,https://doi.org/10.1002/14651858.CD012404.pub2,2021-08-03
4071.0,,pubmed,Effect of acupuncture on in vitro fertilization: An updated systematic review and data mining protocol,Effect of acupuncture on in vitro fertilization: An updated systematic review and data mining protocol,"BACKGROUND: Although many patients try to seek acupuncture to improve in vitro fertilization (IVF) outcomes, evidence regarding its efficacy and acupoints characters are lacking. The aim of this protocol is to evaluate the effectiveness and safety, as well as the acupoints characteristics of acupuncture in the treatment of female undergoing IVF, by conducting a systematic review and data mining. METHODS: The following 6 databases will be searched from their inception to April 30, 2018: PubMed, Chinese National Knowledge Infrastructure, Wanfang, VIP database, Embase, and Cochrane Library. The randomized controlled trials (RCTs) or case-control studies of acupuncture that assess clinical effects and side effects in female undergoing IVF are included. The primary outcome measures will be number of oocytes retrieved, fertilization rate, oocyte cleavage rate, high-quality embryos rate, ovarian hyperstimulation syndrome (OHHS) incidence rate, clinical pregnancy rate (CPR), biochemical pregnancy rate (BPR), implantation rate, and cycle cancellation rate. Two reviewers will independently undertake data extraction and quality assessments. Data will be synthesized by RevMan V.5.3 software. Acupoints characteristics will be excavated using Traditional Chinese Medicine inheritance support system (TCMISS). Reporting bias will be assessed by Funnel plots, Begg test, and Egger test. RESULTS: This review will assess the clinical efficacy and safety, as well as the acupoints characteristics of acupuncture on IVF. CONCLUSION: These findings will summarize the current evidence of acupuncture on IVF outcomes and may provide guidance for clinicians and infertile women to select acupuncture for IVF.","Although many patients try to seek acupuncture to improve in vitro fertilization (IVF) outcomes, evidence regarding its efficacy and acupoints characters are lacking. The aim of this protocol is to evaluate the effectiveness and safety, as well as the acupoints characteristics of acupuncture in the treatment of female undergoing IVF, by conducting a systematic review and data mining. The following 6 databases will be searched from their inception to April 30, 2018: PubMed, Chinese National Knowledge Infrastructure, Wanfang, VIP database, Embase, and Cochrane Library. The randomized controlled trials (RCTs) or case-control studies of acupuncture that assess clinical effects and side effects in female undergoing IVF are included. The primary outcome measures will be number of oocytes retrieved, fertilization rate, oocyte cleavage rate, high-quality embryos rate, ovarian hyperstimulation syndrome (OHHS) incidence rate, clinical pregnancy rate (CPR), biochemical pregnancy rate (BPR), implantation rate, and cycle cancellation rate. Two reviewers will independently undertake data extraction and quality assessments. Data will be synthesized by RevMan V.5.3 software. Acupoints characteristics will be excavated using Traditional Chinese Medicine inheritance support system (TCMISS). Reporting bias will be assessed by Funnel plots, Begg test, and Egger test. This review will assess the clinical efficacy and safety, as well as the acupoints characteristics of acupuncture on IVF. These findings will summarize the current evidence of acupuncture on IVF outcomes and may provide guidance for clinicians and infertile women to select acupuncture for IVF.","Wang, X.
 and Lin, H.
 and Chen, M.
 and Wang, J.
 and Jin, Y.","Wang, Lin, Chen, Wang, Jin",https://dx.doi.org/10.1097/MD.0000000000010998,https://doi.org/10.1097/MD.0000000000010998,2021-08-03
4218.0,,pubmed,Opportunities and challenges in developing deep learning models using electronic health records data: a systematic review,Opportunities and challenges in developing deep learning models using electronic health records data: a systematic review,"Objective: To conduct a systematic review of deep learning models for electronic health record (EHR) data, and illustrate various deep learning architectures for analyzing different data sources and their target applications. We also highlight ongoing research and identify open challenges in building deep learning models of EHRs. Design/method: We searched PubMed and Google Scholar for papers on deep learning studies using EHR data published between January 1, 2010, and January 31, 2018. We summarize them according to these axes: types of analytics tasks, types of deep learning model architectures, special challenges arising from health data and tasks and their potential solutions, as well as evaluation strategies. Results: We surveyed and analyzed multiple aspects of the 98 articles we found and identified the following analytics tasks: disease detection/classification, sequential prediction of clinical events, concept embedding, data augmentation, and EHR data privacy. We then studied how deep architectures were applied to these tasks. We also discussed some special challenges arising from modeling EHR data and reviewed a few popular approaches. Finally, we summarized how performance evaluations were conducted for each task. Discussion: Despite the early success in using deep learning for health analytics applications, there still exist a number of issues to be addressed. We discuss them in detail including data and label availability, the interpretability and transparency of the model, and ease of deployment.","To conduct a systematic review of deep learning models for electronic health record (EHR) data, and illustrate various deep learning architectures for analyzing different data sources and their target applications. We also highlight ongoing research and identify open challenges in building deep learning models of EHRs. We searched PubMed and Google Scholar for papers on deep learning studies using EHR data published between January 1, 2010, and January 31, 2018. We summarize them according to these axes: types of analytics tasks, types of deep learning model architectures, special challenges arising from health data and tasks and their potential solutions, as well as evaluation strategies. We surveyed and analyzed multiple aspects of the 98 articles we found and identified the following analytics tasks: disease detection/classification, sequential prediction of clinical events, concept embedding, data augmentation, and EHR data privacy. We then studied how deep architectures were applied to these tasks. We also discussed some special challenges arising from modeling EHR data and reviewed a few popular approaches. Finally, we summarized how performance evaluations were conducted for each task. Despite the early success in using deep learning for health analytics applications, there still exist a number of issues to be addressed. We discuss them in detail including data and label availability, the interpretability and transparency of the model, and ease of deployment.","Xiao, C.
 and Choi, E.
 and Sun, J.","Xiao, Choi, Sun",https://dx.doi.org/10.1093/jamia/ocy068,https://doi.org/10.1093/jamia/ocy068,2021-08-03
2871.0,,pubmed,Evidence synthesis software,Evidence synthesis software,"It can be challenging to decide which evidence synthesis software to choose when doing a systematic review. This article discusses some of the important questions to consider in relation to the chosen method and synthesis approach. Software can support researchers in a range of ways. Here, a range of review conditions and software solutions. For example, facilitating contemporaneous collaboration across time and geographical space; in-built bias assessment tools; and line-by-line coding for qualitative textual analysis. EPPI-Reviewer is a review software for research synthesis managed by the EPPI-centre, UCL Institute of Education. EPPI-Reviewer has text mining automation technologies. Version 5 supports data sharing and re-use across the systematic review community. Open source software will soon be released. EPPI-Centre will continue to offer the software as a cloud-based service. The software is offered via a subscription with a one-month (extendible) trial available and volume discounts for 'site licences'. It is free to use for Cochrane and Campbell reviews. The next EPPI-Reviewer version is being built in collaboration with National Institute for Health and Care Excellence using 'surveillance' of newly published research to support 'living' iterative reviews. This is achieved using a combination of machine learning and traditional information retrieval technologies to identify the type of research each new publication describes and determine its relevance for a particular review, domain or guideline. While the amount of available knowledge and research is constantly increasing, the ways in which software can support the focus and relevance of data identification are also developing fast. Software advances are maximising the opportunities for the production of relevant and timely reviews.","It can be challenging to decide which evidence synthesis software to choose when doing a systematic review. This article discusses some of the important questions to consider in relation to the chosen method and synthesis approach. Software can support researchers in a range of ways. Here, a range of review conditions and software solutions. For example, facilitating contemporaneous collaboration across time and geographical space; in-built bias assessment tools; and line-by-line coding for qualitative textual analysis. EPPI-Reviewer is a review software for research synthesis managed by the EPPI-centre, UCL Institute of Education. EPPI-Reviewer has text mining automation technologies. Version 5 supports data sharing and re-use across the systematic review community. Open source software will soon be released. EPPI-Centre will continue to offer the software as a cloud-based service. The software is offered via a subscription with a one-month (extendible) trial available and volume discounts for 'site licences'. It is free to use for Cochrane and Campbell reviews. The next EPPI-Reviewer version is being built in collaboration with National Institute for Health and Care Excellence using 'surveillance' of newly published research to support 'living' iterative reviews. This is achieved using a combination of machine learning and traditional information retrieval technologies to identify the type of research each new publication describes and determine its relevance for a particular review, domain or guideline. While the amount of available knowledge and research is constantly increasing, the ways in which software can support the focus and relevance of data identification are also developing fast. Software advances are maximising the opportunities for the production of relevant and timely reviews.","Park, S. E.
 and Thomas, J.","Park, Thomas",not available,https://doi.org/10.1136/bmjebm-2018-110962,2021-08-03
4289.0,,pubmed,A Cost-Effectiveness Analysis of a Pediatric Operating Room in Uganda,A Cost-Effectiveness Analysis of a Pediatric Operating Room in Uganda,"This study examines the cost-effectiveness of constructing a dedicated pediatric operating room (OR) in Uganda, a country where access to surgical care is limited to 4 pediatric surgeons serving a population of over 20 million children under 15 years of age. METHODS: A simulation model using a decision tree template was developed to project the cost and disability-adjusted life-years saved by a pediatric OR in a low-income setting. Parameters are informed by patient outcomes of the surgical procedures performed. Costs of the OR equipment and a literature review were used to calculate the incremental cost-effectiveness ratio of a pediatric OR. One-way and probabilistic sensitivity analysis were performed to assess parameter uncertainty. Economic monetary benefit was calculated using the value of a statistical life approach. RESULTS: A pediatric OR averted a total of 6,447 disability-adjusted life-years /year (95% uncertainty interval 6,288-6,606) and cost $41,182/year (UI 40,539-41,825) in terms of OR installation. The pediatric operating room had an incremental cost-effectiveness ratio of $6.39 per disability-adjusted life-year averted (95% uncertainty interval of 6.19-6.59), or $397.95 (95% uncertainty interval of 385.41-410.67) per life saved based on the country's average life expectancy in 2015. These values were well within the WHO guidelines of cost-effectiveness threshold. The net economic benefit amounted to $5,336,920 for a year of operation, or $16,371 per patient. The model remained robust with one-way and probabilistic sensitivity analyses. CONCLUSION: The construction of a pediatric operating room in Uganda is a cost-effective and worthwhile investment, endorsing future decisions to enhance pediatric surgical capacity in the resource-limited settings of Sub-Saharan Africa.","This study examines the cost-effectiveness of constructing a dedicated pediatric operating room (OR) in Uganda, a country where access to surgical care is limited to 4 pediatric surgeons serving a population of over 20 million children under 15 years of age. A simulation model using a decision tree template was developed to project the cost and disability-adjusted life-years saved by a pediatric OR in a low-income setting. Parameters are informed by patient outcomes of the surgical procedures performed. Costs of the OR equipment and a literature review were used to calculate the incremental cost-effectiveness ratio of a pediatric OR. One-way and probabilistic sensitivity analysis were performed to assess parameter uncertainty. Economic monetary benefit was calculated using the value of a statistical life approach. A pediatric OR averted a total of 6,447 disability-adjusted life-years /year (95% uncertainty interval 6,288-6,606) and cost $41,182/year (UI 40,539-41,825) in terms of OR installation. The pediatric operating room had an incremental cost-effectiveness ratio of $6.39 per disability-adjusted life-year averted (95% uncertainty interval of 6.19-6.59), or $397.95 (95% uncertainty interval of 385.41-410.67) per life saved based on the country's average life expectancy in 2015. These values were well within the WHO guidelines of cost-effectiveness threshold. The net economic benefit amounted to $5,336,920 for a year of operation, or $16,371 per patient. The model remained robust with one-way and probabilistic sensitivity analyses. The construction of a pediatric operating room in Uganda is a cost-effective and worthwhile investment, endorsing future decisions to enhance pediatric surgical capacity in the resource-limited settings of Sub-Saharan Africa.","Yap, A.
 and Muzira, A.
 and Cheung, M.
 and Healy, J.
 and Kakembo, N.
 and Kisa, P.
 and Cunningham, D.
 and Youngson, G.
 and Sekabira, J.
 and Yaesoubi, R.
 and Ozgediz, D.","Yap, Muzira, Cheung, Healy, Kakembo, Kisa, Cunningham, Youngson, Sekabira, Yaesoubi, Ozgediz",https://dx.doi.org/10.1016/j.surg.2018.03.023,https://doi.org/10.1016/j.surg.2018.03.023,2021-08-03
367.0,,pubmed,Short-Term (<8 wk) High-Intensity Interval Training in Diseased Cohorts,Short-Term (&lt;8 wk) High-Intensity Interval Training in Diseased Cohorts,"BACKGROUND AND AIM: Exercise training regimes can lead to improvements in measures of cardiorespiratory fitness (CRF), improved general health, and reduced morbidity and overall mortality risk. High-intensity interval training (HIIT) offers a time-efficient approach to improve CRF in healthy individuals, but the relative benefits of HIIT compared with traditional training methods are unknown in across different disease cohorts. METHODS: This systematic review and meta-analysis compares CRF gains in randomized controlled trials of short-term (<8 wk) HIIT versus either no exercise control (CON) or moderate continuous training (MCT) within diseased cohorts. Literature searches of the following databases were performed: MEDLINE, EMBASE, CINAHL, AMED, and PubMed (all from inception to December 1, 2017), with further searches of Clinicaltrials.gov and citations via Google Scholar. Primary outcomes were effect on CRF variables: VO2peak and anaerobic threshold. RESULTS: Thirty-nine studies met the inclusion criteria. HIIT resulted in a clinically significant increase in VO2peak compared with CON (mean difference [MD] = 3.32 mL.kg.min, 95% confidence interval [CI] = 2.56-2.08). Overall HIIT provided added benefit to VO2peak over MCT (MD = 0.79 mL.kg.min, 95% CI = 0.20-1.39). The benefit of HIIT was most marked in patients with cardiovascular disease when compared with MCT (VO2peak: MD = 1.66 mL.kg.min, 95% CI = 0.60-2.73; anaerobic threshold: MD = 1.61 mL.kg.min, 95% CI = 0.33-2.90). CONCLUSIONS: HIIT elicits improvements in objective measures of CRF within 8 wk in diseased cohorts compared with no intervention. When compared with MCT, HIIT imparts statistically significant additional improvements in measures of CRF, with clinically important additional improvements in VO2peak in cardiovascular patients. Comparative efficacy of HIIT versus MCT combined with an often reduced time commitment may warrant HIIT's promotion as a viable clinical exercise intervention.","Exercise training regimes can lead to improvements in measures of cardiorespiratory fitness (CRF), improved general health, and reduced morbidity and overall mortality risk. High-intensity interval training (HIIT) offers a time-efficient approach to improve CRF in healthy individuals, but the relative benefits of HIIT compared with traditional training methods are unknown in across different disease cohorts. This systematic review and meta-analysis compares CRF gains in randomized controlled trials of short-term (&lt;8 wk) HIIT versus either no exercise control (CON) or moderate continuous training (MCT) within diseased cohorts. Literature searches of the following databases were performed: MEDLINE, EMBASE, CINAHL, AMED, and PubMed (all from inception to December 1, 2017), with further searches of Clinicaltrials.gov and citations via Google Scholar. Primary outcomes were effect on CRF variables: VË™O2peak and anaerobic threshold. Thirty-nine studies met the inclusion criteria. HIIT resulted in a clinically significant increase in VË™O2peak compared with CON (mean difference [MD] = 3.32 mLÂ·kgÂ·min, 95% confidence interval [CI] = 2.56-2.08). Overall HIIT provided added benefit to VË™O2peak over MCT (MD = 0.79 mLÂ·kgÂ·min, 95% CI = 0.20-1.39). The benefit of HIIT was most marked in patients with cardiovascular disease when compared with MCT (VË™O2peak: MD = 1.66 mLÂ·kgÂ·min, 95% CI = 0.60-2.73; anaerobic threshold: MD = 1.61 mLÂ·kgÂ·min, 95% CI = 0.33-2.90). HIIT elicits improvements in objective measures of CRF within 8 wk in diseased cohorts compared with no intervention. When compared with MCT, HIIT imparts statistically significant additional improvements in measures of CRF, with clinically important additional improvements in VË™O2peak in cardiovascular patients. Comparative efficacy of HIIT versus MCT combined with an often reduced time commitment may warrant HIIT's promotion as a viable clinical exercise intervention.","Blackwell, J. E. M.
 and Doleman, B.
 and Herrod, P. J. J.
 and Ricketts, S.
 and Phillips, B. E.
 and Lund, J. N.
 and Williams, J. P.","Blackwell, Doleman, Herrod, Ricketts, Phillips, Lund, Williams",https://dx.doi.org/10.1249/MSS.0000000000001634,https://doi.org/10.1249/MSS.0000000000001634,2021-08-03
3355.0,,pubmed,Association between malocclusion and dental caries in adolescents: a systematic review and meta-analysis,Association between malocclusion and dental caries in adolescents: a systematic review and meta-analysis,"AIM: To evaluate the scientific evidence regarding the association between malocclusion and dental caries in adolescents. METHODS: Searches were conducted of six electronic databases, complemented by manual searching of the reference lists of the selected articles and grey literature. Two independent reviewers performed the selection of the articles, data extraction and the evaluation of the risks of bias through an assessment of methodological quality. Meta-analysis was performed considering the mean decayed, missing and filled teeth (DMFT) index for caries and the Dental Aesthetic Index (DAI) for malocclusion. Heterogeneity was tested using the I<sup>2</sup> statistic and a random effect model was employed. Summary effect measures were calculated as differences in means. RESULTS: The initial search retrieved 2644 studies, only 15 of which were selected for full-text analysis. Four cross-sectional studies were included in the qualitative systematic review. Only one of these studies found no association between malocclusion and dental caries. The meta-analysis of three studies demonstrated that a lower DAI value was significantly associated with a lower mean DMFT index, except for the comparison of DAI 26-30 vs. 31-35. CONCLUSIONS: Based on the studies analysed, the scientific evidence indicates an association between malocclusion and dental caries.","To evaluate the scientific evidence regarding the association between malocclusion and dental caries in adolescents. Searches were conducted of six electronic databases, complemented by manual searching of the reference lists of the selected articles and grey literature. Two independent reviewers performed the selection of the articles, data extraction and the evaluation of the risks of bias through an assessment of methodological quality. Meta-analysis was performed considering the mean decayed, missing and filled teeth (DMFT) index for caries and the Dental Aesthetic Index (DAI) for malocclusion. Heterogeneity was tested using the I<sup>2</sup> statistic and a random effect model was employed. Summary effect measures were calculated as differences in means. The initial search retrieved 2644 studies, only 15 of which were selected for full-text analysis. Four cross-sectional studies were included in the qualitative systematic review. Only one of these studies found no association between malocclusion and dental caries. The meta-analysis of three studies demonstrated that a lower DAI value was significantly associated with a lower mean DMFT index, except for the comparison of DAI 26-30 vs. 31-35. Based on the studies analysed, the scientific evidence indicates an association between malocclusion and dental caries.","Sa-Pinto, A. C.
 and Rego, T. M.
 and Marques, L. S.
 and Martins, C. C.
 and Ramos-Jorge, M. L.
 and Ramos-Jorge, J.","SÃ¡-Pinto, Rego, Marques, Martins, Ramos-Jorge, Ramos-Jorge",https://dx.doi.org/10.1007/s40368-018-0333-0,https://doi.org/10.1007/s40368-018-0333-0,2021-08-03
3357.0,,pubmed,High-throughput non-invasive prenatal testing for fetal rhesus D status in RhD-negative women not known to be sensitised to the RhD antigen: a systematic review and economic evaluation,High-throughput non-invasive prenatal testing for fetal rhesus D status in RhD-negative women not known to be sensitised to the RhD antigen: a systematic review and economic evaluation,"BACKGROUND: High-throughput non-invasive prenatal testing (NIPT) for fetal rhesus (D antigen) (RhD) status could avoid unnecessary treatment with routine anti-D immunoglobulin for RhD-negative women carrying a RhD-negative fetus, although this may lead to an increased risk of RhD sensitisations. OBJECTIVES: To systematically review the evidence on the diagnostic accuracy, clinical effectiveness and implementation of high-throughput NIPT and to develop a cost-effectiveness model. METHODS: We searched MEDLINE and other databases, from inception to February 2016, for studies of high-throughput NIPT free-cell fetal deoxyribonucleic acid (DNA) tests of maternal plasma to determine fetal RhD status in RhD-negative pregnant women who were not known to be sensitised to the RhD antigen. Study quality was assessed with the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) and A Cochrane Risk of Bias Assessment Tool: for Non-Randomised Studies of Interventions (ACROBAT-NRSI). Summary estimates of false-positive rates (FPRs) and false-negative rates (FNRs) were calculated using bivariate models. Clinical effectiveness evidence was used to conduct a simulation study. We developed a de novo probabilistic decision tree-based cohort model that considered four alternative ways in which the results of NIPT could guide the use of anti-D immunoglobulin antenatally and post partum. Sensitivity analyses (SAs) were conducted to address key uncertainties and model assumptions. RESULTS: Eight studies were included in the diagnostic accuracy review, seven studies were included in the clinical effectiveness review and 12 studies were included in the review of implementation. Meta-analyses included women mostly at or post 11 weeks' gestation. The pooled FNR (women at risk of sensitisation) was 0.34% [95% confidence interval (CI) 0.15% to 0.76%] and the pooled FPR (women needlessly receiving anti-D) was 3.86% (95% CI 2.54% to 5.82%). SAs did not materially alter the overall results. Data on clinical outcomes, including sensitisation rates, were limited. Our simulation suggests that NIPT could substantially reduce unnecessary use of antenatal anti-D with only a small increase in the risk of sensitisation. All large implementation studies suggested that large-scale implementation of high-throughput NIPT was feasible. Seven cost-effectiveness studies were included in the review, which found that the potential for the use of NIPT to produce cost savings was dependent on the cost of the test. Our de novo model suggested that high-throughput NIPT is likely to be cost saving compared with the current practice of providing routine antenatal anti-D prophylaxis to all women who are RhD negative. The extent of the cost saving appeared to be sufficient to outweigh the small increase in sensitisations. However, the magnitude of the cost saving is highly sensitive to the cost of NIPT itself. LIMITATIONS: There was very limited evidence relating to the clinical effectiveness of high-throughput NIPT, with no evidence on potential adverse effects. The generalisability of the findings to non-white women and multiple pregnancies is unclear. CONCLUSIONS: High-throughput NIPT is sufficiently accurate to detect fetal RhD status in RhD-negative women from 11 weeks' gestation and would considerably reduce unnecessary treatment with routine anti-D immunoglobulin, potentially resulting in cost savings of between 485,000 and 671,000 per 100,000 pregnancies if the cost of implementing NIPT is in line with that reflected in this evaluation. FUTURE WORK: Further research on the diagnostic accuracy of NIPT in non-white women is needed. STUDY REGISTRATION: This study is registered as PROSPERO CRD42015029497. FUNDING: The National Institute for Health Research Health Technology Assessment programme.","High-throughput non-invasive prenatal testing (NIPT) for fetal rhesus (D antigen) (RhD) status could avoid unnecessary treatment with routine anti-D immunoglobulin for RhD-negative women carrying a RhD-negative fetus, although this may lead to an increased risk of RhD sensitisations. To systematically review the evidence on the diagnostic accuracy, clinical effectiveness and implementation of high-throughput NIPT and to develop a cost-effectiveness model. We searched MEDLINE and other databases, from inception to February 2016, for studies of high-throughput NIPT free-cell fetal deoxyribonucleic acid (DNA) tests of maternal plasma to determine fetal RhD status in RhD-negative pregnant women who were not known to be sensitised to the RhD antigen. Study quality was assessed with the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) and A Cochrane Risk of Bias Assessment Tool: for Non-Randomised Studies of Interventions (ACROBAT-NRSI). Summary estimates of false-positive rates (FPRs) and false-negative rates (FNRs) were calculated using bivariate models. Clinical effectiveness evidence was used to conduct a simulation study. We developed a de novo probabilistic decision tree-based cohort model that considered four alternative ways in which the results of NIPT could guide the use of anti-D immunoglobulin antenatally and post partum. Sensitivity analyses (SAs) were conducted to address key uncertainties and model assumptions. Eight studies were included in the diagnostic accuracy review, seven studies were included in the clinical effectiveness review and 12 studies were included in the review of implementation. Meta-analyses included women mostly at or post 11 weeks' gestation. The pooled FNR (women at risk of sensitisation) was 0.34% [95% confidence interval (CI) 0.15% to 0.76%] and the pooled FPR (women needlessly receiving anti-D) was 3.86% (95% CI 2.54% to 5.82%). SAs did not materially alter the overall results. Data on clinical outcomes, including sensitisation rates, were limited. Our simulation suggests that NIPT could substantially reduce unnecessary use of antenatal anti-D with only a small increase in the risk of sensitisation. All large implementation studies suggested that large-scale implementation of high-throughput NIPT was feasible. Seven cost-effectiveness studies were included in the review, which found that the potential for the use of NIPT to produce cost savings was dependent on the cost of the test. Our de novo model suggested that high-throughput NIPT is likely to be cost saving compared with the current practice of providing routine antenatal anti-D prophylaxis to all women who are RhD negative. The extent of the cost saving appeared to be sufficient to outweigh the small increase in sensitisations. However, the magnitude of the cost saving is highly sensitive to the cost of NIPT itself. There was very limited evidence relating to the clinical effectiveness of high-throughput NIPT, with no evidence on potential adverse effects. The generalisability of the findings to non-white women and multiple pregnancies is unclear. High-throughput NIPT is sufficiently accurate to detect fetal RhD status in RhD-negative women from 11 weeks' gestation and would considerably reduce unnecessary treatment with routine anti-D immunoglobulin, potentially resulting in cost savings of between Â£485,000 and Â£671,000 per 100,000 pregnancies if the cost of implementing NIPT is in line with that reflected in this evaluation. Further research on the diagnostic accuracy of NIPT in non-white women is needed. This study is registered as PROSPERO CRD42015029497. The National Institute for Health Research Health Technology Assessment programme.","Saramago, P.
 and Yang, H.
 and Llewellyn, A.
 and Walker, R.
 and Harden, M.
 and Palmer, S.
 and Griffin, S.
 and Simmonds, M.","Saramago, Yang, Llewellyn, Walker, Harden, Palmer, Griffin, Simmonds",https://dx.doi.org/10.3310/hta22130,https://doi.org/10.3310/hta22130,2021-08-03
248.0,,pubmed,Tool for filtering PubMed search results by sample size,Tool for filtering PubMed search results by sample size,"Objective: The most used search engine for scientific literature, PubMed, provides tools to filter results by several fields. When searching for reports on clinical trials, sample size can be among the most important factors to consider. However, PubMed does not currently provide any means of filtering search results by sample size. Such a filtering tool would be useful in a variety of situations, including meta-analyses or state-of-the-art analyses to support experimental therapies. In this work, a tool was developed to filter articles identified by PubMed based on their reported sample sizes. Materials and Methods: A search engine was designed to send queries to PubMed, retrieve results, and compute estimates of reported sample sizes using a combination of syntactical and machine learning methods. The sample size search tool is publicly available for download at http://ihealth.uemc.es. Its accuracy was assessed against a manually annotated database of 750 random clinical trials returned by PubMed. Results: Validation tests show that the sample size search tool is able to accurately (1) estimate sample size for 70% of abstracts and (2) classify 85% of abstracts into sample size quartiles. Conclusions: The proposed tool was validated as useful for advanced PubMed searches of clinical trials when the user is interested in identifying trials of a given sample size.","The most used search engine for scientific literature, PubMed, provides tools to filter results by several fields. When searching for reports on clinical trials, sample size can be among the most important factors to consider. However, PubMed does not currently provide any means of filtering search results by sample size. Such a filtering tool would be useful in a variety of situations, including meta-analyses or state-of-the-art analyses to support experimental therapies. In this work, a tool was developed to filter articles identified by PubMed based on their reported sample sizes. A search engine was designed to send queries to PubMed, retrieve results, and compute estimates of reported sample sizes using a combination of syntactical and machine learning methods. The sample size search tool is publicly available for download at http://ihealth.uemc.es. Its accuracy was assessed against a manually annotated database of 750 random clinical trials returned by PubMed. Validation tests show that the sample size search tool is able to accurately (1) estimate sample size for 70% of abstracts and (2) classify 85% of abstracts into sample size quartiles. The proposed tool was validated as useful for advanced PubMed searches of clinical trials when the user is interested in identifying trials of a given sample size.","Baladron, C.
 and Santos-Lozano, A.
 and Aguiar, J. M.
 and Lucia, A.
 and Martin-Hernandez, J.","BaladrÃ³n, Santos-Lozano, Aguiar, Lucia, MartÃ­n-HernÃ¡ndez",https://dx.doi.org/10.1093/jamia/ocx155,https://doi.org/10.1093/jamia/ocx155,2021-08-03
196.0,,pubmed,A systematic review of the diagnostic accuracy of automated tests for cognitive impairment,A systematic review of the diagnostic accuracy of automated tests for cognitive impairment,"OBJECTIVE: The aim of this review is to determine whether automated computerised tests accurately identify patients with progressive cognitive impairment and, if so, to investigate their role in monitoring disease progression and/or response to treatment. METHODS: Six electronic databases (Medline, Embase, Cochrane, Institute for Scientific Information, PsycINFO, and ProQuest) were searched from January 2005 to August 2015 to identify papers for inclusion. Studies assessing the diagnostic accuracy of automated computerised tests for mild cognitive impairment (MCI) and early dementia against a reference standard were included. Where possible, sensitivity, specificity, positive predictive value, negative predictive value, and likelihood ratios were calculated. The Quality Assessment of Diagnostic Accuracy Studies tool was used to assess risk of bias. RESULTS: Sixteen studies assessing 11 diagnostic tools for MCI and early dementia were included. No studies were eligible for inclusion in the review of tools for monitoring progressive disease and response to treatment. The overall quality of the studies was good. However, the wide range of tests assessed and the non-standardised reporting of diagnostic accuracy outcomes meant that statistical analysis was not possible. CONCLUSION: Some tests have shown promising results for identifying MCI and early dementia. However, concerns over small sample sizes, lack of replicability of studies, and lack of evidence available make it difficult to make recommendations on the clinical use of the computerised tests for diagnosing, monitoring progression, and treatment response for MCI and early dementia. Research is required to establish stable cut-off points for automated computerised tests used to diagnose patients with MCI or early dementia.","The aim of this review is to determine whether automated computerised tests accurately identify patients with progressive cognitive impairment and, if so, to investigate their role in monitoring disease progression and/or response to treatment. Six electronic databases (Medline, Embase, Cochrane, Institute for Scientific Information, PsycINFO, and ProQuest) were searched from January 2005 to August 2015 to identify papers for inclusion. Studies assessing the diagnostic accuracy of automated computerised tests for mild cognitive impairment (MCI) and early dementia against a reference standard were included. Where possible, sensitivity, specificity, positive predictive value, negative predictive value, and likelihood ratios were calculated. The Quality Assessment of Diagnostic Accuracy Studies tool was used to assess risk of bias. Sixteen studies assessing 11 diagnostic tools for MCI and early dementia were included. No studies were eligible for inclusion in the review of tools for monitoring progressive disease and response to treatment. The overall quality of the studies was good. However, the wide range of tests assessed and the non-standardised reporting of diagnostic accuracy outcomes meant that statistical analysis was not possible. Some tests have shown promising results for identifying MCI and early dementia. However, concerns over small sample sizes, lack of replicability of studies, and lack of evidence available make it difficult to make recommendations on the clinical use of the computerised tests for diagnosing, monitoring progression, and treatment response for MCI and early dementia. Research is required to establish stable cut-off points for automated computerised tests used to diagnose patients with MCI or early dementia.","Aslam, R. W.
 and Bates, V.
 and Dundar, Y.
 and Hounsome, J.
 and Richardson, M.
 and Krishan, A.
 and Dickson, R.
 and Boland, A.
 and Fisher, J.
 and Robinson, L.
 and Sikdar, S.","Aslam, Bates, Dundar, Hounsome, Richardson, Krishan, Dickson, Boland, Fisher, Robinson, Sikdar",https://dx.doi.org/10.1002/gps.4852,https://doi.org/10.1002/gps.4852,2021-08-03
4329.0,,pubmed,Patient-Reported Outcomes After Lateral Wall Sinus Floor Elevation: A Systematic Review,Patient-Reported Outcomes After Lateral Wall Sinus Floor Elevation: A Systematic Review,"OBJECTIVES: The aim of this systematic review is to assess patient-reported outcome measures (PROMs) after a sinus lift elevation by means of a lateral approach. MATERIAL AND METHODS: An electronic search was performed to search for eligible publications reporting PROMs after a lateral wall sinus lift procedure. Selected articles were further scrutinized and underwent a quality check before inclusion in a final study pool. RESULTS: The electronic search provided us with 2444 articles of which 98 were further examined through a full-text analysis. Of these 98 studies, 11 were selected based on our inclusion and exclusion criteria. Results on a different number of PROMs were examined and compared: pain, edema, ability to eat, ability to work, phonetics, daily activities, bleeding, bruising, ability to sleep, bad breath, patient preference, and Oral Health Impact Profile-14 (OHIP-14). Methods of evaluation were 3- to 5-point scales, visual analog scale scores, and OHIP-14 questionnaires. Evaluation time points differed between 1 single evaluation time to a daily registration during 1 week. CONCLUSIONS: A general peak in discomfort could be noticed on day 1 postoperatively with a general decline thereafter. Severe morbidity or discomfort occurred but not in most patients.","The aim of this systematic review is to assess patient-reported outcome measures (PROMs) after a sinus lift elevation by means of a lateral approach. An electronic search was performed to search for eligible publications reporting PROMs after a lateral wall sinus lift procedure. Selected articles were further scrutinized and underwent a quality check before inclusion in a final study pool. The electronic search provided us with 2444 articles of which 98 were further examined through a full-text analysis. Of these 98 studies, 11 were selected based on our inclusion and exclusion criteria. Results on a different number of PROMs were examined and compared: pain, edema, ability to eat, ability to work, phonetics, daily activities, bleeding, bruising, ability to sleep, bad breath, patient preference, and Oral Health Impact Profile-14 (OHIP-14). Methods of evaluation were 3- to 5-point scales, visual analog scale scores, and OHIP-14 questionnaires. Evaluation time points differed between 1 single evaluation time to a daily registration during 1 week. A general peak in discomfort could be noticed on day 1 postoperatively with a general decline thereafter. Severe morbidity or discomfort occurred but not in most patients.","Younes, F.
 and Eghbali, A.
 and Goemaere, T.
 and De Bruyckere, T.
 and Cosyn, J.","Younes, Eghbali, Goemaere, De Bruyckere, Cosyn",https://dx.doi.org/10.1097/ID.0000000000000717,https://doi.org/10.1097/ID.0000000000000717,2021-08-03
2395.0,,pubmed,Machine learning for identifying Randomized Controlled Trials: An evaluation and practitioner's guide,Machine learning for identifying Randomized Controlled Trials: An evaluation and practitioner's guide,"Machine learning (ML) algorithms have proven highly accurate for identifying Randomized Controlled Trials (RCTs) but are not used much in practice, in part because the best way to make use of the technology in a typical workflow is unclear. In this work, we evaluate ML models for RCT classification (support vector machines, convolutional neural networks, and ensemble approaches). We trained and optimized support vector machine and convolutional neural network models on the titles and abstracts of the Cochrane Crowd RCT set. We evaluated the models on an external dataset (Clinical Hedges), allowing direct comparison with traditional database search filters. We estimated area under receiver operating characteristics (AUROC) using the Clinical Hedges dataset. We demonstrate that ML approaches better discriminate between RCTs and non-RCTs than widely used traditional database search filters at all sensitivity levels; our best-performing model also achieved the best results to date for ML in this task (AUROC 0.987, 95% CI, 0.984-0.989). We provide practical guidance on the role of ML in (1) systematic reviews (high-sensitivity strategies) and (2) rapid reviews and clinical question answering (high-precision strategies) together with recommended probability cutoffs for each use case. Finally, we provide open-source software to enable these approaches to be used in practice.","Machine learning (ML) algorithms have proven highly accurate for identifying Randomized Controlled Trials (RCTs) but are not used much in practice, in part because the best way to make use of the technology in a typical workflow is unclear. In this work, we evaluate ML models for RCT classification (support vector machines, convolutional neural networks, and ensemble approaches). We trained and optimized support vector machine and convolutional neural network models on the titles and abstracts of the Cochrane Crowd RCT set. We evaluated the models on an external dataset (Clinical Hedges), allowing direct comparison with traditional database search filters. We estimated area under receiver operating characteristics (AUROC) using the Clinical Hedges dataset. We demonstrate that ML approaches better discriminate between RCTs and non-RCTs than widely used traditional database search filters at all sensitivity levels; our best-performing model also achieved the best results to date for ML in this task (AUROC 0.987, 95% CI, 0.984-0.989). We provide practical guidance on the role of ML in (1) systematic reviews (high-sensitivity strategies) and (2) rapid reviews and clinical question answering (high-precision strategies) together with recommended probability cutoffs for each use case. Finally, we provide open-source software to enable these approaches to be used in practice.","Marshall, I. J.
 and Noel-Storr, A.
 and Kuiper, J.
 and Thomas, J.
 and Wallace, B. C.","Marshall, Noel-Storr, Kuiper, Thomas, Wallace",not available,https://doi.org/10.1002/jrsm.1287,2021-08-03
3546.0,,pubmed,A Neural Candidate-Selector Architecture for Automatic Structured Clinical Text Annotation,A Neural Candidate-Selector Architecture for Automatic Structured Clinical Text Annotation,"We consider the task of automatically annotating free texts describing clinical trials with concepts from a controlled, structured medical vocabulary. Specifically we aim to build a model to infer distinct sets of (ontological) concepts describing complementary clinically salient aspects of the underlying trials: the populations enrolled, the interventions administered and the outcomes measured, i.e., the PICO elements. This important practical problem poses a few key challenges. One issue is that the output space is vast, because the vocabulary comprises many unique concepts. Compounding this problem, annotated data in this domain is expensive to collect and hence sparse. Furthermore, the outputs (sets of concepts for each PICO element) are correlated: specific populations (e.g., diabetics) will render certain intervention concepts likely (insulin therapy) while effectively precluding others (radiation therapy). Such correlations should be exploited. We propose a novel neural model that addresses these challenges. We introduce a Candidate-Selector architecture in which the model considers setes of candidate concepts for PICO elements, and assesses their plausibility conditioned on the input text to be annotated. This relies on a 'candidate set' generator, which may be learned or relies on heuristics. A conditional discriminative neural model then jointly selects candidate concepts, given the input text. We compare the predictive performance of our approach to strong baselines, and show that it outperforms them. Finally, we perform a qualitative evaluation of the generated annotations by asking domain experts to assess their quality.","We consider the task of automatically annotating free texts describing clinical trials with concepts from a controlled, structured medical vocabulary. Specifically we aim to build a model to infer distinct sets of (ontological) concepts describing complementary clinically salient aspects of the underlying trials: the populations enrolled, the interventions administered and the outcomes measured, i.e., the <i>PICO</i> elements. This important practical problem poses a few key challenges. One issue is that the output space is vast, because the vocabulary comprises many unique concepts. Compounding this problem, annotated data in this domain is expensive to collect and hence sparse. Furthermore, the outputs (sets of concepts for each PICO element) are correlated: specific populations (e.g., diabetics) will render certain intervention concepts likely (insulin therapy) while effectively precluding others (radiation therapy). Such correlations should be exploited. We propose a novel neural model that addresses these challenges. We introduce a Candidate-Selector architecture in which the model considers setes of <i>candidate concepts</i> for PICO elements, and assesses their plausibility conditioned on the input text to be annotated. This relies on a 'candidate set' generator, which may be learned or relies on heuristics. A conditional discriminative neural model then jointly selects candidate concepts, given the input text. We compare the predictive performance of our approach to strong baselines, and show that it outperforms them. Finally, we perform a qualitative evaluation of the generated annotations by asking domain experts to assess their quality.","Singh, G.
 and Marshall, I. J.
 and Thomas, J.
 and Shawe-Taylor, J.
 and Wallace, B. C.","Singh, Marshall, Thomas, Shawe-Taylor, Wallace",not available,https://doi.org/10.1145/3132847.3132989,2021-08-03
2568.0,,pubmed,The intriguing evolution of effect sizes in biomedical research over time: smaller but more often statistically significant,The intriguing evolution of effect sizes in biomedical research over time: smaller but more often statistically significant,"Background: In medicine, effect sizes (ESs) allow the effects of independent variables (including risk/protective factors or treatment interventions) on dependent variables (e.g., health outcomes) to be quantified. Given that many public health decisions and health care policies are based on ES estimates, it is important to assess how ESs are used in the biomedical literature and to investigate potential trends in their reporting over time. Results: Through a big data approach, the text mining process automatically extracted 814 120 ESs from 13 322 754 PubMed abstracts. Eligible ESs were risk ratio, odds ratio, and hazard ratio, along with their confidence intervals. Here we show a remarkable decrease of ES values in PubMed abstracts between 1990 and 2015 while, concomitantly, results become more often statistically significant. Medians of ES values have decreased over time for both 'risk' and 'protective' values. This trend was found in nearly all fields of biomedical research, with the most marked downward tendency in genetics. Over the same period, the proportion of statistically significant ESs increased regularly: among the abstracts with at least 1 ES, 74% were statistically significant in 1990-1995, vs 85% in 2010-2015. Conclusions: whereas decreasing ESs could be an intrinsic evolution in biomedical research, the concomitant increase of statistically significant results is more intriguing. Although it is likely that growing sample sizes in biomedical research could explain these results, another explanation may lie in the 'publish or perish' context of scientific research, with the probability of a growing orientation toward sensationalism in research reports. Important provisions must be made to improve the credibility of biomedical research and limit waste of resources.","In medicine, effect sizes (ESs) allow the effects of independent variables (including risk/protective factors or treatment interventions) on dependent variables (e.g., health outcomes) to be quantified. Given that many public health decisions and health care policies are based on ES estimates, it is important to assess how ESs are used in the biomedical literature and to investigate potential trends in their reporting over time. Through a big data approach, the text mining process automatically extracted 814 120 ESs from 13 322 754 PubMed abstracts. Eligible ESs were risk ratio, odds ratio, and hazard ratio, along with their confidence intervals. Here we show a remarkable decrease of ES values in PubMed abstracts between 1990 and 2015 while, concomitantly, results become more often statistically significant. Medians of ES values have decreased over time for both ""risk"" and ""protective"" values. This trend was found in nearly all fields of biomedical research, with the most marked downward tendency in genetics. Over the same period, the proportion of statistically significant ESs increased regularly: among the abstracts with at least 1 ES, 74% were statistically significant in 1990-1995, vs 85% in 2010-2015. whereas decreasing ESs could be an intrinsic evolution in biomedical research, the concomitant increase of statistically significant results is more intriguing. Although it is likely that growing sample sizes in biomedical research could explain these results, another explanation may lie in the ""publish or perish"" context of scientific research, with the probability of a growing orientation toward sensationalism in research reports. Important provisions must be made to improve the credibility of biomedical research and limit waste of resources.","Monsarrat, P.
 and Vergnes, J. N.","Monsarrat, Vergnes",https://dx.doi.org/10.1093/gigascience/gix121,https://doi.org/10.1093/gigascience/gix121,2021-08-03
4081.0,,pubmed,Clinical information extraction applications: A literature review,Clinical information extraction applications: A literature review,"BACKGROUND: With the rapid adoption of electronic health records (EHRs), it is desirable to harvest information and knowledge from EHRs to support automated systems at the point of care and to enable secondary use of EHRs for clinical and translational research. One critical component used to facilitate the secondary use of EHR data is the information extraction (IE) task, which automatically extracts and encodes clinical information from text. OBJECTIVES: In this literature review, we present a review of recent published research on clinical information extraction (IE) applications. METHODS: A literature search was conducted for articles published from January 2009 to September 2016 based on Ovid MEDLINE In-Process & Other Non-Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science, and ACM Digital Library. RESULTS: A total of 1917 publications were identified for title and abstract screening. Of these publications, 263 articles were selected and discussed in this review in terms of publication venues and data sources, clinical IE tools, methods, and applications in the areas of disease- and drug-related studies, and clinical workflow optimizations. CONCLUSIONS: Clinical IE has been used for a wide range of applications, however, there is a considerable gap between clinical studies using EHR data and studies using clinical IE. This study enabled us to gain a more concrete understanding of the gap and to provide potential solutions to bridge this gap.","With the rapid adoption of electronic health records (EHRs), it is desirable to harvest information and knowledge from EHRs to support automated systems at the point of care and to enable secondary use of EHRs for clinical and translational research. One critical component used to facilitate the secondary use of EHR data is the information extraction (IE) task, which automatically extracts and encodes clinical information from text. In this literature review, we present a review of recent published research on clinical information extraction (IE) applications. A literature search was conducted for articles published from January 2009 to September 2016 based on Ovid MEDLINE In-Process &amp; Other Non-Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science, and ACM Digital Library. A total of 1917 publications were identified for title and abstract screening. Of these publications, 263 articles were selected and discussed in this review in terms of publication venues and data sources, clinical IE tools, methods, and applications in the areas of disease- and drug-related studies, and clinical workflow optimizations. Clinical IE has been used for a wide range of applications, however, there is a considerable gap between clinical studies using EHR data and studies using clinical IE. This study enabled us to gain a more concrete understanding of the gap and to provide potential solutions to bridge this gap.","Wang, Y.
 and Wang, L.
 and Rastegar-Mojarad, M.
 and Moon, S.
 and Shen, F.
 and Afzal, N.
 and Liu, S.
 and Zeng, Y.
 and Mehrabi, S.
 and Sohn, S.
 and Liu, H.","Wang, Wang, Rastegar-Mojarad, Moon, Shen, Afzal, Liu, Zeng, Mehrabi, Sohn, Liu",https://dx.doi.org/10.1016/j.jbi.2017.11.011,https://doi.org/10.1016/j.jbi.2017.11.011,2021-08-03
2392.0,,pubmed,Automating Biomedical Evidence Synthesis: RobotReviewer,Automating Biomedical Evidence Synthesis: RobotReviewer,"We present RobotReviewer, an open-source web-based system that uses machine learning and NLP to semi-automate biomedical evidence synthesis, to aid the practice of Evidence-Based Medicine. RobotReviewer processes full-text journal articles (PDFs) describing randomized controlled trials (RCTs). It appraises the reliability of RCTs and extracts text describing key trial characteristics (e.g., descriptions of the population) using novel NLP methods. RobotReviewer then automatically generates a report synthesising this information. Our goal is for RobotReviewer to automatically extract and synthesise the full-range of structured data needed to inform evidence-based practice.","We present <i>RobotReviewer</i>, an open-source web-based system that uses machine learning and NLP to semi-automate biomedical evidence synthesis, to aid the practice of Evidence-Based Medicine. RobotReviewer processes full-text journal articles (PDFs) describing randomized controlled trials (RCTs). It appraises the reliability of RCTs and extracts text describing key trial characteristics (e.g., descriptions of the population) using novel NLP methods. RobotReviewer then automatically generates a report synthesising this information. Our goal is for RobotReviewer to automatically extract and synthesise the full-range of structured data needed to inform evidence-based practice.","Marshall, I. J.
 and Kuiper, J.
 and Banner, E.
 and Wallace, B. C.","Marshall, Kuiper, Banner, Wallace",not available,https://doi.org/10.18653/v1/P17-4002,2021-08-03
3948.0,,pubmed,Characteristics of knowledge content in a curated online evidence library,Characteristics of knowledge content in a curated online evidence library,"Objective: To describe types of recommendations represented in a curated online evidence library, report on the quality of evidence-based recommendations pertaining to diagnostic imaging exams, and assess underlying knowledge representation. Materials and Methods: The evidence library is populated with clinical decision rules, professional society guidelines, and locally developed best practice guidelines. Individual recommendations were graded based on a standard methodology and compared using chi-square test. Strength of evidence ranged from grade 1 (systematic review) through grade 5 (recommendations based on expert opinion). Finally, variations in the underlying representation of these recommendations were identified. Results: The library contains 546 individual imaging-related recommendations. Only 15% (16/106) of recommendations from clinical decision rules were grade 5 vs 83% (526/636) from professional society practice guidelines and local best practice guidelines that cited grade 5 studies (P < .0001). Minor head trauma, pulmonary embolism, and appendicitis were topic areas supported by the highest quality of evidence. Three main variations in underlying representations of recommendations were 'single-decision,' 'branching,' and 'score-based.' Discussion: Most recommendations were grade 5, largely because studies to test and validate many recommendations were absent. Recommendation types vary in amount and complexity and, accordingly, the structure and syntax of statements they generate. However, they can be represented in single-decision, branching, and score-based representations. Conclusion: In a curated evidence library with graded imaging-based recommendations, evidence quality varied widely, with decision rules providing the highest-quality recommendations. The library may be helpful in highlighting evidence gaps, comparing recommendations from varied sources on similar clinical topics, and prioritizing imaging recommendations to inform clinical decision support implementation.","To describe types of recommendations represented in a curated online evidence library, report on the quality of evidence-based recommendations pertaining to diagnostic imaging exams, and assess underlying knowledge representation. The evidence library is populated with clinical decision rules, professional society guidelines, and locally developed best practice guidelines. Individual recommendations were graded based on a standard methodology and compared using chi-square test. Strength of evidence ranged from grade 1 (systematic review) through grade 5 (recommendations based on expert opinion). Finally, variations in the underlying representation of these recommendations were identified. The library contains 546 individual imaging-related recommendations. Only 15% (16/106) of recommendations from clinical decision rules were grade 5 vs 83% (526/636) from professional society practice guidelines and local best practice guidelines that cited grade 5 studies (Pâ€‰&lt;â€‰.0001). Minor head trauma, pulmonary embolism, and appendicitis were topic areas supported by the highest quality of evidence. Three main variations in underlying representations of recommendations were ""single-decision,"" ""branching,"" and ""score-based."" Most recommendations were grade 5, largely because studies to test and validate many recommendations were absent. Recommendation types vary in amount and complexity and, accordingly, the structure and syntax of statements they generate. However, they can be represented in single-decision, branching, and score-based representations. In a curated evidence library with graded imaging-based recommendations, evidence quality varied widely, with decision rules providing the highest-quality recommendations. The library may be helpful in highlighting evidence gaps, comparing recommendations from varied sources on similar clinical topics, and prioritizing imaging recommendations to inform clinical decision support implementation.","Varada, S.
 and Lacson, R.
 and Raja, A. S.
 and Ip, I. K.
 and Schneider, L.
 and Osterbur, D.
 and Bain, P.
 and Vetrano, N.
 and Cellini, J.
 and Mita, C.
 and Coletti, M.
 and Whelan, J.
 and Khorasani, R.","Varada, Lacson, Raja, Ip, Schneider, Osterbur, Bain, Vetrano, Cellini, Mita, Coletti, Whelan, Khorasani",https://dx.doi.org/10.1093/jamia/ocx092,https://doi.org/10.1093/jamia/ocx092,2021-08-03
498.0,,pubmed,Machine learning in laboratory medicine: waiting for the flood?,Machine learning in laboratory medicine: waiting for the flood?,"This review focuses on machine learning and on how methods and models combining data analytics and artificial intelligence have been applied to laboratory medicine so far. Although still in its infancy, the potential for applying machine learning to laboratory data for both diagnostic and prognostic purposes deserves more attention by the readership of this journal, as well as by physician-scientists who will want to take advantage of this new computer-based support in pathology and laboratory medicine.","This review focuses on machine learning and on how methods and models combining data analytics and artificial intelligence have been applied to laboratory medicine so far. Although still in its infancy, the potential for applying machine learning to laboratory data for both diagnostic and prognostic purposes deserves more attention by the readership of this journal, as well as by physician-scientists who will want to take advantage of this new computer-based support in pathology and laboratory medicine.","Cabitza, F.
 and Banfi, G.","Cabitza, Banfi",https://dx.doi.org/10.1515/cclm-2017-0287,https://doi.org/10.1515/cclm-2017-0287,2021-08-03
517.0,,pubmed,Automatable algorithms to identify nonmedical opioid use using electronic data: a systematic review,Automatable algorithms to identify nonmedical opioid use using electronic data: a systematic review,"Objective: Improved methods to identify nonmedical opioid use can help direct health care resources to individuals who need them. Automated algorithms that use large databases of electronic health care claims or records for surveillance are a potential means to achieve this goal. In this systematic review, we reviewed the utility, attempts at validation, and application of such algorithms to detect nonmedical opioid use. Materials and Methods: We searched PubMed and Embase for articles describing automatable algorithms that used electronic health care claims or records to identify patients or prescribers with likely nonmedical opioid use. We assessed algorithm development, validation, and performance characteristics and the settings where they were applied. Study variability precluded a meta-analysis. Results: Of 15 included algorithms, 10 targeted patients, 2 targeted providers, 2 targeted both, and 1 identified medications with high abuse potential. Most patient-focused algorithms (67%) used prescription drug claims and/or medical claims, with diagnosis codes of substance abuse and/or dependence as the reference standard. Eleven algorithms were developed via regression modeling. Four used natural language processing, data mining, audit analysis, or factor analysis. Discussion: Automated algorithms can facilitate population-level surveillance. However, there is no true gold standard for determining nonmedical opioid use. Users must recognize the implications of identifying false positives and, conversely, false negatives. Few algorithms have been applied in real-world settings. Conclusion: Automated algorithms may facilitate identification of patients and/or providers most likely to need more intensive screening and/or intervention for nonmedical opioid use. Additional implementation research in real-world settings would clarify their utility.","Improved methods to identify nonmedical opioid use can help direct health care resources to individuals who need them. Automated algorithms that use large databases of electronic health care claims or records for surveillance are a potential means to achieve this goal. In this systematic review, we reviewed the utility, attempts at validation, and application of such algorithms to detect nonmedical opioid use. We searched PubMed and Embase for articles describing automatable algorithms that used electronic health care claims or records to identify patients or prescribers with likely nonmedical opioid use. We assessed algorithm development, validation, and performance characteristics and the settings where they were applied. Study variability precluded a meta-analysis. Of 15 included algorithms, 10 targeted patients, 2 targeted providers, 2 targeted both, and 1 identified medications with high abuse potential. Most patient-focused algorithms (67%) used prescription drug claims and/or medical claims, with diagnosis codes of substance abuse and/or dependence as the reference standard. Eleven algorithms were developed via regression modeling. Four used natural language processing, data mining, audit analysis, or factor analysis. Automated algorithms can facilitate population-level surveillance. However, there is no true gold standard for determining nonmedical opioid use. Users must recognize the implications of identifying false positives and, conversely, false negatives. Few algorithms have been applied in real-world settings. Automated algorithms may facilitate identification of patients and/or providers most likely to need more intensive screening and/or intervention for nonmedical opioid use. Additional implementation research in real-world settings would clarify their utility.","Canan, C.
 and Polinski, J. M.
 and Alexander, G. C.
 and Kowal, M. K.
 and Brennan, T. A.
 and Shrank, W. H.","Canan, Polinski, Alexander, Kowal, Brennan, Shrank",https://dx.doi.org/10.1093/jamia/ocx066,https://doi.org/10.1093/jamia/ocx066,2021-08-03
3795.0,,pubmed,Living systematic reviews: 2 Combining human and machine effort,Living systematic reviews: 2 Combining human and machine effort,"New approaches to evidence synthesis, which use human effort and machine automation in mutually reinforcing ways, can enhance the feasibility and sustainability of living systematic reviews. Human effort is a scarce and valuable resource, required when automation is impossible or undesirable, and includes contributions from online communities ('crowds') as well as more conventional contributions from review authors and information specialists. Automation can assist with some systematic review tasks, including searching, eligibility assessment, identification and retrieval of full-text reports, extraction of data, and risk of bias assessment. Workflows can be developed in which human effort and machine automation can each enable the other to operate in more effective and efficient ways, offering substantial enhancement to the productivity of systematic reviews. This paper describes and discusses the potential-and limitations-of new ways of undertaking specific tasks in living systematic reviews, identifying areas where these human/machine 'technologies' are already in use, and where further research and development is needed. While the context is living systematic reviews, many of these enabling technologies apply equally to standard approaches to systematic reviewing.","New approaches to evidence synthesis, which use human effort and machine automation in mutually reinforcing ways, can enhance the feasibility and sustainability of living systematic reviews. Human effort is a scarce and valuable resource, required when automation is impossible or undesirable, and includes contributions from online communities (""crowds"") as well as more conventional contributions from review authors and information specialists. Automation can assist with some systematic review tasks, including searching, eligibility assessment, identification and retrieval of full-text reports, extraction of data, and risk of bias assessment. Workflows can be developed in which human effort and machine automation can each enable the other to operate in more effective and efficient ways, offering substantial enhancement to the productivity of systematic reviews. This paper describes and discusses the potential-and limitations-of new ways of undertaking specific tasks in living systematic reviews, identifying areas where these human/machine ""technologies"" are already in use, and where further research and development is needed. While the context is living systematic reviews, many of these enabling technologies apply equally to standard approaches to systematic reviewing.","Thomas, J.
 and Noel-Storr, A.
 and Marshall, I.
 and Wallace, B.
 and McDonald, S.
 and Mavergames, C.
 and Glasziou, P.
 and Shemilt, I.
 and Synnot, A.
 and Turner, T.
 and Elliott, J.
 and Living Systematic Review, Network","Thomas, Noel-Storr, Marshall, Wallace, McDonald, Mavergames, Glasziou, Shemilt, Synnot, Turner, Elliott, Agoritsas, Hilton, Perron, Akl, Hodder, Pestridge, Albrecht, Horsley, Platt, Armstrong, Nguyen, Plovnick, Arno, Ivers, Quinn, Au, Johnston, Rada, Bagg, Jones, Ravaud, Boden, Kahale, Richter, Boisvert, Keshavarz, Ryan, Brandt, Kolakowsky-Hayner, Salama, Brazinova, Nagraj, Salanti, Buchbinder, Lasserson, Santaguida, Champion, Lawrence, Santesso, Chandler, Les, SchÃ¼nemann, Charidimou, Leucht, Shemilt, Chou, Low, Sherifali, Churchill, Maas, Siemieniuk, Cnossen, MacLehose, Simmonds, Cossi, Macleod, Skoetz, Counotte, Marshall, Soares-Weiser, Craigie, Marshall, Srikanth, Dahm, Martin, Sullivan, Danilkewich, MartÃ­nez GarcÃ­a, Synnot, Danko, Mavergames, Taylor, Donoghue, Maxwell, Thayer, Dressler, McAuley, Thomas, Egan, McDonald, Tritton, Elliott, McKenzie, Tsafnat, Elliott, Meerpohl, Tugwell, Etxeandia, Merner, Turgeon, Featherstone, Mondello, Turner, Foxlee, Morley, van Valkenhoef, Garner, Munafo, Vandvik, Gerrity, Munn, Wallace, Glasziou, Murano, Wallace, Green, Newman, Watts, Grimshaw, Nieuwlaat, Weeks, Gurusamy, Nikolakopoulou, Weigl, Haddaway, Noel-Storr, Wells, Hartling, O'Connor, Wiercioch, Hayden, Page, Wolfenden, Helfand, Pahwa, Yepes NuÃ±ez, Higgins, Pardo, Yost, Hill, Pearson",not available,https://doi.org/10.1016/j.jclinepi.2017.08.011,2021-08-03
824.0,,pubmed,50 years of rational-emotive and cognitive-behavioral therapy: A systematic review and meta-analysis,50 years of rational-emotive and cognitive-behavioral therapy: A systematic review and meta-analysis,"OBJECTIVE: Rational emotive behavior therapy (REBT), introduced by Albert Ellis in the late 1950s, is one of the main pillars of cognitive-behavioral therapy. Existing reviews on REBT are overdue by 10 years or more. We aimed to summarize the effectiveness and efficacy of REBT since its beginnings and investigate the alleged mechanisms of change. METHOD: Systematic search identified 84 articles, out of which 68 provided data for between-group analyses and 39 for within-group analyses. RESULTS: We found a medium effect size of REBT compared to other interventions on outcomes (d = 0.58) and on irrational beliefs (d = 0.70), at posttest. For the within-group analyses, we obtained medium effects for both outcomes (d = 0.56) and irrational beliefs (d = 0.61). Several significant moderators emerged. CONCLUSION: REBT is a sound psychological intervention. Directions for future studies are outlined, stemming from the limitations of existing ones.","Rational emotive behavior therapy (REBT), introduced by Albert Ellis in the late 1950s, is one of the main pillars of cognitive-behavioral therapy. Existing reviews on REBT are overdue by 10 years or more. We aimed to summarize the effectiveness and efficacy of REBT since its beginnings and investigate the alleged mechanisms of change. Systematic search identified 84 articles, out of which 68 provided data for between-group analyses and 39 for within-group analyses. We found a medium effect size of REBT compared to other interventions on outcomes (d = 0.58) and on irrational beliefs (d = 0.70), at posttest. For the within-group analyses, we obtained medium effects for both outcomes (d = 0.56) and irrational beliefs (d = 0.61). Several significant moderators emerged. REBT is a sound psychological intervention. Directions for future studies are outlined, stemming from the limitations of existing ones.","David, D.
 and Cotet, C.
 and Matu, S.
 and Mogoase, C.
 and Stefan, S.","David, Cotet, Matu, Mogoase, Stefan",https://dx.doi.org/10.1002/jclp.22514,https://doi.org/10.1002/jclp.22514,2021-08-03
1790.0,,pubmed,Automated telecommunication interventions to promote adherence to cardio-metabolic medications: meta-analysis of effectiveness and meta-regression of behaviour change techniques,Automated telecommunication interventions to promote adherence to cardio-metabolic medications: meta-analysis of effectiveness and meta-regression of behaviour change techniques,"Automated telecommunication interventions, including short message service and interactive voice response, are increasingly being used to promote adherence to medications prescribed for cardio-metabolic conditions. This systematic review aimed to comprehensively assess the effectiveness of such interventions to support medication adherence, and to identify the behaviour change techniques (BCTs) and other intervention characteristics that are positively associated with greater intervention effectiveness. Meta-analysis of 17 randomised controlled trials showed a small but statistically significant effect on medication adherence, OR = 1.89, 95% CI [1.51, 2.36], I<sup>2</sup> = 89%, N = 25,101. Multivariable meta-regression analysis including eight BCTs explained 88% of the observed variance in effect size (ES). The BCTs 'tailored' and 'information about health consequences' were positively and significantly associated with ES. Future studies could explore whether the inclusion of these and/or additional techniques (e.g., 'implementation intentions') would increase the effect of automated telecommunication interventions, using rigorous designs and objective outcome measures.","Automated telecommunication interventions, including short message service and interactive voice response, are increasingly being used to promote adherence to medications prescribed for cardio-metabolic conditions. This systematic review aimed to comprehensively assess the effectiveness of such interventions to support medication adherence, and to identify the behaviour change techniques (BCTs) and other intervention characteristics that are positively associated with greater intervention effectiveness. Meta-analysis of 17 randomised controlled trials showed a small but statistically significant effect on medication adherence, ORâ€‰=â€‰1.89, 95% CI [1.51, 2.36], I<sup>2</sup>â€‰=â€‰89%, Nâ€‰=â€‰25,101. Multivariable meta-regression analysis including eight BCTs explained 88% of the observed variance in effect size (ES). The BCTs 'tailored' and 'information about health consequences' were positively and significantly associated with ES. Future studies could explore whether the inclusion of these and/or additional techniques (e.g., 'implementation intentions') would increase the effect of automated telecommunication interventions, using rigorous designs and objective outcome measures.","Kassavou, A.
 and Sutton, S.","Kassavou, Sutton",https://dx.doi.org/10.1080/17437199.2017.1365617,https://doi.org/10.1080/17437199.2017.1365617,2021-08-03
3084.0,,pubmed,Ecological niche modeling to determine potential niche of Vaccinia virus: a case only study,Ecological niche modeling to determine potential niche of Vaccinia virus: a case only study,"BACKGROUND: Emerging and understudied pathogens often lack information that most commonly used analytical tools require, such as negative controls or baseline data; thus, new analytical strategies are needed to analyze transmission patterns and drivers of disease emergence. Zoonotic infections with Vaccinia virus (VACV) were first reported in Brazil in 1999, VACV is an emerging zoonotic Orthopoxvirus, which primarily infects dairy cattle and farmers in close contact with infected cows. Prospective studies of emerging pathogens could provide critical data that would inform public health planning and response to outbreaks. By using the location of 87-recorded outbreaks and publicly available bioclimatic data, we demonstrate one such approach. Using an ecological niche model (ENM) algorithm, we identify the environmental conditions under which VACV outbreaks have occurred, and determine additional locations in two affected countries that may be susceptible to transmission. Further, we show how suitability for the virus responds to different levels of various environmental factors and highlight the most important factors in determining its transmission. METHODS: A literature review was performed and the geospatial coordinates of 87 molecularly confirmed VACV outbreaks in Brazil were identified. An ENM was generated using MaxENT software by combining principal component analysis results of 19 bioclim spatial layers, and 25 randomly selected subsets of the original list of 87 outbreaks. RESULTS: The final ENM predicted all areas where Brazilian outbreaks occurred, one out of five of the Colombian outbreak regions and identified new regions within Brazil that are suitable for transmission based on bioclimatic factors. Further, the most important factors in determining transmission suitability are precipitation of the wettest quarter, annual precipitation, mean temperature of the coldest quarter and mean diurnal range. CONCLUSION: The analyses here provide a means by which to study patterns of an emerging infectious disease and identify regions that are potentially suitable for its transmission, in spite of the paucity of high-quality critical data. Policy and methods for the control of infectious diseases often use a reactionary model, addressing diseases only after significant impact on human health has ensued. The methodology used in the present work allows the identification of areas where disease is likely to appear, which could be used for directed intervention.","Emerging and understudied pathogens often lack information that most commonly used analytical tools require, such as negative controls or baseline data; thus, new analytical strategies are needed to analyze transmission patterns and drivers of disease emergence. Zoonotic infections with Vaccinia virus (VACV) were first reported in Brazil in 1999, VACV is an emerging zoonotic Orthopoxvirus, which primarily infects dairy cattle and farmers in close contact with infected cows. Prospective studies of emerging pathogens could provide critical data that would inform public health planning and response to outbreaks. By using the location of 87-recorded outbreaks and publicly available bioclimatic data, we demonstrate one such approach. Using an ecological niche model (ENM) algorithm, we identify the environmental conditions under which VACV outbreaks have occurred, and determine additional locations in two affected countries that may be susceptible to transmission. Further, we show how suitability for the virus responds to different levels of various environmental factors and highlight the most important factors in determining its transmission. A literature review was performed and the geospatial coordinates of 87 molecularly confirmed VACV outbreaks in Brazil were identified. An ENM was generated using MaxENT software by combining principal component analysis results of 19 bioclim spatial layers, and 25 randomly selected subsets of the original list of 87 outbreaks. The final ENM predicted all areas where Brazilian outbreaks occurred, one out of five of the Colombian outbreak regions and identified new regions within Brazil that are suitable for transmission based on bioclimatic factors. Further, the most important factors in determining transmission suitability are precipitation of the wettest quarter, annual precipitation, mean temperature of the coldest quarter and mean diurnal range. The analyses here provide a means by which to study patterns of an emerging infectious disease and identify regions that are potentially suitable for its transmission, in spite of the paucity of high-quality critical data. Policy and methods for the control of infectious diseases often use a reactionary model, addressing diseases only after significant impact on human health has ensued. The methodology used in the present work allows the identification of areas where disease is likely to appear, which could be used for directed intervention.","Quiner, C. A.
 and Nakazawa, Y.","Quiner, Nakazawa",https://dx.doi.org/10.1186/s12942-017-0100-1,https://doi.org/10.1186/s12942-017-0100-1,2021-08-03
3513.0,,pubmed,Increasing value and reducing waste in data extraction for systematic reviews: tracking data in data extraction forms,Increasing value and reducing waste in data extraction for systematic reviews: tracking data in data extraction forms,"Data extraction is one of the most time-consuming tasks in performing a systematic review. Extraction is often onto some sort of form. Sharing completed forms can be used to check quality and accuracy of extraction or for re-cycling data to other researchers for updating. However, validating each piece of extracted data is time-consuming and linking to source problematic. In this methodology paper, we summarize three methods for reporting the location of data in original full-text reports, comparing their advantages and disadvantages.","Data extraction is one of the most time-consuming tasks in performing a systematic review. Extraction is often onto some sort of form. Sharing completed forms can be used to check quality and accuracy of extraction or for re-cycling data to other researchers for updating. However, validating each piece of extracted data is time-consuming and linking to source problematic.In this methodology paper, we summarize three methods for reporting the location of data in original full-text reports, comparing their advantages and disadvantages.","Shokraneh, F.
 and Adams, C. E.","Shokraneh, Adams",not available,https://doi.org/10.1186/s13643-017-0546-z,2021-08-03
2451.0,,pubmed,External Validity of Electronic Sniffers for Automated Recognition of Acute Respiratory Distress Syndrome,External Validity of Electronic Sniffers for Automated Recognition of Acute Respiratory Distress Syndrome,"INTRODUCTION: Automated electronic sniffers may be useful for early detection of acute respiratory distress syndrome (ARDS) for institution of treatment or clinical trial screening. METHODS: In a prospective cohort of 2929 critically ill patients, we retrospectively applied published sniffer algorithms for automated detection of acute lung injury to assess their utility in diagnosis of ARDS in the first 4 ICU days. Radiographic full-text reports were searched for 'edema' OR ('bilateral' AND 'infiltrate') and a more detailed algorithm for descriptions consistent with ARDS. Patients were flagged as possible ARDS if a radiograph met search criteria and had a PaO<sub>2</sub>/FiO<sub>2</sub> or SpO<sub>2</sub>/FiO<sub>2</sub> of 300 or 315, respectively. Test characteristics of the electronic sniffers and clinical suspicion of ARDS were compared to a gold standard of 2-physician adjudicated ARDS. RESULTS: Thirty percent of 2841 patients included in the analysis had gold standard diagnosis of ARDS. The simpler algorithm had sensitivity for ARDS of 78.9%, specificity of 52%, positive predictive value (PPV) of 41%, and negative predictive value (NPV) of 85.3% over the 4-day study period. The more detailed algorithm had sensitivity of 88.2%, specificity of 55.4%, PPV of 45.6%, and NPV of 91.7%. Both algorithms were more sensitive but less specific than clinician suspicion, which had sensitivity of 40.7%, specificity of 94.8%, PPV of 78.2%, and NPV of 77.7%. CONCLUSIONS: Published electronic sniffer algorithms for ARDS may be useful automated screening tools for ARDS and improve on clinical recognition, but they are limited to screening rather than diagnosis because their specificity is poor.","Automated electronic sniffers may be useful for early detection of acute respiratory distress syndrome (ARDS) for institution of treatment or clinical trial screening. In a prospective cohort of 2929 critically ill patients, we retrospectively applied published sniffer algorithms for automated detection of acute lung injury to assess their utility in diagnosis of ARDS in the first 4 ICU days. Radiographic full-text reports were searched for ""edema"" OR (""bilateral"" AND ""infiltrate"") and a more detailed algorithm for descriptions consistent with ARDS. Patients were flagged as possible ARDS if a radiograph met search criteria and had a PaO<sub>2</sub>/FiO<sub>2</sub> or SpO<sub>2</sub>/FiO<sub>2</sub> of 300 or 315, respectively. Test characteristics of the electronic sniffers and clinical suspicion of ARDS were compared to a gold standard of 2-physician adjudicated ARDS. Thirty percent of 2841 patients included in the analysis had gold standard diagnosis of ARDS. The simpler algorithm had sensitivity for ARDS of 78.9%, specificity of 52%, positive predictive value (PPV) of 41%, and negative predictive value (NPV) of 85.3% over the 4-day study period. The more detailed algorithm had sensitivity of 88.2%, specificity of 55.4%, PPV of 45.6%, and NPV of 91.7%. Both algorithms were more sensitive but less specific than clinician suspicion, which had sensitivity of 40.7%, specificity of 94.8%, PPV of 78.2%, and NPV of 77.7%. Published electronic sniffer algorithms for ARDS may be useful automated screening tools for ARDS and improve on clinical recognition, but they are limited to screening rather than diagnosis because their specificity is poor.","McKown, A. C.
 and Brown, R. M.
 and Ware, L. B.
 and Wanderer, J. P.","McKown, Brown, Ware, Wanderer",https://dx.doi.org/10.1177/0885066617720159,https://doi.org/10.1177/0885066617720159,2021-08-03
1962.0,,pubmed,Natural language processing systems for capturing and standardizing unstructured clinical information: A systematic review,Natural language processing systems for capturing and standardizing unstructured clinical information: A systematic review,"We followed a systematic approach based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses to identify existing clinical natural language processing (NLP) systems that generate structured information from unstructured free text. Seven literature databases were searched with a query combining the concepts of natural language processing and structured data capture. Two reviewers screened all records for relevance during two screening phases, and information about clinical NLP systems was collected from the final set of papers. A total of 7149 records (after removing duplicates) were retrieved and screened, and 86 were determined to fit the review criteria. These papers contained information about 71 different clinical NLP systems, which were then analyzed. The NLP systems address a wide variety of important clinical and research tasks. Certain tasks are well addressed by the existing systems, while others remain as open challenges that only a small number of systems attempt, such as extraction of temporal information or normalization of concepts to standard terminologies. This review has identified many NLP systems capable of processing clinical free text and generating structured output, and the information collected and evaluated here will be important for prioritizing development of new approaches for clinical NLP.","We followed a systematic approach based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses to identify existing clinical natural language processing (NLP) systems that generate structured information from unstructured free text. Seven literature databases were searched with a query combining the concepts of natural language processing and structured data capture. Two reviewers screened all records for relevance during two screening phases, and information about clinical NLP systems was collected from the final set of papers. A total of 7149 records (after removing duplicates) were retrieved and screened, and 86 were determined to fit the review criteria. These papers contained information about 71 different clinical NLP systems, which were then analyzed. The NLP systems address a wide variety of important clinical and research tasks. Certain tasks are well addressed by the existing systems, while others remain as open challenges that only a small number of systems attempt, such as extraction of temporal information or normalization of concepts to standard terminologies. This review has identified many NLP systems capable of processing clinical free text and generating structured output, and the information collected and evaluated here will be important for prioritizing development of new approaches for clinical NLP.","Kreimeyer, K.
 and Foster, M.
 and Pandey, A.
 and Arya, N.
 and Halford, G.
 and Jones, S. F.
 and Forshee, R.
 and Walderhaug, M.
 and Botsis, T.","Kreimeyer, Foster, Pandey, Arya, Halford, Jones, Forshee, Walderhaug, Botsis",https://dx.doi.org/10.1016/j.jbi.2017.07.012,https://doi.org/10.1016/j.jbi.2017.07.012,2021-08-03
4121.0,,pubmed,Screening strategies for atrial fibrillation: a systematic review and cost-effectiveness analysis,Screening strategies for atrial fibrillation: a systematic review and cost-effectiveness analysis,"BACKGROUND: Atrial fibrillation (AF) is a common cardiac arrhythmia that increases the risk of thromboembolic events. Anticoagulation therapy to prevent AF-related stroke has been shown to be cost-effective. A national screening programme for AF may prevent AF-related events, but would involve a substantial investment of NHS resources. OBJECTIVES: To conduct a systematic review of the diagnostic test accuracy (DTA) of screening tests for AF, update a systematic review of comparative studies evaluating screening strategies for AF, develop an economic model to compare the cost-effectiveness of different screening strategies and review observational studies of AF screening to provide inputs to the model. DESIGN: Systematic review, meta-analysis and cost-effectiveness analysis. SETTING: Primary care. PARTICIPANTS: Adults. INTERVENTION: Screening strategies, defined by screening test, age at initial and final screens, screening interval and format of screening {systematic opportunistic screening [individuals offered screening if they consult with their general practitioner (GP)] or systematic population screening (when all eligible individuals are invited to screening)}. MAIN OUTCOME MEASURES: Sensitivity, specificity and diagnostic odds ratios; the odds ratio of detecting new AF cases compared with no screening; and the mean incremental net benefit compared with no screening. REVIEW METHODS: Two reviewers screened the search results, extracted data and assessed the risk of bias. A DTA meta-analysis was perfomed, and a decision tree and Markov model was used to evaluate the cost-effectiveness of the screening strategies. RESULTS: Diagnostic test accuracy depended on the screening test and how it was interpreted. In general, the screening tests identified in our review had high sensitivity (> 0.9). Systematic population and systematic opportunistic screening strategies were found to be similarly effective, with an estimated 170 individuals needed to be screened to detect one additional AF case compared with no screening. Systematic opportunistic screening was more likely to be cost-effective than systematic population screening, as long as the uptake of opportunistic screening observed in randomised controlled trials translates to practice. Modified blood pressure monitors, photoplethysmography or nurse pulse palpation were more likely to be cost-effective than other screening tests. A screening strategy with an initial screening age of 65 years and repeated screens every 5 years until age 80 years was likely to be cost-effective, provided that compliance with treatment does not decline with increasing age. CONCLUSIONS: A national screening programme for AF is likely to represent a cost-effective use of resources. Systematic opportunistic screening is more likely to be cost-effective than systematic population screening. Nurse pulse palpation or modified blood pressure monitors would be appropriate screening tests, with confirmation by diagnostic 12-lead electrocardiography interpreted by a trained GP, with referral to a specialist in the case of an unclear diagnosis. Implementation strategies to operationalise uptake of systematic opportunistic screening in primary care should accompany any screening recommendations. LIMITATIONS: Many inputs for the economic model relied on a single trial [the Screening for Atrial Fibrillation in the Elderly (SAFE) study] and DTA results were based on a few studies at high risk of bias/of low applicability. FUTURE WORK: Comparative studies measuring long-term outcomes of screening strategies and DTA studies for new, emerging technologies and to replicate the results for photoplethysmography and GP interpretation of 12-lead electrocardiography in a screening population. STUDY REGISTRATION: This study is registered as PROSPERO CRD42014013739. FUNDING: The National Institute for Health Research Health Technology Assessment programme.","Atrial fibrillation (AF) is a common cardiac arrhythmia that increases the risk of thromboembolic events. Anticoagulation therapy to prevent AF-related stroke has been shown to be cost-effective. A national screening programme for AF may prevent AF-related events, but would involve a substantial investment of NHS resources. To conduct a systematic review of the diagnostic test accuracy (DTA) of screening tests for AF, update a systematic review of comparative studies evaluating screening strategies for AF, develop an economic model to compare the cost-effectiveness of different screening strategies and review observational studies of AF screening to provide inputs to the model. Systematic review, meta-analysis and cost-effectiveness analysis. Primary care. Adults. Screening strategies, defined by screening test, age at initial and final screens, screening interval and format of screening {systematic opportunistic screening [individuals offered screening if they consult with their general practitioner (GP)] or systematic population screening (when all eligible individuals are invited to screening)}. Sensitivity, specificity and diagnostic odds ratios; the odds ratio of detecting new AF cases compared with no screening; and the mean incremental net benefit compared with no screening. Two reviewers screened the search results, extracted data and assessed the risk of bias. A DTA meta-analysis was perfomed, and a decision tree and Markov model was used to evaluate the cost-effectiveness of the screening strategies. Diagnostic test accuracy depended on the screening test and how it was interpreted. In general, the screening tests identified in our review had high sensitivity (&gt;â€‰0.9). Systematic population and systematic opportunistic screening strategies were found to be similarly effective, with an estimated 170 individuals needed to be screened to detect one additional AF case compared with no screening. Systematic opportunistic screening was more likely to be cost-effective than systematic population screening, as long as the uptake of opportunistic screening observed in randomised controlled trials translates to practice. Modified blood pressure monitors, photoplethysmography or nurse pulse palpation were more likely to be cost-effective than other screening tests. A screening strategy with an initial screening age of 65 years and repeated screens every 5 years until age 80 years was likely to be cost-effective, provided that compliance with treatment does not decline with increasing age. A national screening programme for AF is likely to represent a cost-effective use of resources. Systematic opportunistic screening is more likely to be cost-effective than systematic population screening. Nurse pulse palpation or modified blood pressure monitors would be appropriate screening tests, with confirmation by diagnostic 12-lead electrocardiography interpreted by a trained GP, with referral to a specialist in the case of an unclear diagnosis. Implementation strategies to operationalise uptake of systematic opportunistic screening in primary care should accompany any screening recommendations. Many inputs for the economic model relied on a single trial [the Screening for Atrial Fibrillation in the Elderly (SAFE) study] and DTA results were based on a few studies at high risk of bias/of low applicability. Comparative studies measuring long-term outcomes of screening strategies and DTA studies for new, emerging technologies and to replicate the results for photoplethysmography and GP interpretation of 12-lead electrocardiography in a screening population. This study is registered as PROSPERO CRD42014013739. The National Institute for Health Research Health Technology Assessment programme.","Welton, N. J.
 and McAleenan, A.
 and Thom, H. H.
 and Davies, P.
 and Hollingworth, W.
 and Higgins, J. P.
 and Okoli, G.
 and Sterne, J. A.
 and Feder, G.
 and Eaton, D.
 and Hingorani, A.
 and Fawsitt, C.
 and Lobban, T.
 and Bryden, P.
 and Richards, A.
 and Sofat, R.","Welton, McAleenan, Thom, Davies, Hollingworth, Higgins, Okoli, Sterne, Feder, Eaton, Hingorani, Fawsitt, Lobban, Bryden, Richards, Sofat",not available,https://doi.org/10.3310/hta21290,2021-08-03
1155.0,,pubmed,"Multiplex tests to identify gastrointestinal bacteria, viruses and parasites in people with suspected infectious gastroenteritis: a systematic review and economic analysis","Multiplex tests to identify gastrointestinal bacteria, viruses and parasites in people with suspected infectious gastroenteritis: a systematic review and economic analysis","BACKGROUND: Gastroenteritis is a common, transient disorder usually caused by infection and characterised by the acute onset of diarrhoea. Multiplex gastrointestinal pathogen panel (GPP) tests simultaneously identify common bacterial, viral and parasitic pathogens using molecular testing. By providing test results more rapidly than conventional testing methods, GPP tests might positively influence the treatment and management of patients presenting in hospital or in the community. OBJECTIVE: To systematically review the evidence for GPP tests [xTAG<sup> R</sup> (Luminex, Toronto, ON, Canada), FilmArray (BioFire Diagnostics, Salt Lake City, UT, USA) and Faecal Pathogens B (AusDiagnostics, Beaconsfield, NSW, Australia)] and to develop a de novo economic model to compare the cost-effectiveness of GPP tests with conventional testing in England and Wales. DATA SOURCES: Multiple electronic databases including MEDLINE, EMBASE, Web of Science and the Cochrane Database were searched from inception to January 2016 (with supplementary searches of other online resources). REVIEW METHODS: Eligible studies included patients with acute diarrhoea; comparing GPP tests with standard microbiology techniques; and patient, management, test accuracy or cost-effectiveness outcomes. Quality assessment of eligible studies used tailored Quality Assessment of Diagnostic Accuracy Studies-2, Consolidated Health Economic Evaluation Reporting Standards and Philips checklists. The meta-analysis included positive and negative agreement estimated for each pathogen. A de novo decision tree model compared patients managed with GPP testing or comparable coverage with patients managed using conventional tests, within the Public Health England pathway. Economic models included hospital and community management of patients with suspected gastroenteritis. The model estimated costs (in 2014/15 prices) and quality-adjusted life-year losses from a NHS and Personal Social Services perspective. RESULTS: Twenty-three studies informed the review of clinical evidence (17 xTAG, four FilmArray, two xTAG and FilmArray, 0 Faecal Pathogens B). No study provided an adequate reference standard with which to compare the test accuracy of GPP with conventional tests. A meta-analysis (of 10 studies) found considerable heterogeneity; however, GPP testing produces a greater number of pathogen-positive findings than conventional testing. It is unclear whether or not these additional 'positives' are clinically important. The review identified no robust evidence to inform consequent clinical management of patients. There is considerable uncertainty about the cost-effectiveness of GPP panels used to test for suspected infectious gastroenteritis in hospital and community settings. Uncertainties in the model include length of stay, assumptions about false-positive findings and the costs of tests. Although there is potential for cost-effectiveness in both settings, key modelling assumptions need to be verified and model findings remain tentative. LIMITATIONS: No test-treat trials were retrieved. The economic model reflects one pattern of care, which will vary across the NHS. CONCLUSIONS: The systematic review and cost-effectiveness model identify uncertainties about the adoption of GPP tests within the NHS. GPP testing will generally correctly identify pathogens identified by conventional testing; however, these tests also generate considerable additional positive results of uncertain clinical importance. FUTURE WORK: An independent reference standard may not exist to evaluate alternative approaches to testing. A test-treat trial might ascertain whether or not additional GPP 'positives' are clinically important or result in overdiagnoses, whether or not earlier diagnosis leads to earlier discharge in patients and what the health consequences of earlier intervention are. Future work might also consider the public health impact of different testing treatments, as test results form the basis for public health surveillance. STUDY REGISTRATION: This study is registered as PROSPERO CRD2016033320. FUNDING: The National Institute for Health Research Health Technology Assessment programme.","Gastroenteritis is a common, transient disorder usually caused by infection and characterised by the acute onset of diarrhoea. Multiplex gastrointestinal pathogen panel (GPP) tests simultaneously identify common bacterial, viral and parasitic pathogens using molecular testing. By providing test results more rapidly than conventional testing methods, GPP tests might positively influence the treatment and management of patients presenting in hospital or in the community. To systematically review the evidence for GPP tests [xTAG<sup>Â®</sup> (Luminex, Toronto, ON, Canada), FilmArray (BioFire Diagnostics, Salt Lake City, UT, USA) and Faecal Pathogens B (AusDiagnostics, Beaconsfield, NSW, Australia)] and to develop a de novo economic model to compare the cost-effectiveness of GPP tests with conventional testing in England and Wales. Multiple electronic databases including MEDLINE, EMBASE, Web of Science and the Cochrane Database were searched from inception to January 2016 (with supplementary searches of other online resources). Eligible studies included patients with acute diarrhoea; comparing GPP tests with standard microbiology techniques; and patient, management, test accuracy or cost-effectiveness outcomes. Quality assessment of eligible studies used tailored Quality Assessment of Diagnostic Accuracy Studies-2, Consolidated Health Economic Evaluation Reporting Standards and Philips checklists. The meta-analysis included positive and negative agreement estimated for each pathogen. A de novo decision tree model compared patients managed with GPP testing or comparable coverage with patients managed using conventional tests, within the Public Health England pathway. Economic models included hospital and community management of patients with suspected gastroenteritis. The model estimated costs (in 2014/15 prices) and quality-adjusted life-year losses from a NHS and Personal Social Services perspective. Twenty-three studies informed the review of clinical evidence (17 xTAG, four FilmArray, two xTAG and FilmArray, 0 Faecal Pathogens B). No study provided an adequate reference standard with which to compare the test accuracy of GPP with conventional tests. A meta-analysis (of 10 studies) found considerable heterogeneity; however, GPP testing produces a greater number of pathogen-positive findings than conventional testing. It is unclear whether or not these additional 'positives' are clinically important. The review identified no robust evidence to inform consequent clinical management of patients. There is considerable uncertainty about the cost-effectiveness of GPP panels used to test for suspected infectious gastroenteritis in hospital and community settings. Uncertainties in the model include length of stay, assumptions about false-positive findings and the costs of tests. Although there is potential for cost-effectiveness in both settings, key modelling assumptions need to be verified and model findings remain tentative. No test-treat trials were retrieved. The economic model reflects one pattern of care, which will vary across the NHS. The systematic review and cost-effectiveness model identify uncertainties about the adoption of GPP tests within the NHS. GPP testing will generally correctly identify pathogens identified by conventional testing; however, these tests also generate considerable additional positive results of uncertain clinical importance. An independent reference standard may not exist to evaluate alternative approaches to testing. A test-treat trial might ascertain whether or not additional GPP 'positives' are clinically important or result in overdiagnoses, whether or not earlier diagnosis leads to earlier discharge in patients and what the health consequences of earlier intervention are. Future work might also consider the public health impact of different testing treatments, as test results form the basis for public health surveillance. This study is registered as PROSPERO CRD2016033320. The National Institute for Health Research Health Technology Assessment programme.","Freeman, K.
 and Mistry, H.
 and Tsertsvadze, A.
 and Royle, P.
 and McCarthy, N.
 and Taylor-Phillips, S.
 and Manuel, R.
 and Mason, J.","Freeman, Mistry, Tsertsvadze, Royle, McCarthy, Taylor-Phillips, Manuel, Mason",https://dx.doi.org/10.3310/hta21230,https://doi.org/10.3310/hta21230,2021-08-03
3563.0,,pubmed,Asynchronous automated electronic laboratory result notifications: a systematic review,Asynchronous automated electronic laboratory result notifications: a systematic review,"Objective: To systematically review the literature pertaining to asynchronous automated electronic notifications of laboratory results to clinicians. Methods: PubMed, Web of Science, and the Cochrane Collaboration were queried for studies pertaining to automated electronic notifications of laboratory results. A title review was performed on the primary results, with a further abstract review and full review to produce the final set of included articles. Results: The full review included 34 articles, representing 19 institutions. Of these, 19 reported implementation and design of systems, 11 reported quasi-experimental studies, 3 reported a randomized controlled trial, and 1 was a meta-analysis. Twenty-seven articles included alerts of critical results, while 5 focused on urgent notifications and 2 on elective notifications. There was considerable variability in clinical setting, system implementation, and results presented. Conclusion: Several asynchronous automated electronic notification systems for laboratory results have been evaluated, most from >10 years ago. Further research on the effect of notifications on clinicians as well as the use of modern electronic health records and new methods of notification is warranted to determine their effects on workflow and clinical outcomes.","To systematically review the literature pertaining to asynchronous automated electronic notifications of laboratory results to clinicians. PubMed, Web of Science, and the Cochrane Collaboration were queried for studies pertaining to automated electronic notifications of laboratory results. A title review was performed on the primary results, with a further abstract review and full review to produce the final set of included articles. The full review included 34 articles, representing 19 institutions. Of these, 19 reported implementation and design of systems, 11 reported quasi-experimental studies, 3 reported a randomized controlled trial, and 1 was a meta-analysis. Twenty-seven articles included alerts of critical results, while 5 focused on urgent notifications and 2 on elective notifications. There was considerable variability in clinical setting, system implementation, and results presented. Several asynchronous automated electronic notification systems for laboratory results have been evaluated, most from &gt;10 years ago. Further research on the effect of notifications on clinicians as well as the use of modern electronic health records and new methods of notification is warranted to determine their effects on workflow and clinical outcomes.","Slovis, B. H.
 and Nahass, T. A.
 and Salmasian, H.
 and Kuperman, G.
 and Vawdrey, D. K.","Slovis, Nahass, Salmasian, Kuperman, Vawdrey",https://dx.doi.org/10.1093/jamia/ocx047,https://doi.org/10.1093/jamia/ocx047,2021-08-03
3065.0,,pubmed,Treatment of Low Bone Density or Osteoporosis to Prevent Fractures in Men and Women: A Clinical Practice Guideline Update From the American College of Physicians,Treatment of Low Bone Density or Osteoporosis to Prevent Fractures in Men and Women: A Clinical Practice Guideline Update From the American College of Physicians,"Description: This guideline updates the 2008 American College of Physicians (ACP) recommendations on treatment of low bone density and osteoporosis to prevent fractures in men and women. This guideline is endorsed by the American Academy of Family Physicians. Methods: The ACP Clinical Guidelines Committee based these recommendations on a systematic review of randomized controlled trials; systematic reviews; large observational studies (for adverse events); and case reports (for rare events) that were published between 2 January 2005 and 3 June 2011. The review was updated to July 2016 by using a machine-learning method, and a limited update to October 2016 was done. Clinical outcomes evaluated were fractures and adverse events. This guideline focuses on the comparative benefits and risks of short- and long-term pharmacologic treatments for low bone density, including pharmaceutical prescriptions, calcium, vitamin D, and estrogen. Evidence was graded according to the GRADE (Grading of Recommendations Assessment, Development and Evaluation) system. Target Audience and Patient Population: The target audience for this guideline includes all clinicians. The target patient population includes men and women with low bone density and osteoporosis. Recommendation 1: ACP recommends that clinicians offer pharmacologic treatment with alendronate, risedronate, zoledronic acid, or denosumab to reduce the risk for hip and vertebral fractures in women who have known osteoporosis. (Grade: strong recommendation; high-quality evidence). Recommendation 2: ACP recommends that clinicians treat osteoporotic women with pharmacologic therapy for 5 years. (Grade: weak recommendation; low-quality evidence). Recommendation 3: ACP recommends that clinicians offer pharmacologic treatment with bisphosphonates to reduce the risk for vertebral fracture in men who have clinically recognized osteoporosis. (Grade: weak recommendation; low-quality evidence). Recommendation 4: ACP recommends against bone density monitoring during the 5-year pharmacologic treatment period for osteoporosis in women. (Grade: weak recommendation; low-quality evidence). Recommendation 5: ACP recommends against using menopausal estrogen therapy or menopausal estrogen plus progestogen therapy or raloxifene for the treatment of osteoporosis in women. (Grade: strong recommendation; moderate-quality evidence). Recommendation 6: ACP recommends that clinicians should make the decision whether to treat osteopenic women 65 years of age or older who are at a high risk for fracture based on a discussion of patient preferences, fracture risk profile, and benefits, harms, and costs of medications. (Grade: weak recommendation; low-quality evidence).","This guideline updates the 2008 American College of Physicians (ACP) recommendations on treatment of low bone density and osteoporosis to prevent fractures in men and women. This guideline is endorsed by the American Academy of Family Physicians. The ACP Clinical Guidelines Committee based these recommendations on a systematic review of randomized controlled trials; systematic reviews; large observational studies (for adverse events); and case reports (for rare events) that were published between 2 January 2005 and 3 June 2011. The review was updated to July 2016 by using a machine-learning method, and a limited update to October 2016 was done. Clinical outcomes evaluated were fractures and adverse events. This guideline focuses on the comparative benefits and risks of short- and long-term pharmacologic treatments for low bone density, including pharmaceutical prescriptions, calcium, vitamin D, and estrogen. Evidence was graded according to the GRADE (Grading of Recommendations Assessment, Development and Evaluation) system. The target audience for this guideline includes all clinicians. The target patient population includes men and women with low bone density and osteoporosis. ACP recommends that clinicians offer pharmacologic treatment with alendronate, risedronate, zoledronic acid, or denosumab to reduce the risk for hip and vertebral fractures in women who have known osteoporosis. (Grade: strong recommendation; high-quality evidence). ACP recommends that clinicians treat osteoporotic women with pharmacologic therapy for 5 years. (Grade: weak recommendation; low-quality evidence). ACP recommends that clinicians offer pharmacologic treatment with bisphosphonates to reduce the risk for vertebral fracture in men who have clinically recognized osteoporosis. (Grade: weak recommendation; low-quality evidence). ACP recommends against bone density monitoring during the 5-year pharmacologic treatment period for osteoporosis in women. (Grade: weak recommendation; low-quality evidence). ACP recommends against using menopausal estrogen therapy or menopausal estrogen plus progestogen therapy or raloxifene for the treatment of osteoporosis in women. (Grade: strong recommendation; moderate-quality evidence). ACP recommends that clinicians should make the decision whether to treat osteopenic women 65 years of age or older who are at a high risk for fracture based on a discussion of patient preferences, fracture risk profile, and benefits, harms, and costs of medications. (Grade: weak recommendation; low-quality evidence).","Qaseem, A.
 and Forciea, M. A.
 and McLean, R. M.
 and Denberg, T. D.
 and Clinical Guidelines Committee of the American College of, Physicians","Qaseem, Forciea, McLean, Denberg, Barry, Cooke, Fitterman, Harris, Humphrey, Kansagara, McLean, Mir, SchÃ¼nemann, Denberg, Forciea, Barry, Cooke, Fitterman, Harris, Humphrey, Kansagara, McLean, Mir, SchÃ¼nemann, Wilt",https://dx.doi.org/10.7326/M15-1361,https://doi.org/10.7326/M15-1361,2021-08-03
170.0,,pubmed,Collaborative writing applications in healthcare: effects on professional practice and healthcare outcomes,Collaborative writing applications in healthcare: effects on professional practice and healthcare outcomes,"BACKGROUND: Collaborative writing applications (CWAs), such as wikis and Google Documents, hold the potential to improve the use of evidence in both public health and healthcare. Although a growing body of literature indicates that CWAs could have positive effects on healthcare, such as improved collaboration, behavioural change, learning, knowledge management, and adaptation of knowledge to local context, this has never been assessed systematically. Moreover, several questions regarding safety, reliability, and legal aspects exist. OBJECTIVES: The objectives of this review were to (1) assess the effects of the use of CWAs on process (including the behaviour of healthcare professionals) and patient outcomes, (2) critically appraise and summarise current evidence on the use of resources, costs, and cost-effectiveness associated with CWAs to improve professional practices and patient outcomes, and (3) explore the effects of different CWA features (e.g. open versus closed) and different implementation factors (e.g. the presence of a moderator) on process and patient outcomes. SEARCH METHODS: We searched CENTRAL, MEDLINE, Embase, and 11 other electronic databases. We searched the grey literature, two trial registries, CWA websites, individual journals, and conference proceedings. We also contacted authors and experts in the field. We did not apply date or language limits. We searched for published literature to August 2016, and grey literature to September 2015. SELECTION CRITERIA: We included randomised controlled trials (RCTs), non-randomised controlled trials (NRCTs), controlled before-and-after (CBA) studies, interrupted time series (ITS) studies, and repeated measures studies (RMS), in which CWAs were used as an intervention to improve the process of care, patient outcomes, or healthcare costs. DATA COLLECTION AND ANALYSIS: Teams of two review authors independently assessed the eligibility of studies. Disagreements were resolved by discussion, and when consensus was not reached, a third review author was consulted. MAIN RESULTS: We screened 11,993 studies identified from the electronic database searches and 346 studies from grey literature sources. We analysed the full text of 99 studies. None of the studies met the eligibility criteria; two potentially relevant studies are ongoing. AUTHORS' CONCLUSIONS: While there is a high number of published studies about CWAs, indicating that this is an active field of research, additional studies using rigorous experimental designs are needed to assess their impact and cost-effectiveness on process and patient outcomes.","Collaborative writing applications (CWAs), such as wikis and Google Documents, hold the potential to improve the use of evidence in both public health and healthcare. Although a growing body of literature indicates that CWAs could have positive effects on healthcare, such as improved collaboration, behavioural change, learning, knowledge management, and adaptation of knowledge to local context, this has never been assessed systematically. Moreover, several questions regarding safety, reliability, and legal aspects exist. The objectives of this review were to (1) assess the effects of the use of CWAs on process (including the behaviour of healthcare professionals) and patient outcomes, (2) critically appraise and summarise current evidence on the use of resources, costs, and cost-effectiveness associated with CWAs to improve professional practices and patient outcomes, and (3) explore the effects of different CWA features (e.g. open versus closed) and different implementation factors (e.g. the presence of a moderator) on process and patient outcomes. We searched CENTRAL, MEDLINE, Embase, and 11 other electronic databases. We searched the grey literature, two trial registries, CWA websites, individual journals, and conference proceedings. We also contacted authors and experts in the field. We did not apply date or language limits. We searched for published literature to August 2016, and grey literature to September 2015. We included randomised controlled trials (RCTs), non-randomised controlled trials (NRCTs), controlled before-and-after (CBA) studies, interrupted time series (ITS) studies, and repeated measures studies (RMS), in which CWAs were used as an intervention to improve the process of care, patient outcomes, or healthcare costs. Teams of two review authors independently assessed the eligibility of studies. Disagreements were resolved by discussion, and when consensus was not reached, a third review author was consulted. We screened 11,993 studies identified from the electronic database searches and 346 studies from grey literature sources. We analysed the full text of 99 studies. None of the studies met the eligibility criteria; two potentially relevant studies are ongoing. While there is a high number of published studies about CWAs, indicating that this is an active field of research, additional studies using rigorous experimental designs are needed to assess their impact and cost-effectiveness on process and patient outcomes.","Archambault, P. M.
 and van de Belt, T. H.
 and Kuziemsky, C.
 and Plaisance, A.
 and Dupuis, A.
 and McGinn, C. A.
 and Francois, R.
 and Gagnon, M. P.
 and Turgeon, A. F.
 and Horsley, T.
 and Witteman, W.
 and Poitras, J.
 and Lapointe, J.
 and Brand, K.
 and Lachaine, J.
 and Legare, F.","Archambault, van de Belt, Kuziemsky, Plaisance, Dupuis, McGinn, Francois, Gagnon, Turgeon, Horsley, Witteman, Poitras, Lapointe, Brand, Lachaine, LÃ©garÃ©",https://dx.doi.org/10.1002/14651858.CD011388.pub2,https://doi.org/10.1002/14651858.CD011388.pub2,2021-08-03
3287.0,,pubmed,The need to approximate the use-case in clinical machine learning,The need to approximate the use-case in clinical machine learning,"The availability of smartphone and wearable sensor technology is leading to a rapid accumulation of human subject data, and machine learning is emerging as a technique to map those data into clinical predictions. As machine learning algorithms are increasingly used to support clinical decision making, it is vital to reliably quantify their prediction accuracy. Cross-validation (CV) is the standard approach where the accuracy of such algorithms is evaluated on part of the data the algorithm has not seen during training. However, for this procedure to be meaningful, the relationship between the training and the validation set should mimic the relationship between the training set and the dataset expected for the clinical use. Here we compared two popular CV methods: record-wise and subject-wise. While the subject-wise method mirrors the clinically relevant use-case scenario of diagnosis in newly recruited subjects, the record-wise strategy has no such interpretation. Using both a publicly available dataset and a simulation, we found that record-wise CV often massively overestimates the prediction accuracy of the algorithms. We also conducted a systematic review of the relevant literature, and found that this overly optimistic method was used by almost half of the retrieved studies that used accelerometers, wearable sensors, or smartphones to predict clinical outcomes. As we move towards an era of machine learning-based diagnosis and treatment, using proper methods to evaluate their accuracy is crucial, as inaccurate results can mislead both clinicians and data scientists.","The availability of smartphone and wearable sensor technology is leading to a rapid accumulation of human subject data, and machine learning is emerging as a technique to map those data into clinical predictions. As machine learning algorithms are increasingly used to support clinical decision making, it is vital to reliably quantify their prediction accuracy. Cross-validation (CV) is the standard approach where the accuracy of such algorithms is evaluated on part of the data the algorithm has not seen during training. However, for this procedure to be meaningful, the relationship between the training and the validation set should mimic the relationship between the training set and the dataset expected for the clinical use. Here we compared two popular CV methods: record-wise and subject-wise. While the subject-wise method mirrors the clinically relevant use-case scenario of diagnosis in newly recruited subjects, the record-wise strategy has no such interpretation. Using both a publicly available dataset and a simulation, we found that record-wise CV often massively overestimates the prediction accuracy of the algorithms. We also conducted a systematic review of the relevant literature, and found that this overly optimistic method was used by almost half of the retrieved studies that used accelerometers, wearable sensors, or smartphones to predict clinical outcomes. As we move towards an era of machine learning-based diagnosis and treatment, using proper methods to evaluate their accuracy is crucial, as inaccurate results can mislead both clinicians and data scientists.","Saeb, S.
 and Lonini, L.
 and Jayaraman, A.
 and Mohr, D. C.
 and Kording, K. P.","Saeb, Lonini, Jayaraman, Mohr, Kording",https://dx.doi.org/10.1093/gigascience/gix019,https://doi.org/10.1093/gigascience/gix019,2021-08-03
2045.0,,pubmed,Text mining for improved exposure assessment,Text mining for improved exposure assessment,"Chemical exposure assessments are based on information collected via different methods, such as biomonitoring, personal monitoring, environmental monitoring and questionnaires. The vast amount of chemical-specific exposure information available from web-based databases, such as PubMed, is undoubtedly a great asset to the scientific community. However, manual retrieval of relevant published information is an extremely time consuming task and overviewing the data is nearly impossible. Here, we present the development of an automatic classifier for chemical exposure information. First, nearly 3700 abstracts were manually annotated by an expert in exposure sciences according to a taxonomy exclusively created for exposure information. Natural Language Processing (NLP) techniques were used to extract semantic and syntactic features relevant to chemical exposure text. Using these features, we trained a supervised machine learning algorithm to automatically classify PubMed abstracts according to the exposure taxonomy. The resulting classifier demonstrates good performance in the intrinsic evaluation. We also show that the classifier improves information retrieval of chemical exposure data compared to keyword-based PubMed searches. Case studies demonstrate that the classifier can be used to assist researchers by facilitating information retrieval and classification, enabling data gap recognition and overviewing available scientific literature using chemical-specific publication profiles. Finally, we identify challenges to be addressed in future development of the system.","Chemical exposure assessments are based on information collected via different methods, such as biomonitoring, personal monitoring, environmental monitoring and questionnaires. The vast amount of chemical-specific exposure information available from web-based databases, such as PubMed, is undoubtedly a great asset to the scientific community. However, manual retrieval of relevant published information is an extremely time consuming task and overviewing the data is nearly impossible. Here, we present the development of an automatic classifier for chemical exposure information. First, nearly 3700 abstracts were manually annotated by an expert in exposure sciences according to a taxonomy exclusively created for exposure information. Natural Language Processing (NLP) techniques were used to extract semantic and syntactic features relevant to chemical exposure text. Using these features, we trained a supervised machine learning algorithm to automatically classify PubMed abstracts according to the exposure taxonomy. The resulting classifier demonstrates good performance in the intrinsic evaluation. We also show that the classifier improves information retrieval of chemical exposure data compared to keyword-based PubMed searches. Case studies demonstrate that the classifier can be used to assist researchers by facilitating information retrieval and classification, enabling data gap recognition and overviewing available scientific literature using chemical-specific publication profiles. Finally, we identify challenges to be addressed in future development of the system.","Larsson, K.
 and Baker, S.
 and Silins, I.
 and Guo, Y.
 and Stenius, U.
 and Korhonen, A.
 and Berglund, M.","Larsson, Baker, Silins, Guo, Stenius, Korhonen, Berglund",https://dx.doi.org/10.1371/journal.pone.0173132,https://doi.org/10.1371/journal.pone.0173132,2021-08-03
440.0,,pubmed,Non anti-coagulant factors associated with filter life in continuous renal replacement therapy (CRRT): a systematic review and meta-analysis,Non anti-coagulant factors associated with filter life in continuous renal replacement therapy (CRRT): a systematic review and meta-analysis,"BACKGROUND: Optimising filter life and performance efficiency in continuous renal replacement therapy has been a focus of considerable recent research. Larger high quality studies have predominantly focussed on optimal anticoagulation however CRRT is complex and filter life is also affected by vascular access, circuit and management factors. We performed a systematic search of the literature to identify and quantify the effect of vascular access, circuit and patient factors that affect filter life and presented the results as a meta-analysis. METHODS: A systematic review and meta-analysis was performed by searching Pubmed (MEDLINE) and Ovid EMBASE libraries from inception to 29<sup>th</sup> February 2016 for all studies with a comparator or independent variable relating to CRRT circuits and reporting filter life. Included studies documented filter life in hours with a comparator other than anti-coagulation intervention. All studies comparing anticoagulation interventions were searched for regression or hazard models pertaining to other sources of variation in filter life. RESULTS: Eight hundred nineteen abstracts were identified of which 364 were selected for full text analysis. 24 presented data on patient modifiers of circuit life, 14 on vascular access modifiers and 34 on circuit related factors. Risk of bias was high and findings are hypothesis generating. Ranking of vascular access site by filter longevity favours: tunnelled semi-permanent catheters, femoral, internal jugular and subclavian last. There is inconsistency in the difference reported between femoral and jugular catheters. Amongst published literature, modality of CRRT consistently favoured continuous veno-venous haemodiafiltration (CVVHD-F) with an associated 44% lower failure rate compared to CVVH. There was a trend favouring higher blood flow rates. There is insufficient data to determine advantages of haemofilter membranes. Patient factors associated with a statistically significant worsening of filter life included mechanical ventilation, elevated SOFA or LOD score, elevations in ionized calcium, elevated platelet count, red cell transfusion, platelet factor 4 (PF-4) antibodies, and elevated fibrinogen. Majority of studies are observational or report circuit factors in sub-analysis. Risk of bias is high and findings require targeted investigations to confirm. CONCLUSION: The interaction of patient, pathology, anticoagulation, vascular access, circuit and staff factors contribute to CRRT filter life. There remains an ambiguity from published data as to which site and side should be the first choice for vascular access placement and what interaction this has with patient factors and timing. Early consideration of tunnelled semi-permanent access may provide optimal filter life if longer periods of CRRT are anticipated. There remains an absence of robust evidence outside of anti-coagulation strategies despite over 20 years of therapy delivery however trends favour CVVHD-F over CVVH.","Optimising filter life and performance efficiency in continuous renal replacement therapy has been a focus of considerable recent research. Larger high quality studies have predominantly focussed on optimal anticoagulation however CRRT is complex and filter life is also affected by vascular access, circuit and management factors. We performed a systematic search of the literature to identify and quantify the effect of vascular access, circuit and patient factors that affect filter life and presented the results as a meta-analysis. A systematic review and meta-analysis was performed by searching Pubmed (MEDLINE) and Ovid EMBASE libraries from inception to 29<sup>th</sup> February 2016 for all studies with a comparator or independent variable relating to CRRT circuits and reporting filter life. Included studies documented filter life in hours with a comparator other than anti-coagulation intervention. All studies comparing anticoagulation interventions were searched for regression or hazard models pertaining to other sources of variation in filter life. Eight hundred nineteen abstracts were identified of which 364 were selected for full text analysis. 24 presented data on patient modifiers of circuit life, 14 on vascular access modifiers and 34 on circuit related factors. Risk of bias was high and findings are hypothesis generating. Ranking of vascular access site by filter longevity favours: tunnelled semi-permanent catheters, femoral, internal jugular and subclavian last. There is inconsistency in the difference reported between femoral and jugular catheters. Amongst published literature, modality of CRRT consistently favoured continuous veno-venous haemodiafiltration (CVVHD-F) with an associated 44% lower failure rate compared to CVVH. There was a trend favouring higher blood flow rates. There is insufficient data to determine advantages of haemofilter membranes. Patient factors associated with a statistically significant worsening of filter life included mechanical ventilation, elevated SOFA or LOD score, elevations in ionized calcium, elevated platelet count, red cell transfusion, platelet factor 4 (PF-4) antibodies, and elevated fibrinogen. Majority of studies are observational or report circuit factors in sub-analysis. Risk of bias is high and findings require targeted investigations to confirm. The interaction of patient, pathology, anticoagulation, vascular access, circuit and staff factors contribute to CRRT filter life. There remains an ambiguity from published data as to which site and side should be the first choice for vascular access placement and what interaction this has with patient factors and timing. Early consideration of tunnelled semi-permanent access may provide optimal filter life if longer periods of CRRT are anticipated. There remains an absence of robust evidence outside of anti-coagulation strategies despite over 20Â years of therapy delivery however trends favour CVVHD-F over CVVH.","Brain, M.
 and Winson, E.
 and Roodenburg, O.
 and McNeil, J.","Brain, Winson, Roodenburg, McNeil",https://dx.doi.org/10.1186/s12882-017-0445-5,https://doi.org/10.1186/s12882-017-0445-5,2021-08-03
878.0,,pubmed,MetaMap Lite: an evaluation of a new Java implementation of MetaMap,MetaMap Lite: an evaluation of a new Java implementation of MetaMap,"MetaMap is a widely used named entity recognition tool that identifies concepts from the Unified Medical Language System Metathesaurus in text. This study presents MetaMap Lite, an implementation of some of the basic MetaMap functions in Java. On several collections of biomedical literature and clinical text, MetaMap Lite demonstrated real-time speed and precision, recall, and F1 scores comparable to or exceeding those of MetaMap and other popular biomedical text processing tools, clinical Text Analysis and Knowledge Extraction System (cTAKES) and DNorm.","MetaMap is a widely used named entity recognition tool that identifies concepts from the Unified Medical Language System Metathesaurus in text. This study presents MetaMap Lite, an implementation of some of the basic MetaMap functions in Java. On several collections of biomedical literature and clinical text, MetaMap Lite demonstrated real-time speed and precision, recall, and F1 scores comparable to or exceeding those of MetaMap and other popular biomedical text processing tools, clinical Text Analysis and Knowledge Extraction System (cTAKES) and DNorm.","Demner-Fushman, D.
 and Rogers, W. J.
 and Aronson, A. R.","Demner-Fushman, Rogers, Aronson",not available,https://doi.org/10.1093/jamia/ocw177,2021-08-03
2806.0,,pubmed,Rayyan-a web and mobile app for systematic reviews,Rayyan-a web and mobile app for systematic reviews,"BACKGROUND: Synthesis of multiple randomized controlled trials (RCTs) in a systematic review can summarize the effects of individual outcomes and provide numerical answers about the effectiveness of interventions. Filtering of searches is time consuming, and no single method fulfills the principal requirements of speed with accuracy. Automation of systematic reviews is driven by a necessity to expedite the availability of current best evidence for policy and clinical decision-making. We developed Rayyan ( http://rayyan.qcri.org ), a free web and mobile app, that helps expedite the initial screening of abstracts and titles using a process of semi-automation while incorporating a high level of usability. For the beta testing phase, we used two published Cochrane reviews in which included studies had been selected manually. Their searches, with 1030 records and 273 records, were uploaded to Rayyan. Different features of Rayyan were tested using these two reviews. We also conducted a survey of Rayyan's users and collected feedback through a built-in feature. RESULTS: Pilot testing of Rayyan focused on usability, accuracy against manual methods, and the added value of the prediction feature. The 'taster' review (273 records) allowed a quick overview of Rayyan for early comments on usability. The second review (1030 records) required several iterations to identify the previously identified 11 trials. The 'suggestions' and 'hints,' based on the 'prediction model,' appeared as testing progressed beyond five included studies. Post rollout user experiences and a reflexive response by the developers enabled real-time modifications and improvements. The survey respondents reported 40% average time savings when using Rayyan compared to others tools, with 34% of the respondents reporting more than 50% time savings. In addition, around 75% of the respondents mentioned that screening and labeling studies as well as collaborating on reviews to be the two most important features of Rayyan. As of November 2016, Rayyan users exceed 2000 from over 60 countries conducting hundreds of reviews totaling more than 1.6M citations. Feedback from users, obtained mostly through the app web site and a recent survey, has highlighted the ease in exploration of searches, the time saved, and simplicity in sharing and comparing include-exclude decisions. The strongest features of the app, identified and reported in user feedback, were its ability to help in screening and collaboration as well as the time savings it affords to users. CONCLUSIONS: Rayyan is responsive and intuitive in use with significant potential to lighten the load of reviewers.","Synthesis of multiple randomized controlled trials (RCTs) in a systematic review can summarize the effects of individual outcomes and provide numerical answers about the effectiveness of interventions. Filtering of searches is time consuming, and no single method fulfills the principal requirements of speed with accuracy. Automation of systematic reviews is driven by a necessity to expedite the availability of current best evidence for policy and clinical decision-making. We developed Rayyan ( http://rayyan.qcri.org ), a free web and mobile app, that helps expedite the initial screening of abstracts and titles using a process of semi-automation while incorporating a high level of usability. For the beta testing phase, we used two published Cochrane reviews in which included studies had been selected manually. Their searches, with 1030 records and 273 records, were uploaded to Rayyan. Different features of Rayyan were tested using these two reviews. We also conducted a survey of Rayyan's users and collected feedback through a built-in feature. Pilot testing of Rayyan focused on usability, accuracy against manual methods, and the added value of the prediction feature. The ""taster"" review (273 records) allowed a quick overview of Rayyan for early comments on usability. The second review (1030 records) required several iterations to identify the previously identified 11 trials. The ""suggestions"" and ""hints,"" based on the ""prediction model,"" appeared as testing progressed beyond five included studies. Post rollout user experiences and a reflexive response by the developers enabled real-time modifications and improvements. The survey respondents reported 40% average time savings when using Rayyan compared to others tools, with 34% of the respondents reporting more than 50% time savings. In addition, around 75% of the respondents mentioned that screening and labeling studies as well as collaborating on reviews to be the two most important features of Rayyan. As of November 2016, Rayyan users exceed 2000 from over 60 countries conducting hundreds of reviews totaling more than 1.6M citations. Feedback from users, obtained mostly through the app web site and a recent survey, has highlighted the ease in exploration of searches, the time saved, and simplicity in sharing and comparing include-exclude decisions. The strongest features of the app, identified and reported in user feedback, were its ability to help in screening and collaboration as well as the time savings it affords to users. Rayyan is responsive and intuitive in use with significant potential to lighten the load of reviewers.","Ouzzani, M.
 and Hammady, H.
 and Fedorowicz, Z.
 and Elmagarmid, A.","Ouzzani, Hammady, Fedorowicz, Elmagarmid",not available,https://doi.org/10.1186/s13643-016-0384-4,2021-08-03
1064.0,,pubmed,"The identification and treatment of women with hyperglycaemia in pregnancy: an analysis of individual participant data, systematic reviews, meta-analyses and an economic evaluation","The identification and treatment of women with hyperglycaemia in pregnancy: an analysis of individual participant data, systematic reviews, meta-analyses and an economic evaluation","BACKGROUND: Gestational diabetes mellitus (GDM) is associated with a higher risk of important adverse outcomes. Practice varies and the best strategy for identifying and treating GDM is unclear. AIM: To estimate the clinical effectiveness and cost-effectiveness of strategies for identifying and treating women with GDM. METHODS: We analysed individual participant data (IPD) from birth cohorts and conducted systematic reviews to estimate the association of maternal glucose levels with adverse perinatal outcomes; GDM prevalence; maternal characteristics/risk factors for GDM; and the effectiveness and costs of treatments. The cost-effectiveness of various strategies was estimated using a decision tree model, along with a value of information analysis to assess where future research might be worthwhile. Detailed systematic searches of MEDLINE<sup> R</sup> and MEDLINE In-Process & Other Non-Indexed Citations<sup> R</sup>, EMBASE, Cumulative Index to Nursing and Allied Health Literature Plus, Cochrane Central Register of Controlled Trials, Cochrane Database of Systematic Reviews, Database of Abstracts of Reviews of Effects, Health Technology Assessment database, NHS Economic Evaluation Database, Maternity and Infant Care database and the Cochrane Methodology Register were undertaken from inception up to October 2014. RESULTS: We identified 58 studies examining maternal glucose levels and outcome associations. Analyses using IPD alone and the systematic review demonstrated continuous linear associations of fasting and post-load glucose levels with adverse perinatal outcomes, with no clear threshold below which there is no increased risk. Using IPD, we estimated glucose thresholds to identify infants at high risk of being born large for gestational age or with high adiposity; for South Asian (SA) women these thresholds were fasting and post-load glucose levels of 5.2 mmol/l and 7.2 mmol/l, respectively and for white British (WB) women they were 5.4 and 7.5 mmol/l, respectively. Prevalence using IPD and published data varied from 1.2% to 24.2% (depending on criteria and population) and was consistently two to three times higher in SA women than in WB women. Lowering thresholds to identify GDM, particularly in women of SA origin, identifies more women at risk, but increases costs. Maternal characteristics did not accurately identify women with GDM; there was limited evidence that in some populations risk factors may be useful for identifying low-risk women. Dietary modification additional to routine care reduced the risk of most adverse perinatal outcomes. Metformin (Glucophage,<sup> R</sup> Teva UK Ltd, Eastbourne, UK) and insulin were more effective than glibenclamide (Aurobindo Pharma - Milpharm Ltd, South Ruislip, Middlesex, UK). For all strategies to identify and treat GDM, the costs exceeded the health benefits. A policy of no screening/testing or treatment offered the maximum expected net monetary benefit (NMB) of 1184 at a cost-effectiveness threshold of 20,000 per quality-adjusted life-year (QALY). The NMB for the three best-performing strategies in each category (screen only, then treat; screen, test, then treat; and test all, then treat) ranged between -1197 and -1210. Further research to reduce uncertainty around potential longer-term benefits for the mothers and offspring, find ways of improving the accuracy of identifying women with GDM, and reduce costs of identification and treatment would be worthwhile. LIMITATIONS: We did not have access to IPD from populations in the UK outside of England. Few observational studies reported longer-term associations, and treatment trials have generally reported only perinatal outcomes. CONCLUSIONS: Using the national standard cost-effectiveness threshold of 20,000 per QALY it is not cost-effective to routinely identify pregnant women for treatment of hyperglycaemia. Further research to provide evidence on longer-term outcomes, and more cost-effective ways to detect and treat GDM, would be valuable. STUDY REGISTRATION: This study is registered as PROSPERO CRD42013004608. FUNDING: The National Institute for Health Research Health Technology Assessment programme.","Gestational diabetes mellitus (GDM) is associated with a higher risk of important adverse outcomes. Practice varies and the best strategy for identifying and treating GDM is unclear. To estimate the clinical effectiveness and cost-effectiveness of strategies for identifying and treating women with GDM. We analysed individual participant data (IPD) from birth cohorts and conducted systematic reviews to estimate the association of maternal glucose levels with adverse perinatal outcomes; GDM prevalence; maternal characteristics/risk factors for GDM; and the effectiveness and costs of treatments. The cost-effectiveness of various strategies was estimated using a decision tree model, along with a value of information analysis to assess where future research might be worthwhile. Detailed systematic searches of MEDLINE<sup>Â®</sup> and MEDLINE In-Process &amp; Other Non-Indexed Citations<sup>Â®</sup>, EMBASE, Cumulative Index to Nursing and Allied Health Literature Plus, Cochrane Central Register of Controlled Trials, Cochrane Database of Systematic Reviews, Database of Abstracts of Reviews of Effects, Health Technology Assessment database, NHS Economic Evaluation Database, Maternity and Infant Care database and the Cochrane Methodology Register were undertaken from inception up to October 2014. We identified 58 studies examining maternal glucose levels and outcome associations. Analyses using IPD alone and the systematic review demonstrated continuous linear associations of fasting and post-load glucose levels with adverse perinatal outcomes, with no clear threshold below which there is no increased risk. Using IPD, we estimated glucose thresholds to identify infants at high risk of being born large for gestational age or with high adiposity; for South Asian (SA) women these thresholds were fasting and post-load glucose levels of 5.2â€‰mmol/l and 7.2â€‰mmol/l, respectively and for white British (WB) women they were 5.4 and 7.5â€‰mmol/l, respectively. Prevalence using IPD and published data varied from 1.2% to 24.2% (depending on criteria and population) and was consistently two to three times higher in SA women than in WB women. Lowering thresholds to identify GDM, particularly in women of SA origin, identifies more women at risk, but increases costs. Maternal characteristics did not accurately identify women with GDM; there was limited evidence that in some populations risk factors may be useful for identifying low-risk women. Dietary modification additional to routine care reduced the risk of most adverse perinatal outcomes. Metformin (Glucophage,<sup>Â®</sup> Teva UK Ltd, Eastbourne, UK) and insulin were more effective than glibenclamide (Aurobindo Pharma - Milpharm Ltd, South Ruislip, Middlesex, UK). For all strategies to identify and treat GDM, the costs exceeded the health benefits. A policy of no screening/testing or treatment offered the maximum expected net monetary benefit (NMB) of Â£1184 at a cost-effectiveness threshold of Â£20,000 per quality-adjusted life-year (QALY). The NMB for the three best-performing strategies in each category (screen only, then treat; screen, test, then treat; and test all, then treat) ranged between -Â£1197 and -Â£1210. Further research to reduce uncertainty around potential longer-term benefits for the mothers and offspring, find ways of improving the accuracy of identifying women with GDM, and reduce costs of identification and treatment would be worthwhile. We did not have access to IPD from populations in the UK outside of England. Few observational studies reported longer-term associations, and treatment trials have generally reported only perinatal outcomes. Using the national standard cost-effectiveness threshold of Â£20,000 per QALY it is not cost-effective to routinely identify pregnant women for treatment of hyperglycaemia. Further research to provide evidence on longer-term outcomes, and more cost-effective ways to detect and treat GDM, would be valuable. This study is registered as PROSPERO CRD42013004608. The National Institute for Health Research Health Technology Assessment programme.","Farrar, D.
 and Simmonds, M.
 and Griffin, S.
 and Duarte, A.
 and Lawlor, D. A.
 and Sculpher, M.
 and Fairley, L.
 and Golder, S.
 and Tuffnell, D.
 and Bland, M.
 and Dunne, F.
 and Whitelaw, D.
 and Wright, J.
 and Sheldon, T. A.","Farrar, Simmonds, Griffin, Duarte, Lawlor, Sculpher, Fairley, Golder, Tuffnell, Bland, Dunne, Whitelaw, Wright, Sheldon",not available,https://doi.org/10.3310/hta20860,2021-08-03
295.0,,pubmed,Auditory verbal hallucinations and continuum models of psychosis: A systematic review of the healthy voice-hearer literature,Auditory verbal hallucinations and continuum models of psychosis: A systematic review of the healthy voice-hearer literature,"Recent decades have seen a surge of research interest in the phenomenon of healthy individuals who experience auditory verbal hallucinations, yet do not exhibit distress or need for care. The aims of the present systematic review are to provide a comprehensive overview of this research and examine how healthy voice-hearers may best be conceptualised in relation to the diagnostic versus 'quasi-' and 'fully-dimensional' continuum models of psychosis. A systematic literature search was conducted, resulting in a total of 398 article titles and abstracts that were scrutinised for appropriateness to the present objective. Seventy articles were identified for full-text analysis, of which 36 met criteria for inclusion. Subjective perceptual experience of voices, such as loudness or location (i.e., inside/outside head), is similar in clinical and non-clinical groups, although clinical voice-hearers have more frequent voices, more negative voice content, and an older age of onset. Groups differ significantly in beliefs about voices, control over voices, voice-related distress, and affective difficulties. Cognitive biases, reduced global functioning, and psychiatric symptoms such as delusions, appear more prevalent in healthy voice-hearers than in healthy controls, yet less than in clinical samples. Transition to mental health difficulties is increased in HVHs, yet only occurs in a minority and is predicted by previous mood problems and voice distress. Whilst healthy voice-hearers show similar brain activity during hallucinatory experiences to clinical voice-hearers, other neuroimaging measures, such as mismatch negativity, have been inconclusive. Risk factors such as familial and childhood trauma appear similar between clinical and non-clinical voice-hearers. Overall the results of the present systematic review support a continuum view rather than a diagnostic model, but cannot distinguish between 'quasi' and 'fully' dimensional models. Healthy voice-hearers may be a key resource in informing transdiagnostic approaches to research of auditory hallucinations.","Recent decades have seen a surge of research interest in the phenomenon of healthy individuals who experience auditory verbal hallucinations, yet do not exhibit distress or need for care. The aims of the present systematic review are to provide a comprehensive overview of this research and examine how healthy voice-hearers may best be conceptualised in relation to the diagnostic versus 'quasi-' and 'fully-dimensional' continuum models of psychosis. A systematic literature search was conducted, resulting in a total of 398 article titles and abstracts that were scrutinised for appropriateness to the present objective. Seventy articles were identified for full-text analysis, of which 36 met criteria for inclusion. Subjective perceptual experience of voices, such as loudness or location (i.e., inside/outside head), is similar in clinical and non-clinical groups, although clinical voice-hearers have more frequent voices, more negative voice content, and an older age of onset. Groups differ significantly in beliefs about voices, control over voices, voice-related distress, and affective difficulties. Cognitive biases, reduced global functioning, and psychiatric symptoms such as delusions, appear more prevalent in healthy voice-hearers than in healthy controls, yet less than in clinical samples. Transition to mental health difficulties is increased in HVHs, yet only occurs in a minority and is predicted by previous mood problems and voice distress. Whilst healthy voice-hearers show similar brain activity during hallucinatory experiences to clinical voice-hearers, other neuroimaging measures, such as mismatch negativity, have been inconclusive. Risk factors such as familial and childhood trauma appear similar between clinical and non-clinical voice-hearers. Overall the results of the present systematic review support a continuum view rather than a diagnostic model, but cannot distinguish between 'quasi' and 'fully' dimensional models. Healthy voice-hearers may be a key resource in informing transdiagnostic approaches to research of auditory hallucinations.","Baumeister, D.
 and Sedgwick, O.
 and Howes, O.
 and Peters, E.","Baumeister, Sedgwick, Howes, Peters",https://dx.doi.org/10.1016/j.cpr.2016.10.010,https://doi.org/10.1016/j.cpr.2016.10.010,2021-08-03
4020.0,,pubmed,Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision,Extracting PICO Sentences from Clinical Trial Reports using <i>Supervised Distant Supervision</i>,"Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations. To this end, we propose a novel method - supervised distant supervision (SDS) - that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by learning to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction.","<i>Systematic reviews</i> underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a <i>PICO</i> criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive <i>distant supervision</i> (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations. To this end, we propose a novel method - <i>supervised distant supervision</i> (SDS) - that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by <i>learning</i> to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction.","Wallace, B. C.
 and Kuiper, J.
 and Sharma, A.
 and Zhu, M. B.
 and Marshall, I. J.","Wallace, Kuiper, Sharma, Zhu, Marshall",not available,https://www.google.com/search?q=Extracting+PICO+Sentences+from+Clinical+Trial+Reports+using+<i>Supervised+Distant+Supervision</i>.,2021-08-03
3881.0,,pubmed,An evidence synthesis of the international knowledge base for new care models to inform and mobilise knowledge for multispecialty community providers (MCPs),An evidence synthesis of the international knowledge base for new care models to inform and mobilise knowledge for multispecialty community providers (MCPs),"BACKGROUND: NHS England's Five Year Forward View (NHS England, Five Year Forward View, 2014) formally introduced a strategy for new models of care driven by simultaneous pressures to contain costs, improve care and deliver services closer to home through integrated models. This synthesis focuses on a multispecialty community provider (MCP) model. This new model of care seeks to overcome the limitations in current models of care, often based around single condition-focused pathways, in contrast to patient-focused delivery (Royal College of General Practitioners, The 2022 GP: compendium of evidence, 2012) which offers greater continuity of care in recognition of complex needs and multimorbidity. METHODS: The synthesis, an innovative combination of best fit framework synthesis and realist synthesis, will develop a 'blueprint' which articulates how and why MCP models work, to inform design of future iterations of the MCP model. A systematic search will be conducted to identify research and practice-derived evidence to achieve a balance that captures the historical legacy of MCP models but focuses on contemporary evidence. Sources will include bibliographic databases including MEDLINE, PreMEDLINE, CINAHL, Embase, HMIC and Cochrane Library; and grey literature sources. The Best Fit synthesis methodology will be combined with a synthesis following realist principles which are particularly suited to exploring what works, when, for whom and in what circumstances. DISCUSSION: The aim of this synthesis is to provide decision makers in health and social care with a practical evidence base relating to the multispecialty community provider (MCP) model of care. Systematic review registration: prospero crd42016039552 .","NHS England's Five Year Forward View (NHS England, Five Year Forward View, 2014) formally introduced a strategy for new models of care driven by simultaneous pressures to contain costs, improve care and deliver services closer to home through integrated models. This synthesis focuses on a multispecialty community provider (MCP) model. This new model of care seeks to overcome the limitations in current models of care, often based around single condition-focused pathways, in contrast to patient-focused delivery (Royal College of General Practitioners, The 2022 GP: compendium of evidence, 2012) which offers greater continuity of care in recognition of complex needs and multimorbidity. The synthesis, an innovative combination of best fit framework synthesis and realist synthesis, will develop a ""blueprint"" which articulates how and why MCP models work, to inform design of future iterations of the MCP model. A systematic search will be conducted to identify research and practice-derived evidence to achieve a balance that captures the historical legacy of MCP models but focuses on contemporary evidence. Sources will include bibliographic databases including MEDLINE, PreMEDLINE, CINAHL, Embase, HMIC and Cochrane Library; and grey literature sources. The Best Fit synthesis methodology will be combined with a synthesis following realist principles which are particularly suited to exploring what works, when, for whom and in what circumstances. The aim of this synthesis is to provide decision makers in health and social care with a practical evidence base relating to the multispecialty community provider (MCP) model of care. PROSPERO CRD42016039552 .","Turner, A.
 and Mulla, A.
 and Booth, A.
 and Aldridge, S.
 and Stevens, S.
 and Battye, F.
 and Spilsbury, P.","Turner, Mulla, Booth, Aldridge, Stevens, Battye, Spilsbury",not available,https://doi.org/10.1186/s13643-016-0346-x,2021-08-03
2500.0,,pubmed,Use of emergency department electronic medical records for automated epidemiological surveillance of suicide attempts: a French pilot study,Use of emergency department electronic medical records for automated epidemiological surveillance of suicide attempts: a French pilot study,"The aim of this study was to determine whether an expert system based on automated processing of electronic health records (EHRs) could provide a more accurate estimate of the annual rate of emergency department (ED) visits for suicide attempts in France, as compared to the current national surveillance system based on manual coding by emergency practitioners. A feasibility study was conducted at Lyon University Hospital, using data for all ED patient visits in 2012. After automatic data extraction and pre-processing, including automatic coding of medical free-text through use of the Unified Medical Language System, seven different machine-learning methods were used to classify the reasons for ED visits into 'suicide attempts' versus 'other reasons'. The performance of these different methods was compared by using the F-measure. In a test sample of 444 patients admitted to the ED in 2012 (98 suicide attempts, 48 cases of suicidal ideation, and 292 controls with no recorded non-fatal suicidal behaviour), the F-measure for automatic detection of suicide attempts ranged from 70.4% to 95.3%. The random forest and naive Bayes methods performed best. This study demonstrates that machine-learning methods can improve the quality of epidemiological indicators as compared to current national surveillance of suicide attempts.","The aim of this study was to determine whether an expert system based on automated processing of electronic health records (EHRs) could provide a more accurate estimate of the annual rate of emergency department (ED) visits for suicide attempts in France, as compared to the current national surveillance system based on manual coding by emergency practitioners. A feasibility study was conducted at Lyon University Hospital, using data for all ED patient visits in 2012. After automatic data extraction and pre-processing, including automatic coding of medical free-text through use of the Unified Medical Language System, seven different machine-learning methods were used to classify the reasons for ED visits into ""suicide attempts"" versus ""other reasons"". The performance of these different methods was compared by using the F-measure. In a test sample of 444 patients admitted to the ED in 2012 (98 suicide attempts, 48 cases of suicidal ideation, and 292 controls with no recorded non-fatal suicidal behaviour), the F-measure for automatic detection of suicide attempts ranged from 70.4% to 95.3%. The random forest and naÃ¯ve Bayes methods performed best. This study demonstrates that machine-learning methods can improve the quality of epidemiological indicators as compared to current national surveillance of suicide attempts.","Metzger, M. H.
 and Tvardik, N.
 and Gicquel, Q.
 and Bouvry, C.
 and Poulet, E.
 and Potinet-Pagliaroli, V.","Metzger, Tvardik, Gicquel, Bouvry, Poulet, Potinet-Pagliaroli",https://dx.doi.org/10.1002/mpr.1522,https://doi.org/10.1002/mpr.1522,2021-08-03
75.0,,pubmed,"Which method is best for the induction of labour? A systematic review, network meta-analysis and cost-effectiveness analysis","Which method is best for the induction of labour? A systematic review, network meta-analysis and cost-effectiveness analysis","BACKGROUND: More than 150,000 pregnant women in England and Wales have their labour induced each year. Multiple pharmacological, mechanical and complementary methods are available to induce labour. OBJECTIVE: To assess the relative effectiveness, safety and cost-effectiveness of labour induction methods and, data permitting, effects in different clinical subgroups. METHODS: We carried out a systematic review using Cochrane methods. The Cochrane Pregnancy and Childbirth Group's Trials Register was searched (March 2014). This contains over 22,000 reports of controlled trials (published from 1923 onwards) retrieved from weekly searches of OVID MEDLINE (1966 to current); Cochrane Central Register of Controlled Trials (The Cochrane Library); EMBASE (1982 to current); Cumulative Index to Nursing and Allied Health Literature (1984 to current); ClinicalTrials.gov; the World Health Organization International Clinical Trials Registry Portal; and hand-searching of relevant conference proceedings and journals. We included randomised controlled trials examining interventions to induce labour compared with placebo, no treatment or other interventions in women eligible for third-trimester induction. We included outcomes relating to efficacy, safety and acceptability to women. In addition, for the economic analysis we searched the Database of Abstracts of Reviews of Effects, and Economic Evaluations Databases, NHS Economic Evaluation Database and the Health Technology Assessment database. We carried out a network meta-analysis (NMA) using all of the available evidence, both direct and indirect, to produce estimates of the relative effects of each treatment compared with others in a network. We developed a de novo decision tree model to estimate the cost-effectiveness of various methods. The costs included were the intervention and other hospital costs incurred (price year 2012-13). We reviewed the literature to identify preference-based utilities for the health-related outcomes in the model. We calculated incremental cost-effectiveness ratios, expected costs, utilities and net benefit. We represent uncertainty in the optimal intervention using cost-effectiveness acceptability curves. RESULTS: We identified 1190 studies; 611 were eligible for inclusion. The interventions most likely to achieve vaginal delivery (VD) within 24 hours were intravenous oxytocin with amniotomy [posterior rank 2; 95% credible intervals (CrIs) 1 to 9] and higher-dose (>= 50 micro g) vaginal misoprostol (rank 3; 95% CrI 1 to 6). Compared with placebo, several treatments reduced the odds of caesarean section, but we observed considerable uncertainty in treatment rankings. For uterine hyperstimulation, double-balloon catheter had the highest probability of being among the best three treatments, whereas vaginal misoprostol (>= 50 micro g) was most likely to increase the odds of excessive uterine activity. For other safety outcomes there were insufficient data or there was too much uncertainty to identify which treatments performed 'best'. Few studies collected information on women's views. Owing to incomplete reporting of the VD within 24 hours outcome, the cost-effectiveness analysis could compare only 20 interventions. The analysis suggested that most interventions have similar utility and differ mainly in cost. With a caveat of considerable uncertainty, titrated (low-dose) misoprostol solution and buccal/sublingual misoprostol had the highest likelihood of being cost-effective. LIMITATIONS: There was considerable uncertainty in findings and there were insufficient data for some planned subgroup analyses. CONCLUSIONS: Overall, misoprostol and oxytocin with amniotomy (for women with favourable cervix) is more successful than other agents in achieving VD within 24 hours. The ranking according to safety of different methods was less clear. The cost-effectiveness analysis suggested that titrated (low-dose) oral misoprostol solution resulted in the highest utility, whereas buccal/sublingual misoprostol had the lowest cost. There was a high degree of uncertainty as to the most cost-effective intervention. FUTURE WORK: Future trials should be powered to detect a method that is more cost-effective than misoprostol solution and report outcomes included in this NMA. STUDY REGISTRATION: This study is registered as PROSPERO CRD42013005116. FUNDING: The National Institute for Health Research Health Technology Assessment programme.","More than 150,000 pregnant women in England and Wales have their labour induced each year. Multiple pharmacological, mechanical and complementary methods are available to induce labour. To assess the relative effectiveness, safety and cost-effectiveness of labour induction methods and, data permitting, effects in different clinical subgroups. We carried out a systematic review using Cochrane methods. The Cochrane Pregnancy and Childbirth Group's Trials Register was searched (March 2014). This contains over 22,000 reports of controlled trials (published from 1923 onwards) retrieved from weekly searches of OVID MEDLINE (1966 to current); Cochrane Central Register of Controlled Trials (The Cochrane Library); EMBASE (1982 to current); Cumulative Index to Nursing and Allied Health Literature (1984 to current); ClinicalTrials.gov; the World Health Organization International Clinical Trials Registry Portal; and hand-searching of relevant conference proceedings and journals. We included randomised controlled trials examining interventions to induce labour compared with placebo, no treatment or other interventions in women eligible for third-trimester induction. We included outcomes relating to efficacy, safety and acceptability to women. In addition, for the economic analysis we searched the Database of Abstracts of Reviews of Effects, and Economic Evaluations Databases, NHS Economic Evaluation Database and the Health Technology Assessment database. We carried out a network meta-analysis (NMA) using all of the available evidence, both direct and indirect, to produce estimates of the relative effects of each treatment compared with others in a network. We developed a de novo decision tree model to estimate the cost-effectiveness of various methods. The costs included were the intervention and other hospital costs incurred (price year 2012-13). We reviewed the literature to identify preference-based utilities for the health-related outcomes in the model. We calculated incremental cost-effectiveness ratios, expected costs, utilities and net benefit. We represent uncertainty in the optimal intervention using cost-effectiveness acceptability curves. We identified 1190 studies; 611 were eligible for inclusion. The interventions most likely to achieve vaginal delivery (VD) within 24 hours were intravenous oxytocin with amniotomy [posterior rank 2; 95% credible intervals (CrIs) 1 to 9] and higher-dose (â‰¥â€‰50â€‰Âµg) vaginal misoprostol (rank 3; 95% CrI 1 to 6). Compared with placebo, several treatments reduced the odds of caesarean section, but we observed considerable uncertainty in treatment rankings. For uterine hyperstimulation, double-balloon catheter had the highest probability of being among the best three treatments, whereas vaginal misoprostol (â‰¥â€‰50â€‰Âµg) was most likely to increase the odds of excessive uterine activity. For other safety outcomes there were insufficient data or there was too much uncertainty to identify which treatments performed 'best'. Few studies collected information on women's views. Owing to incomplete reporting of the VD within 24 hours outcome, the cost-effectiveness analysis could compare only 20 interventions. The analysis suggested that most interventions have similar utility and differ mainly in cost. With a caveat of considerable uncertainty, titrated (low-dose) misoprostol solution and buccal/sublingual misoprostol had the highest likelihood of being cost-effective. There was considerable uncertainty in findings and there were insufficient data for some planned subgroup analyses. Overall, misoprostol and oxytocin with amniotomy (for women with favourable cervix) is more successful than other agents in achieving VD within 24 hours. The ranking according to safety of different methods was less clear. The cost-effectiveness analysis suggested that titrated (low-dose) oral misoprostol solution resulted in the highest utility, whereas buccal/sublingual misoprostol had the lowest cost. There was a high degree of uncertainty as to the most cost-effective intervention. Future trials should be powered to detect a method that is more cost-effective than misoprostol solution and report outcomes included in this NMA. This study is registered as PROSPERO CRD42013005116. The National Institute for Health Research Health Technology Assessment programme.","Alfirevic, Z.
 and Keeney, E.
 and Dowswell, T.
 and Welton, N. J.
 and Medley, N.
 and Dias, S.
 and Jones, L. V.
 and Gyte, G.
 and Caldwell, D. M.","Alfirevic, Keeney, Dowswell, Welton, Medley, Dias, Jones, Gyte, Caldwell",https://dx.doi.org/10.3310/hta20650,https://doi.org/10.3310/hta20650,2021-08-03
1967.0,,pubmed,Is Single-Stage Prosthetic Reconstruction Cost Effective? A Cost-Utility Analysis for the Use of Direct-to-Implant Breast Reconstruction Relative to Expander-Implant Reconstruction in Postmastectomy Patients,Is Single-Stage Prosthetic Reconstruction Cost Effective? A Cost-Utility Analysis for the Use of Direct-to-Implant Breast Reconstruction Relative to Expander-Implant Reconstruction in Postmastectomy Patients,"BACKGROUND: Prosthetic breast reconstruction is most commonly performed using the two-stage (expander-implant) technique. However, with the advent of skin-sparing mastectomy and the use of acellular dermal matrices, one-stage prosthetic reconstruction has become more feasible. Prior studies have suggested that one-stage reconstruction has economic advantages relative to two-stage reconstruction despite a higher revision rate. This is the first cost-utility analysis to compare the cost and quality of life of both procedures to guide patient care. METHODS: A comprehensive literature review was conducted using the MEDLINE, EMBASE, and Cochrane databases to include studies directly comparing matched patient cohorts undergoing single-stage or staged prosthetic reconstruction. Six studies were selected examining 791 direct-to-implant reconstructions and 1142 expander-implant reconstructions. Costs were derived adopting both patient and third-party payer perspectives. Utilities were derived by surveying an expert panel. Probabilities of clinically relevant complications were combined with cost and utility estimates to fit into a decision tree analysis. RESULTS: The overall complication rate was 35 percent for single-stage reconstruction and 34 percent for expander-implant reconstruction. The authors' baseline analysis using Medicare reimbursement revealed a cost decrease of $525.25 and a clinical benefit of 0.89 quality-adjusted life-year when performing single-stage reconstructions, yielding a negative incremental cost-utility ratio. When using national billing, the incremental cost-utility further decreased, indicating that direct-to-implant breast reconstruction was the dominant strategy. Sensitivity analysis confirmed the robustness of the authors' conclusions. CONCLUSIONS: Direct-to-implant breast reconstruction is the dominant strategy when used appropriately. Surgeons are encouraged to consider single-stage reconstruction when feasible in properly selected patients.","Prosthetic breast reconstruction is most commonly performed using the two-stage (expander-implant) technique. However, with the advent of skin-sparing mastectomy and the use of acellular dermal matrices, one-stage prosthetic reconstruction has become more feasible. Prior studies have suggested that one-stage reconstruction has economic advantages relative to two-stage reconstruction despite a higher revision rate. This is the first cost-utility analysis to compare the cost and quality of life of both procedures to guide patient care. A comprehensive literature review was conducted using the MEDLINE, EMBASE, and Cochrane databases to include studies directly comparing matched patient cohorts undergoing single-stage or staged prosthetic reconstruction. Six studies were selected examining 791 direct-to-implant reconstructions and 1142 expander-implant reconstructions. Costs were derived adopting both patient and third-party payer perspectives. Utilities were derived by surveying an expert panel. Probabilities of clinically relevant complications were combined with cost and utility estimates to fit into a decision tree analysis. The overall complication rate was 35 percent for single-stage reconstruction and 34 percent for expander-implant reconstruction. The authors' baseline analysis using Medicare reimbursement revealed a cost decrease of $525.25 and a clinical benefit of 0.89 quality-adjusted life-year when performing single-stage reconstructions, yielding a negative incremental cost-utility ratio. When using national billing, the incremental cost-utility further decreased, indicating that direct-to-implant breast reconstruction was the dominant strategy. Sensitivity analysis confirmed the robustness of the authors' conclusions. Direct-to-implant breast reconstruction is the dominant strategy when used appropriately. Surgeons are encouraged to consider single-stage reconstruction when feasible in properly selected patients.","Krishnan, N. M.
 and Fischer, J. P.
 and Basta, M. N.
 and Nahabedian, M. Y.","Krishnan, Fischer, Basta, Nahabedian",https://dx.doi.org/10.1097/PRS.0000000000002428,https://doi.org/10.1097/PRS.0000000000002428,2021-08-03
2308.0,,pubmed,Automation bias and verification complexity: a systematic review,Automation bias and verification complexity: a systematic review,"Introduction: While potentially reducing decision errors, decision support systems can introduce new types of errors. Automation bias (AB) happens when users become overreliant on decision support, which reduces vigilance in information seeking and processing. Most research originates from the human factors literature, where the prevailing view is that AB occurs only in multitasking environments. Objectives: This review seeks to compare the human factors and health care literature, focusing on the apparent association of AB with multitasking and task complexity. Data sources: EMBASE, Medline, Compendex, Inspec, IEEE Xplore, Scopus, Web of Science, PsycINFO, and Business Source Premiere from 1983 to 2015. Study selection: Evaluation studies where task execution was assisted by automation and resulted in errors were included. Participants needed to be able to verify automation correctness and perform the task manually. Methods: Tasks were identified and grouped. Task and automation type and presence of multitasking were noted. Each task was rated for its verification complexity. Results: Of 890 papers identified, 40 met the inclusion criteria; 6 were in health care. Contrary to the prevailing human factors view, AB was found in single tasks, typically involving diagnosis rather than monitoring, and with high verification complexity. Limitations: The literature is fragmented, with large discrepancies in how AB is reported. Few studies reported the statistical significance of AB compared to a control condition. Conclusion: AB appears to be associated with the degree of cognitive load experienced in decision tasks, and appears to not be uniquely associated with multitasking. Strategies to minimize AB might focus on cognitive load reduction.","While potentially reducing decision errors, decision support systems can introduce new types of errors. Automation bias (AB) happens when users become overreliant on decision support, which reduces vigilance in information seeking and processing. Most research originates from the human factors literature, where the prevailing view is that AB occurs only in multitasking environments. This review seeks to compare the human factors and health care literature, focusing on the apparent association of AB with multitasking and task complexity. EMBASE, Medline, Compendex, Inspec, IEEE Xplore, Scopus, Web of Science, PsycINFO, and Business Source Premiere from 1983 to 2015. Evaluation studies where task execution was assisted by automation and resulted in errors were included. Participants needed to be able to verify automation correctness and perform the task manually. Tasks were identified and grouped. Task and automation type and presence of multitasking were noted. Each task was rated for its verification complexity. Of 890 papers identified, 40 met the inclusion criteria; 6 were in health care. Contrary to the prevailing human factors view, AB was found in single tasks, typically involving diagnosis rather than monitoring, and with high verification complexity. The literature is fragmented, with large discrepancies in how AB is reported. Few studies reported the statistical significance of AB compared to a control condition. AB appears to be associated with the degree of cognitive load experienced in decision tasks, and appears to not be uniquely associated with multitasking. Strategies to minimize AB might focus on cognitive load reduction.","Lyell, D.
 and Coiera, E.","Lyell, Coiera",https://dx.doi.org/10.1093/jamia/ocw105,https://doi.org/10.1093/jamia/ocw105,2021-08-03
209.0,,pubmed,A systematic review of economic models used to assess the cost-effectiveness of strategies for identifying latent tuberculosis in high-risk groups,A systematic review of economic models used to assess the cost-effectiveness of strategies for identifying latent tuberculosis inÂ high-risk groups,"BACKGROUND: Timely diagnosis and treatment of latent tuberculosis infection (LTBI) through screening remains a key public health priority. Although globally it is recommended to screen people at high risk of developing TB, the economic evidence underpinning these recommendations is limited. This review critically appraised studies that had used a decision-analytical modelling framework to estimate the cost-effectiveness of interferon gamma release assays (IGRAs) compared to tuberculin skin test (TST) for detecting LTBI in high risk populations. METHODS: A comprehensive search of MEDLINE, EMBASE, NHS-EED was undertaken from 2009 up to June 2015. Studies were screened and extracted by independent reviewers. The study quality was assessed using the Consolidated Health Economic Evaluation Reporting Standards (CHEERS) and the Philips' checklist, respectively. A narrative synthesis of the included studies was undertaken. RESULTS: Ten studies were included in this review. Two economic evaluations were conducted in a child population, six in an immunocompromised population and two in a recently arrived population from a country with a high incidence of TB. Most studies (n = 7) used a decision tree structure with Markov nodes. In general, all models were clearly described in terms of reporting quality, but were subject to limitations to structure and model inputs. Models have not elaborated on their setting or the perspective of the studies was not consistent with their analyses. Other concerns were related to derivation of prevalence, test accuracy and transition probabilities. CONCLUSION: Current methods available highlight limitations in the clinical effectiveness literature, model structures and assumptions, which impact on the robustness of the cost-effectiveness results. These models available are useful, but limited on the information that can be used to inform on future cost-effectiveness analysis. Until consideration is given on deriving the performance of tests used to identify LTBI that progresses to active TB, and the development of more comprehensive models, the economic benefit of LTBI testing with TST/IGRAs in high risk populations will remain unanswered.","Timely diagnosis and treatment of latent tuberculosis infection (LTBI) through screening remains a key public health priority. Although globally it is recommended to screen people at high risk of developing TB, the economic evidence underpinning these recommendations is limited. This review critically appraised studies that had used a decision-analytical modelling framework to estimate the cost-effectiveness of interferon gamma release assays (IGRAs) compared to tuberculin skin test (TST) for detecting LTBI in high risk populations. A comprehensive search of MEDLINE, EMBASE, NHS-EED was undertaken from 2009 up to June 2015. Studies were screened and extracted by independent reviewers. The study quality was assessed using the Consolidated Health Economic Evaluation Reporting Standards (CHEERS) and the Philips' checklist, respectively. A narrative synthesis of the included studies was undertaken. Ten studies were included in this review. Two economic evaluations were conducted in a child population, six in an immunocompromised population and two in a recently arrived population from a country with a high incidence of TB. Most studies (nÂ =Â 7) used a decision tree structure with Markov nodes. In general, all models were clearly described in terms of reporting quality, but were subject to limitations to structure and model inputs. Models have not elaborated on their setting or the perspective of the studies was not consistent with their analyses. Other concerns were related to derivation of prevalence, test accuracy and transition probabilities. Current methods available highlight limitations in the clinical effectiveness literature, model structures and assumptions, which impact on the robustness of the cost-effectiveness results. These models available are useful, but limited on the information that can be used to inform on future cost-effectiveness analysis. Until consideration is given on deriving the performance of tests used to identify LTBI that progresses to active TB, and the development of more comprehensive models, the economic benefit of LTBI testing with TST/IGRAs in high risk populations will remain unanswered.","Auguste, P.
 and Tsertsvadze, A.
 and Court, R.
 and Pink, J.","Auguste, Tsertsvadze, Court, Pink",https://dx.doi.org/10.1016/j.tube.2016.04.007,https://doi.org/10.1016/j.tube.2016.04.007,2021-08-03
3509.0,,pubmed,Identification of Inpatient Falls Using Automated Review of Text-Based Medical Records,Identification of Inpatient Falls Using Automated Review of Text-Based Medical Records,"OBJECTIVES: Although falls are among the most common adverse event in hospitals, they are difficult to measure and often unreported. Mechanisms to track falls include incident reporting and medical records review. Because of limitations of each method, researchers suggest multimodal approaches. Although incident reporting is commonly used, medical records review is limited by the need to read a high volume of clinical notes. Natural language processing (NLP) is 1 potential mechanism to automate this process. METHOD: We compared automated NLP to manual chart review and incident reporting as a method to detect falls among inpatients. First, we developed an NLP algorithm to identify inpatient progress notes describing falls. Second, we compared the NLP algorithm to manual records review in identifying inpatient progress notes that describe falls. Third, we compared the NLP algorithm to the incident reporting system in identifying falls. RESULTS: When examining individual inpatient notes, our NLP algorithm was highly specific (0.97) but had low sensitivity (0.44) when compared with our manual records review. However, when considering groups of inpatient notes, all describing the same fall, our NLP algorithm had a large improvement in sensitivity (0.80) with some loss of specificity (0.65) compared with incident reporting. CONCLUSIONS: National language processing represents a promising method to automate review of inpatient medical records to identify falls.","Although falls are among the most common adverse event in hospitals, they are difficult to measure and often unreported. Mechanisms to track falls include incident reporting and medical records review. Because of limitations of each method, researchers suggest multimodal approaches. Although incident reporting is commonly used, medical records review is limited by the need to read a high volume of clinical notes. Natural language processing (NLP) is 1 potential mechanism to automate this process. We compared automated NLP to manual chart review and incident reporting as a method to detect falls among inpatients. First, we developed an NLP algorithm to identify inpatient progress notes describing falls. Second, we compared the NLP algorithm to manual records review in identifying inpatient progress notes that describe falls. Third, we compared the NLP algorithm to the incident reporting system in identifying falls. When examining individual inpatient notes, our NLP algorithm was highly specific (0.97) but had low sensitivity (0.44) when compared with our manual records review. However, when considering groups of inpatient notes, all describing the same fall, our NLP algorithm had a large improvement in sensitivity (0.80) with some loss of specificity (0.65) compared with incident reporting. National language processing represents a promising method to automate review of inpatient medical records to identify falls.","Shiner, B.
 and Neily, J.
 and Mills, P. D.
 and Watts, B. V.","Shiner, Neily, Mills, Watts",not available,https://doi.org/10.1097/PTS.0000000000000275,2021-08-03
1558.0,,pubmed,SWIFT-Review: a text-mining workbench for systematic review,SWIFT-Review: a text-mining workbench for systematic review,"BACKGROUND: There is growing interest in using machine learning approaches to priority rank studies and reduce human burden in screening literature when conducting systematic reviews. In addition, identifying addressable questions during the problem formulation phase of systematic review can be challenging, especially for topics having a large literature base. Here, we assess the performance of the SWIFT-Review priority ranking algorithm for identifying studies relevant to a given research question. We also explore the use of SWIFT-Review during problem formulation to identify, categorize, and visualize research areas that are data rich/data poor within a large literature corpus. METHODS: Twenty case studies, including 15 public data sets, representing a range of complexity and size, were used to assess the priority ranking performance of SWIFT-Review. For each study, seed sets of manually annotated included and excluded titles and abstracts were used for machine training. The remaining references were then ranked for relevance using an algorithm that considers term frequency and latent Dirichlet allocation (LDA) topic modeling. This ranking was evaluated with respect to (1) the number of studies screened in order to identify 95 % of known relevant studies and (2) the 'Work Saved over Sampling' (WSS) performance metric. To assess SWIFT-Review for use in problem formulation, PubMed literature search results for 171 chemicals implicated as EDCs were uploaded into SWIFT-Review (264,588 studies) and categorized based on evidence stream and health outcome. Patterns of search results were surveyed and visualized using a variety of interactive graphics. RESULTS: Compared with the reported performance of other tools using the same datasets, the SWIFT-Review ranking procedure obtained the highest scores on 11 out of 15 of the public datasets. Overall, these results suggest that using machine learning to triage documents for screening has the potential to save, on average, more than 50 % of the screening effort ordinarily required when using un-ordered document lists. In addition, the tagging and annotation capabilities of SWIFT-Review can be useful during the activities of scoping and problem formulation. CONCLUSIONS: Text-mining and machine learning software such as SWIFT-Review can be valuable tools to reduce the human screening burden and assist in problem formulation.","There is growing interest in using machine learning approaches to priority rank studies and reduce human burden in screening literature when conducting systematic reviews. In addition, identifying addressable questions during the problem formulation phase of systematic review can be challenging, especially for topics having a large literature base. Here, we assess the performance of the SWIFT-Review priority ranking algorithm for identifying studies relevant to a given research question. We also explore the use of SWIFT-Review during problem formulation to identify, categorize, and visualize research areas that are data rich/data poor within a large literature corpus. Twenty case studies, including 15 public data sets, representing a range of complexity and size, were used to assess the priority ranking performance of SWIFT-Review. For each study, seed sets of manually annotated included and excluded titles and abstracts were used for machine training. The remaining references were then ranked for relevance using an algorithm that considers term frequency and latent Dirichlet allocation (LDA) topic modeling. This ranking was evaluated with respect to (1) the number of studies screened in order to identify 95 % of known relevant studies and (2) the ""Work Saved over Sampling"" (WSS) performance metric. To assess SWIFT-Review for use in problem formulation, PubMed literature search results for 171 chemicals implicated as EDCs were uploaded into SWIFT-Review (264,588 studies) and categorized based on evidence stream and health outcome. Patterns of search results were surveyed and visualized using a variety of interactive graphics. Compared with the reported performance of other tools using the same datasets, the SWIFT-Review ranking procedure obtained the highest scores on 11 out of 15 of the public datasets. Overall, these results suggest that using machine learning to triage documents for screening has the potential to save, on average, more than 50 % of the screening effort ordinarily required when using un-ordered document lists. In addition, the tagging and annotation capabilities of SWIFT-Review can be useful during the activities of scoping and problem formulation. Text-mining and machine learning software such as SWIFT-Review can be valuable tools to reduce the human screening burden and assist in problem formulation.","Howard, B. E.
 and Phillips, J.
 and Miller, K.
 and Tandon, A.
 and Mav, D.
 and Shah, M. R.
 and Holmgren, S.
 and Pelch, K. E.
 and Walker, V.
 and Rooney, A. A.
 and Macleod, M.
 and Shah, R. R.
 and Thayer, K.","Howard, Phillips, Miller, Tandon, Mav, Shah, Holmgren, Pelch, Walker, Rooney, Macleod, Shah, Thayer",not available,https://doi.org/10.1186/s13643-016-0263-z,2021-08-03
74.0,,pubmed,"Methods to induce labour: a systematic review, network meta-analysis and cost-effectiveness analysis","Methods to induce labour: a systematic review, network meta-analysis and cost-effectiveness analysis","OBJECTIVES: To compare the clinical effectiveness and cost-effectiveness of labour induction methods. METHODS: We conducted a systematic review of randomised trials comparing interventions for third-trimester labour induction (search date: March 2014). Network meta-analysis was possible for six of nine prespecified key outcomes: vaginal delivery within 24 hours (VD24), caesarean section, uterine hyperstimulation, neonatal intensive care unit (NICU) admissions, instrumental delivery and infant Apgar scores. We developed a decision-tree model from a UK NHS perspective and calculated incremental cost-effectiveness ratios, expected costs, utilities and net benefit, and cost-effectiveness acceptability curves. MAIN RESULTS: In all, 611 studies comparing 31 active interventions were included. Intravenous oxytocin with amniotomy and vaginal misoprostol (>=50 mug) were most likely to achieve VD24. Titrated low-dose oral misoprostol achieved the lowest odds of caesarean section, but there was considerable uncertainty in ranking estimates. Vaginal (>=50 mug) and buccal/sublingual misoprostol were most likely to increase uterine hyperstimulation with high uncertainty in ranking estimates. Compared with placebo, extra-amniotic prostaglandin E2 reduced NICU admissions. There were insufficient data to conduct analyses for maternal and neonatal mortality and serious morbidity or maternal satisfaction. Conclusions were robust after exclusion of studies at high risk of bias. Due to poor reporting of VD24, the cost-effectiveness analysis compared a subset of 20 interventions. There was considerable uncertainty in estimates, but buccal/sublingual and titrated (low-dose) misoprostol showed the highest probability of being most cost-effective. CONCLUSIONS: Future trials should be designed and powered to detect a method that is more cost-effective than low-dose titrated oral misoprostol. TWEETABLE ABSTRACT: New study ranks methods to induce labour in pregnant women on effectiveness and cost.","To compare the clinical effectiveness and cost-effectiveness of labour induction methods. We conducted a systematic review of randomised trials comparing interventions for third-trimester labour induction (search date: March 2014). Network meta-analysis was possible for six of nine prespecified key outcomes: vaginal delivery within 24 hours (VD24), caesarean section, uterine hyperstimulation, neonatal intensive care unit (NICU) admissions, instrumental delivery and infant Apgar scores. We developed a decision-tree model from a UK NHS perspective and calculated incremental cost-effectiveness ratios, expected costs, utilities and net benefit, and cost-effectiveness acceptability curves. In all, 611 studies comparing 31 active interventions were included. Intravenous oxytocin with amniotomy and vaginal misoprostol (â‰¥50 Î¼g) were most likely to achieve VD24. Titrated low-dose oral misoprostol achieved the lowest odds of caesarean section, but there was considerable uncertainty in ranking estimates. Vaginal (â‰¥50 Î¼g) and buccal/sublingual misoprostol were most likely to increase uterine hyperstimulation with high uncertainty in ranking estimates. Compared with placebo, extra-amniotic prostaglandin E2 reduced NICU admissions. There were insufficient data to conduct analyses for maternal and neonatal mortality and serious morbidity or maternal satisfaction. Conclusions were robust after exclusion of studies at high risk of bias. Due to poor reporting of VD24, the cost-effectiveness analysis compared a subset of 20 interventions. There was considerable uncertainty in estimates, but buccal/sublingual and titrated (low-dose) misoprostol showed the highest probability of being most cost-effective. Future trials should be designed and powered to detect a method that is more cost-effective than low-dose titrated oral misoprostol. New study ranks methods to induce labour in pregnant women on effectiveness and cost.","Alfirevic, Z.
 and Keeney, E.
 and Dowswell, T.
 and Welton, N. J.
 and Medley, N.
 and Dias, S.
 and Jones, L. V.
 and Caldwell, D. M.","Alfirevic, Keeney, Dowswell, Welton, Medley, Dias, Jones, Caldwell",https://dx.doi.org/10.1111/1471-0528.13981,https://doi.org/10.1111/1471-0528.13981,2021-08-03
79.0,,pubmed,Grouping chemicals for health risk assessment: A text mining-based case study of polychlorinated biphenyls (PCBs),Grouping chemicals for health risk assessment: A text mining-based case study of polychlorinated biphenyls (PCBs),"As many chemicals act as carcinogens, chemical health risk assessment is critically important. A notoriously time consuming process, risk assessment could be greatly supported by classifying chemicals with similar toxicological profiles so that they can be assessed in groups rather than individually. We have previously developed a text mining (TM)-based tool that can automatically identify the mode of action (MOA) of a carcinogen based on the scientific evidence in literature, and it can measure the MOA similarity between chemicals on the basis of their literature profiles (Korhonen et al., 2009, 2012). A new version of the tool (2.0) was recently released and here we apply this tool for the first time to investigate and identify meaningful groups of chemicals for risk assessment. We used published literature on polychlorinated biphenyls (PCBs)-persistent, widely spread toxic organic compounds comprising of 209 different congeners. Although chemically similar, these compounds are heterogeneous in terms of MOA. We show that our TM tool, when applied to 1648 PubMed abstracts, produces a MOA profile for a subgroup of dioxin-like PCBs (DL-PCBs) which differs clearly from that for the rest of PCBs. This suggests that the tool could be used to effectively identify homogenous groups of chemicals and, when integrated in real-life risk assessment, could help and significantly improve the efficiency of the process.","As many chemicals act as carcinogens, chemical health risk assessment is critically important. A notoriously time consuming process, risk assessment could be greatly supported by classifying chemicals with similar toxicological profiles so that they can be assessed in groups rather than individually. We have previously developed a text mining (TM)-based tool that can automatically identify the mode of action (MOA) of a carcinogen based on the scientific evidence in literature, and it can measure the MOA similarity between chemicals on the basis of their literature profiles (Korhonen et al., 2009, 2012). A new version of the tool (2.0) was recently released and here we apply this tool for the first time to investigate and identify meaningful groups of chemicals for risk assessment. We used published literature on polychlorinated biphenyls (PCBs)-persistent, widely spread toxic organic compounds comprising of 209 different congeners. Although chemically similar, these compounds are heterogeneous in terms of MOA. We show that our TM tool, when applied to 1648 PubMed abstracts, produces a MOA profile for a subgroup of dioxin-like PCBs (DL-PCBs) which differs clearly from that for the rest of PCBs. This suggests that the tool could be used to effectively identify homogenous groups of chemicals and, when integrated in real-life risk assessment, could help and significantly improve the efficiency of the process.","Ali, I.
 and Guo, Y.
 and Silins, I.
 and Hogberg, J.
 and Stenius, U.
 and Korhonen, A.","Ali, Guo, Silins, HÃ¶gberg, Stenius, Korhonen",not available,https://doi.org/10.1016/j.toxlet.2015.11.003,2021-08-03
2347.0,,pubmed,Evaluation for Blunt Cerebrovascular Injury: Review of the Literature and a Cost-Effectiveness Analysis,Evaluation for Blunt Cerebrovascular Injury: Review of the Literature and a Cost-Effectiveness Analysis,"BACKGROUND AND PURPOSE: Evaluation for blunt cerebrovascular injury has generated immense controversy with wide variations in recommendations regarding the need for evaluation and the optimal imaging technique. We review the literature and determine the most cost-effective strategy for evaluating blunt cerebrovascular injury in trauma patients. MATERIALS AND METHODS: A comprehensive literature review was performed with data extracted to create a decision-tree analysis for 5 different strategies: anticoagulation for high-risk (based on the Denver screening criteria) patients, selective DSA or CTA (only high-risk patients), and DSA or CTA for all trauma patients. The economic evaluation was based on a health care payer perspective during a 1-year horizon. Statistical analyses were performed. The cost-effectiveness was compared through 2 main indicators: the incremental cost-effectiveness ratio and net monetary benefit. RESULTS: Selective anticoagulation in high-risk patients was shown to be the most cost-effective strategy, with the lowest cost and greatest effectiveness (an average cost of $21.08 and average quality-adjusted life year of 0.7231). Selective CTA has comparable utility and only a slightly higher cost (an average cost of $48.84 and average quality-adjusted life year of 0.7229). DSA, whether performed selectively or for all patients, was not optimal from both the cost and utility perspectives. Sensitivity analyses demonstrated these results to be robust for a wide range of parameter values. CONCLUSIONS: Selective CTA in high-risk patients is the optimal and cost-effective imaging strategy. It remains the dominant strategy over DSA, even assuming a low CTA sensitivity and irrespective of the proportion of patients at high-risk and the incidence of blunt cerebrovascular injury in high-risk patients.","Evaluation for blunt cerebrovascular injury has generated immense controversy with wide variations in recommendations regarding the need for evaluation and the optimal imaging technique. We review the literature and determine the most cost-effective strategy for evaluating blunt cerebrovascular injury in trauma patients. A comprehensive literature review was performed with data extracted to create a decision-tree analysis for 5 different strategies: anticoagulation for high-risk (based on the Denver screening criteria) patients, selective DSA or CTA (only high-risk patients), and DSA or CTA for all trauma patients. The economic evaluation was based on a health care payer perspective during a 1-year horizon. Statistical analyses were performed. The cost-effectiveness was compared through 2 main indicators: the incremental cost-effectiveness ratio and net monetary benefit. Selective anticoagulation in high-risk patients was shown to be the most cost-effective strategy, with the lowest cost and greatest effectiveness (an average cost of $21.08 and average quality-adjusted life year of 0.7231). Selective CTA has comparable utility and only a slightly higher cost (an average cost of $48.84 and average quality-adjusted life year of 0.7229). DSA, whether performed selectively or for all patients, was not optimal from both the cost and utility perspectives. Sensitivity analyses demonstrated these results to be robust for a wide range of parameter values. Selective CTA in high-risk patients is the optimal and cost-effective imaging strategy. It remains the dominant strategy over DSA, even assuming a low CTA sensitivity and irrespective of the proportion of patients at high-risk and the incidence of blunt cerebrovascular injury in high-risk patients.","Malhotra, A.
 and Wu, X.
 and Kalra, V. B.
 and Schindler, J.
 and Matouk, C. C.
 and Forman, H. P.","Malhotra, Wu, Kalra, Schindler, Matouk, Forman",https://dx.doi.org/10.3174/ajnr.A4515,https://doi.org/10.3174/ajnr.A4515,2021-08-03
2856.0,,pubmed,Depression Symptom Severity and Cardiorespiratory Fitness in Healthy and Depressed Adults: A Systematic Review and Meta-Analysis,Depression Symptom Severity and Cardiorespiratory Fitness in Healthy and Depressed Adults: A Systematic Review and Meta-Analysis,"BACKGROUND: Depression symptom severity and cardiorespiratory fitness (CRF) are significant predictors of mortality and disability. However, the relationship between the two is unclear. OBJECTIVE: This meta-analysis assessed the relationship between depression symptom severity and CRF in healthy and depressed adults (aged 18 years and over). SEARCH METHODS: The PubMed, Cochrane Library, Google Scholar and ProQuest databases were browsed for relevant English-language studies published from January 2000 to August 2014. SELECTION CRITERIA: Studies reporting a correlation between a depression scale and maximum oxygen consumption (VO2peak), as well as studies from the data of which such a correlation could be calculated, were included in this analysis. DATA ANALYSIS: Correlation coefficients (CCs) were converted to Fisher's z values, and the analysis was performed using a random-effects model. Then, summary effects and 95% confidence intervals (CIs) were converted back to CCs. RESULTS: Sixteen studies (totalling 4039 participants) were included in this analysis. A modest correlation between depression symptom severity and CRF was found (CC -0.16, 95% CI -0.21 to -0.10), appearing stronger in male participants (CC - 0.22, 95% CI -0.26 to -0.18) than in female participants (CC -0.12, 95% CI -0.19 to -0.05; p = 0.01). There was no difference in the summary effect between healthy and depressed adults (p = 0.43). Heterogeneity was moderate (I2 = 33%; p = 0.09). CONCLUSIONS: Depression symptom severity is inversely correlated with CRF, and this correlation is stronger in men than in women. Clinical and prognostic implications of the correlation are discussed. These findings should stimulate further research on the effects of treating one variable on the other.","Depression symptom severity and cardiorespiratory fitness (CRF) are significant predictors of mortality and disability. However, the relationship between the two is unclear. This meta-analysis assessed the relationship between depression symptom severity and CRF in healthy and depressed adults (aged 18 years and over). The PubMed, Cochrane Library, Google Scholar and ProQuest databases were browsed for relevant English-language studies published from January 2000 to August 2014. Studies reporting a correlation between a depression scale and maximum oxygen consumption (VO2peak), as well as studies from the data of which such a correlation could be calculated, were included in this analysis. Correlation coefficients (CCs) were converted to Fisher's z values, and the analysis was performed using a random-effects model. Then, summary effects and 95% confidence intervals (CIs) were converted back to CCs. Sixteen studies (totalling 4039 participants) were included in this analysis. A modest correlation between depression symptom severity and CRF was found (CC -0.16, 95% CI -0.21 to -0.10), appearing stronger in male participants (CC - 0.22, 95% CI -0.26 to -0.18) than in female participants (CC -0.12, 95% CI -0.19 to -0.05; p = 0.01). There was no difference in the summary effect between healthy and depressed adults (p = 0.43). Heterogeneity was moderate (I2 = 33%; p = 0.09). Depression symptom severity is inversely correlated with CRF, and this correlation is stronger in men than in women. Clinical and prognostic implications of the correlation are discussed. These findings should stimulate further research on the effects of treating one variable on the other.","Papasavvas, T.
 and Bonow, R. O.
 and Alhashemi, M.
 and Micklewright, D.","Papasavvas, Bonow, Alhashemi, Micklewright",https://dx.doi.org/10.1007/s40279-015-0409-5,https://doi.org/10.1007/s40279-015-0409-5,2021-08-03
618.0,,pubmed,An automatic system to identify heart disease risk factors in clinical texts over time,An automatic system to identify heart disease risk factors in clinical texts over time,"Despite recent progress in prediction and prevention, heart disease remains a leading cause of death. One preliminary step in heart disease prediction and prevention is risk factor identification. Many studies have been proposed to identify risk factors associated with heart disease; however, none have attempted to identify all risk factors. In 2014, the National Center of Informatics for Integrating Biology and Beside (i2b2) issued a clinical natural language processing (NLP) challenge that involved a track (track 2) for identifying heart disease risk factors in clinical texts over time. This track aimed to identify medically relevant information related to heart disease risk and track the progression over sets of longitudinal patient medical records. Identification of tags and attributes associated with disease presence and progression, risk factors, and medications in patient medical history were required. Our participation led to development of a hybrid pipeline system based on both machine learning-based and rule-based approaches. Evaluation using the challenge corpus revealed that our system achieved an F1-score of 92.68%, making it the top-ranked system (without additional annotations) of the 2014 i2b2 clinical NLP challenge.","Despite recent progress in prediction and prevention, heart disease remains a leading cause of death. One preliminary step in heart disease prediction and prevention is risk factor identification. Many studies have been proposed to identify risk factors associated with heart disease; however, none have attempted to identify all risk factors. In 2014, the National Center of Informatics for Integrating Biology and Beside (i2b2) issued a clinical natural language processing (NLP) challenge that involved a track (track 2) for identifying heart disease risk factors in clinical texts over time. This track aimed to identify medically relevant information related to heart disease risk and track the progression over sets of longitudinal patient medical records. Identification of tags and attributes associated with disease presence and progression, risk factors, and medications in patient medical history were required. Our participation led to development of a hybrid pipeline system based on both machine learning-based and rule-based approaches. Evaluation using the challenge corpus revealed that our system achieved an F1-score of 92.68%, making it the top-ranked system (without additional annotations) of the 2014 i2b2 clinical NLP challenge.","Chen, Q.
 and Li, H.
 and Tang, B.
 and Wang, X.
 and Liu, X.
 and Liu, Z.
 and Liu, S.
 and Wang, W.
 and Deng, Q.
 and Zhu, S.
 and Chen, Y.
 and Wang, J.","Chen, Li, Tang, Wang, Liu, Liu, Liu, Wang, Deng, Zhu, Chen, Wang",https://dx.doi.org/10.1016/j.jbi.2015.09.002,https://doi.org/10.1016/j.jbi.2015.09.002,2021-08-03
3.0,,pubmed,Text mining applications in psychiatry: a systematic literature review,Text mining applications in psychiatry: a systematic literature review,"The expansion of biomedical literature is creating the need for efficient tools to keep pace with increasing volumes of information. Text mining (TM) approaches are becoming essential to facilitate the automated extraction of useful biomedical information from unstructured text. We reviewed the applications of TM in psychiatry, and explored its advantages and limitations. A systematic review of the literature was carried out using the CINAHL, Medline, EMBASE, PsycINFO and Cochrane databases. In this review, 1103 papers were screened, and 38 were included as applications of TM in psychiatric research. Using TM and content analysis, we identified four major areas of application: (1) Psychopathology (i.e. observational studies focusing on mental illnesses) (2) the Patient perspective (i.e. patients' thoughts and opinions), (3) Medical records (i.e. safety issues, quality of care and description of treatments), and (4) Medical literature (i.e. identification of new scientific information in the literature). The information sources were qualitative studies, Internet postings, medical records and biomedical literature. Our work demonstrates that TM can contribute to complex research tasks in psychiatry. We discuss the benefits, limits, and further applications of this tool in the future. Copyright Â© 2015 John Wiley & Sons, Ltd.","The expansion of biomedical literature is creating the need for efficient tools to keep pace with increasing volumes of information. Text mining (TM) approaches are becoming essential to facilitate the automated extraction of useful biomedical information from unstructured text. We reviewed the applications of TM in psychiatry, and explored its advantages and limitations. A systematic review of the literature was carried out using the CINAHL, Medline, EMBASE, PsycINFO and Cochrane databases. In this review, 1103 papers were screened, and 38 were included as applications of TM in psychiatric research. Using TM and content analysis, we identified four major areas of application: (1) Psychopathology (i.e. observational studies focusing on mental illnesses) (2) the Patient perspective (i.e. patients' thoughts and opinions), (3) Medical records (i.e. safety issues, quality of care and description of treatments), and (4) Medical literature (i.e. identification of new scientific information in the literature). The information sources were qualitative studies, Internet postings, medical records and biomedical literature. Our work demonstrates that TM can contribute to complex research tasks in psychiatry. We discuss the benefits, limits, and further applications of this tool in the future. Copyright Â© 2015 John Wiley &amp; Sons, Ltd.","Abbe, A.
 and Grouin, C.
 and Zweigenbaum, P.
 and Falissard, B.","Abbe, Grouin, Zweigenbaum, Falissard",not available,https://doi.org/10.1002/mpr.1481,2021-08-03
2394.0,,pubmed,RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials,RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials,"OBJECTIVE: To develop and evaluate RobotReviewer, a machine learning (ML) system that automatically assesses bias in clinical trials. From a (PDF-formatted) trial report, the system should determine risks of bias for the domains defined by the Cochrane Risk of Bias (RoB) tool, and extract supporting text for these judgments. METHODS: We algorithmically annotated 12,808 trial PDFs using data from the Cochrane Database of Systematic Reviews (CDSR). Trials were labeled as being at low or high/unclear risk of bias for each domain, and sentences were labeled as being informative or not. This dataset was used to train a multi-task ML model. We estimated the accuracy of ML judgments versus humans by comparing trials with two or more independent RoB assessments in the CDSR. Twenty blinded experienced reviewers rated the relevance of supporting text, comparing ML output with equivalent (human-extracted) text from the CDSR. RESULTS: By retrieving the top 3 candidate sentences per document (top3 recall), the best ML text was rated more relevant than text from the CDSR, but not significantly (60.4% ML text rated 'highly relevant' v 56.5% of text from reviews; difference +3.9%, [-3.2% to +10.9%]). Model RoB judgments were less accurate than those from published reviews, though the difference was <10% (overall accuracy 71.0% with ML v 78.3% with CDSR). CONCLUSION: Risk of bias assessment may be automated with reasonable accuracy. Automatically identified text supporting bias assessment is of equal quality to the manually identified text in the CDSR. This technology could substantially reduce reviewer workload and expedite evidence syntheses.","To develop and evaluate RobotReviewer, a machine learning (ML) system that automatically assesses bias in clinical trials. From a (PDF-formatted) trial report, the system should determine risks of bias for the domains defined by the Cochrane Risk of Bias (RoB) tool, and extract supporting text for these judgments. We algorithmically annotated 12,808 trial PDFs using data from the Cochrane Database of Systematic Reviews (CDSR). Trials were labeled as being at low or high/unclear risk of bias for each domain, and sentences were labeled as being informative or not. This dataset was used to train a multi-task ML model. We estimated the accuracy of ML judgments versus humans by comparing trials with two or more independent RoB assessments in the CDSR. Twenty blinded experienced reviewers rated the relevance of supporting text, comparing ML output with equivalent (human-extracted) text from the CDSR. By retrieving the top 3 candidate sentences per document (top3 recall), the best ML text was rated more relevant than text from the CDSR, but not significantly (60.4% ML text rated 'highly relevant' v 56.5% of text from reviews; difference +3.9%, [-3.2% to +10.9%]). Model RoB judgments were less accurate than those from published reviews, though the difference was &lt;10% (overall accuracy 71.0% with ML v 78.3% with CDSR). Risk of bias assessment may be automated with reasonable accuracy. Automatically identified text supporting bias assessment is of equal quality to the manually identified text in the CDSR. This technology could substantially reduce reviewer workload and expedite evidence syntheses.","Marshall, I. J.
 and Kuiper, J.
 and Wallace, B. C.","Marshall, Kuiper, Wallace",not available,https://doi.org/10.1093/jamia/ocv044,2021-08-03
3318.0,,pubmed,The biology of cancer-related fatigue: a review of the literature,The biology of cancer-related fatigue: a review of the literature,"PURPOSE: Understanding the etiology of cancer-related fatigue (CRF) is critical to identify targets to develop therapies to reduce CRF burden. The goal of this systematic review was to expand on the initial work by the National Cancer Institute CRF Working Group to understand the state of the science related to the biology of CRF and, specifically, to evaluate studies that examined the relationships between biomarkers and CRF and to develop an etiologic model of CRF to guide researchers on pathways to explore or therapeutic targets to investigate. METHODS: This review was completed by the Multinational Association of Supportive Care in Cancer Fatigue Study Group-Biomarker Working Group. The initial search used three terms (biomarkers, fatigue, cancer), which yielded 11,129 articles. After removing duplicates, 9145 articles remained. Titles were assessed for the keywords 'cancer' and 'fatigue' resulting in 3811 articles. Articles published before 2010 and those with samples <50 were excluded, leaving 75 articles for full-text review. Of the 75 articles, 28 were further excluded for not investigating the associations of biomarkers and CRF. RESULTS: Of the 47 articles reviewed, 25 were cross-sectional and 22 were longitudinal studies. More than half (about 70 %) were published recently (2010-2013). Almost half (45 %) enrolled breast cancer participants. The majority of studies assessed fatigue using self-report questionnaires, and only two studies used clinical parameters to measure fatigue. CONCLUSIONS: The findings from this review suggest that CRF is linked to immune/inflammatory, metabolic, neuroendocrine, and genetic biomarkers. We also identified gaps in knowledge and made recommendations for future research.","Understanding the etiology of cancer-related fatigue (CRF) is critical to identify targets to develop therapies to reduce CRF burden. The goal of this systematic review was to expand on the initial work by the National Cancer Institute CRF Working Group to understand the state of the science related to the biology of CRF and, specifically, to evaluate studies that examined the relationships between biomarkers and CRF and to develop an etiologic model of CRF to guide researchers on pathways to explore or therapeutic targets to investigate. This review was completed by the Multinational Association of Supportive Care in Cancer Fatigue Study Group-Biomarker Working Group. The initial search used three terms (biomarkers, fatigue, cancer), which yielded 11,129 articles. After removing duplicates, 9145 articles remained. Titles were assessed for the keywords ""cancer"" and ""fatigue"" resulting in 3811 articles. Articles published before 2010 and those with samples &lt;50 were excluded, leaving 75 articles for full-text review. Of the 75 articles, 28 were further excluded for not investigating the associations of biomarkers and CRF. Of the 47 articles reviewed, 25 were cross-sectional and 22 were longitudinal studies. More than half (about 70 %) were published recently (2010-2013). Almost half (45 %) enrolled breast cancer participants. The majority of studies assessed fatigue using self-report questionnaires, and only two studies used clinical parameters to measure fatigue. The findings from this review suggest that CRF is linked to immune/inflammatory, metabolic, neuroendocrine, and genetic biomarkers. We also identified gaps in knowledge and made recommendations for future research.","Saligan, L. N.
 and Olson, K.
 and Filler, K.
 and Larkin, D.
 and Cramp, F.
 and Yennurajalingam, S.
 and Escalante, C. P.
 and del Giglio, A.
 and Kober, K. M.
 and Kamath, J.
 and Palesh, O.
 and Mustian, K.
 and Multinational Association of Supportive Care in Cancer Fatigue Study Group-Biomarker Working, Group","Saligan, Olson, Filler, Larkin, Cramp, Yennurajalingam, Sriram, Escalante, del Giglio, Kober, Kamath, Palesh, Mustian",https://dx.doi.org/10.1007/s00520-015-2763-0,https://doi.org/10.1007/s00520-015-2763-0,2021-08-03
1968.0,,pubmed,The cost effectiveness of the DIEP flap relative to the muscle-sparing TRAM flap in postmastectomy breast reconstruction,The cost effectiveness of the DIEP flap relative to the muscle-sparing TRAM flap in postmastectomy breast reconstruction,"BACKGROUND: The deep inferior epigastric perforator (DIEP) flap has gained notoriety because of its proposed benefit in decreasing donor-site morbidity but has been associated with longer operative times, higher perfusion-related complications, and increased cost relative to muscle-sparing free transverse rectus abdominis myocutaneous (TRAM) flaps. The authors performed the first cost-utility analysis examining the cost effectiveness of DIEP flaps relative to muscle-sparing free TRAM flaps in women who underwent mastectomy. METHODS: A comprehensive literature review was conducted using the MED- LINE, Embase, and Cochrane library databases to include studies directly comparing DIEP to muscle-sparing free TRAM flaps in matched patient cohorts. Eight studies were included, examining 740 DIEP flaps and 807 muscle-sparing free TRAM flaps. Costs were derived adopting both societal and third-party payer perspectives. Utilities were derived from a previous cost-utility analysis. Probabilities of clinically relevant complications were combined with cost and utility estimates to fit into a decision tree analysis. RESULTS: The overall complication rates were 24.7 percent and 21.8 percent for DIEP and muscle-sparing free TRAM flaps, respectively. The authors' baseline analysis using Medicare reimbursement revealed a cost decrease of $69.42 and a clinical benefit of 0.0035 quality-adjusted life-year when performing DIEP flap surgery relative to muscle-sparing free TRAM flap surgery, yielding an incremental cost-utility ratio of -$19,834.29. When using societal costs, the incremental cost-utility ratio increased to $87,800. CONCLUSION: DIEP flaps are cost effective relative to muscle-sparing free TRAM flaps when patients are carefully selected based on perforator anatomy and surgery is performed by experienced surgeons.","The deep inferior epigastric perforator (DIEP) flap has gained notoriety because of its proposed benefit in decreasing donor-site morbidity but has been associated with longer operative times, higher perfusion-related complications, and increased cost relative to muscle-sparing free transverse rectus abdominis myocutaneous (TRAM) flaps. The authors performed the first cost-utility analysis examining the cost effectiveness of DIEP flaps relative to muscle-sparing free TRAM flaps in women who underwent mastectomy. A comprehensive literature review was conducted using the MED- LINE, Embase, and Cochrane library databases to include studies directly comparing DIEP to muscle-sparing free TRAM flaps in matched patient cohorts. Eight studies were included, examining 740 DIEP flaps and 807 muscle-sparing free TRAM flaps. Costs were derived adopting both societal and third-party payer perspectives. Utilities were derived from a previous cost-utility analysis. Probabilities of clinically relevant complications were combined with cost and utility estimates to fit into a decision tree analysis. The overall complication rates were 24.7 percent and 21.8 percent for DIEP and muscle-sparing free TRAM flaps, respectively. The authors' baseline analysis using Medicare reimbursement revealed a cost decrease of $69.42 and a clinical benefit of 0.0035 quality-adjusted life-year when performing DIEP flap surgery relative to muscle-sparing free TRAM flap surgery, yielding an incremental cost-utility ratio of -$19,834.29. When using societal costs, the incremental cost-utility ratio increased to $87,800. DIEP flaps are cost effective relative to muscle-sparing free TRAM flaps when patients are carefully selected based on perforator anatomy and surgery is performed by experienced surgeons.","Krishnan, N. M.
 and Purnell, C.
 and Nahabedian, M. Y.
 and Freed, G. L.
 and Nigriny, J. F.
 and Rosen, J. M.
 and Rosson, G. D.","Krishnan, Purnell, Nahabedian, Freed, Nigriny, Rosen, Rosson",https://dx.doi.org/10.1097/PRS.0000000000001125,https://doi.org/10.1097/PRS.0000000000001125,2021-08-03
593.0,,pubmed,A Cost-Utility Analysis Comparing the Sartorius versus the Rectus Femoris Flap in the Treatment of the Infected Vascular Groin Graft Wound,A Cost-Utility Analysis Comparing the Sartorius versus the Rectus Femoris Flap in the Treatment of the Infected Vascular Groin Graft Wound,"BACKGROUND: The purpose of this study was to examine the sartorius and rectus femoris flaps as reasonable coverage options for the infected vascular groin graft wound. The authors' goal was to perform a cost-utility analysis of the sartorius flap versus the rectus femoris flap in the treatment of an infected vascular groin graft. METHODS: Cost-utility methodology involved a literature review compiling outcomes for specific flap interventions, obtaining utility scores for complications to estimate quality-adjusted life-years, accruing costs using Diagnosis-Related Group and Current Procedural Terminology codes for each intervention, and developing a decision tree that could portray the more cost-effective strategy. The authors also performed sensitivity analysis to check the robustness of their data. Szilyagi III and Samson III and IV grades of infected groin grafts were included in the study. RESULTS: Twenty-six studies were used pooling 296 patients (234 sartorius flaps and 62 rectus flaps). Decision tree analysis noted that the rectus femoris flap was the more cost-effective option. It was the dominant treatment option given that it was more clinically effective by an additional 0.30 quality- adjusted life-years, with the sartorius flap option costing an additional $2241.88. The sartorius flap had a 13.68 percent major complication rate versus an 8.6 percent major complication rate for the rectus femoris flap. One-way sensitivity analysis showed that the sartorius flap became a cost-effective option if its major complication rate was less than or equal to 8.89 percent. CONCLUSION: The rectus femoris flap in the treatment of the infected vascular groin graft is a cost-effective option compared with the sartorius flap.","The purpose of this study was to examine the sartorius and rectus femoris flaps as reasonable coverage options for the infected vascular groin graft wound. The authors' goal was to perform a cost-utility analysis of the sartorius flap versus the rectus femoris flap in the treatment of an infected vascular groin graft. Cost-utility methodology involved a literature review compiling outcomes for specific flap interventions, obtaining utility scores for complications to estimate quality-adjusted life-years, accruing costs using Diagnosis-Related Group and Current Procedural Terminology codes for each intervention, and developing a decision tree that could portray the more cost-effective strategy. The authors also performed sensitivity analysis to check the robustness of their data. Szilyagi III and Samson III and IV grades of infected groin grafts were included in the study. Twenty-six studies were used pooling 296 patients (234 sartorius flaps and 62 rectus flaps). Decision tree analysis noted that the rectus femoris flap was the more cost-effective option. It was the dominant treatment option given that it was more clinically effective by an additional 0.30 quality- adjusted life-years, with the sartorius flap option costing an additional $2241.88. The sartorius flap had a 13.68 percent major complication rate versus an 8.6 percent major complication rate for the rectus femoris flap. One-way sensitivity analysis showed that the sartorius flap became a cost-effective option if its major complication rate was less than or equal to 8.89 percent. The rectus femoris flap in the treatment of the infected vascular groin graft is a cost-effective option compared with the sartorius flap.","Chatterjee, A.
 and Kosowski, T.
 and Pyfer, B.
 and Fisher, C. S.
 and Tchou, J. C.
 and Maddali, S.","Chatterjee, Kosowski, Pyfer, Fisher, Tchou, Maddali",https://dx.doi.org/10.1097/PRS.0000000000001267,https://doi.org/10.1097/PRS.0000000000001267,2021-08-03
594.0,,pubmed,The use of mesh versus primary fascial closure of the abdominal donor site when using a transverse rectus abdominis myocutaneous flap for breast reconstruction: a cost-utility analysis,The use of mesh versus primary fascial closure of the abdominal donor site when using a transverse rectus abdominis myocutaneous flap for breast reconstruction: a cost-utility analysis,"BACKGROUND: During breast reconstruction using the transverse rectus abdominis myocutaneous (TRAM) flap, the use of mesh for abdominal donor-site closure provides for a technology that potentially offers clinical benefit yet incurs an added cost. The authors' goal was to determine whether it is cost effective to use mesh during abdominal donor-site closure when performing a TRAM flap for breast reconstruction. METHODS: A literature review was conducted to identify and collect published hernia and bulge rates at abdominal TRAM flap donor sites closed either primarily or with mesh. A decision tree analysis was performed. Outcome probabilities, costs of complications, and expert utility estimates were populated into the decision tree model to evaluate the cost-utility of using mesh in TRAM abdominal donor-site closure. One-way sensitivity analyses were performed to verify the robustness of the results. RESULTS: The authors' literature review resulted in 10 articles describing 1195 patients who had TRAM abdominal donor-site closure primarily and 696 patients who had donor-site closure performed with mesh. Pooled hernia/bulge complication rates for these two groups were 7.87 percent and 4.45 percent, respectively. The use of mesh was more clinically effective based on total quality-adjusted life-years gained of 30.53 compared with 30.41 when performing primary fascial closure alone. The incremental additional cost incurred by the mesh arm when running the decision tree model was $693.14. This difference in cost, divided by the difference in clinical efficacy (0.12), results in an incremental cost-utility ratio value of $5776.17 per quality-adjusted life-year gained when using mesh, making it cost effective (when using a willingness-to-pay threshold of $50,000). One-way sensitivity analysis revealed the following: (1) using mesh was a cost effective option, provided that the price of mesh was less than or equal to $5970; (2) mesh was cost effective when its use led to a hernia/bulge rate less than or equal to 7.25 percent; and (3) primary facial closure was cost effective when its use led to a hernia/bulge rate less than or equal to 4.75 percent. CONCLUSION: The use of mesh when repairing the abdominal donor site during a pedicled or free TRAM flap breast reconstruction is cost effective compared with primary fascial closure alone.","During breast reconstruction using the transverse rectus abdominis myocutaneous (TRAM) flap, the use of mesh for abdominal donor-site closure provides for a technology that potentially offers clinical benefit yet incurs an added cost. The authors' goal was to determine whether it is cost effective to use mesh during abdominal donor-site closure when performing a TRAM flap for breast reconstruction. A literature review was conducted to identify and collect published hernia and bulge rates at abdominal TRAM flap donor sites closed either primarily or with mesh. A decision tree analysis was performed. Outcome probabilities, costs of complications, and expert utility estimates were populated into the decision tree model to evaluate the cost-utility of using mesh in TRAM abdominal donor-site closure. One-way sensitivity analyses were performed to verify the robustness of the results. The authors' literature review resulted in 10 articles describing 1195 patients who had TRAM abdominal donor-site closure primarily and 696 patients who had donor-site closure performed with mesh. Pooled hernia/bulge complication rates for these two groups were 7.87 percent and 4.45 percent, respectively. The use of mesh was more clinically effective based on total quality-adjusted life-years gained of 30.53 compared with 30.41 when performing primary fascial closure alone. The incremental additional cost incurred by the mesh arm when running the decision tree model was $693.14. This difference in cost, divided by the difference in clinical efficacy (0.12), results in an incremental cost-utility ratio value of $5776.17 per quality-adjusted life-year gained when using mesh, making it cost effective (when using a willingness-to-pay threshold of $50,000). One-way sensitivity analysis revealed the following: (1) using mesh was a cost effective option, provided that the price of mesh was less than or equal to $5970; (2) mesh was cost effective when its use led to a hernia/bulge rate less than or equal to 7.25 percent; and (3) primary facial closure was cost effective when its use led to a hernia/bulge rate less than or equal to 4.75 percent. The use of mesh when repairing the abdominal donor site during a pedicled or free TRAM flap breast reconstruction is cost effective compared with primary fascial closure alone.","Chatterjee, A.
 and Ramkumar, D. B.
 and Dawli, T. B.
 and Nigriny, J. F.
 and Stotland, M. A.
 and Ridgway, E. B.","Chatterjee, Ramkumar, Dawli, Nigriny, Stotland, Ridgway",https://dx.doi.org/10.1097/PRS.0000000000000957,https://doi.org/10.1097/PRS.0000000000000957,2021-08-03
3023.0,,pubmed,Retained surgical item identification on imaging studies: a training module for radiology residents,Retained surgical item identification on imaging studies: a training module for radiology residents,"PURPOSE: Many hospitals experience one or more retained surgical item events per year, with risk of patient morbidity and medicolegal consequences. We hypothesized that the confidence and performance of the radiologist would be enhanced by prior training in retained surgical item detection and by prior exposure to commonly employed surgical instruments and devices. METHODS: A training module for radiology residents was created through literature review, expert consultation, and imaging of commonly employed surgical instruments and devices. A survey assessing resident command of background knowledge, policy, and image-based retained surgical item questions was created. Additionally resident confidence for hospital policy and retained surgical item identification was assessed. A pre-module survey and confidence questionnaire were administered to first- through fourth- year residents. For one month, the training module was available online for independent review. Subsequently, a post-module survey and confidence questionnaire were completed by participants. T tests were performed to evaluate pre- and posttest means for survey performance and confidence questions. RESULTS: Mean post-module survey performance significantly improved compared with pre-module performance. Mean confidence levels for ability to incidentally identify a retained surgical item on a radiograph obtained for another indication and current understanding of the institution's policy regarding retained surgical items were also significantly increased. CONCLUSION: The knowledge base, diagnostic performance, and confidence of radiology residents were significantly enhanced by online teaching module training in retained surgical item detection.","Many hospitals experience one or more retained surgical item events per year, with risk of patient morbidity and medicolegal consequences. We hypothesized that the confidence and performance of the radiologist would be enhanced by prior training in retained surgical item detection and by prior exposure to commonly employed surgical instruments and devices. A training module for radiology residents was created through literature review, expert consultation, and imaging of commonly employed surgical instruments and devices. A survey assessing resident command of background knowledge, policy, and image-based retained surgical item questions was created. Additionally resident confidence for hospital policy and retained surgical item identification was assessed. A pre-module survey and confidence questionnaire were administered to first- through fourth- year residents. For one month, the training module was available online for independent review. Subsequently, a post-module survey and confidence questionnaire were completed by participants. T tests were performed to evaluate pre- and posttest means for survey performance and confidence questions. Mean post-module survey performance significantly improved compared with pre-module performance. Mean confidence levels for ability to incidentally identify a retained surgical item on a radiograph obtained for another indication and current understanding of the institution's policy regarding retained surgical items were also significantly increased. The knowledge base, diagnostic performance, and confidence of radiology residents were significantly enhanced by online teaching module training in retained surgical item detection.","Porter, K. K.
 and Bailey, P. D.
 and Woods, R.
 and Scott, W. W., Jr.
 and Johnson, P. T.","Porter, Bailey, Woods, Scott, Johnson",https://dx.doi.org/10.1007/s11548-015-1154-9,https://doi.org/10.1007/s11548-015-1154-9,2021-08-03
2822.0,,pubmed,Vitamin D and assisted reproduction: should vitamin D be routinely screened and repleted prior to ART? A systematic review,Vitamin D and assisted reproduction: should vitamin D be routinely screened and repleted prior to ART? A systematic review,"PURPOSE: To review the current literature regarding the role of vitamin D status in pregnancy outcomes in women undergoing assisted reproductive technology (ART) and to assess cost-effectiveness of routine vitamin D deficiency screening and repletion prior to initiation of ART. METHODS: A systematic literature review was conducted using PubMed. Relevant study outcomes were compared among the selected studies. A cost-benefit analysis was performed using a decision tree mathematical model with sensitivity analyses from the perspective of direct societal cost. Published data were used to estimate probabilities and costs in 2014 US dollars. RESULTS: Thirty-four articles were retrieved, of which eight met inclusion criteria. One study demonstrated a negative relationship between vitamin D status and ART outcomes, while two studies showed no association. The remaining five studies concluded that ART outcomes improved after vitamin D repletion. CONCLUSION: The majority of reviewed studies reported a decrement in ART outcomes in patients with vitamin D deficiency. Cost-benefit analyses suggested that screening and supplementing vitamin D prior to ART might be cost effective, but further evidence is needed. Given the absence of Level I evidence regarding vitamin D status and ART outcomes, full endorsement of routine vitamin D screening and supplementation prior to ART is premature.","To review the current literature regarding the role of vitamin D status in pregnancy outcomes in women undergoing assisted reproductive technology (ART) and to assess cost-effectiveness of routine vitamin D deficiency screening and repletion prior to initiation of ART. A systematic literature review was conducted using PubMed. Relevant study outcomes were compared among the selected studies. A cost-benefit analysis was performed using a decision tree mathematical model with sensitivity analyses from the perspective of direct societal cost. Published data were used to estimate probabilities and costs in 2014 US dollars. Thirty-four articles were retrieved, of which eight met inclusion criteria. One study demonstrated a negative relationship between vitamin D status and ART outcomes, while two studies showed no association. The remaining five studies concluded that ART outcomes improved after vitamin D repletion. The majority of reviewed studies reported a decrement in ART outcomes in patients with vitamin D deficiency. Cost-benefit analyses suggested that screening and supplementing vitamin D prior to ART might be cost effective, but further evidence is needed. Given the absence of Level I evidence regarding vitamin D status and ART outcomes, full endorsement of routine vitamin D screening and supplementation prior to ART is premature.","Pacis, M. M.
 and Fortin, C. N.
 and Zarek, S. M.
 and Mumford, S. L.
 and Segars, J. H.","Pacis, Fortin, Zarek, Mumford, Segars",https://dx.doi.org/10.1007/s10815-014-0407-9,https://doi.org/10.1007/s10815-014-0407-9,2021-08-03
2778.0,,pubmed,Techniques for identifying cross-disciplinary and 'hard-to-detect' evidence for systematic review,Techniques for identifying cross-disciplinary and 'hard-to-detect' evidence for systematic review,"Driven by necessity in our own complex review, we developed alternative systematic ways of identifying relevant evidence where the key concepts are generally not focal to the primary studies' aims and are found across multiple disciplines-that is, hard-to-detect evidence. Specifically, we sought to identify evidence on community engagement in public health interventions that aim to reduce health inequalities. Our initial search strategy used text mining to identify synonyms for the concept 'community engagement'. We conducted a systematic search for reviews on public health interventions, supplemented by searches of trials databases. We then used information in the reviews' evidence tables to gather more information about the included studies than was evident in the primary studies' own titles or abstracts. We identified 319 primary studies cited in reviews after full-text screening. In this paper, we retrospectively reflect on the challenges and benefits of the approach taken. We estimate that more than a quarter of the studies that were identified would have been missed by typical searching and screening methods. This identification strategy was highly effective and could be useful for reviews of broad research questions, or where the key concepts are unlikely to be the main focus of primary research.","Driven by necessity in our own complex review, we developed alternative systematic ways of identifying relevant evidence where the key concepts are generally not focal to the primary studies' aims and are found across multiple disciplines-that is, hard-to-detect evidence. Specifically, we sought to identify evidence on community engagement in public health interventions that aim to reduce health inequalities. Our initial search strategy used text mining to identify synonyms for the concept 'community engagement'. We conducted a systematic search for reviews on public health interventions, supplemented by searches of trials databases. We then used information in the reviews' evidence tables to gather more information about the included studies than was evident in the primary studies' own titles or abstracts. We identified 319 primary studies cited in reviews after full-text screening. In this paper, we retrospectively reflect on the challenges and benefits of the approach taken. We estimate that more than a quarter of the studies that were identified would have been missed by typical searching and screening methods. This identification strategy was highly effective and could be useful for reviews of broad research questions, or where the key concepts are unlikely to be the main focus of primary research.","O'Mara-Eves, A.
 and Brunton, G.
 and McDaid, D.
 and Kavanagh, J.
 and Oliver, S.
 and Thomas, J.","O'Mara-Eves, Brunton, McDaid, Kavanagh, Oliver, Thomas",not available,https://doi.org/10.1002/jrsm.1094,2021-08-03
2458.0,,pubmed,Sentinel lymph node status in vulval cancer: systematic reviews of test accuracy and decision-analytic model-based economic evaluation,Sentinel lymph node status in vulval cancer: systematic reviews of test accuracy and decision-analytic model-based economic evaluation,"BACKGROUND: Vulval cancer causes 3-5% of all gynaecological malignancies and requires surgical removal and inguinofemoral lymphadenectomy (IFL). Complications affect > 50% of patients, including groin wound infection, lymphoedema and cellulitis. A sentinel lymph node (SLN) is the first groin node with the highest probability of malignancy. SLN biopsy would be useful if it could accurately identify patients in whom cancer has spread to the groin, without removing all groin nodes. SLNs can be identified by isosulfan blue dye and/or technetium-99 ((99m)Tc) radioactive tracer during lymphoscintigraphy. The blue dye/(99m)Tc procedure only detects SLN, not metastases - this requires histological examination, which can include ultrastaging and staining with conventional haematoxylin and eosin (H&E) or immunohistochemistry. OBJECTIVES: To determine the test accuracy and cost-effectiveness of the SLN biopsy with (99m)Tc and/or blue dye compared with IFL or clinical follow-up for test negatives in vulval cancer, through systematic reviews and economic evaluation. DATA SOURCES: Standard medical databases, including MEDLINE, EMBASE, Science Citation Index and The Cochrane Library, medical search gateways, reference lists of review articles and included studies were searched to January 2011. METHODS: For accuracy and effectiveness, standard methods were used and reported according to Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Searches were to January 2011, with no language restrictions. Meta-analyses were carried out with Meta-Disc version 1.4 (Javier Zamora, Madrid, Spain) for accuracy; none was appropriate for effectiveness. The economic evaluation from a NHS perspective used a decision-tree model in DATA TreeAge Pro Healthcare 2001 (TreeAge Software, Inc., Williamstown, MA, USA). Six options (blue dye with H&E, blue dye with ultrastaging, (99m)Tc with H&E, (99m)Tc with ultrastaging, blue dye/(99m)Tc with H&E, blue dye/(99m)Tc with ultrastaging) were compared with IFL. Deterministic and probabilistic sensitivity analyses were conducted. RESULTS: For accuracy, of the 26 included studies, most evaluated (99m)Tc/blue dye combined. Four studies had clinical follow-up only for test negatives and five had clinical follow-up for all and IFL for test negatives. Numbers with no SLN found were difficult to distinguish from those with negative SLN biopsies. The largest group of 11 studies using (99m)Tc/blue dye, ultrastaging and immunohistochemistry had a pooled sensitivity of 95.6% [95% confidence interval (CI) 91.5% to 98.1%] and a specificity of 100% (95% CI 99.0% to 100%). Mean SLN detection rates were 94.6% for (99m)Tc, 68.7% for blue dye and 97.7% for both. One study measured global health status quality of life (QoL) and found no difference between SLN biopsy and IFL. One patient preference evaluation showed that 66% preferred IFL rather than a 5% false-negative rate from SLN biopsy. For effectiveness, of 14,038 references, one randomised controlled trial, three case-control studies and 13 case series were found. Approximately 50% died from vulval cancer and 50% from other causes during follow-ups. Recurrences were in the ratio of approximately 4 : 2 : 1 vulval, groin and distant, with more recurrences in node-positive patients. No studies reported QoL. For cost per death averted, IFL was less costly and more effective than strategies using SLN biopsy. For morbidity-free survival and long-term morbidity-free survival, (99m)Tc with ultrastaging was most cost-effective. Strategies with blue dye only and H&E only were never cost-effective. The incremental cost-effectiveness ratio for (99m)Tc with ultrastaging compared with IFL was 4300 per case of morbidity-free survival and 7100 per long-term morbidity-free survival. LIMITATIONS: The main limitations of this study include the lack of good-quality evidence on accuracy, effectiveness and QoL. A large project such as this takes time to publish, so the most recent studies are not included. CONCLUSIONS: A sensitive and specific combined metastatic SLN detection test and information on generic QoL in vulval cancer is urgently required. FUNDING: The National Institute for Health Research Health Technology Assessment programme.","Vulval cancer causes 3-5% of all gynaecological malignancies and requires surgical removal and inguinofemoral lymphadenectomy (IFL). Complications affect &gt;â€‰50% of patients, including groin wound infection, lymphoedema and cellulitis. A sentinel lymph node (SLN) is the first groin node with the highest probability of malignancy. SLN biopsy would be useful if it could accurately identify patients in whom cancer has spread to the groin, without removing all groin nodes. SLNs can be identified by isosulfan blue dye and/or technetium-99 ((99m)Tc) radioactive tracer during lymphoscintigraphy. The blue dye/(99m)Tc procedure only detects SLN, not metastases - this requires histological examination, which can include ultrastaging and staining with conventional haematoxylin and eosin (H&amp;E) or immunohistochemistry. To determine the test accuracy and cost-effectiveness of the SLN biopsy with (99m)Tc and/or blue dye compared with IFL or clinical follow-up for test negatives in vulval cancer, through systematic reviews and economic evaluation. Standard medical databases, including MEDLINE, EMBASE, Science Citation Index and The Cochrane Library, medical search gateways, reference lists of review articles and included studies were searched to January 2011. For accuracy and effectiveness, standard methods were used and reported according to Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Searches were to January 2011, with no language restrictions. Meta-analyses were carried out with Meta-Disc version 1.4 (Javier Zamora, Madrid, Spain) for accuracy; none was appropriate for effectiveness. The economic evaluation from a NHS perspective used a decision-tree model in DATA TreeAge Pro Healthcare 2001 (TreeAge Software, Inc., Williamstown, MA, USA). Six options (blue dye with H&amp;E, blue dye with ultrastaging, (99m)Tc with H&amp;E, (99m)Tc with ultrastaging, blue dye/(99m)Tc with H&amp;E, blue dye/(99m)Tc with ultrastaging) were compared with IFL. Deterministic and probabilistic sensitivity analyses were conducted. For accuracy, of the 26 included studies, most evaluated (99m)Tc/blue dye combined. Four studies had clinical follow-up only for test negatives and five had clinical follow-up for all and IFL for test negatives. Numbers with no SLN found were difficult to distinguish from those with negative SLN biopsies. The largest group of 11 studies using (99m)Tc/blue dye, ultrastaging and immunohistochemistry had a pooled sensitivity of 95.6% [95% confidence interval (CI) 91.5% to 98.1%] and a specificity of 100% (95% CI 99.0% to 100%). Mean SLN detection rates were 94.6% for (99m)Tc, 68.7% for blue dye and 97.7% for both. One study measured global health status quality of life (QoL) and found no difference between SLN biopsy and IFL. One patient preference evaluation showed that 66% preferred IFL rather than a 5% false-negative rate from SLN biopsy. For effectiveness, of 14,038 references, one randomised controlled trial, three case-control studies and 13 case series were found. Approximately 50% died from vulval cancer and 50% from other causes during follow-ups. Recurrences were in the ratio of approximately 4â€‰:â€‰2â€‰:â€‰1 vulval, groin and distant, with more recurrences in node-positive patients. No studies reported QoL. For cost per death averted, IFL was less costly and more effective than strategies using SLN biopsy. For morbidity-free survival and long-term morbidity-free survival, (99m)Tc with ultrastaging was most cost-effective. Strategies with blue dye only and H&amp;E only were never cost-effective. The incremental cost-effectiveness ratio for (99m)Tc with ultrastaging compared with IFL was Â£4300 per case of morbidity-free survival and Â£7100 per long-term morbidity-free survival. The main limitations of this study include the lack of good-quality evidence on accuracy, effectiveness and QoL. A large project such as this takes time to publish, so the most recent studies are not included. A sensitive and specific combined metastatic SLN detection test and information on generic QoL in vulval cancer is urgently required. The National Institute for Health Research Health Technology Assessment programme.","Meads, C.
 and Sutton, A.
 and Malysiak, S.
 and Kowalska, M.
 and Zapalska, A.
 and Rogozinska, E.
 and Baldwin, P.
 and Rosenthal, A.
 and Ganesan, R.
 and Borowiack, E.
 and Barton, P.
 and Roberts, T.
 and Sundar, S.
 and Khan, K.","Meads, Sutton, MaÅ‚ysiak, Kowalska, Zapalska, Rogozinska, Baldwin, Rosenthal, Ganesan, Borowiack, Barton, Roberts, Sundar, Khan",https://dx.doi.org/10.3310/hta17600,https://doi.org/10.3310/hta17600,2021-08-03
1344.0,,pubmed,Comparing five alternative methods of breast reconstruction surgery: a cost-effectiveness analysis,Comparing five alternative methods of breast reconstruction surgery: a cost-effectiveness analysis,"BACKGROUND: The purpose of this study was to assess the cost-effectiveness of five standardized procedures for breast reconstruction to delineate the best reconstructive approach in postmastectomy patients in the settings of nonirradiated and irradiated chest walls. METHODS: A decision tree was used to model five breast reconstruction procedures from the provider perspective to evaluate cost-effectiveness. Procedures included autologous flaps with pedicled tissue, autologous flaps with free tissue, latissimus dorsi flaps with breast implants, expanders with implant exchange, and immediate implant placement. All methods were compared with a 'do-nothing' alternative. Data for model parameters were collected through a systematic review, and patient health utilities were calculated from an ad hoc survey of reconstructive surgeons. Results were measured in cost (2011 U.S. dollars) per quality-adjusted life-year. Univariate sensitivity analyses and Bayesian multivariate probabilistic sensitivity analysis were conducted. RESULTS: Pedicled autologous tissue and free autologous tissue reconstruction were cost-effective compared with the do-nothing alternative. Pedicled autologous tissue was the slightly more cost-effective of the two. The other procedures were not found to be cost-effective. The results were robust to a number of sensitivity analyses, although the margin between pedicled and free autologous tissue reconstruction is small and affected by some parameter values. CONCLUSIONS: Autologous pedicled tissue was slightly more cost-effective than free tissue reconstruction in irradiated and nonirradiated patients. Implant-based techniques were not cost-effective. This is in agreement with the growing trend at academic institutions to encourage autologous tissue reconstruction because of its natural recreation of the breast contour, suppleness, and resiliency in the setting of irradiated recipient beds.","The purpose of this study was to assess the cost-effectiveness of five standardized procedures for breast reconstruction to delineate the best reconstructive approach in postmastectomy patients in the settings of nonirradiated and irradiated chest walls. A decision tree was used to model five breast reconstruction procedures from the provider perspective to evaluate cost-effectiveness. Procedures included autologous flaps with pedicled tissue, autologous flaps with free tissue, latissimus dorsi flaps with breast implants, expanders with implant exchange, and immediate implant placement. All methods were compared with a ""do-nothing"" alternative. Data for model parameters were collected through a systematic review, and patient health utilities were calculated from an ad hoc survey of reconstructive surgeons. Results were measured in cost (2011 U.S. dollars) per quality-adjusted life-year. Univariate sensitivity analyses and Bayesian multivariate probabilistic sensitivity analysis were conducted. Pedicled autologous tissue and free autologous tissue reconstruction were cost-effective compared with the do-nothing alternative. Pedicled autologous tissue was the slightly more cost-effective of the two. The other procedures were not found to be cost-effective. The results were robust to a number of sensitivity analyses, although the margin between pedicled and free autologous tissue reconstruction is small and affected by some parameter values. Autologous pedicled tissue was slightly more cost-effective than free tissue reconstruction in irradiated and nonirradiated patients. Implant-based techniques were not cost-effective. This is in agreement with the growing trend at academic institutions to encourage autologous tissue reconstruction because of its natural recreation of the breast contour, suppleness, and resiliency in the setting of irradiated recipient beds.","Grover, R.
 and Padula, W. V.
 and Van Vliet, M.
 and Ridgway, E. B.","Grover, Padula, Van Vliet, Ridgway",https://dx.doi.org/10.1097/PRS.0b013e3182a48b10,https://doi.org/10.1097/PRS.0b013e3182a48b10,2021-08-03
3731.0,,pubmed,Cost-effectiveness of sentinel lymph node biopsy vs inguinofemoral lymphadenectomy in women with vulval cancer,Cost-effectiveness of sentinel lymph node biopsy vs inguinofemoral lymphadenectomy in women with vulval cancer,"BACKGROUND: This study examines the cost-effectiveness of sentinel lymph node biopsy, a potentially less morbid procedure, compared with inguinofemoral lymphadenectomy (IFL) among women with stage I and stage II vulval squamous cell carcinoma. METHODS: A model-based economic evaluation was undertaken based on clinical evidence from a systematic review of published sources. A decision tree model was developed with the structure being informed by clinical input, taking the perspective of the health-care provider. RESULTS: For overall survival for 2 years, IFL was found to be the most cost-effective option and dominated all other strategies, being the least costly and most effective. For morbidity-free related outcomes for 2 years, sentinel lymph node (SLN) biopsy with 99mTc and blue dye and haematoxylin & eosin (H&E) histopathology, with ultrastaging and immunohistochemistry reserved for those that test negative following H&E is likely to be the most effective approach. CONCLUSION: SLN biopsy using 99mTc and blue dye with ultrastaging may be considered the most cost-effective strategy based on the outcome of survival free of morbidity for 2 years. The findings here also indicate that using blue dye and H&E for the identification of the SLN and the identification of metastasis, respectively, are not sensitive enough to be used on their own.","This study examines the cost-effectiveness of sentinel lymph node biopsy, a potentially less morbid procedure, compared with inguinofemoral lymphadenectomy (IFL) among women with stage I and stage II vulval squamous cell carcinoma. A model-based economic evaluation was undertaken based on clinical evidence from a systematic review of published sources. A decision tree model was developed with the structure being informed by clinical input, taking the perspective of the health-care provider. For overall survival for 2 years, IFL was found to be the most cost-effective option and dominated all other strategies, being the least costly and most effective. For morbidity-free related outcomes for 2 years, sentinel lymph node (SLN) biopsy with 99mTc and blue dye and haematoxylin &amp; eosin (H&amp;E) histopathology, with ultrastaging and immunohistochemistry reserved for those that test negative following H&amp;E is likely to be the most effective approach. SLN biopsy using 99mTc and blue dye with ultrastaging may be considered the most cost-effective strategy based on the outcome of survival free of morbidity for 2 years. The findings here also indicate that using blue dye and H&amp;E for the identification of the SLN and the identification of metastasis, respectively, are not sensitive enough to be used on their own.","Sutton, A. J.
 and Barton, P.
 and Sundar, S.
 and Meads, C.
 and Rosenthal, A. N.
 and Baldwin, P.
 and Khan, K.
 and Roberts, T. E.","Sutton, Barton, Sundar, Meads, Rosenthal, Baldwin, Khan, Roberts",https://dx.doi.org/10.1038/bjc.2013.631,https://doi.org/10.1038/bjc.2013.631,2021-08-03
3723.0,,pubmed,Multivariate classification of blood oxygen level-dependent FMRI data with diagnostic intention: a clinical perspective,Multivariate classification of blood oxygen level-dependent FMRI data with diagnostic intention: a clinical perspective,"SUMMARY: There has been a recent upsurge of reports about applications of pattern-recognition techniques from the field of machine learning to functional MR imaging data as a diagnostic tool for systemic brain disease or psychiatric disorders. Entities studied include depression, schizophrenia, attention deficit hyperactivity disorder, and neurodegenerative disorders like Alzheimer dementia. We review these recent studies which-despite the optimism from some articles-predominantly constitute explorative efforts at the proof-of-concept level. There is some evidence that, in particular, support vector machines seem to be promising. However, the field is still far from real clinical application, and much work has to be done regarding data preprocessing, model optimization, and validation. Reporting standards are proposed to facilitate future meta-analyses or systematic reviews.","There has been a recent upsurge of reports about applications of pattern-recognition techniques from the field of machine learning to functional MR imaging data as a diagnostic tool for systemic brain disease or psychiatric disorders. Entities studied include depression, schizophrenia, attention deficit hyperactivity disorder, and neurodegenerative disorders like Alzheimer dementia. We review these recent studies which-despite the optimism from some articles-predominantly constitute explorative efforts at the proof-of-concept level. There is some evidence that, in particular, support vector machines seem to be promising. However, the field is still far from real clinical application, and much work has to be done regarding data preprocessing, model optimization, and validation. Reporting standards are proposed to facilitate future meta-analyses or systematic reviews.","Sundermann, B.
 and Herr, D.
 and Schwindt, W.
 and Pfleiderer, B.","Sundermann, Herr, Schwindt, Pfleiderer",https://dx.doi.org/10.3174/ajnr.A3713,https://doi.org/10.3174/ajnr.A3713,2021-08-03
4472.0,,pubmed,Anxiolytic-like effects of antisauvagine-30 in mice are not mediated by CRF2 receptors,Anxiolytic-like effects of antisauvagine-30 in mice are not mediated by CRF2 receptors,"The role of brain corticotropin-releasing factor type 2 (CRF2) receptors in behavioral stress responses remains controversial. Conflicting findings suggest pro-stress, anti-stress or no effects of impeding CRF2 signaling. Previous studies have used antisauvagine-30 as a selective CRF2 antagonist. The present study tested the hypotheses that 1) potential anxiolytic-like actions of intracerebroventricular (i.c.v.) administration of antisauvagine-30 also are present in mice lacking CRF2 receptors and 2) potential anxiolytic-like effects of antisauvagine-30 are not shared by the more selective CRF2 antagonist astressin2-B. Cannulated, male CRF2 receptor knockout (n = 22) and wildtype littermate mice (n = 21) backcrossed onto a C57BL/6J genetic background were tested in the marble burying, elevated plus-maze, and shock-induced freezing tests following pretreatment (i.c.v.) with vehicle, antisauvagine-30 or astressin2-B. Antisauvagine-30 reduced shock-induced freezing equally in wildtype and CRF2 knockout mice. In contrast, neither astressin2-B nor CRF2 genotype influenced shock-induced freezing. Neither CRF antagonist nor CRF2 genotype influenced anxiety-like behavior in the plus-maze or marble burying tests. A literature review showed that the typical antisauvagine-30 concentration infused in previous intracranial studies (~1 mM) was 3 orders greater than its IC50 to block CRF1-mediated cAMP responses and 4 orders greater than its binding constants (Kd , Ki ) for CRF1 receptors. Thus, increasing, previously used doses of antisauvagine-30 also exert non-CRF2-mediated effects, perhaps via CRF1. The results do not support the hypothesis that brain CRF2 receptors tonically promote anxiogenic-like behavior. Utilization of CRF2 antagonists, such as astressin2-B, at doses that are more subtype-selective, can better clarify the significance of brain CRF2 systems in stress-related behavior.","The role of brain corticotropin-releasing factor type 2 (CRF2) receptors in behavioral stress responses remains controversial. Conflicting findings suggest pro-stress, anti-stress or no effects of impeding CRF2 signaling. Previous studies have used antisauvagine-30 as a selective CRF2 antagonist. The present study tested the hypotheses that 1) potential anxiolytic-like actions of intracerebroventricular (i.c.v.) administration of antisauvagine-30 also are present in mice lacking CRF2 receptors and 2) potential anxiolytic-like effects of antisauvagine-30 are not shared by the more selective CRF2 antagonist astressin2-B. Cannulated, male CRF2 receptor knockout (nâ€Š=â€Š22) and wildtype littermate mice (nâ€Š=â€Š21) backcrossed onto a C57BL/6J genetic background were tested in the marble burying, elevated plus-maze, and shock-induced freezing tests following pretreatment (i.c.v.) with vehicle, antisauvagine-30 or astressin2-B. Antisauvagine-30 reduced shock-induced freezing equally in wildtype and CRF2 knockout mice. In contrast, neither astressin2-B nor CRF2 genotype influenced shock-induced freezing. Neither CRF antagonist nor CRF2 genotype influenced anxiety-like behavior in the plus-maze or marble burying tests. A literature review showed that the typical antisauvagine-30 concentration infused in previous intracranial studies (âˆ¼1 mM) was 3 orders greater than its IC50 to block CRF1-mediated cAMP responses and 4 orders greater than its binding constants (Kd , Ki ) for CRF1 receptors. Thus, increasing, previously used doses of antisauvagine-30 also exert non-CRF2-mediated effects, perhaps via CRF1. The results do not support the hypothesis that brain CRF2 receptors tonically promote anxiogenic-like behavior. Utilization of CRF2 antagonists, such as astressin2-B, at doses that are more subtype-selective, can better clarify the significance of brain CRF2 systems in stress-related behavior.","Zorrilla, E. P.
 and Roberts, A. J.
 and Rivier, J. E.
 and Koob, G. F.","Zorrilla, Roberts, Rivier, Koob",https://dx.doi.org/10.1371/journal.pone.0063942,https://doi.org/10.1371/journal.pone.0063942,2021-08-03
870.0,,pubmed,Using the electronic medical record to identify community-acquired pneumonia: toward a replicable automated strategy,Using the electronic medical record to identify community-acquired pneumonia: toward a replicable automated strategy,"BACKGROUND: Timely information about disease severity can be central to the detection and management of outbreaks of acute respiratory infections (ARI), including influenza. We asked if two resources: 1) free text, and 2) structured data from an electronic medical record (EMR) could complement each other to identify patients with pneumonia, an ARI severity landmark. METHODS: A manual EMR review of 2747 outpatient ARI visits with associated chest imaging identified x-ray reports that could support the diagnosis of pneumonia (kappa score = 0.88 (95% CI 0.82:0.93)), along with attendant cases with Possible Pneumonia (adds either cough, sputum, fever/chills/night sweats, dyspnea or pleuritic chest pain) or with Pneumonia-in-Plan (adds pneumonia stated as a likely diagnosis by the provider). The x-ray reports served as a reference to develop a text classifier using machine-learning software that did not require custom coding. To identify pneumonia cases, the classifier was combined with EMR-based structured data and with text analyses aimed at ARI symptoms in clinical notes. RESULTS: 370 reference cases with Possible Pneumonia and 250 with Pneumonia-in-Plan were identified. The x-ray report text classifier increased the positive predictive value of otherwise identical EMR-based case-detection algorithms by 20-70%, while retaining sensitivities of 58-75%. These performance gains were independent of the case definitions and of whether patients were admitted to the hospital or sent home. Text analyses seeking ARI symptoms in clinical notes did not add further value. CONCLUSION: Specialized software development is not required for automated text analyses to help identify pneumonia patients. These results begin to map an efficient, replicable strategy through which EMR data can be used to stratify ARI severity.","Timely information about disease severity can be central to the detection and management of outbreaks of acute respiratory infections (ARI), including influenza. We asked if two resources: 1) free text, and 2) structured data from an electronic medical record (EMR) could complement each other to identify patients with pneumonia, an ARI severity landmark. A manual EMR review of 2747 outpatient ARI visits with associated chest imaging identified x-ray reports that could support the diagnosis of pneumonia (kappa score â€Š=â€Š0.88 (95% CI 0.82âˆ¶0.93)), along with attendant cases with Possible Pneumonia (adds either cough, sputum, fever/chills/night sweats, dyspnea or pleuritic chest pain) or with Pneumonia-in-Plan (adds pneumonia stated as a likely diagnosis by the provider). The x-ray reports served as a reference to develop a text classifier using machine-learning software that did not require custom coding. To identify pneumonia cases, the classifier was combined with EMR-based structured data and with text analyses aimed at ARI symptoms in clinical notes. 370 reference cases with Possible Pneumonia and 250 with Pneumonia-in-Plan were identified. The x-ray report text classifier increased the positive predictive value of otherwise identical EMR-based case-detection algorithms by 20-70%, while retaining sensitivities of 58-75%. These performance gains were independent of the case definitions and of whether patients were admitted to the hospital or sent home. Text analyses seeking ARI symptoms in clinical notes did not add further value. Specialized software development is not required for automated text analyses to help identify pneumonia patients. These results begin to map an efficient, replicable strategy through which EMR data can be used to stratify ARI severity.","DeLisle, S.
 and Kim, B.
 and Deepak, J.
 and Siddiqui, T.
 and Gundlapalli, A.
 and Samore, M.
 and D'Avolio, L.","DeLisle, Kim, Deepak, Siddiqui, Gundlapalli, Samore, D'Avolio",https://dx.doi.org/10.1371/journal.pone.0070944,https://doi.org/10.1371/journal.pone.0070944,2021-08-03
4137.0,,pubmed,The Nordic prescription databases as a resource for pharmacoepidemiological research--a literature review,The Nordic prescription databases as a resource for pharmacoepidemiological research--a literature review,"PURPOSE: All five Nordic countries have nationwide prescription databases covering all dispensed drugs, with potential for linkage to outcomes. The aim of this review is to present an overview of therapeutic areas studied and methods applied in pharmacoepidemiologic studies using data from these databases. METHODS: The study consists of a Medline-based structured literature review of scientific papers published during 2005-2010 using data from the prescription databases in Denmark, Finland, Iceland, Norway, and Sweden, covering 25 million inhabitants. Relevant studies were analyzed in terms of pharmacological group, study population, outcomes examined, type of study (drug utilization vs. effect of drug therapy), country of origin, and extent of cross-national collaboration. RESULTS: A total of 515 studies were identified. Of these, 262 were conducted in Denmark, 97 in Finland, 4 in Iceland, 87 in Norway, and 61 in Sweden. Four studies used data from more than one Nordic country. The most commonly studied drugs were those acting on the nervous system, followed by cardiovascular drugs and gastrointestinal/endocrine drugs. A total of 228 studies examined drug utilization and 263 focused on the effects and safety of drug therapy. Pregnant women were the most commonly studied population in safety studies, whereas prescribers' adherence to guidelines was the most frequent topic of drug utilization studies. CONCLUSIONS: The Nordic prescription databases, with their possibility of record-linkage, represent an outstanding resource for assessing the beneficial and adverse effects of drug use in large populations, under routine care conditions, and with the potential for long-term follow-up.","All five Nordic countries have nationwide prescription databases covering all dispensed drugs, with potential for linkage to outcomes. The aim of this review is to present an overview of therapeutic areas studied and methods applied in pharmacoepidemiologic studies using data from these databases. The study consists of a Medline-based structured literature review of scientific papers published during 2005-2010 using data from the prescription databases in Denmark, Finland, Iceland, Norway, and Sweden, covering 25 million inhabitants. Relevant studies were analyzed in terms of pharmacological group, study population, outcomes examined, type of study (drug utilization vs. effect of drug therapy), country of origin, and extent of cross-national collaboration. A total of 515 studies were identified. Of these, 262 were conducted in Denmark, 97 in Finland, 4 in Iceland, 87 in Norway, and 61 in Sweden. Four studies used data from more than one Nordic country. The most commonly studied drugs were those acting on the nervous system, followed by cardiovascular drugs and gastrointestinal/endocrine drugs. A total of 228 studies examined drug utilization and 263 focused on the effects and safety of drug therapy. Pregnant women were the most commonly studied population in safety studies, whereas prescribers' adherence to guidelines was the most frequent topic of drug utilization studies. The Nordic prescription databases, with their possibility of record-linkage, represent an outstanding resource for assessing the beneficial and adverse effects of drug use in large populations, under routine care conditions, and with the potential for long-term follow-up.","Wettermark, B.
 and Zoega, H.
 and Furu, K.
 and Korhonen, M.
 and Hallas, J.
 and Norgaard, M.
 and Almarsdottir, A.
 and Andersen, M.
 and Andersson Sundell, K.
 and Bergman, U.
 and Helin-Salmivaara, A.
 and Hoffmann, M.
 and Kieler, H.
 and Martikainen, J.
 and Mortensen, M.
 and Petzold, M.
 and Wallach-Kildemoes, H.
 and Wallin, C.
 and Sorensen, H.","Wettermark, ZoÃ«ga, Furu, Korhonen, Hallas, NÃ¸rgaard, Almarsdottir, Andersen, Andersson Sundell, Bergman, Helin-Salmivaara, Hoffmann, Kieler, Martikainen, Mortensen, Petzold, Wallach-Kildemoes, Wallin, SÃ¸rensen",https://dx.doi.org/10.1002/pds.3457,https://doi.org/10.1002/pds.3457,2021-08-03
344.0,,pubmed,The diagnostic utility and cost-effectiveness of selective nerve root blocks in patients considered for lumbar decompression surgery: a systematic review and economic model,The diagnostic utility and cost-effectiveness of selective nerve root blocks in patients considered for lumbar decompression surgery: a systematic review and economic model,"BACKGROUND: Diagnostic selective nerve root block (SNRB) involves injection of local anaesthetic, sometimes in conjunction with corticosteroids, around spinal nerves. It is used to identify symptomatic nerve roots in patients with probable radicular pain that is not fully concordant with imaging findings. OBJECTIVES: (1) Determine the diagnostic accuracy of SNRB in patients with low back and radiating pain in a lower limb; (2) evaluate whether or not accuracy varies by patient subgroups; (3) review injection-related adverse events; and (4) evaluate the cost-effectiveness of SNRB. DATA SOURCES: MEDLINE, EMBASE, Science Citation Index, Bioscience Information Service (BIOSIS), Latin American and Caribbean Health Sciences Literature (LILACS) and grey literature databases were searched from inception to August 2011. Reference lists of included studies were screened. METHODS: A systematic review (SR) of studies that assessed the accuracy of SNRB or adverse events in patients with low back pain and symptoms in a lower limb for the diagnosis of lumbar radiculopathy. Study quality was assessed using the quality assessment of diagnostic accuracy studies (QUADAS)-2 checklist. We used random-effects meta-analysis to pool diagnostic accuracy data. Decision tree and Markov models were developed, combining SR results with information on the costs and outcomes of surgical and non-surgical care. Uncertainty was assessed using probabilistic and deterministic sensitivity analyses. RESULTS: Five studies assessed diagnostic accuracy: three diagnostic cohort and two within-patient case-control studies. All were judged to be at high risk of bias and had high concerns regarding applicability. In individual studies, sensitivity ranged from 57% [95% confidence interval (CI) 43% to 70%] to 100% (95% CI 76% to 100%) and specificity from 9.5% (95% CI 1% to 30%) to 86% (95% CI 76% to 93%). The most reliable estimate was judged to come from two cohort studies that used post-surgery outcome as the reference standard; summary sensitivity and specificity were 93% (95% CI 86% to 97%) and 26% (95% CI 5% to 68%), respectively. No study provided sufficient detail to judge whether or not accuracy varied by patient subgroup. Seven studies assessed adverse events. There were no major or permanent complications; minor complications were reported in 0-6% of patients. The addition of SNRB to the diagnostic work-up was not cost-effective with an incremental cost per quality-adjusted life-year of 1,576,007. Sensitivity analyses confirmed that SNRB was unlikely to be a cost-effective method for diagnosis and planning surgical therapy. LIMITATIONS: We identified very few studies; all were at high risk of bias. The conduct and interpretation of SNRBs varied and there was no gold standard for diagnosis. Limited information about the impact of SNRB on subsequent care and the long-term costs and benefits of surgery increased uncertainty about cost-effectiveness. CONCLUSIONS: There were few studies that estimated the diagnostic accuracy of SNRB in patients with radiculopathy and all were limited by the difficulty of making a reference standard diagnosis. Summary estimates suggest that specificity is low, but results are based on a small number of studies at a high risk of bias. Based on current weak evidence, it is unlikely that SNRB is a cost-effective method for identifying the symptomatic nerve root prior to lumbar spine surgery. Future research should focus on randomised controlled trials to evaluate whether or not SNRB improves patient outcomes at acceptable cost. FUNDING: The National Institute for Health Research Health Technology Assessment programme.","Diagnostic selective nerve root block (SNRB) involves injection of local anaesthetic, sometimes in conjunction with corticosteroids, around spinal nerves. It is used to identify symptomatic nerve roots in patients with probable radicular pain that is not fully concordant with imaging findings. (1) Determine the diagnostic accuracy of SNRB in patients with low back and radiating pain in a lower limb; (2) evaluate whether or not accuracy varies by patient subgroups; (3) review injection-related adverse events; and (4) evaluate the cost-effectiveness of SNRB. MEDLINE, EMBASE, Science Citation Index, Bioscience Information Service (BIOSIS), Latin American and Caribbean Health Sciences Literature (LILACS) and grey literature databases were searched from inception to August 2011. Reference lists of included studies were screened. A systematic review (SR) of studies that assessed the accuracy of SNRB or adverse events in patients with low back pain and symptoms in a lower limb for the diagnosis of lumbar radiculopathy. Study quality was assessed using the quality assessment of diagnostic accuracy studies (QUADAS)-2 checklist. We used random-effects meta-analysis to pool diagnostic accuracy data. Decision tree and Markov models were developed, combining SR results with information on the costs and outcomes of surgical and non-surgical care. Uncertainty was assessed using probabilistic and deterministic sensitivity analyses. Five studies assessed diagnostic accuracy: three diagnostic cohort and two within-patient case-control studies. All were judged to be at high risk of bias and had high concerns regarding applicability. In individual studies, sensitivity ranged from 57% [95% confidence interval (CI) 43% to 70%] to 100% (95% CI 76% to 100%) and specificity from 9.5% (95% CI 1% to 30%) to 86% (95% CI 76% to 93%). The most reliable estimate was judged to come from two cohort studies that used post-surgery outcome as the reference standard; summary sensitivity and specificity were 93% (95% CI 86% to 97%) and 26% (95% CI 5% to 68%), respectively. No study provided sufficient detail to judge whether or not accuracy varied by patient subgroup. Seven studies assessed adverse events. There were no major or permanent complications; minor complications were reported in 0-6% of patients. The addition of SNRB to the diagnostic work-up was not cost-effective with an incremental cost per quality-adjusted life-year of Â£1,576,007. Sensitivity analyses confirmed that SNRB was unlikely to be a cost-effective method for diagnosis and planning surgical therapy. We identified very few studies; all were at high risk of bias. The conduct and interpretation of SNRBs varied and there was no gold standard for diagnosis. Limited information about the impact of SNRB on subsequent care and the long-term costs and benefits of surgery increased uncertainty about cost-effectiveness. There were few studies that estimated the diagnostic accuracy of SNRB in patients with radiculopathy and all were limited by the difficulty of making a reference standard diagnosis. Summary estimates suggest that specificity is low, but results are based on a small number of studies at a high risk of bias. Based on current weak evidence, it is unlikely that SNRB is a cost-effective method for identifying the symptomatic nerve root prior to lumbar spine surgery. Future research should focus on randomised controlled trials to evaluate whether or not SNRB improves patient outcomes at acceptable cost. The National Institute for Health Research Health Technology Assessment programme.","Beynon, R.
 and Hawkins, J.
 and Laing, R.
 and Higgins, N.
 and Whiting, P.
 and Jameson, C.
 and Sterne, J. A.
 and Vergara, P.
 and Hollingworth, W.","Beynon, Hawkins, Laing, Higgins, Whiting, Jameson, Sterne, Vergara, Hollingworth",https://dx.doi.org/10.3310/hta17190,https://doi.org/10.3310/hta17190,2021-08-03
1503.0,,pubmed,Towards the automatic computational assessment of enlarged perivascular spaces on brain magnetic resonance images: a systematic review,Towards the automatic computational assessment of enlarged perivascular spaces on brain magnetic resonance images: a systematic review,"Enlarged perivascular spaces (EPVS), visible in brain MRI, are an important marker of small vessel disease and neuroinflammation. We systematically evaluated the literature up to June 2012 on possible methods for their computational assessment and analyzed confounds with lacunes and small white matter hyperintensities. We found six studies that assessed/identified EPVS computationally by seven different methods, and four studies that described techniques to automatically segment similar structures and are potentially suitable for EPVS segmentation. T2-weighted MRI was the only sequence that identified all EPVS, but FLAIR and T1-weighted images were useful in their differentiation. Inconsistency within the literature regarding their diameter and terminology, and overlap in shape, intensity, location, and size with lacunes, conspires against their differentiation and the accuracy and reproducibility of any computational segmentation technique. The most promising approach will need to combine various MR sequences and consider all these features for accurate EPVS determination.","Enlarged perivascular spaces (EPVS), visible in brain MRI, are an important marker of small vessel disease and neuroinflammation. We systematically evaluated the literature up to June 2012 on possible methods for their computational assessment and analyzed confounds with lacunes and small white matter hyperintensities. We found six studies that assessed/identified EPVS computationally by seven different methods, and four studies that described techniques to automatically segment similar structures and are potentially suitable for EPVS segmentation. T2-weighted MRI was the only sequence that identified all EPVS, but FLAIR and T1-weighted images were useful in their differentiation. Inconsistency within the literature regarding their diameter and terminology, and overlap in shape, intensity, location, and size with lacunes, conspires against their differentiation and the accuracy and reproducibility of any computational segmentation technique. The most promising approach will need to combine various MR sequences and consider all these features for accurate EPVS determination.","Hernandez Mdel, C.
 and Piper, R. J.
 and Wang, X.
 and Deary, I. J.
 and Wardlaw, J. M.","HernÃ¡ndez, Piper, Wang, Deary, Wardlaw",https://dx.doi.org/10.1002/jmri.24047,https://doi.org/10.1002/jmri.24047,2021-08-03
3408.0,,pubmed,"BRENDA in 2013: integrated reactions, kinetic data, enzyme function data, improved disease classification: new options and contents in BRENDA","BRENDA in 2013: integrated reactions, kinetic data, enzyme function data, improved disease classification: new options and contents in BRENDA","The BRENDA (BRaunschweig ENzyme DAtabase) enzyme portal (http://www.brenda-enzymes.org) is the main information system of functional biochemical and molecular enzyme data and provides access to seven interconnected databases. BRENDA contains 2.7 million manually annotated data on enzyme occurrence, function, kinetics and molecular properties. Each entry is connected to a reference and the source organism. Enzyme ligands are stored with their structures and can be accessed via their names, synonyms or via a structure search. FRENDA (Full Reference ENzyme DAta) and AMENDA (Automatic Mining of ENzyme DAta) are based on text mining methods and represent a complete survey of PubMed abstracts with information on enzymes in different organisms, tissues or organelles. The supplemental database DRENDA provides more than 910 000 new EC number-disease relations in more than 510 000 references from automatic search and a classification of enzyme-disease-related information. KENDA (Kinetic ENzyme DAta), a new amendment extracts and displays kinetic values from PubMed abstracts. The integration of the EnzymeDetector offers an automatic comparison, evaluation and prediction of enzyme function annotations for prokaryotic genomes. The biochemical reaction database BKM-react contains non-redundant enzyme-catalysed and spontaneous reactions and was developed to facilitate and accelerate the construction of biochemical models.","The BRENDA (BRaunschweig ENzyme DAtabase) enzyme portal (http://www.brenda-enzymes.org) is the main information system of functional biochemical and molecular enzyme data and provides access to seven interconnected databases. BRENDA contains 2.7 million manually annotated data on enzyme occurrence, function, kinetics and molecular properties. Each entry is connected to a reference and the source organism. Enzyme ligands are stored with their structures and can be accessed via their names, synonyms or via a structure search. FRENDA (Full Reference ENzyme DAta) and AMENDA (Automatic Mining of ENzyme DAta) are based on text mining methods and represent a complete survey of PubMed abstracts with information on enzymes in different organisms, tissues or organelles. The supplemental database DRENDA provides more than 910 000 new EC number-disease relations in more than 510 000 references from automatic search and a classification of enzyme-disease-related information. KENDA (Kinetic ENzyme DAta), a new amendment extracts and displays kinetic values from PubMed abstracts. The integration of the EnzymeDetector offers an automatic comparison, evaluation and prediction of enzyme function annotations for prokaryotic genomes. The biochemical reaction database BKM-react contains non-redundant enzyme-catalysed and spontaneous reactions and was developed to facilitate and accelerate the construction of biochemical models.","Schomburg, I.
 and Chang, A.
 and Placzek, S.
 and Sohngen, C.
 and Rother, M.
 and Lang, M.
 and Munaretto, C.
 and Ulas, S.
 and Stelzer, M.
 and Grote, A.
 and Scheer, M.
 and Schomburg, D.","Schomburg, Chang, Placzek, SÃ¶hngen, Rother, Lang, Munaretto, Ulas, Stelzer, Grote, Scheer, Schomburg",https://dx.doi.org/10.1093/nar/gks1049,https://doi.org/10.1093/nar/gks1049,2021-08-03
4139.0,,pubmed,Pharmacogenomics knowledge for personalized medicine,Pharmacogenomics knowledge for personalized medicine,"The Pharmacogenomics Knowledgebase (PharmGKB) is a resource that collects, curates, and disseminates information about the impact of human genetic variation on drug responses. It provides clinically relevant information, including dosing guidelines, annotated drug labels, and potentially actionable gene-drug associations and genotype-phenotype relationships. Curators assign levels of evidence to variant-drug associations using well-defined criteria based on careful literature review. Thus, PharmGKB is a useful source of high-quality information supporting personalized medicine-implementation projects.","The Pharmacogenomics Knowledgebase (PharmGKB) is a resource that collects, curates, and disseminates information about the impact of human genetic variation on drug responses. It provides clinically relevant information, including dosing guidelines, annotated drug labels, and potentially actionable gene-drug associations and genotype-phenotype relationships. Curators assign levels of evidence to variant-drug associations using well-defined criteria based on careful literature review. Thus, PharmGKB is a useful source of high-quality information supporting personalized medicine-implementation projects.","Whirl-Carrillo, M.
 and McDonagh, E. M.
 and Hebert, J. M.
 and Gong, L.
 and Sangkuhl, K.
 and Thorn, C. F.
 and Altman, R. B.
 and Klein, T. E.","Whirl-Carrillo, McDonagh, Hebert, Gong, Sangkuhl, Thorn, Altman, Klein",https://dx.doi.org/10.1038/clpt.2012.96,https://doi.org/10.1038/clpt.2012.96,2021-08-03
3698.0,,pubmed,Predicting the risk of psychosis onset: advances and prospects,Predicting the risk of psychosis onset: advances and prospects,"AIM: To conduct a systematic review of the methods and performance characteristics of models developed for predicting the onset of psychosis. METHODS: We performed a comprehensive literature search restricted to English articles and identified using PubMed, Medline and PsychINFO, as well as the reference lists of published studies and reviews. Inclusion criteria included the selection of more than one variable to predict psychosis or schizophrenia onset, and selection of individuals at familial risk or clinical high risk. Eighteen studies met these criteria, and we compared these studies based on the subjects selected, predictor variables used and the choice of statistical or machine learning methods. RESULTS: Quality of life and life functioning as well as structural brain imaging emerged as the most promising predictors of psychosis onset, particularly when they were coupled with appropriate dimensionality reduction methods and predictive model algorithms like the support vector machine (SVM). Balanced accuracy ranged from 100% to 78% in four studies using the SVM, and 67% to 81% in 14 studies using general linear models. CONCLUSIONS: Performance of the predictive models improves with quality of life measures, life functioning measures, structural brain imaging data, as well as with the use of methods like SVM. Despite these advances, the overall performance of psychosis predictive models is still modest. In the future, performance can potentially be improved by including genetic variant and new functional imaging data in addition to the predictors that are used currently.","To conduct a systematic review of the methods and performance characteristics of models developed for predicting the onset of psychosis. We performed a comprehensive literature search restricted to English articles and identified using PubMed, Medline and PsychINFO, as well as the reference lists of published studies and reviews. Inclusion criteria included the selection of more than one variable to predict psychosis or schizophrenia onset, and selection of individuals at familial risk or clinical high risk. Eighteen studies met these criteria, and we compared these studies based on the subjects selected, predictor variables used and the choice of statistical or machine learning methods. Quality of life and life functioning as well as structural brain imaging emerged as the most promising predictors of psychosis onset, particularly when they were coupled with appropriate dimensionality reduction methods and predictive model algorithms like the support vector machine (SVM). Balanced accuracy ranged from 100% to 78% in four studies using the SVM, and 67% to 81% in 14 studies using general linear models. Performance of the predictive models improves with quality of life measures, life functioning measures, structural brain imaging data, as well as with the use of methods like SVM. Despite these advances, the overall performance of psychosis predictive models is still modest. In the future, performance can potentially be improved by including genetic variant and new functional imaging data in addition to the predictors that are used currently.","Strobl, E. V.
 and Eack, S. M.
 and Swaminathan, V.
 and Visweswaran, S.","Strobl, Eack, Swaminathan, Visweswaran",https://dx.doi.org/10.1111/j.1751-7893.2012.00383.x,https://doi.org/10.1111/j.1751-7893.2012.00383.x,2021-08-03
2824.0,,pubmed,Stent placement versus surgery for coarctation of the thoracic aorta,Stent placement versus surgery for coarctation of the thoracic aorta,"BACKGROUND: Coarctation of the aorta (CoA) accounts for 5% to 7% of congenital heart disease, with an incidence of 0.3 to 0.4 per 1000 live births. Surgery was the only choice of therapy for CoA until 1982 when balloon angioplasty became an available alternative for its treatment. Re-coarctation, aneurysm and aortic dissection remain the disadvantages of both treatments. To avoid those disadvantages, in 1990 endovascular stents were introduced for native coarctation and re-coarctation and since then they have become an alternative approach to surgical repair. The best approach to treat the CoA, whether open surgery or by stent placement, is not clear. OBJECTIVES: To analyze the effectiveness and safety of stent placement compared with open surgery in patients with coarctation of the thoracic aorta. SEARCH METHODS: The Cochrane Peripheral Vascular Diseases Group searched their Specialised Register (last searched September 2011) and CENTRAL (2011, Issue 3). We also searched MEDLINE, EMBASE, CINAHL, AMED, Web of Science and LILACS (last searched in September 2011). We evaluated the located references and applied the inclusion criteria to selected studies. There was no restriction on language. SELECTION CRITERIA: Randomized or quasi-randomized controlled clinical trials that compared patients with CoA undergoing open surgery or stent placement. DATA COLLECTION AND ANALYSIS: The review authors independently assessed the studies identified for eligibility for inclusion. We excluded studies after a consensus meeting. MAIN RESULTS: All identified studies were screened and had the selection criteria applied to the title and abstract. In total, we selected five studies for full-text analysis. After detailed evaluation, we excluded all studies because there was no comparison between stent placement and open surgery. AUTHORS' CONCLUSIONS: There is insufficient evidence with regards to the best treatment for coarctation of the thoracic aorta. This review suggests a need to perform a randomized controlled clinical trial with emphasis on the allocation method, evaluation of primary outcomes, size and quality of the sample, and long-term follow-up.","Coarctation of the aorta (CoA) accounts for 5% to 7% of congenital heart disease, with an incidence of 0.3 to 0.4 per 1000 live births. Surgery was the only choice of therapy for CoA until 1982 when balloon angioplasty became an available alternative for its treatment. Re-coarctation, aneurysm and aortic dissection remain the disadvantages of both treatments. To avoid those disadvantages, in 1990 endovascular stents were introduced for native coarctation and re-coarctation and since then they have become an alternative approach to surgical repair. The best approach to treat the CoA, whether open surgery or by stent placement, is not clear. To analyze the effectiveness and safety of stent placement compared with open surgery in patients with coarctation of the thoracic aorta. The Cochrane Peripheral Vascular Diseases Group searched their Specialised Register (last searched September 2011) and CENTRAL (2011, Issue 3). We also searched MEDLINE, EMBASE, CINAHL, AMED, Web of Science and LILACS (last searched in September 2011). We evaluated the located references and applied the inclusion criteria to selected studies. There was no restriction on language. Randomized or quasi-randomized controlled clinical trials that compared patients with CoA undergoing open surgery or stent placement. The review authors independently assessed the studies identified for eligibility for inclusion. We excluded studies after a consensus meeting. All identified studies were screened and had the selection criteria applied to the title and abstract. In total, we selected five studies for full-text analysis. After detailed evaluation, we excluded all studies because there was no comparison between stent placement and open surgery. There is insufficient evidence with regards to the best treatment for coarctation of the thoracic aorta. This review suggests a need to perform a randomized controlled clinical trial with emphasis on the allocation method, evaluation of primary outcomes, size and quality of the sample, and long-term follow-up.","Padua, L. M.
 and Garcia, L. C.
 and Rubira, C. J.
 and de Oliveira Carvalho, P. E.","PÃ¡dua, Garcia, Rubira, de Oliveira Carvalho",https://dx.doi.org/10.1002/14651858.CD008204.pub2,https://doi.org/10.1002/14651858.CD008204.pub2,2021-08-03
2478.0,,pubmed,A review of transcriptome studies combined with data mining reveals novel potential markers of malignant pleural mesothelioma,A review of transcriptome studies combined with data mining reveals novel potential markers of malignant pleural mesothelioma,"Malignant pleural mesothelioma (MPM), a cancer of the serosal pleural cavities, is one of the most aggressive human tumors. In order to identify genes crucial for the onset and progression of MPM, we performed an extensive literature review focused on transcriptome studies (RTS). In this kind of studies a great number of transcripts are analyzed without formulating any a priori hypothesis, thus preventing any bias coming from previously established knowledge that could lead to an over-representation of specific genes. Each study was thoroughly analyzed paying particular attention to: (i) the employed microarray platform, (ii) the number and type of samples, (iii) the fold-change, and (iv) the statistical significance of deregulated genes. We also performed data mining (DM) on MPM using three different tools (Coremine, SNPs3D, and GeneProspector). Results from RTS and DM were compared in order to restrict the number of genes potentially deregulated in MPM. Our main requirement for a gene to be a 'mesothelioma gene' (MG) is to be reproducibly deregulated among independent studies and confirmed by DM. A list of MGs was thus produced, including PTGS2, BIRC5, ASS1, JUNB, MCM2, AURKA, FGF2, MKI67, CAV1, SFRP1, CCNB1, CDK4, and MSLN that might represent potential novel biomarkers or therapeutic targets for MPM. Moreover, it was found a sub-group of MGs including ASS1, JUNB, PTGS2, EEF2, SULF1, TOP2A, AURKA, BIRC5, CAV1, IFITM1, PCNA, and PKM2 that could explain, at least in part, the mechanisms of resistance to cisplatin, one first-line chemotherapeutic drug used for the disease. Finally, the pathway analysis showed that co-regulation networks related to the cross-talk between MPM and its micro-environment, in particular involving the adhesion molecules, integrins, and cytokines, might have an important role in MPM. Future studies are warranted to better characterize the role played by these genes in MPM.","Malignant pleural mesothelioma (MPM), a cancer of the serosal pleural cavities, is one of the most aggressive human tumors. In order to identify genes crucial for the onset and progression of MPM, we performed an extensive literature review focused on transcriptome studies (RTS). In this kind of studies a great number of transcripts are analyzed without formulating any a priori hypothesis, thus preventing any bias coming from previously established knowledge that could lead to an over-representation of specific genes. Each study was thoroughly analyzed paying particular attention to: (i) the employed microarray platform, (ii) the number and type of samples, (iii) the fold-change, and (iv) the statistical significance of deregulated genes. We also performed data mining (DM) on MPM using three different tools (Coremine, SNPs3D, and GeneProspector). Results from RTS and DM were compared in order to restrict the number of genes potentially deregulated in MPM. Our main requirement for a gene to be a ""mesothelioma gene"" (MG) is to be reproducibly deregulated among independent studies and confirmed by DM. A list of MGs was thus produced, including PTGS2, BIRC5, ASS1, JUNB, MCM2, AURKA, FGF2, MKI67, CAV1, SFRP1, CCNB1, CDK4, and MSLN that might represent potential novel biomarkers or therapeutic targets for MPM. Moreover, it was found a sub-group of MGs including ASS1, JUNB, PTGS2, EEF2, SULF1, TOP2A, AURKA, BIRC5, CAV1, IFITM1, PCNA, and PKM2 that could explain, at least in part, the mechanisms of resistance to cisplatin, one first-line chemotherapeutic drug used for the disease. Finally, the pathway analysis showed that co-regulation networks related to the cross-talk between MPM and its micro-environment, in particular involving the adhesion molecules, integrins, and cytokines, might have an important role in MPM. Future studies are warranted to better characterize the role played by these genes in MPM.","Melaiu, O.
 and Cristaudo, A.
 and Melissari, E.
 and Di Russo, M.
 and Bonotti, A.
 and Bruno, R.
 and Foddis, R.
 and Gemignani, F.
 and Pellegrini, S.
 and Landi, S.","Melaiu, Cristaudo, Melissari, Di Russo, Bonotti, Bruno, Foddis, Gemignani, Pellegrini, Landi",https://dx.doi.org/10.1016/j.mrrev.2011.12.003,https://doi.org/10.1016/j.mrrev.2011.12.003,2021-08-03
3447.0,,pubmed,Cost sensitive hierarchical document classification to triage PubMed abstracts for manual curation,Cost sensitive hierarchical document classification to triage PubMed abstracts for manual curation,"BACKGROUND: The Immune Epitope Database (IEDB) project manually curates information from published journal articles that describe immune epitopes derived from a wide variety of organisms and associated with different diseases. In the past, abstracts of scientific articles were retrieved by broad keyword queries of PubMed, and were classified as relevant (curatable) or irrelevant (not curatable) to the scope of the database by a Naive Bayes classifier. The curatable abstracts were subsequently manually classified into categories corresponding to different disease domains. Over the past four years, we have examined how to further improve this approach in order to enhance classification performance and to reduce the need for manual intervention. RESULTS: Utilizing 89,884 abstracts classified by a domain expert as curatable or uncuratable, we found that a SVM classifier outperformed the previously used Naive Bayes classifier for curatability predictions with an AUC of 0.899 and 0.854, respectively. Next, using a non-hierarchical and a hierarchical application of SVM classifiers trained on 22,833 curatable abstracts manually classified into three levels of disease specific categories we demonstrated that a hierarchical application of SVM classifiers outperformed non-hierarchical SVM classifiers for categorization. Finally, to optimize the hierarchical SVM classifiers' error profile for the curation process, cost sensitivity functions were developed to avoid serious misclassifications. We tested our design on a benchmark dataset of 1,388 references and achieved an overall category prediction accuracy of 94.4%, 93.9%, and 82.1% at the three levels of categorization, respectively. CONCLUSIONS: A hierarchical application of SVM algorithms with cost sensitive output weighting enabled high quality reference classification with few serious misclassifications. This enabled us to significantly reduce the manual component of abstract categorization. Our findings are relevant to other databases that are developing their own document classifier schema and the datasets we make available provide large scale real-life benchmark sets for method developers.","The Immune Epitope Database (IEDB) project manually curates information from published journal articles that describe immune epitopes derived from a wide variety of organisms and associated with different diseases. In the past, abstracts of scientific articles were retrieved by broad keyword queries of PubMed, and were classified as relevant (curatable) or irrelevant (not curatable) to the scope of the database by a NaÃ¯ve Bayes classifier. The curatable abstracts were subsequently manually classified into categories corresponding to different disease domains. Over the past four years, we have examined how to further improve this approach in order to enhance classification performance and to reduce the need for manual intervention. Utilizing 89,884 abstracts classified by a domain expert as curatable or uncuratable, we found that a SVM classifier outperformed the previously used NaÃ¯ve Bayes classifier for curatability predictions with an AUC of 0.899 and 0.854, respectively. Next, using a non-hierarchical and a hierarchical application of SVM classifiers trained on 22,833 curatable abstracts manually classified into three levels of disease specific categories we demonstrated that a hierarchical application of SVM classifiers outperformed non-hierarchical SVM classifiers for categorization. Finally, to optimize the hierarchical SVM classifiers' error profile for the curation process, cost sensitivity functions were developed to avoid serious misclassifications. We tested our design on a benchmark dataset of 1,388 references and achieved an overall category prediction accuracy of 94.4%, 93.9%, and 82.1% at the three levels of categorization, respectively. A hierarchical application of SVM algorithms with cost sensitive output weighting enabled high quality reference classification with few serious misclassifications. This enabled us to significantly reduce the manual component of abstract categorization. Our findings are relevant to other databases that are developing their own document classifier schema and the datasets we make available provide large scale real-life benchmark sets for method developers.","Seymour, E.
 and Damle, R.
 and Sette, A.
 and Peters, B.","Seymour, Damle, Sette, Peters",not available,https://doi.org/10.1186/1471-2105-12-482,2021-08-03
2307.0,,pubmed,Using statistical text mining to supplement the development of an ontology,Using statistical text mining to supplement the development of an ontology,"Statistical text mining was used to supplement efforts to develop a clinical vocabulary for post-traumatic stress disorder (PTSD) in the VA. A set of outpatient progress notes was collected for a cohort of 405 unique veterans with PTSD and a comparison group of 392 with other psychological conditions at one VA hospital. Two methods were employed: (1) 'multi-model term scoring' used stepwise logistic regression to develop 21 separate models by varying three frequency weight and seven term weight options and (2) 'iterative term refinement' which used a standard stop list followed by clinical review to eliminate non-clinical terms and terms not related to PTSD. Combined results of the two methods were reviewed by two clinicians resulting in 226 unique PTSD related terms. Results of the statistical text mining methods were compared with ongoing efforts to identify terms based on literature review, focus groups with clinicians treating PTSD and review of an existing vocabulary, lending support to the contributions of the STM analyses.","Statistical text mining was used to supplement efforts to develop a clinical vocabulary for post-traumatic stress disorder (PTSD) in the VA. A set of outpatient progress notes was collected for a cohort of 405 unique veterans with PTSD and a comparison group of 392 with other psychological conditions at one VA hospital. Two methods were employed: (1) ""multi-model term scoring"" used stepwise logistic regression to develop 21 separate models by varying three frequency weight and seven term weight options and (2) ""iterative term refinement"" which used a standard stop list followed by clinical review to eliminate non-clinical terms and terms not related to PTSD. Combined results of the two methods were reviewed by two clinicians resulting in 226 unique PTSD related terms. Results of the statistical text mining methods were compared with ongoing efforts to identify terms based on literature review, focus groups with clinicians treating PTSD and review of an existing vocabulary, lending support to the contributions of the STM analyses.","Luther, S.
 and Berndt, D.
 and Finch, D.
 and Richardson, M.
 and Hickling, E.
 and Hickam, D.","Luther, Berndt, Finch, Richardson, Hickling, Hickam",not available,https://doi.org/10.1016/j.jbi.2011.11.001,2021-08-03
827.0,,pubmed,Implantable loop recorders are cost-effective when used to investigate transient loss of consciousness which is either suspected to be arrhythmic or remains unexplained,Implantable loop recorders are cost-effective when used to investigate transient loss of consciousness which is either suspected to be arrhythmic or remains unexplained,"AIMS: To assess the cost-effectiveness of implantable loop recorders (ILRs) in people with transient loss of consciousness (TLoC), which, after initial assessment and specialist cardiovascular assessment, is either suspected to be arrhythmic in origin or remains unexplained. This analysis was conducted to inform clinical guideline recommendations made by the National Institute for Health and Clinical Excellence (NICE) on the management of TLoC. METHODS AND RESULTS: Decision analytic modelling was used to estimate the costs and benefits of using ILRs compared with a strategy of no further diagnostic testing. Diagnostic outcomes were estimated from a systematic review and used to populate a decision tree model. To capture the main consequences of diagnosis, the costs and benefits of treatment for several clinically significant arrhythmias were estimated within the model. We used a cost-utility approach, in which benefits are measured using quality adjusted life years (QALYs), and took a UK National Health Service (NHS) and personal social services perspective. The cost per QALY was 17,400 in patients with unexplained syncope and 16,400 in patients with suspected arrhythmic syncope. Sensitivity analysis found that the cost-effectiveness estimates are fairly robust despite the areas of uncertainty identified in the evidence and assumptions used to inform the model. CONCLUSIONS: Implantable loop recorder monitoring is likely to be a cost-effective strategy in people presenting to the UK NHS who are experiencing infrequent episodes of TLoC which either remain unexplained or are suspected to be arrhythmic after initial assessment and specialist cardiovascular assessment. Implantable loop recorder monitoring has been recommended by NICE for these populations.","To assess the cost-effectiveness of implantable loop recorders (ILRs) in people with transient loss of consciousness (TLoC), which, after initial assessment and specialist cardiovascular assessment, is either suspected to be arrhythmic in origin or remains unexplained. This analysis was conducted to inform clinical guideline recommendations made by the National Institute for Health and Clinical Excellence (NICE) on the management of TLoC. Decision analytic modelling was used to estimate the costs and benefits of using ILRs compared with a strategy of no further diagnostic testing. Diagnostic outcomes were estimated from a systematic review and used to populate a decision tree model. To capture the main consequences of diagnosis, the costs and benefits of treatment for several clinically significant arrhythmias were estimated within the model. We used a cost-utility approach, in which benefits are measured using quality adjusted life years (QALYs), and took a UK National Health Service (NHS) and personal social services perspective. The cost per QALY was Â£17,400 in patients with unexplained syncope and Â£16,400 in patients with suspected arrhythmic syncope. Sensitivity analysis found that the cost-effectiveness estimates are fairly robust despite the areas of uncertainty identified in the evidence and assumptions used to inform the model. Implantable loop recorder monitoring is likely to be a cost-effective strategy in people presenting to the UK NHS who are experiencing infrequent episodes of TLoC which either remain unexplained or are suspected to be arrhythmic after initial assessment and specialist cardiovascular assessment. Implantable loop recorder monitoring has been recommended by NICE for these populations.","Davis, S.
 and Westby, M.
 and Pitcher, D.
 and Petkar, S.","Davis, Westby, Pitcher, Petkar",https://dx.doi.org/10.1093/europace/eur343,https://doi.org/10.1093/europace/eur343,2021-08-03
933.0,,pubmed,The promise of mHealth: daily activity monitoring and outcome assessments by wearable sensors,The promise of mHealth: daily activity monitoring and outcome assessments by wearable sensors,"Mobile health tools that enable clinicians and researchers to monitor the type, quantity, and quality of everyday activities of patients and trial participants have long been needed to improve daily care, design more clinically meaningful randomized trials of interventions, and establish cost-effective, evidence-based practices. Inexpensive, unobtrusive wireless sensors, including accelerometers, gyroscopes, and pressure-sensitive textiles, combined with Internet-based communications and machine-learning algorithms trained to recognize upper- and lower-extremity movements, have begun to fulfill this need. Continuous data from ankle triaxial accelerometers, for example, can be transmitted from the home and community via WiFi or a smartphone to a remote data analysis server. Reports can include the walking speed and duration of every bout of ambulation, spatiotemporal symmetries between the legs, and the type, duration, and energy used during exercise. For daily care, this readily accessible flow of real-world information allows clinicians to monitor the amount and quality of exercise for risk factor management and compliance in the practice of skills. Feedback may motivate better self-management as well as serve home-based rehabilitation efforts. Monitoring patients with chronic diseases and after hospitalization or the start of new medications for a decline in daily activity may help detect medical complications before rehospitalization becomes necessary. For clinical trials, repeated laboratory-quality assessments of key activities in the community, rather than by clinic testing, self-report, and ordinal scales, may reduce the cost and burden of travel, improve recruitment and retention, and capture more reliable, valid, and responsive ratio-scaled outcome measures that are not mere surrogates for changes in daily impairment, disability, and functioning.","Mobile health tools that enable clinicians and researchers to monitor the type, quantity, and quality of everyday activities of patients and trial participants have long been needed to improve daily care, design more clinically meaningful randomized trials of interventions, and establish cost-effective, evidence-based practices. Inexpensive, unobtrusive wireless sensors, including accelerometers, gyroscopes, and pressure-sensitive textiles, combined with Internet-based communications and machine-learning algorithms trained to recognize upper- and lower-extremity movements, have begun to fulfill this need. Continuous data from ankle triaxial accelerometers, for example, can be transmitted from the home and community via WiFi or a smartphone to a remote data analysis server. Reports can include the walking speed and duration of every bout of ambulation, spatiotemporal symmetries between the legs, and the type, duration, and energy used during exercise. For daily care, this readily accessible flow of real-world information allows clinicians to monitor the amount and quality of exercise for risk factor management and compliance in the practice of skills. Feedback may motivate better self-management as well as serve home-based rehabilitation efforts. Monitoring patients with chronic diseases and after hospitalization or the start of new medications for a decline in daily activity may help detect medical complications before rehospitalization becomes necessary. For clinical trials, repeated laboratory-quality assessments of key activities in the community, rather than by clinic testing, self-report, and ordinal scales, may reduce the cost and burden of travel, improve recruitment and retention, and capture more reliable, valid, and responsive ratio-scaled outcome measures that are not mere surrogates for changes in daily impairment, disability, and functioning.","Dobkin, B. H.
 and Dorsch, A.","Dobkin, Dorsch",https://dx.doi.org/10.1177/1545968311425908,https://doi.org/10.1177/1545968311425908,2021-08-03
3794.0,,pubmed,Applications of text mining within systematic reviews,Applications of text mining within systematic reviews,"Systematic reviews are a widely accepted research method. However, it is increasingly difficult to conduct them to fit with policy and practice timescales, particularly in areas which do not have well indexed, comprehensive bibliographic databases. Text mining technologies offer one possible way forward in reducing the amount of time systematic reviews take to conduct. They can facilitate the identification of relevant literature, its rapid description or categorization, and its summarization. In this paper, we describe the application of four text mining technologies, namely, automatic term recognition, document clustering, classification and summarization, which support the identification of relevant studies in systematic reviews. The contributions of text mining technologies to improve reviewing efficiency are considered and their strengths and weaknesses explored. We conclude that these technologies do have the potential to assist at various stages of the review process. However, they are relatively unknown in the systematic reviewing community, and substantial evaluation and methods development are required before their possible impact can be fully assessed. Copyright Â© 2011 John Wiley & Sons, Ltd.","Systematic reviews are a widely accepted research method. However, it is increasingly difficult to conduct them to fit with policy and practice timescales, particularly in areas which do not have well indexed, comprehensive bibliographic databases. Text mining technologies offer one possible way forward in reducing the amount of time systematic reviews take to conduct. They can facilitate the identification of relevant literature, its rapid description or categorization, and its summarization. In this paper, we describe the application of four text mining technologies, namely, automatic term recognition, document clustering, classification and summarization, which support the identification of relevant studies in systematic reviews. The contributions of text mining technologies to improve reviewing efficiency are considered and their strengths and weaknesses explored. We conclude that these technologies do have the potential to assist at various stages of the review process. However, they are relatively unknown in the systematic reviewing community, and substantial evaluation and methods development are required before their possible impact can be fully assessed. Copyright Â© 2011 John Wiley &amp; Sons, Ltd.","Thomas, J.
 and McNaught, J.
 and Ananiadou, S.","Thomas, McNaught, Ananiadou",not available,https://doi.org/10.1002/jrsm.27,2021-08-03
3188.0,,pubmed,Economic evaluation of enhanced staff contact for the promotion of breastfeeding for low birth weight infants,Economic evaluation of enhanced staff contact for the promotion of breastfeeding for low birth weight infants,"OBJECTIVES: There is evidence that breastmilk feeding reduces mortality and short and long-term morbidity among infants born too soon or too small. The aim of this study was to evaluate the cost-effectiveness of enhanced staff contact for mothers with infants in a neonatal unit with a birth weight of 500-2,500 g from the perspective of the UK National Health Service. METHODS: A decision-tree model linked clinical outcomes with long-term health outcomes. The study population was divided into three weight bands: 500-999 g, 1000-1,749 g, and 1,750-2,500 g. Clinical and resource use data were obtained from literature reviews. The measure of benefit was quality-adjusted life-years. Uncertainty was evaluated using cost-effectiveness acceptability curves and sensitivity analyses. RESULTS: The intervention was less costly and more effective than the comparator in the base-case analysis for each birth weight group. The results were quite robust to the sensitivity analyses performed. CONCLUSIONS: This is the first economic evaluation in this complex field and offers a model to be developed in future research. The results provide preliminary indications that enhanced staff contact may be cost-effective. However, the limited evidence available, and the limited UK data in particular, suggest that further research is required to provide results with confidence.","There is evidence that breastmilk feeding reduces mortality and short and long-term morbidity among infants born too soon or too small. The aim of this study was to evaluate the cost-effectiveness of enhanced staff contact for mothers with infants in a neonatal unit with a birth weight of 500-2,500 g from the perspective of the UK National Health Service. A decision-tree model linked clinical outcomes with long-term health outcomes. The study population was divided into three weight bands: 500-999 g, 1000-1,749 g, and 1,750-2,500 g. Clinical and resource use data were obtained from literature reviews. The measure of benefit was quality-adjusted life-years. Uncertainty was evaluated using cost-effectiveness acceptability curves and sensitivity analyses. The intervention was less costly and more effective than the comparator in the base-case analysis for each birth weight group. The results were quite robust to the sensitivity analyses performed. This is the first economic evaluation in this complex field and offers a model to be developed in future research. The results provide preliminary indications that enhanced staff contact may be cost-effective. However, the limited evidence available, and the limited UK data in particular, suggest that further research is required to provide results with confidence.","Rice, S. J.
 and Craig, D.
 and McCormick, F.
 and Renfrew, M. J.
 and Williams, A. F.","Rice, Craig, McCormick, Renfrew, Williams",https://dx.doi.org/10.1017/S0266462310000115,https://doi.org/10.1017/S0266462310000115,2021-08-03
2895.0,,pubmed,Screening for postnatal depression in primary care: cost effectiveness analysis,Screening for postnatal depression in primary care: cost effectiveness analysis,"OBJECTIVE: To evaluate the cost effectiveness of routine screening for postnatal depression in primary care. DESIGN: Cost effectiveness analysis with a decision model of alternative methods of screening for depression, including standardised postnatal depression and generic depression instruments. The performance of screening instruments was derived from a systematic review and bivariate meta-analysis at a range of instrument cut points; estimates of other relevant parameters were derived from literature sources and relevant databases. A decision tree considered the full treatment pathway from the possible onset of postnatal depression through identification, treatment, and possible relapse. SETTING: Primary care. PARTICIPANTS: A hypothetical population of women assessed for postnatal depression either via routine care only or supplemented by use of formal identification methods six weeks postnatally, as recommended in recent guidelines. MAIN OUTCOME MEASURES: Costs expressed in 2006-7 prices and impact on health outcomes expressed in terms of quality adjusted life years (QALYs). The time horizon of the analysis was one year. RESULTS: The routine application of either postnatal or general depression questionnaires did not seem to be cost effective compared with routine care only. The Edinburgh postnatal depression scale (at a cut point of 16) had an incremental cost effectiveness ratio (ICER) of pound 41,103 (euro 45,398, $67,130) per QALY compared with routine care only. The ICER for all other strategies ranged from pound 49,928 to pound 272,463 per QALY versus routine care only, while the probability that no formal identification strategy was cost effective was 88% (59%) at a cost effectiveness threshold of pound 20,000 ( pound 30,000) per QALY. While sensitivity analysis indicated that the cost of managing incorrectly identified depression (false positive result) was an important driver of the model, formal identification approaches did not seem to be cost effective at any feasible estimate of this cost. CONCLUSIONS: Formal identification methods for postnatal depression do not seem to represent value for money for the NHS. The major determinant of cost effectiveness seems to be the potential additional costs of managing women incorrectly diagnosed as depressed. Formal identification methods for postnatal depression do not currently satisfy the National Screening Committee's criteria for the adoption of a screening strategy as part of national health policy. [References: 28]","To evaluate the cost effectiveness of routine screening for postnatal depression in primary care. Cost effectiveness analysis with a decision model of alternative methods of screening for depression, including standardised postnatal depression and generic depression instruments. The performance of screening instruments was derived from a systematic review and bivariate meta-analysis at a range of instrument cut points; estimates of other relevant parameters were derived from literature sources and relevant databases. A decision tree considered the full treatment pathway from the possible onset of postnatal depression through identification, treatment, and possible relapse. Primary care. A hypothetical population of women assessed for postnatal depression either via routine care only or supplemented by use of formal identification methods six weeks postnatally, as recommended in recent guidelines. Costs expressed in 2006-7 prices and impact on health outcomes expressed in terms of quality adjusted life years (QALYs). The time horizon of the analysis was one year. The routine application of either postnatal or general depression questionnaires did not seem to be cost effective compared with routine care only. The Edinburgh postnatal depression scale (at a cut point of 16) had an incremental cost effectiveness ratio (ICER) of pound 41,103 (euro 45,398, $67,130) per QALY compared with routine care only. The ICER for all other strategies ranged from pound 49,928 to pound 272,463 per QALY versus routine care only, while the probability that no formal identification strategy was cost effective was 88% (59%) at a cost effectiveness threshold of pound 20,000 ( pound 30,000) per QALY. While sensitivity analysis indicated that the cost of managing incorrectly identified depression (false positive result) was an important driver of the model, formal identification approaches did not seem to be cost effective at any feasible estimate of this cost. Formal identification methods for postnatal depression do not seem to represent value for money for the NHS. The major determinant of cost effectiveness seems to be the potential additional costs of managing women incorrectly diagnosed as depressed. Formal identification methods for postnatal depression do not currently satisfy the National Screening Committee's criteria for the adoption of a screening strategy as part of national health policy.","Paulden, M.
 and Palmer, S.
 and Hewitt, C.
 and Gilbody, S.","Paulden, Palmer, Hewitt, Gilbody",https://dx.doi.org/10.1136/bmj.b5203,https://doi.org/10.1136/bmj.b5203,2021-08-03
3221.0,,pubmed,Colour vision testing for diabetic retinopathy: a systematic review of diagnostic accuracy and economic evaluation,Colour vision testing for diabetic retinopathy: a systematic review of diagnostic accuracy and economic evaluation,"OBJECTIVE: To determine the diagnostic performance and cost-effectiveness of colour vision testing (CVT) to identify and monitor the progression of diabetic retinopathy (DR). DATA SOURCES: Major electronic databases including MEDLINE, EMBASE, Cumulative Index to Nursing and Allied Health Literature, and Cochrane Database of Systematic Reviews were searched from inception to September 2008. REVIEW METHODS: A systematic review of the evidence was carried out according to standard methods. An online survey of National Screening Programme for Diabetic Retinopathy (NSPDR) clinical leads and programme managers assessed the diagnostic tools used routinely by local centres and their views on future research priorities. A decision tree and Markov model was developed to estimate the incremental costs and effects of adding CVT to the current NSPDR. RESULTS: In total, 25 studies on CVT met the inclusion criteria for the review, including 18 presenting 2 x 2 diagnostic accuracy data. The quality of studies and reporting was generally poor. Automated or computerised CVTs reported variable sensitivities (63-97%) and specificities (71-95%). One study reported good diagnostic accuracy estimates for computerised CVT plus retinal photography for detection of sight-threatening DR, but it included few cases of retinopathy in total. Results for pseudoisochromatic plates, anomaloscopes and colour arrangement tests were largely inadequate for DR screening, with Youden indices (sensitivity + specificity - 100%) close to zero. No studies were located that addressed patient preferences relating to CVT for DR. Retinal photography is universally employed as the primary method for retinal screening by centres responding to the online survey; none used CVT. The review of the economic evaluation literature found no previous studies describing the cost and effects of any type of CVT. Our economic evaluation suggested that adding CVT to the current national screening programme could be cost-effective if it adequately increases sensitivity and is relatively inexpensive. The deterministic base-case analysis indicated that the cost per quality-adjusted life-year gained may be 6364 pounds and 12,432 pounds for type 1 and type 2 diabetes respectively. However, probabilistic sensitivity analysis highlighted the substantial probability that CVT is not diagnostically accurate enough to be either an effective or a cost-effective addition to current screening methods. The results of the economic model should be treated with caution as the model is based on only one small study. CONCLUSIONS: There is insufficient evidence to support the use of CVT alone, or in combination with retinal photography, as a method for screening for retinopathy in patients with diabetes. Better quality diagnostic accuracy studies directly comparing the incremental value of CVT in addition to retinal photography are needed before drawing conclusions on cost-effectiveness. The most frequently cited preference for future research was the use of optical coherence tomography for the detection of clinically significant macular oedema. [References: 94]","To determine the diagnostic performance and cost-effectiveness of colour vision testing (CVT) to identify and monitor the progression of diabetic retinopathy (DR). Major electronic databases including MEDLINE, EMBASE, Cumulative Index to Nursing and Allied Health Literature, and Cochrane Database of Systematic Reviews were searched from inception to September 2008. A systematic review of the evidence was carried out according to standard methods. An online survey of National Screening Programme for Diabetic Retinopathy (NSPDR) clinical leads and programme managers assessed the diagnostic tools used routinely by local centres and their views on future research priorities. A decision tree and Markov model was developed to estimate the incremental costs and effects of adding CVT to the current NSPDR. In total, 25 studies on CVT met the inclusion criteria for the review, including 18 presenting 2 x 2 diagnostic accuracy data. The quality of studies and reporting was generally poor. Automated or computerised CVTs reported variable sensitivities (63-97%) and specificities (71-95%). One study reported good diagnostic accuracy estimates for computerised CVT plus retinal photography for detection of sight-threatening DR, but it included few cases of retinopathy in total. Results for pseudoisochromatic plates, anomaloscopes and colour arrangement tests were largely inadequate for DR screening, with Youden indices (sensitivity + specificity - 100%) close to zero. No studies were located that addressed patient preferences relating to CVT for DR. Retinal photography is universally employed as the primary method for retinal screening by centres responding to the online survey; none used CVT. The review of the economic evaluation literature found no previous studies describing the cost and effects of any type of CVT. Our economic evaluation suggested that adding CVT to the current national screening programme could be cost-effective if it adequately increases sensitivity and is relatively inexpensive. The deterministic base-case analysis indicated that the cost per quality-adjusted life-year gained may be 6364 pounds and 12,432 pounds for type 1 and type 2 diabetes respectively. However, probabilistic sensitivity analysis highlighted the substantial probability that CVT is not diagnostically accurate enough to be either an effective or a cost-effective addition to current screening methods. The results of the economic model should be treated with caution as the model is based on only one small study. There is insufficient evidence to support the use of CVT alone, or in combination with retinal photography, as a method for screening for retinopathy in patients with diabetes. Better quality diagnostic accuracy studies directly comparing the incremental value of CVT in addition to retinal photography are needed before drawing conclusions on cost-effectiveness. The most frequently cited preference for future research was the use of optical coherence tomography for the detection of clinically significant macular oedema.","Rodgers, M.
 and Hodges, R.
 and Hawkins, J.
 and Hollingworth, W.
 and Duffy, S.
 and McKibbin, M.
 and Mansfield, M.
 and Harbord, R.
 and Sterne, J.
 and Glasziou, P.
 and Whiting, P.
 and Westwood, M.","Rodgers, Hodges, Hawkins, Hollingworth, Duffy, McKibbin, Mansfield, Harbord, Sterne, Glasziou, Whiting, Westwood",not available,https://doi.org/10.3310/hta13600,2021-08-03
2586.0,,pubmed,The value of intraoperative parathyroid hormone monitoring in localized primary hyperparathyroidism: a cost analysis,The value of intraoperative parathyroid hormone monitoring in localized primary hyperparathyroidism: a cost analysis,"BACKGROUND: Minimally invasive parathyroidectomy (MIP) is the preferred approach to primary hyperparathyroidism (PHPT) when a single adenoma can be localized preoperatively. The added value of intraoperative parathyroid hormone (IOPTH) monitoring remains debated because its ability to prevent failed parathyroidectomy due to unrecognized multiple gland disease (MGD) must be balanced against assay-related costs. We used a decision tree and cost analysis model to examine IOPTH monitoring in localized PHPT. METHODS: Literature review identified 17 studies involving 4,280 unique patients, permitting estimation of base case costs and probabilities. Sensitivity analyses were performed to evaluate the uncertainty of the assumptions associated with IOPTH monitoring and surgical outcomes. IOPTH cost, MGD rate, and reoperation cost were varied to evaluate potential cost savings from IOPTH. RESULTS: The base case assumption was that in well-localized PHPT, IOPTH monitoring would increase the success rate of MIP from 96.3 to 98.8%. The cost of IOPTH varied with operating room time used. IOPTH reduced overall treatment costs only when total assay-related costs fell below $110 per case. Inaccurate localization and high reoperation cost both independently increased the value of IOPTH monitoring. The IOPTH strategy was cost saving when the rate of unrecognized MGD exceeded 6% or if the cost of reoperation exceeded $12,000 (compared with initial MIP cost of $3733). Setting the positive predictive value of IOPTH at 100% and reducing the false-negative rate to 0% did not substantially alter these findings. CONCLUSIONS: Institution-specific factors influence the value of IOPTH. In this model, IOPTH increased the cure rate marginally while incurring approximately 4% additional cost.","Minimally invasive parathyroidectomy (MIP) is the preferred approach to primary hyperparathyroidism (PHPT) when a single adenoma can be localized preoperatively. The added value of intraoperative parathyroid hormone (IOPTH) monitoring remains debated because its ability to prevent failed parathyroidectomy due to unrecognized multiple gland disease (MGD) must be balanced against assay-related costs. We used a decision tree and cost analysis model to examine IOPTH monitoring in localized PHPT. Literature review identified 17 studies involving 4,280 unique patients, permitting estimation of base case costs and probabilities. Sensitivity analyses were performed to evaluate the uncertainty of the assumptions associated with IOPTH monitoring and surgical outcomes. IOPTH cost, MGD rate, and reoperation cost were varied to evaluate potential cost savings from IOPTH. The base case assumption was that in well-localized PHPT, IOPTH monitoring would increase the success rate of MIP from 96.3 to 98.8%. The cost of IOPTH varied with operating room time used. IOPTH reduced overall treatment costs only when total assay-related costs fell below $110 per case. Inaccurate localization and high reoperation cost both independently increased the value of IOPTH monitoring. The IOPTH strategy was cost saving when the rate of unrecognized MGD exceeded 6% or if the cost of reoperation exceeded $12,000 (compared with initial MIP cost of $3733). Setting the positive predictive value of IOPTH at 100% and reducing the false-negative rate to 0% did not substantially alter these findings. Institution-specific factors influence the value of IOPTH. In this model, IOPTH increased the cure rate marginally while incurring approximately 4% additional cost.","Morris, L. F.
 and Zanocco, K.
 and Ituarte, P. H.
 and Ro, K.
 and Duh, Q. Y.
 and Sturgeon, C.
 and Yeh, M. W.","Morris, Zanocco, Ituarte, Ro, Duh, Sturgeon, Yeh",https://dx.doi.org/10.1245/s10434-009-0773-1,https://doi.org/10.1245/s10434-009-0773-1,2021-08-03
3171.0,,pubmed,Breastfeeding promotion for infants in neonatal units: a systematic review and economic analysis,Breastfeeding promotion for infants in neonatal units: a systematic review and economic analysis,"OBJECTIVES: To evaluate the effectiveness and cost-effectiveness of interventions that promote or inhibit breastfeeding or feeding with breastmilk for infants admitted to neonatal units, and to identify an agenda for future research. DATA SOURCES: Electronic databases were searched (including MEDLINE and MEDLINE In-Process Citations, EMBASE, CINAHL, Maternity and Infant Care, PsycINFO, British Nursing Index and Archive, Health Management Information Consortium, Cochrane Central Register of Controlled Trials, Science Citation Index, Pascal, Latin American and Caribbean Health Sciences, MetaRegister of Controlled Trials, Cochrane Database of Systematic Reviews, Database of Abstracts of Reviews of Effectiveness, Health Technology Assessment Database, National Research Register) from inception to February 2008. Advisors identified further published or unpublished material. REVIEW METHODS: All papers fulfilled eligibility criteria covering participants, interventions, study design and outcomes. Results from primary studies were assessed and summarised in a qualitative synthesis for each type of intervention and across types of intervention. To estimate long-term cost utility, a decision tree was developed to synthesise data on enhanced staff contact, breastmilk effectiveness, incidence of necrotising enterocolitis (NEC) and sepsis, resource use, survival and utilities. RESULTS: Forty-eight studies met the selection criteria for the effectiveness review, of which 65% (31/48) were RCTs, and 17% (8/48) were conducted in the UK. Seven were rated as good quality and 28 as moderate quality. No studies met the selection criteria for the health economics review. There is strong evidence that short periods of kangaroo skin-to-skin contact increased the duration of any breastfeeding for 1 month after discharge [risk ratio (RR) 4.76, 95% confidence interval (CI) 1.19 to 19.10] and for more than 6 weeks (RR 1.95, 95% CI 1.03 to 3.70) among clinically stable infants in industrialised settings. There is strong evidence for the effectiveness of peer support at home (in Manila) for mothers of term, low birthweight infants on any breastfeeding up to 24 weeks (RR 2.18, 95% CI 1.45 to 3.29) and exclusive breastfeeding from birth to 6 months (RR 65.94, 95% CI 4.12 to 1055.70), and for the effectiveness of peer support in hospital and at home for mothers of infants in Special Care Baby Units on providing any breastmilk at 12 weeks [odds ratio (OR) 2.81, 95% CI 1.11 to 7.14; p = 0.01]. There is more limited evidence for the effectiveness of skilled professional support in a US Neonatal Intensive Care Unit on infants receiving any breastmilk at discharge (OR 2.0, 95% CI 1.2 to 3.2, p = 0.004). Multidisciplinary staff training may increase knowledge and can increase initiation rates and duration of breastfeeding, although evidence is limited. Lack of staff training is an important barrier to implementation of effective interventions. Baby Friendly accreditation of the associated maternity hospital results in improvements in several breastfeeding-related outcomes for infants in neonatal units. Limited evidence suggests that cup feeding (versus bottle feeding) may increase breastfeeding at discharge and reduce the frequency of oxygen desaturation. Breastmilk expression using simultaneous pumping with an electric pump has advantages in the first 2 weeks. Pharmaceutical galactagogues have little benefit among mothers who have recently given birth. Our economic analysis found that additional skilled professional support in hospital was more effective and less costly (due to reduced neonatal illness) than normal staff contact. Additional support ranged from 0.009 quality-adjusted life-years (QALYs) to 0.251 QALYs more beneficial per infant and ranged from 66 pounds to 586 pounds cheaper per infant across the birthweight subpopulations. Donor milk would become cost-effective given improved mechanisms for its provision. CONCLUSIONS: Despite the limitations of the evidence base, kangaroo skin-to-skin contact, peer support, simultaneous breastmilk pumping, multidisciplinary staff training and the Baby Friendly accreditation of the associated maternity hospital have been shown to be effective, and skilled support from trained staff in hospital has been shown to be potentially cost-effective. All these point to future research priorities. Many of these interventions inter-relate: it is unlikely that specific clinical interventions will be effective if used alone. There is a need for national surveillance of feeding, health and cost outcomes for infants and mothers in neonatal units; to assist this goal, we propose consensus definitions of the initiation and duration of breastfeeding/breastmilk feeding with specific reference to infants admitted to neonatal units and their mothers. [References: 288]","To evaluate the effectiveness and cost-effectiveness of interventions that promote or inhibit breastfeeding or feeding with breastmilk for infants admitted to neonatal units, and to identify an agenda for future research. Electronic databases were searched (including MEDLINE and MEDLINE In-Process Citations, EMBASE, CINAHL, Maternity and Infant Care, PsycINFO, British Nursing Index and Archive, Health Management Information Consortium, Cochrane Central Register of Controlled Trials, Science Citation Index, Pascal, Latin American and Caribbean Health Sciences, MetaRegister of Controlled Trials, Cochrane Database of Systematic Reviews, Database of Abstracts of Reviews of Effectiveness, Health Technology Assessment Database, National Research Register) from inception to February 2008. Advisors identified further published or unpublished material. All papers fulfilled eligibility criteria covering participants, interventions, study design and outcomes. Results from primary studies were assessed and summarised in a qualitative synthesis for each type of intervention and across types of intervention. To estimate long-term cost utility, a decision tree was developed to synthesise data on enhanced staff contact, breastmilk effectiveness, incidence of necrotising enterocolitis (NEC) and sepsis, resource use, survival and utilities. Forty-eight studies met the selection criteria for the effectiveness review, of which 65% (31/48) were RCTs, and 17% (8/48) were conducted in the UK. Seven were rated as good quality and 28 as moderate quality. No studies met the selection criteria for the health economics review. There is strong evidence that short periods of kangaroo skin-to-skin contact increased the duration of any breastfeeding for 1 month after discharge [risk ratio (RR) 4.76, 95% confidence interval (CI) 1.19 to 19.10] and for more than 6 weeks (RR 1.95, 95% CI 1.03 to 3.70) among clinically stable infants in industrialised settings. There is strong evidence for the effectiveness of peer support at home (in Manila) for mothers of term, low birthweight infants on any breastfeeding up to 24 weeks (RR 2.18, 95% CI 1.45 to 3.29) and exclusive breastfeeding from birth to 6 months (RR 65.94, 95% CI 4.12 to 1055.70), and for the effectiveness of peer support in hospital and at home for mothers of infants in Special Care Baby Units on providing any breastmilk at 12 weeks [odds ratio (OR) 2.81, 95% CI 1.11 to 7.14; p = 0.01]. There is more limited evidence for the effectiveness of skilled professional support in a US Neonatal Intensive Care Unit on infants receiving any breastmilk at discharge (OR 2.0, 95% CI 1.2 to 3.2, p = 0.004). Multidisciplinary staff training may increase knowledge and can increase initiation rates and duration of breastfeeding, although evidence is limited. Lack of staff training is an important barrier to implementation of effective interventions. Baby Friendly accreditation of the associated maternity hospital results in improvements in several breastfeeding-related outcomes for infants in neonatal units. Limited evidence suggests that cup feeding (versus bottle feeding) may increase breastfeeding at discharge and reduce the frequency of oxygen desaturation. Breastmilk expression using simultaneous pumping with an electric pump has advantages in the first 2 weeks. Pharmaceutical galactagogues have little benefit among mothers who have recently given birth. Our economic analysis found that additional skilled professional support in hospital was more effective and less costly (due to reduced neonatal illness) than normal staff contact. Additional support ranged from 0.009 quality-adjusted life-years (QALYs) to 0.251 QALYs more beneficial per infant and ranged from 66 pounds to 586 pounds cheaper per infant across the birthweight subpopulations. Donor milk would become cost-effective given improved mechanisms for its provision. Despite the limitations of the evidence base, kangaroo skin-to-skin contact, peer support, simultaneous breastmilk pumping, multidisciplinary staff training and the Baby Friendly accreditation of the associated maternity hospital have been shown to be effective, and skilled support from trained staff in hospital has been shown to be potentially cost-effective. All these point to future research priorities. Many of these interventions inter-relate: it is unlikely that specific clinical interventions will be effective if used alone. There is a need for national surveillance of feeding, health and cost outcomes for infants and mothers in neonatal units; to assist this goal, we propose consensus definitions of the initiation and duration of breastfeeding/breastmilk feeding with specific reference to infants admitted to neonatal units and their mothers.","Renfrew, M. J.
 and Craig, D.
 and Dyson, L.
 and McCormick, F.
 and Rice, S.
 and King, S. E.
 and Misso, K.
 and Stenhouse, E.
 and Williams, A. F.","Renfrew, Craig, Dyson, McCormick, Rice, King, Misso, Stenhouse, Williams",https://dx.doi.org/10.3310/hta13400,https://doi.org/10.3310/hta13400,2021-08-03
2369.0,,pubmed,"Systematic review and individual patient data meta-analysis of diagnosis of heart failure, with modelling of implications of different diagnostic strategies in primary care","Systematic review and individual patient data meta-analysis of diagnosis of heart failure, with modelling of implications of different diagnostic strategies in primary care","OBJECTIVES: To assess the accuracy in diagnosing heart failure of clinical features and potential primary care investigations, and to perform a decision analysis to test the impact of plausible diagnostic strategies on costs and diagnostic yield in the UK health-care setting. DATA SOURCES: MEDLINE and CINAHL were searched from inception to 7 July 2006. 'Grey literature' databases and conference proceedings were searched and authors of relevant studies contacted for data that could not be extracted from the published papers. REVIEW METHODS: A systematic review of the clinical evidence was carried out according to standard methods. Individual patient data (IPD) analysis was performed on nine studies, and a logistic regression model to predict heart failure was developed on one of the data sets and validated on the other data sets. Cost-effectiveness modelling was based on a decision tree that compared different plausible investigation strategies. RESULTS: Dyspnoea was the only symptom or sign with high sensitivity (89%), but it had poor specificity (51%). Clinical features with relatively high specificity included history of myocardial infarction (89%), orthopnoea (89%), oedema (72%), elevated jugular venous pressure (70%), cardiomegaly (85%), added heart sounds (99%), lung crepitations (81%) and hepatomegaly (97%). However, the sensitivity of these features was low, ranging from 11% (added heart sounds) to 53% (oedema). Electrocardiography (ECG), B-type natriuretic peptides (BNP) and N-terminal pro-B-type natriuretic peptides (NT-proBNP) all had high sensitivities (89%, 93% and 93% respectively). Chest X-ray was moderately specific (76-83%) but insensitive (67-68%). BNP was more accurate than ECG, with a relative diagnostic odds ratio of ECG/BNP of 0.32 (95% CI 0.12-0.87). There was no difference between the diagnostic accuracy of BNP and NT-proBNP. A model based upon simple clinical features and BNP derived from one data set was found to have good validity when applied to other data sets. A model substituting ECG for BNP was less predictive. From this a simple clinical rule was developed: in a patient presenting with symptoms such as breathlessness in whom heart failure is suspected, refer directly to echocardiography if the patient has a history of myocardial infarction or basal crepitations or is a male with ankle oedema; otherwise, carry out a BNP test and refer for echocardiography depending on the results of the test. On the basis of the cost-effectiveness analysis carried out, such a decision rule is likely to be considered cost-effective to the NHS in terms of cost per additional case detected. The cost-effectiveness analysis further suggested that, if likely benefit to the patient in terms of improved life expectancy is taken into account, the optimum strategy would be to refer all patients with symptoms suggestive of heart failure directly for echocardiography. CONCLUSIONS: The analysis suggests the need for important changes to the NICE recommendations. First, BNP (or NT-proBNP) should be recommended over ECG and, second, some patients should be referred straight for echocardiography without undergoing any preliminary investigation. Future work should include evaluation of the clinical rule described above in clinical practice. [References: 197]","To assess the accuracy in diagnosing heart failure of clinical features and potential primary care investigations, and to perform a decision analysis to test the impact of plausible diagnostic strategies on costs and diagnostic yield in the UK health-care setting. MEDLINE and CINAHL were searched from inception to 7 July 2006. 'Grey literature' databases and conference proceedings were searched and authors of relevant studies contacted for data that could not be extracted from the published papers. A systematic review of the clinical evidence was carried out according to standard methods. Individual patient data (IPD) analysis was performed on nine studies, and a logistic regression model to predict heart failure was developed on one of the data sets and validated on the other data sets. Cost-effectiveness modelling was based on a decision tree that compared different plausible investigation strategies. Dyspnoea was the only symptom or sign with high sensitivity (89%), but it had poor specificity (51%). Clinical features with relatively high specificity included history of myocardial infarction (89%), orthopnoea (89%), oedema (72%), elevated jugular venous pressure (70%), cardiomegaly (85%), added heart sounds (99%), lung crepitations (81%) and hepatomegaly (97%). However, the sensitivity of these features was low, ranging from 11% (added heart sounds) to 53% (oedema). Electrocardiography (ECG), B-type natriuretic peptides (BNP) and N-terminal pro-B-type natriuretic peptides (NT-proBNP) all had high sensitivities (89%, 93% and 93% respectively). Chest X-ray was moderately specific (76-83%) but insensitive (67-68%). BNP was more accurate than ECG, with a relative diagnostic odds ratio of ECG/BNP of 0.32 (95% CI 0.12-0.87). There was no difference between the diagnostic accuracy of BNP and NT-proBNP. A model based upon simple clinical features and BNP derived from one data set was found to have good validity when applied to other data sets. A model substituting ECG for BNP was less predictive. From this a simple clinical rule was developed: in a patient presenting with symptoms such as breathlessness in whom heart failure is suspected, refer directly to echocardiography if the patient has a history of myocardial infarction or basal crepitations or is a male with ankle oedema; otherwise, carry out a BNP test and refer for echocardiography depending on the results of the test. On the basis of the cost-effectiveness analysis carried out, such a decision rule is likely to be considered cost-effective to the NHS in terms of cost per additional case detected. The cost-effectiveness analysis further suggested that, if likely benefit to the patient in terms of improved life expectancy is taken into account, the optimum strategy would be to refer all patients with symptoms suggestive of heart failure directly for echocardiography. The analysis suggests the need for important changes to the NICE recommendations. First, BNP (or NT-proBNP) should be recommended over ECG and, second, some patients should be referred straight for echocardiography without undergoing any preliminary investigation. Future work should include evaluation of the clinical rule described above in clinical practice.","Mant, J.
 and Doust, J.
 and Roalfe, A.
 and Barton, P.
 and Cowie, M. R.
 and Glasziou, P.
 and Mant, D.
 and McManus, R. J.
 and Holder, R.
 and Deeks, J.
 and Fletcher, K.
 and Qume, M.
 and Sohanpal, S.
 and Sanders, S.
 and Hobbs, F. D.","Mant, Doust, Roalfe, Barton, Cowie, Glasziou, Mant, McManus, Holder, Deeks, Fletcher, Qume, Sohanpal, Sanders, Hobbs",https://dx.doi.org/10.3310/hta13320,https://doi.org/10.3310/hta13320,2021-08-03
2459.0,,pubmed,Methods of prediction and prevention of pre-eclampsia: systematic reviews of accuracy and effectiveness literature with economic modelling,Methods of prediction and prevention of pre-eclampsia: systematic reviews of accuracy and effectiveness literature with economic modelling,"OBJECTIVES: To investigate the accuracy of predictive tests for pre-eclampsia and the effectiveness of preventative interventions for pre-eclampsia. Also to assess the cost-effectiveness of strategies (test-intervention combinations) to predict and prevent pre-eclampsia. DATA SOURCES: Major electronic databases were searched to January 2005 at least. REVIEW METHODS: Systematic reviews were carried out for test accuracy and effectiveness. Quality assessment was carried out using standard tools. For test accuracy, meta-analyses used a bivariate approach. Effectiveness reviews were conducted under the auspices of the Cochrane Pregnancy and Childbirth Group and used standard Cochrane review methods. The economic evaluation was from an NHS perspective and used a decision tree model. RESULTS: For the 27 tests reviewed, the quality of included studies was generally poor. Some tests appeared to have high specificity, but at the expense of compromised sensitivity. Tests that reached specificities above 90% were body mass index greater than 34, alpha-foetoprotein and uterine artery Doppler (bilateral notching). The only Doppler test with a sensitivity of over 60% was resistance index and combinations of indices. A few tests not commonly found in routine practice, such as kallikreinuria and SDS-PAGE proteinuria, seemed to offer the promise of high sensitivity, without compromising specificity, but these would require further investigation. For the 16 effectiveness reviews, the quality of included studies was variable. The largest review was of antiplatelet agents, primarily low-dose aspirin, and included 51 trials (36,500 women). This was the only review where the intervention was shown to prevent both pre-eclampsia and its consequences for the baby. Calcium supplementation also reduced the risk of pre-eclampsia, but with some uncertainty about the impact on outcomes for the baby. The only other intervention associated with a reduction in RR of pre-eclampsia was rest at home, with or without a nutritional supplement, for women with normal blood pressure. However, this review included just two small trials and its results should be interpreted with caution. The cost of most of the tests was modest, ranging from 5 pounds for blood tests such as serum uric acid to approximately 20 pounds for Doppler tests. Similarly, the cost of most interventions was also modest. In contrast, the best estimate of additional average cost associated with an average case of pre-eclampsia was high at approximately 9000 pounds. The results of the modelling revealed that prior testing with the test accuracy sensitivities and specificities identified appeared to offer little as a way of improving cost-effectiveness. Based on the evidence reviewed, none of the tests appeared sufficiently accurate to be clinically useful and the results of the model favoured no-test/treat-all strategies. Rest at home without any initial testing appeared to be the most cost-effective 'test-treatment' combination. Calcium supplementation to all women, without any initial testing, appeared to be the second most cost-effective. The economic model provided little support that any form of Doppler test has sufficiently high sensitivity and specificity to be cost-effective for the early identification of pre-eclampsia. It also suggested that the pattern of cost-effectiveness was no different in high-risk mothers than the low-risk mothers considered in the base case. CONCLUSIONS: The tests evaluated are not sufficiently accurate, in our opinion, to suggest their routine use in clinical practice. Calcium and antiplatelet agents, primarily low-dose aspirin, were the interventions shown to prevent pre-eclampsia. The most cost-effective approach to reducing pre-eclampsia is likely to be the provision of an effective, affordable and safe intervention applied to all mothers without prior testing to assess levels of risk. It is probably premature to suggest the implementation of a treat-all intervention strategy at present, however the feasibility and acceptability of this to women could be explored. Rigorous evaluation is needed of tests with modest cost whose initial assessments suggest that they may have high levels of both sensitivity and specificity. Similarly, there is a need for high-quality, adequately powered randomised controlled trials to investigate whether interventions such as advice to rest are indeed effective in reducing pre-eclampsia. In future, an economic model should be developed that considers not just pre-eclampsia, but other related outcomes, particularly those relevant to the infant such as perinatal death, preterm birth and small for gestational age. Such a modelling project should make provision for primary data collection on the safety of interventions and their associated costs. [References: 156]","To investigate the accuracy of predictive tests for pre-eclampsia and the effectiveness of preventative interventions for pre-eclampsia. Also to assess the cost-effectiveness of strategies (test-intervention combinations) to predict and prevent pre-eclampsia. Major electronic databases were searched to January 2005 at least. Systematic reviews were carried out for test accuracy and effectiveness. Quality assessment was carried out using standard tools. For test accuracy, meta-analyses used a bivariate approach. Effectiveness reviews were conducted under the auspices of the Cochrane Pregnancy and Childbirth Group and used standard Cochrane review methods. The economic evaluation was from an NHS perspective and used a decision tree model. For the 27 tests reviewed, the quality of included studies was generally poor. Some tests appeared to have high specificity, but at the expense of compromised sensitivity. Tests that reached specificities above 90% were body mass index greater than 34, alpha-foetoprotein and uterine artery Doppler (bilateral notching). The only Doppler test with a sensitivity of over 60% was resistance index and combinations of indices. A few tests not commonly found in routine practice, such as kallikreinuria and SDS-PAGE proteinuria, seemed to offer the promise of high sensitivity, without compromising specificity, but these would require further investigation. For the 16 effectiveness reviews, the quality of included studies was variable. The largest review was of antiplatelet agents, primarily low-dose aspirin, and included 51 trials (36,500 women). This was the only review where the intervention was shown to prevent both pre-eclampsia and its consequences for the baby. Calcium supplementation also reduced the risk of pre-eclampsia, but with some uncertainty about the impact on outcomes for the baby. The only other intervention associated with a reduction in RR of pre-eclampsia was rest at home, with or without a nutritional supplement, for women with normal blood pressure. However, this review included just two small trials and its results should be interpreted with caution. The cost of most of the tests was modest, ranging from 5 pounds for blood tests such as serum uric acid to approximately 20 pounds for Doppler tests. Similarly, the cost of most interventions was also modest. In contrast, the best estimate of additional average cost associated with an average case of pre-eclampsia was high at approximately 9000 pounds. The results of the modelling revealed that prior testing with the test accuracy sensitivities and specificities identified appeared to offer little as a way of improving cost-effectiveness. Based on the evidence reviewed, none of the tests appeared sufficiently accurate to be clinically useful and the results of the model favoured no-test/treat-all strategies. Rest at home without any initial testing appeared to be the most cost-effective 'test-treatment' combination. Calcium supplementation to all women, without any initial testing, appeared to be the second most cost-effective. The economic model provided little support that any form of Doppler test has sufficiently high sensitivity and specificity to be cost-effective for the early identification of pre-eclampsia. It also suggested that the pattern of cost-effectiveness was no different in high-risk mothers than the low-risk mothers considered in the base case. The tests evaluated are not sufficiently accurate, in our opinion, to suggest their routine use in clinical practice. Calcium and antiplatelet agents, primarily low-dose aspirin, were the interventions shown to prevent pre-eclampsia. The most cost-effective approach to reducing pre-eclampsia is likely to be the provision of an effective, affordable and safe intervention applied to all mothers without prior testing to assess levels of risk. It is probably premature to suggest the implementation of a treat-all intervention strategy at present, however the feasibility and acceptability of this to women could be explored. Rigorous evaluation is needed of tests with modest cost whose initial assessments suggest that they may have high levels of both sensitivity and specificity. Similarly, there is a need for high-quality, adequately powered randomised controlled trials to investigate whether interventions such as advice to rest are indeed effective in reducing pre-eclampsia. In future, an economic model should be developed that considers not just pre-eclampsia, but other related outcomes, particularly those relevant to the infant such as perinatal death, preterm birth and small for gestational age. Such a modelling project should make provision for primary data collection on the safety of interventions and their associated costs.","Meads, C. A.
 and Cnossen, J. S.
 and Meher, S.
 and Juarez-Garcia, A.
 and ter Riet, G.
 and Duley, L.
 and Roberts, T. E.
 and Mol, B. W.
 and van der Post, J. A.
 and Leeflang, M. M.
 and Barton, P. M.
 and Hyde, C. J.
 and Gupta, J. K.
 and Khan, K. S.","Meads, Cnossen, Meher, Juarez-Garcia, ter Riet, Duley, Roberts, Mol, van der Post, Leeflang, Barton, Hyde, Gupta, Khan",not available,https://doi.org/10.3310/hta12060,2021-08-03
1505.0,,pubmed,The value of myocardial perfusion scintigraphy in the diagnosis and management of angina and myocardial infarction: a probabilistic economic analysis,The value of myocardial perfusion scintigraphy in the diagnosis and management of angina and myocardial infarction: a probabilistic economic analysis,"BACKGROUND AND AIM: Coronary heart disease (CHD) is the most common cause of death in the United Kingdom, accounting for more than 120,000 deaths in 2001, among the highest rates in the world. This study reports an economic evaluation of single photon emission computed tomography myocardial perfusion scintigraphy (SPECT) for the diagnosis and management of coronary artery disease (CAD). METHODS: Strategies involving SPECT with and without stress electrocardiography (ECG) and coronary angiography (CA) were compared to diagnostic strategies not involving SPECT. The diagnosis decision was modeled with a decision tree model and long-term costs and consequences using a Markov model. Data to populate the models were obtained from a series of systematic reviews. Unlike earlier evaluations, a probabilistic analysis was included to assess the statistical imprecision of the results. The results are presented in terms of incremental cost per quality-adjusted life year (QALY). RESULTS: At prevalence levels of CAD of 10.5%, SPECT-based strategies are cost-effective; ECG-CA is highly unlikely to be optimal. At a ceiling ratio of Pound 20,000 per QALY, SPECT-CA has a 90% likelihood of being optimal. Beyond this threshold, this strategy becomes less likely to be cost-effective. At more than Pound 75,000 per QALY, coronary angiography is most likely to be optimal. For higher levels of prevalence (around 50%) and more than a Pound 10,000 per QALY threshold, coronary angiography is the optimal decision. CONCLUSIONS: SPECT-based strategies are likely to be cost-effective when risk of CAD is modest (10.5%). Sensitivity analyses show these strategies dominated non-SPECT-based strategies for risk of CAD up to 4%. At higher levels of prevalence, invasive strategies may become worthwhile. Finally, sensitivity analyses show stress echocardiography as a potentially cost-effective option, and further research to assess the relative cost-effectiveness of echocardiography should also be performed.","Coronary heart disease (CHD) is the most common cause of death in the United Kingdom, accounting for more than 120,000 deaths in 2001, among the highest rates in the world. This study reports an economic evaluation of single photon emission computed tomography myocardial perfusion scintigraphy (SPECT) for the diagnosis and management of coronary artery disease (CAD). Strategies involving SPECT with and without stress electrocardiography (ECG) and coronary angiography (CA) were compared to diagnostic strategies not involving SPECT. The diagnosis decision was modeled with a decision tree model and long-term costs and consequences using a Markov model. Data to populate the models were obtained from a series of systematic reviews. Unlike earlier evaluations, a probabilistic analysis was included to assess the statistical imprecision of the results. The results are presented in terms of incremental cost per quality-adjusted life year (QALY). At prevalence levels of CAD of 10.5%, SPECT-based strategies are cost-effective; ECG-CA is highly unlikely to be optimal. At a ceiling ratio of Pound 20,000 per QALY, SPECT-CA has a 90% likelihood of being optimal. Beyond this threshold, this strategy becomes less likely to be cost-effective. At more than Pound 75,000 per QALY, coronary angiography is most likely to be optimal. For higher levels of prevalence (around 50%) and more than a Pound 10,000 per QALY threshold, coronary angiography is the optimal decision. SPECT-based strategies are likely to be cost-effective when risk of CAD is modest (10.5%). Sensitivity analyses show these strategies dominated non-SPECT-based strategies for risk of CAD up to 4%. At higher levels of prevalence, invasive strategies may become worthwhile. Finally, sensitivity analyses show stress echocardiography as a potentially cost-effective option, and further research to assess the relative cost-effectiveness of echocardiography should also be performed.","Hernandez, R.
 and Vale, L.","HernÃ¡ndez, Vale",not available,https://doi.org/10.1177/0272989X07306111,2021-08-03
3089.0,,pubmed,Automated vs continuous ambulatory peritoneal dialysis: a systematic review of randomized controlled trials,Automated vs continuous ambulatory peritoneal dialysis: a systematic review of randomized controlled trials,"BACKGROUND: A systematic review of randomized controlled trials (RCTs) comparing continuous ambulatory peritoneal dialysis (CAPD) with all forms of automated peritoneal dialysis (APD) was performed to assess their comparative clinical effectiveness. METHODS: The Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE, EMBASE and CINAHL, were searched for relevant RCTs. Analysis was by a random effects model and results expressed as relative risk (RR) and weighted mean difference (WMD) with 95% confidence intervals (CI). RESULTS: Three trials (139 patients) were identified. APD when compared to CAPD was found to have significantly lower peritonitis rates (two trials, 107 patients, rate ratio 0.54, 95% CI 0.35-0.83) and hospitalization rates (one trial, 82 patients, rate ratio 0.60, 95% CI 0.39-0.93) but not exit-site infection rates (two trials, 107 patients, rate ratio 1.00, 95% CI 0.56-1.76). However no differences were detected between APD and CAPD in respect to risk of mortality (RR 1.49, 95% CI 0.51-4.37), peritonitis (RR 0.75, 95% CI 0.50-1.11), switching from the original peritoneal dialysis (PD) modality to a different dialysis modality including an alternative form of PD (RR 0.50, 95% CI 0.25-1.02), PD catheter removal (RR 0.64, 95% CI 0.27-1.48) and hospital admissions (RR 0.96, 95% CI 0.43-2.17). Patients on APD were found to have significantly more time for work, family and social activities. CONCLUSIONS: APD appears to be more beneficial than CAPD, in terms of reducing peritonitis rates and with respect to certain social issues that impact on patients' quality of life. Further, adequately powered trials are required to confirm the benefits for APD found in this review and detect differences with respect to other clinically important outcomes that may have been missed by the trials included in this review due to their small size and short follow-up periods. [References: 32]","A systematic review of randomized controlled trials (RCTs) comparing continuous ambulatory peritoneal dialysis (CAPD) with all forms of automated peritoneal dialysis (APD) was performed to assess their comparative clinical effectiveness. The Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE, EMBASE and CINAHL, were searched for relevant RCTs. Analysis was by a random effects model and results expressed as relative risk (RR) and weighted mean difference (WMD) with 95% confidence intervals (CI). Three trials (139 patients) were identified. APD when compared to CAPD was found to have significantly lower peritonitis rates (two trials, 107 patients, rate ratio 0.54, 95% CI 0.35-0.83) and hospitalization rates (one trial, 82 patients, rate ratio 0.60, 95% CI 0.39-0.93) but not exit-site infection rates (two trials, 107 patients, rate ratio 1.00, 95% CI 0.56-1.76). However no differences were detected between APD and CAPD in respect to risk of mortality (RR 1.49, 95% CI 0.51-4.37), peritonitis (RR 0.75, 95% CI 0.50-1.11), switching from the original peritoneal dialysis (PD) modality to a different dialysis modality including an alternative form of PD (RR 0.50, 95% CI 0.25-1.02), PD catheter removal (RR 0.64, 95% CI 0.27-1.48) and hospital admissions (RR 0.96, 95% CI 0.43-2.17). Patients on APD were found to have significantly more time for work, family and social activities. APD appears to be more beneficial than CAPD, in terms of reducing peritonitis rates and with respect to certain social issues that impact on patients' quality of life. Further, adequately powered trials are required to confirm the benefits for APD found in this review and detect differences with respect to other clinically important outcomes that may have been missed by the trials included in this review due to their small size and short follow-up periods.","Rabindranath, K. S.
 and Adams, J.
 and Ali, T. Z.
 and Daly, C.
 and Vale, L.
 and Macleod, A. M.","Rabindranath, Adams, Ali, Daly, Vale, Macleod",not available,https://doi.org/10.1093/ndt/gfm515,2021-08-03
713.0,,pubmed,"A systematic review of duplex ultrasound, magnetic resonance angiography and computed tomography angiography for the diagnosis and assessment of symptomatic, lower limb peripheral arterial disease","A systematic review of duplex ultrasound, magnetic resonance angiography and computed tomography angiography for the diagnosis and assessment of symptomatic, lower limb peripheral arterial disease","OBJECTIVES: To determine the diagnostic accuracy and cost-effectiveness of duplex ultrasound (DUS), magnetic resonance angiography (MRA), and computed tomography angiography (CTA), as alternatives to contrast angiography (CA), for the assessment of lower limb peripheral arterial disease (PAD). DATA SOURCES: Ten electronic databases were searched in April 2004, with an update in May 2005. Six key journals and bibliographies of included studies were also searched and experts in the field were consulted. REVIEW METHODS: Data extraction and quality assessment were performed in duplicate. Data were analysed according to test type and diagnostic threshold. For the economic analysis, a decision tree was developed and a probabilistic sensitivity analysis performed to incorporate statistical uncertainty into the cost-effectiveness analysis. RESULTS: A total of 113 studies met the inclusion criteria (including six economic evaluations). For the detection of stenosis greater than 50% in the whole leg, contrast-enhanced (CE) MRA (14 studies) had the highest diagnostic accuracy, with sensitivity ranging from 92 to 99.5% and specificity from 64 to 99%. Two-dimensional (2D) time-of-flight (TOF) MRA (11 studies) was less accurate, with sensitivity ranging from 79 to 94% and specificity from 74 to 92%. 2D phase-contrast (PC) MRA (one study) had a sensitivity of 98% and specificity of 74%. CTA (seven studies) also appeared slightly inferior to CE MRA, with a sensitivity ranging from 89 to 99% and specificity from 83 to 97%, but better than DUS (28 studies), which had a sensitivity ranging from 80 to 98% and specificity from 89 to 99%. There was some indication that CE MRA and DUS were more accurate for detecting stenoses/occlusions above the knee than below the knee or in the pedal artery. The four studies of patient attitudes strongly suggested that patients preferred CE MRA to CA. CA was considered the most uncomfortable test, followed by CE MRA, with CTA being the least uncomfortable. Half of the patients (from a sample who did not suffer from claustrophobia and had no metallic implants) expressed no preference between undergoing TOF MRA or DUS; most of those who did express a preference favoured TOF MRA. In the 55 studies identified for adverse events, MRA was associated with the highest reported proportion. However, the most severe adverse events were more common in patients undergoing CA; although these were rare for both tests. The economic evaluation showed DUS dominated the other alternatives when the whole leg was assessed, by presenting higher effectiveness at a lower cost per quality-adjusted life-year (QALY; i.e. 13,646 pounds per QALY). When the assessment was limited to a section of the leg, either above the knee or below the knee, 2D TOF MRA was the most cost-effective preoperative diagnostic strategy. The incremental cost per QALY for below-the-knee comparisons was equal to 37,024 pounds when 2D TOF MRA was compared with DUS. For above-the-knee comparisons, 2D TOF MRA presented the lowest cost and slightly lower effectiveness compared with CE MRA, with a cost per QALY equal to 13,442 pounds. CONCLUSIONS: The results of the review suggest that CE MRA has a better overall diagnostic accuracy than CTA or DUS, and that CE MRA is generally preferred by patients over CA. Where available, CE MRA may be a viable alternative to CA. The only controlled trial suggested that the results of DUS were comparable to those of CA, in terms of surgical planning and outcome. This finding conflicts with the results of diagnostic accuracy studies, which reported poor estimates of accuracy for DUS in comparison with CA. There was insufficient evidence to evaluate the usefulness of CTA for the assessment of PAD, particularly newer techniques. The results of the economic modelling suggest that for PAD patients for whom the whole leg is evaluated by a preoperative diagnostic test, DUS dominates the other alternatives by presenting higher effectiveness at a lower cost per QALY. However, when the analysis of stenosis is limited to a section of the leg, either above the knee or below the knee, 2D TOF MRA appears to be the most cost-effective preoperative diagnostic strategy. Further research is needed into a number of areas including the relative clinical effectiveness of the available imaging tests, in terms of surgical planning and postoperative outcome. [References: 692]","To determine the diagnostic accuracy and cost-effectiveness of duplex ultrasound (DUS), magnetic resonance angiography (MRA), and computed tomography angiography (CTA), as alternatives to contrast angiography (CA), for the assessment of lower limb peripheral arterial disease (PAD). Ten electronic databases were searched in April 2004, with an update in May 2005. Six key journals and bibliographies of included studies were also searched and experts in the field were consulted. Data extraction and quality assessment were performed in duplicate. Data were analysed according to test type and diagnostic threshold. For the economic analysis, a decision tree was developed and a probabilistic sensitivity analysis performed to incorporate statistical uncertainty into the cost-effectiveness analysis. A total of 113 studies met the inclusion criteria (including six economic evaluations). For the detection of stenosis greater than 50% in the whole leg, contrast-enhanced (CE) MRA (14 studies) had the highest diagnostic accuracy, with sensitivity ranging from 92 to 99.5% and specificity from 64 to 99%. Two-dimensional (2D) time-of-flight (TOF) MRA (11 studies) was less accurate, with sensitivity ranging from 79 to 94% and specificity from 74 to 92%. 2D phase-contrast (PC) MRA (one study) had a sensitivity of 98% and specificity of 74%. CTA (seven studies) also appeared slightly inferior to CE MRA, with a sensitivity ranging from 89 to 99% and specificity from 83 to 97%, but better than DUS (28 studies), which had a sensitivity ranging from 80 to 98% and specificity from 89 to 99%. There was some indication that CE MRA and DUS were more accurate for detecting stenoses/occlusions above the knee than below the knee or in the pedal artery. The four studies of patient attitudes strongly suggested that patients preferred CE MRA to CA. CA was considered the most uncomfortable test, followed by CE MRA, with CTA being the least uncomfortable. Half of the patients (from a sample who did not suffer from claustrophobia and had no metallic implants) expressed no preference between undergoing TOF MRA or DUS; most of those who did express a preference favoured TOF MRA. In the 55 studies identified for adverse events, MRA was associated with the highest reported proportion. However, the most severe adverse events were more common in patients undergoing CA; although these were rare for both tests. The economic evaluation showed DUS dominated the other alternatives when the whole leg was assessed, by presenting higher effectiveness at a lower cost per quality-adjusted life-year (QALY; i.e. 13,646 pounds per QALY). When the assessment was limited to a section of the leg, either above the knee or below the knee, 2D TOF MRA was the most cost-effective preoperative diagnostic strategy. The incremental cost per QALY for below-the-knee comparisons was equal to 37,024 pounds when 2D TOF MRA was compared with DUS. For above-the-knee comparisons, 2D TOF MRA presented the lowest cost and slightly lower effectiveness compared with CE MRA, with a cost per QALY equal to 13,442 pounds. The results of the review suggest that CE MRA has a better overall diagnostic accuracy than CTA or DUS, and that CE MRA is generally preferred by patients over CA. Where available, CE MRA may be a viable alternative to CA. The only controlled trial suggested that the results of DUS were comparable to those of CA, in terms of surgical planning and outcome. This finding conflicts with the results of diagnostic accuracy studies, which reported poor estimates of accuracy for DUS in comparison with CA. There was insufficient evidence to evaluate the usefulness of CTA for the assessment of PAD, particularly newer techniques. The results of the economic modelling suggest that for PAD patients for whom the whole leg is evaluated by a preoperative diagnostic test, DUS dominates the other alternatives by presenting higher effectiveness at a lower cost per QALY. However, when the analysis of stenosis is limited to a section of the leg, either above the knee or below the knee, 2D TOF MRA appears to be the most cost-effective preoperative diagnostic strategy. Further research is needed into a number of areas including the relative clinical effectiveness of the available imaging tests, in terms of surgical planning and postoperative outcome.","Collins, R.
 and Cranny, G.
 and Burch, J.
 and Aguiar-Ibanez, R.
 and Craig, D.
 and Wright, K.
 and Berry, E.
 and Gough, M.
 and Kleijnen, J.
 and Westwood, M.","Collins, Cranny, Burch, Aguiar-IbÃ¡Ã±ez, Craig, Wright, Berry, Gough, Kleijnen, Westwood",not available,https://doi.org/10.3310/hta11200,2021-08-03
17305.0,arxiv,acl,XOR QA: Cross-lingual Open-Retrieval Question Answering,XOR QA: Cross-lingual Open-Retrieval Question Answering,"Multilingual question answering tasks typically assume answers exist in the same language as the question. Yet in practice, many languages face both information scarcity---where languages have few reference articles---and information asymmetry---where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on questions from TyDi QA lacking same-language answers. Our task formulation, called Cross-lingual Open Retrieval Question Answering (XOR QA), includes 40k information-seeking questions from across 7 diverse non-English languages. Based on this dataset, we introduce three new tasks that involve cross-lingual document retrieval using multi-lingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington.edu/xorqa.","Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity---where languages have few reference articles---and information asymmetry---where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington.edu/xorqa/.","['Akari Asai', 'Jungo Kasai', 'Jonathan H. Clark', 'Kenton Lee', 'Eunsol Choi', 'Hannaneh Hajishirzi']","Asai, Akari; Kasai, Jungo; Clark, Jonathan; Lee, Kenton; Choi, Eunsol; Hajishirzi, Hannaneh",https://export.arxiv.org/abs/2010.11856,https://aclanthology.org/2021.naacl-main.46,2021-08-03
17270.0,arxiv,acl,SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer   Matching Retrieval,SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval,"We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, Natuarl Question, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.","We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.","['Tiancheng Zhao', 'Xiaopeng Lu', 'Kyusong Lee']","Zhao, Tiancheng; Lu, Xiaopeng; Lee, Kyusong",https://export.arxiv.org/abs/2009.13013,https://aclanthology.org/2021.naacl-main.47,2021-08-03
17299.0,arxiv,acl,Technical Question Answering across Tasks and Domains,Technical Question Answering across Tasks and Domains,"Building automatic technical support system is an important yet challenge task. Conceptually, to answer a user question on a technical forum, a human expert has to first retrieve relevant documents, and then read them carefully to identify the answer snippet. Despite huge success the researchers have achieved in coping with general domain question answering (QA), much less attentions have been paid for investigating technical QA. Specifically, existing methods suffer from several unique challenges (i) the question and answer rarely overlaps substantially and (ii) very limited data size. In this paper, we propose a novel framework of deep transfer learning to effectively address technical QA across tasks and domains. To this end, we present an adjustable joint learning approach for document retrieval and reading comprehension tasks. Our experiments on the TechQA demonstrates superior performance compared with state-of-the-art methods.","Building automatic technical support system is an important yet challenge task. Conceptually, to answer a user question on a technical forum, a human expert has to first retrieve relevant documents, and then read them carefully to identify the answer snippet. Despite huge success the researchers have achieved in coping with general domain question answering (QA), much less attentions have been paid for investigating technical QA. Specifically, existing methods suffer from several unique challenges (i) the question and answer rarely overlaps substantially and (ii) very limited data size. In this paper, we propose a novel framework of deep transfer learning to effectively address technical QA across tasks and domains. To this end, we present an adjustable joint learning approach for document retrieval and reading comprehension tasks. Our experiments on the TechQA demonstrates superior performance compared with state-of-the-art methods.","['Wenhao Yu', 'Lingfei Wu', 'Yu Deng', 'Qingkai Zeng', 'Ruchi Mahindru', 'Sinem Guven', 'Meng Jiang']","Yu, Wenhao; Wu, Lingfei; Deng, Yu; Zeng, Qingkai; Mahindru, Ruchi; Guven, Sinem; Jiang, Meng",https://export.arxiv.org/abs/2010.09780,https://aclanthology.org/2021.naacl-industry.23,2021-08-03
9892.0,,acl,Scalable Evaluation and Improvement of Document Set Expansion via Neural   Positive-Unlabeled Learning,Scalable Evaluation and Improvement of Document Set Expansion via Neural Positive-Unlabeled Learning,"We consider the situation in which a user has collected a small set of documents on a cohesive topic, and they want to retrieve additional documents on this topic from a large collection. Information Retrieval (IR) solutions treat the document set as a query, and look for similar documents in the collection. We propose to extend the IR approach by treating the problem as an instance of positive-unlabeled (PU) learning---i.e., learning binary classifiers from only positive and unlabeled data, where the positive data corresponds to the query documents, and the unlabeled data is the results returned by the IR engine. Utilizing PU learning for text with big neural networks is a largely unexplored field. We discuss various challenges in applying PU learning to the setting, including an unknown class prior, extremely imbalanced data and large-scale accurate evaluation of models, and we propose solutions and empirically validate them. We demonstrate the effectiveness of the method using a series of experiments of retrieving PubMed abstracts adhering to fine-grained topics. We demonstrate improvements over the base IR solution and other baselines. Implementation is available at https://github.com/sayaendo/document-set-expansion-pu.","We consider the situation in which a user has collected a small set of documents on a cohesive topic, and they want to retrieve additional documents on this topic from a large collection. Information Retrieval (IR) solutions treat the document set as a query, and look for similar documents in the collection. We propose to extend the IR approach by treating the problem as an instance of positive-unlabeled (PU) learning---i.e., learning binary classifiers from only positive (the query documents) and unlabeled (the results of the IR engine) data. Utilizing PU learning for text with big neural networks is a largely unexplored field. We discuss various challenges in applying PU learning to the setting, showing that the standard implementations of state-of-the-art PU solutions fail. We propose solutions for each of the challenges and empirically validate them with ablation tests. We demonstrate the effectiveness of the new method using a series of experiments of retrieving PubMed abstracts adhering to fine-grained topics, showing improvements over the common IR solution and other baselines.","['Alon Jacovi', 'Gang Niu', 'Yoav Goldberg', 'Masashi Sugiyama']","Jacovi, Alon; Niu, Gang; Goldberg, Yoav; Sugiyama, Masashi",https://export.arxiv.org/abs/1910.13339,https://aclanthology.org/2021.eacl-main.47,2021-08-03
17163.0,arxiv,acl,Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of   Current Evaluation Protocols,Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of Current Evaluation Protocols,"As conversational AI-based dialogue management has increasingly become a trending topic, the need for a standardized and reliable evaluation procedure grows even more pressing. The current state of affairs suggests various evaluation protocols to assess chat-oriented dialogue management systems, rendering it difficult to conduct fair comparative studies across different approaches and gain an insightful understanding of their values. To foster this research, a more robust evaluation protocol must be set in place. This paper presents a comprehensive synthesis of both automated and human evaluation methods on dialogue systems, identifying their shortcomings while accumulating evidence towards the most effective evaluation dimensions. A total of 20 papers from the last two years are surveyed to analyze three types of evaluation protocols: automated, static, and interactive. Finally, the evaluation dimensions used in these papers are compared against our expert evaluation on the system-user dialogue data collected from the Alexa Prize 2020.","As conversational AI-based dialogue management has increasingly become a trending topic, the need for a standardized and reliable evaluation procedure grows even more pressing. The current state of affairs suggests various evaluation protocols to assess chat-oriented dialogue management systems, rendering it difficult to conduct fair comparative studies across different approaches and gain an insightful understanding of their values. To foster this research, a more robust evaluation protocol must be set in place. This paper presents a comprehensive synthesis of both automated and human evaluation methods on dialogue systems, identifying their shortcomings while accumulating evidence towards the most effective evaluation dimensions. A total of 20 papers from the last two years are surveyed to analyze three types of evaluation protocols: automated, static, and interactive. Finally, the evaluation dimensions used in these papers are compared against our expert evaluation on the system-user dialogue data collected from the Alexa Prize 2020.","['Sarah E. Finch', 'Jinho D. Choi']","Finch, Sarah E.; Choi, Jinho D.",https://export.arxiv.org/abs/2006.06110,https://aclanthology.org/2020.sigdial-1.29,2021-08-03
17225.0,arxiv,acl,LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for   Multi-Granular Propaganda Span Identification,LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for Multi-Granular Propaganda Span Identification,"In this paper we describe our submission for the task of Propaganda Span Identification in news articles. We introduce a BERT-BiLSTM based span-level propaganda classification model that identifies which token spans within the sentence are indicative of propaganda. The ""multi-granular"" model incorporates linguistic knowledge at various levels of text granularity, including word, sentence and document level syntactic, semantic and pragmatic affect features, which significantly improve model performance, compared to its language-agnostic variant. To facilitate better representation learning, we also collect a corpus of 10k news articles, and use it for fine-tuning the model. The final model is a majority-voting ensemble which learns different propaganda class boundaries by leveraging different subsets of incorporated knowledge and attains $4^{th}$ position on the test leaderboard. Our final model and code is released at https://github.com/sopu/PropagandaSemEval2020.","In this paper we describe our submission for the task of Propaganda Span Identification in news articles. We introduce a BERT-BiLSTM based span-level propaganda classification model that identifies which token spans within the sentence are indicative of propaganda. The ''multi-granular'' model incorporates linguistic knowledge at various levels of text granularity, including word, sentence and document level syntactic, semantic and pragmatic affect features, which significantly improve model performance, compared to its language-agnostic variant. To facilitate better representation learning, we also collect a corpus of 10k news articles, and use it for fine-tuning the model. The final model is a majority-voting ensemble which learns different propaganda class boundaries by leveraging different subsets of incorporated knowledge.","['Sopan Khosla', 'Rishabh Joshi', 'Ritam Dutt', 'Alan W Black', 'Yulia Tsvetkov']","Khosla, Sopan; Joshi, Rishabh; Dutt, Ritam; Black, Alan W; Tsvetkov, Yulia",https://export.arxiv.org/abs/2008.04820,https://aclanthology.org/2020.semeval-1.230,2021-08-03
17288.0,arxiv,acl,Scaling Systematic Literature Reviews with Machine Learning Pipelines,Scaling Systematic Literature Reviews with Machine Learning Pipelines,"Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a systematic review are easily done automatically: searching for documents can be done via APIs and scrapers, selection of relevant documents can be done via binary classification, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these tasks. We construct a pipeline that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of classifiers to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of data extraction with varying difficulty in annotation, and five different neural architectures to do the extraction. We find that we can get surprising accuracy and generalisability of the whole pipeline system with only 2 weeks of human-expert annotation, which is only 15% of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.","Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a systematic review are easily done automatically: searching for documents can be done via APIs and scrapers, selection of relevant documents can be done via binary classification, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these tasks. We construct a pipeline that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of classifiers to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of data extraction with varying difficulty in annotation, and five different neural architectures to do the extraction. We find that we can get surprising accuracy and generalisability of the whole pipeline system with only 2 weeks of human-expert annotation, which is only 15% of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.","['Seraphina Goldfarb-Tarrant', 'Alexander Robertson', 'Jasmina Lazic', 'Theodora Tsouloufi', 'Louise Donnison', 'Karen Smyth']","Goldfarb-Tarrant, Seraphina; Robertson, Alexander; Lazic, Jasmina; Tsouloufi, Theodora; Donnison, Louise; Smyth, Karen",https://export.arxiv.org/abs/2010.04665,https://aclanthology.org/2020.sdp-1.21,2021-08-03
9894.0,,acl,CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data,CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data,"Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.","Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.","['Guillaume Wenzek', 'Marie-Anne Lachaux', 'Alexis Conneau', 'Vishrav Chaudhary', 'Francisco Guzmán', 'Armand Joulin', 'Edouard Grave']","Wenzek, Guillaume; Lachaux, Marie-Anne; Conneau, Alexis; Chaudhary, Vishrav; Guzm'an, Francisco; Joulin, Armand; Grave, Edouard",https://export.arxiv.org/abs/1911.00359,https://aclanthology.org/2020.lrec-1.494,2021-08-03
9698.0,,acl,A Survey on Natural Language Processing for Fake News Detection,A Survey on Natural Language Processing for Fake News Detection,"Fake news detection is a critical yet challenging problem in Natural Language Processing (NLP). The rapid rise of social networking platforms has not only yielded a vast increase in information accessibility but has also accelerated the spread of fake news. Thus, the effect of fake news has been growing, sometimes extending to the offline world and threatening public safety. Given the massive amount of Web content, automatic fake news detection is a practical NLP problem useful to all online content providers, in order to reduce the human time and effort to detect and prevent the spread of fake news. In this paper, we describe the challenges involved in fake news detection and also describe related tasks. We systematically review and compare the task formulations, datasets and NLP solutions that have been developed for this task, and also discuss the potentials and limitations of them. Based on our insights, we outline promising research directions, including more fine-grained, detailed, fair, and practical detection models. We also highlight the difference between fake news detection and other related tasks, and the importance of NLP solutions for fake news detection.","Fake news detection is a critical yet challenging problem in Natural Language Processing (NLP). The rapid rise of social networking platforms has not only yielded a vast increase in information accessibility but has also accelerated the spread of fake news. Thus, the effect of fake news has been growing, sometimes extending to the offline world and threatening public safety. Given the massive amount of Web content, automatic fake news detection is a practical NLP problem useful to all online content providers, in order to reduce the human time and effort to detect and prevent the spread of fake news. In this paper, we describe the challenges involved in fake news detection and also describe related tasks. We systematically review and compare the task formulations, datasets and NLP solutions that have been developed for this task, and also discuss the potentials and limitations of them. Based on our insights, we outline promising research directions, including more fine-grained, detailed, fair, and practical detection models. We also highlight the difference between fake news detection and other related tasks, and the importance of NLP solutions for fake news detection.","['Ray Oshikawa', 'Jing Qian', 'William Yang Wang']","Oshikawa, Ray; Qian, Jing; Wang, William Yang",https://export.arxiv.org/abs/1811.00770,https://aclanthology.org/2020.lrec-1.747,2021-08-03
17204.0,arxiv,acl,COVID-19 Knowledge Graph: Accelerating Information Retrieval and   Discovery for Scientific Literature,COVID-19 Knowledge Graph: Accelerating Information Retrieval and Discovery for Scientific Literature,"The coronavirus disease (COVID-19) has claimed the lives of over 350,000 people and infected more than 6 million people worldwide. Several search engines have surfaced to provide researchers with additional tools to find and retrieve information from the rapidly growing corpora on COVID-19. These engines lack extraction and visualization tools necessary to retrieve and interpret complex relations inherent to scientific literature. Moreover, because these engines mainly rely upon semantic information, their ability to capture complex global relationships across documents is limited, which reduces the quality of similarity-based article recommendations for users. In this work, we present the COVID-19 Knowledge Graph (CKG), a heterogeneous graph for extracting and visualizing complex relationships between COVID-19 scientific articles. The CKG combines semantic information with document topological information for the application of similar document retrieval. The CKG is constructed using the latent schema of the data, and then enriched with biomedical entity information extracted from the unstructured text of articles using scalable AWS technologies to form relations in the graph. Finally, we propose a document similarity engine that leverages low-dimensional graph embeddings from the CKG with semantic embeddings for similar article retrieval. Analysis demonstrates the quality of relationships in the CKG and shows that it can be used to uncover meaningful information in COVID-19 scientific articles. The CKG helps power www.cord19.aws and is publicly available.","The coronavirus disease (COVID-19) has claimed the lives of over one million people and infected more than thirty-five million people worldwide. Several search engines have surfaced to provide researchers with additional tools to find and retrieve information from the rapidly growing corpora on COVID19. These engines lack extraction and visualization tools necessary to retrieve and interpret complex relations inherent to scientific literature. Moreover, because these engines mainly rely upon semantic information, their ability to capture complex global relationships across documents is limited, which reduces the quality of similarity-based article recommendations for users. In this work, we present the COVID-19 Knowledge Graph (CKG), a heterogeneous graph for extracting and visualizing complex relationships between COVID-19 scientific articles. The CKG combines semantic information with document topological information for the application of similar document retrieval. The CKG is constructed using the latent schema of the data, and then enriched with biomedical entity information extracted from the unstructured text of articles using scalable AWS technologies to form relations in the graph. Finally, we propose a document similarity engine that leverages low-dimensional graph embeddings from the CKG with semantic embeddings for similar article retrieval. Analysis demonstrates the quality of relationships in the CKG and shows that it can be used to uncover meaningful information in COVID-19 scientific articles. The CKG helps power www.cord19.aws and is publicly available.","['Colby Wise', 'Vassilis N. Ioannidis', 'Miguel Romero Calvo', 'Xiang Song', 'George Price', 'Ninad Kulkarni', 'Ryan Brand', 'Parminder Bhatia', 'George Karypis']","Wise, Colby; Romero Calvo, Miguel; Bhatia, Pariminder; Ioannidis, Vassilis; Karypus, George; Price, George; Song, Xiang; Brand, Ryan; Kulkani, Ninad",https://export.arxiv.org/abs/2007.12731,https://aclanthology.org/2020.knlp-1.1,2021-08-03
17282.0,arxiv,acl,PolicyQA: A Reading Comprehension Dataset for Privacy Policies,PolicyQA: A Reading Comprehension Dataset for Privacy Policies,"Privacy policy documents are long and verbose. A question answering (QA) system can assist users in finding the information that is relevant and important to them. Prior studies in this domain frame the QA task as retrieving the most relevant text segment or a list of sentences from the policy document given a question. On the contrary, we argue that providing users with a short text span from policy documents reduces the burden of searching the target information from a lengthy text segment. In this paper, we present PolicyQA, a dataset that contains 25,017 reading comprehension style examples curated from an existing corpus of 115 website privacy policies. PolicyQA provides 714 human-annotated questions written for a wide range of privacy practices. We evaluate two existing neural QA models and perform rigorous analysis to reveal the advantages and challenges offered by PolicyQA.","Privacy policy documents are long and verbose. A question answering (QA) system can assist users in finding the information that is relevant and important to them. Prior studies in this domain frame the QA task as retrieving the most relevant text segment or a list of sentences from the policy document given a question. On the contrary, we argue that providing users with a short text span from policy documents reduces the burden of searching the target information from a lengthy text segment. In this paper, we present PolicyQA, a dataset that contains 25,017 reading comprehension style examples curated from an existing corpus of 115 website privacy policies. PolicyQA provides 714 human-annotated questions written for a wide range of privacy practices. We evaluate two existing neural QA models and perform rigorous analysis to reveal the advantages and challenges offered by PolicyQA.","['Wasi Uddin Ahmad', 'Jianfeng Chi', 'Yuan Tian', 'Kai-Wei Chang']","Ahmad, Wasi; Chi, Jianfeng; Tian, Yuan; Chang, Kai-Wei",https://export.arxiv.org/abs/2010.02557,https://aclanthology.org/2020.findings-emnlp.66,2021-08-03
17257.0,arxiv,acl,BERT-QE: Contextualized Query Expansion for Document Re-ranking,BERT-QE: Contextualized Query Expansion for Document Re-ranking,"Query expansion aims to mitigate the mismatch between the language used in a query and in a document. Query expansion methods can suffer from introducing non-relevant information when expanding the query, however. To bridge this gap, inspired by recent advances in applying contextualized models like BERT to the document retrieval task, this paper proposes a novel query expansion model that leverages the strength of the BERT model to better select relevant information for expansion. In evaluations on the standard TREC Robust04 and GOV2 test collections, the proposed BERT-QE model significantly outperforms BERT-Large models commonly used for document retrieval.","Query expansion aims to mitigate the mismatch between the language used in a query and in a document. However, query expansion methods can suffer from introducing non-relevant information when expanding the query. To bridge this gap, inspired by recent advances in applying contextualized models like BERT to the document retrieval task, this paper proposes a novel query expansion model that leverages the strength of the BERT model to select relevant document chunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test collections, the proposed BERT-QE model significantly outperforms BERT-Large models.","['Zhi Zheng', 'Kai Hui', 'Ben He', 'Xianpei Han', 'Le Sun', 'Andrew Yates']","Zheng, Zhi; Hui, Kai; He, Ben; Han, Xianpei; Sun, Le; Yates, Andrew",https://export.arxiv.org/abs/2009.07258,https://aclanthology.org/2020.findings-emnlp.424,2021-08-03
17166.0,arxiv,acl,Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually   Grounded Speech,Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech,"The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network's architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.","The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network's architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.","['William N. Havard', 'Jean-Pierre Chevrot', 'Laurent Besacier']","Havard, William; Besacier, Laurent; Chevrot, Jean-Pierre",https://export.arxiv.org/abs/2006.08387,https://aclanthology.org/2020.conll-1.22,2021-08-03
17152.0,arxiv,acl,Conversational Machine Comprehension: a Literature Review,Conversational Machine Comprehension: a Literature Review,"Conversational Machine Comprehension (CMC) is a research track in conversational AI which expects the machine to understand an open-domain text and thereafter engage in a multi-turn conversation to answer questions related to the text. While most of the research in Machine Reading Comprehension (MRC) revolves around single-turn question answering, multi-turn CMC has recently gained prominence, thanks to the advancement in natural language understanding via neural language models like BERT and the introduction of large-scale conversational datasets like CoQA and QuAC. The rise in interest has, however, led to a flurry of concurrent publications, each with a different yet structurally similar modeling approach and an inconsistent view of the surrounding literature. With the volume of model submissions to conversational datasets increasing every year, there exists a need to consolidate the scattered knowledge in this domain to streamline future research. This literature review, therefore, is a first-of-its-kind attempt at providing a holistic overview of CMC, with an emphasis on the common trends across recently published models, specifically in their approach to tackling conversational history. It focuses on synthesizing a generic framework for CMC models, rather than describing the models individually. The review is intended to serve as a compendium for future researchers in this domain.","Conversational Machine Comprehension (CMC), a research track in conversational AI, expects the machine to understand an open-domain natural language text and thereafter engage in a multi-turn conversation to answer questions related to the text. While most of the research in Machine Reading Comprehension (MRC) revolves around single-turn question answering (QA), multi-turn CMC has recently gained prominence, thanks to the advancement in natural language understanding via neural language models such as BERT and the introduction of large-scale conversational datasets such as CoQA and QuAC. The rise in interest has, however, led to a flurry of concurrent publications, each with a different yet structurally similar modeling approach and an inconsistent view of the surrounding literature. With the volume of model submissions to conversational datasets increasing every year, there exists a need to consolidate the scattered knowledge in this domain to streamline future research. This literature review attempts at providing a holistic overview of CMC with an emphasis on the common trends across recently published models, specifically in their approach to tackling conversational history. The review synthesizes a generic framework for CMC models while highlighting the differences in recent approaches and intends to serve as a compendium of CMC for future researchers.","['Somil Gupta', 'Bhanu Pratap Singh Rawat']","Gupta, Somil; Rawat, Bhanu Pratap Singh; Yu, Hong",https://export.arxiv.org/abs/2006.00671,https://aclanthology.org/2020.coling-main.247,2021-08-03
17121.0,arxiv,acl,Cross-lingual Information Retrieval with BERT,Cross-lingual Information Retrieval with BERT,"Multiple neural language models have been developed recently, e.g., BERT and XLNet, and achieved impressive results in various NLP tasks including sentence classification, question answering and document ranking. In this paper, we explore the use of the popular bidirectional language model, BERT, to model and learn the relevance between English queries and foreign-language documents in the task of cross-lingual information retrieval. A deep relevance matching model based on BERT is introduced and trained by finetuning a pretrained multilingual BERT model with weak supervision, using home-made CLIR training data derived from parallel corpora. Experimental results of the retrieval of Lithuanian documents against short English queries show that our model is effective and outperforms the competitive baseline approaches.","Multiple neural language models have been developed recently, e.g., BERT and XLNet, and achieved impressive results in various NLP tasks including sentence classification, question answering and document ranking. In this paper, we explore the use of the popular bidirectional language model, BERT, to model and learn the relevance between English queries and foreign-language documents in the task of cross-lingual information retrieval. A deep relevance matching model based on BERT is introduced and trained by finetuning a pretrained multilingual BERT model with weak supervision, using home-made CLIR training data derived from parallel corpora. Experimental results of the retrieval of Lithuanian documents against short English queries show that our model is effective and outperforms the competitive baseline approaches.","['Zhuolin Jiang', 'Amro El-Jaroudi', 'William Hartmann', 'Damianos Karakos', 'Lingjun Zhao']","Jiang, Zhuolin; El-Jaroudi, Amro; Hartmann, William; Karakos, Damianos; Zhao, Lingjun",https://export.arxiv.org/abs/2004.13005,https://aclanthology.org/2020.clssts-1.5,2021-08-03
17123.0,arxiv,acl,Conversational Word Embedding for Retrieval-Based Dialog System,Conversational Word Embedding for Retrieval-Based Dialog System,"Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs $ \left\langle{post, reply} \right\rangle$ to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply. To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level. We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems. The experiment results show that PR-Embedding can improve the quality of the selected response. PR-Embedding source code is available at https://github.com/wtma/PR-Embedding","Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs textlesspost, replytextgreater to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.The experiment results show that PR-Embedding can improve the quality of the selected response.","['Wentao Ma', 'Yiming Cui', 'Ting Liu', 'Dong Wang', 'Shijin Wang', 'Guoping Hu']","Ma, Wentao; Cui, Yiming; Liu, Ting; Wang, Dong; Wang, Shijin; Hu, Guoping",https://export.arxiv.org/abs/2004.13249,https://aclanthology.org/2020.acl-main.127,2021-08-03
9967.0,,acl,Massively Multilingual Document Alignment with Cross-lingual   Sentence-Mover's Distance,Massively Multilingual Document Alignment with Cross-lingual Sentence-Mover's Distance,"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. Such aligned data can be used for a variety of NLP tasks from training cross-lingual representations to mining parallel bitexts for machine translation training. In this paper we develop an unsupervised scoring function that leverages cross-lingual sentence embeddings to compute the semantic distance between documents in different languages. These semantic distances are then used to guide a document alignment algorithm to properly pair cross-lingual web documents across a variety of low, mid, and high-resource language pairs. Recognizing that our proposed scoring function and other state of the art methods are computationally intractable for long web documents, we utilize a more tractable greedy algorithm that performs comparably. We experimentally demonstrate that our distance metric performs better alignment than current baselines outperforming them by 7% on high-resource language pairs, 15% on mid-resource language pairs, and 22% on low-resource language pairs","Document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. Such aligned data can be used for a variety of NLP tasks from training cross-lingual representations to mining parallel data for machine translation. In this paper we develop an unsupervised scoring function that leverages cross-lingual sentence embeddings to compute the semantic distance between documents in different languages. These semantic distances are then used to guide a document alignment algorithm to properly pair cross-lingual web documents across a variety of low, mid, and high-resource language pairs. Recognizing that our proposed scoring function and other state of the art methods are computationally intractable for long web documents, we utilize a more tractable greedy algorithm that performs comparably. We experimentally demonstrate that our distance metric performs better alignment than current baselines outperforming them by 7% on high-resource language pairs, 15% on mid-resource language pairs, and 22% on low-resource language pairs.","['Ahmed El-Kishky', 'Francisco Guzmán']","El-Kishky, Ahmed; Guzm'an, Francisco",https://export.arxiv.org/abs/2002.00761,https://aclanthology.org/2020.aacl-main.62,2021-08-03
9814.0,,acl,Transfer Learning in Biomedical Natural Language Processing: An   Evaluation of BERT and ELMo on Ten Benchmarking Datasets,Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets,"Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ncbi-nlp/BLUE_Benchmark.","Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE_Benchmark.","['Yifan Peng', 'Shankai Yan', 'Zhiyong Lu']","Peng, Yifan; Yan, Shankai; Lu, Zhiyong",https://export.arxiv.org/abs/1906.05474,https://aclanthology.org/W19-5006,2021-08-03
9810.0,,acl,Deep Contextualized Biomedical Abbreviation Expansion,Deep Contextualized Biomedical Abbreviation Expansion,"Automatic identification and expansion of ambiguous abbreviations are essential for biomedical natural language processing applications, such as information retrieval and question answering systems. In this paper, we present DEep Contextualized Biomedical. Abbreviation Expansion (DECBAE) model. DECBAE automatically collects substantial and relatively clean annotated contexts for 950 ambiguous abbreviations from PubMed abstracts using a simple heuristic. Then it utilizes BioELMo to extract the contextualized features of words, and feed those features to abbreviation-specific bidirectional LSTMs, where the hidden states of the ambiguous abbreviations are used to assign the exact definitions. Our DECBAE model outperforms other baselines by large margins, achieving average accuracy of 0.961 and macro-F1 of 0.917 on the dataset. It also surpasses human performance for expanding a sample abbreviation, and remains robust in imbalanced, low-resources and clinical settings.","Automatic identification and expansion of ambiguous abbreviations are essential for biomedical natural language processing applications, such as information retrieval and question answering systems. In this paper, we present DEep Contextualized Biomedical Abbreviation Expansion (DECBAE) model. DECBAE automatically collects substantial and relatively clean annotated contexts for 950 ambiguous abbreviations from PubMed abstracts using a simple heuristic. Then it utilizes BioELMo to extract the contextualized features of words, and feed those features to abbreviation-specific bidirectional LSTMs, where the hidden states of the ambiguous abbreviations are used to assign the exact definitions. Our DECBAE model outperforms other baselines by large margins, achieving average accuracy of 0.961 and macro-F1 of 0.917 on the dataset. It also surpasses human performance for expanding a sample abbreviation, and remains robust in imbalanced, low-resources and clinical settings.","['Qiao Jin', 'Jinling Liu', 'Xinghua Lu']","Jin, Qiao; Liu, Jinling; Lu, Xinghua",https://export.arxiv.org/abs/1906.03360,https://aclanthology.org/W19-5010,2021-08-03
9813.0,,acl,Neural Arabic Question Answering,Neural Arabic Question Answering,"This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.","This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.","['Hussein Mozannar', 'Karl El Hajal', 'Elie Maamary', 'Hazem Hajj']","Mozannar, Hussein; Maamary, Elie; El Hajal, Karl; Hajj, Hazem",https://export.arxiv.org/abs/1906.05394,https://aclanthology.org/W19-4612,2021-08-03
9789.0,,acl,On the Feasibility of Automated Detection of Allusive Text Reuse,On the Feasibility of Automated Detection of Allusive Text Reuse,"The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely---commonly based on none or very few shared words. Arguably, lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and ontologies can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves retrieval performance with respect to a windowing approach, and that (ii) retrieval performance can be moderately boosted with distributional semantics.","The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely --- commonly based on none or very few shared words. Arguably, lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and ontologies can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves retrieval performance with respect to a windowing approach, and that (ii) retrieval performance can be moderately boosted with distributional semantics.","['Enrique Manjavacas', 'Brian Long', 'Mike Kestemont']","Manjavacas, Enrique; Long, Brian; Kestemont, Mike",https://export.arxiv.org/abs/1905.02973,https://aclanthology.org/W19-2514,2021-08-03
9770.0,,acl,Probing Biomedical Embeddings from Language Models,Probing Biomedical Embeddings from Language Models,"Contextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks. Pre-training on domain-specific corpora, such as biomedical articles, further improves their performance. In this paper, we conduct probing experiments to determine what additional information is carried intrinsically by the in-domain trained contextualized embeddings. For this we use the pre-trained LMs as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers. We compare BERT, ELMo, BioBERT and BioELMo, a biomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We use visualization and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority.","Contextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks. Pre-training on domain-specific corpora, such as biomedical articles, further improves their performance. In this paper, we conduct probing experiments to determine what additional information is carried intrinsically by the in-domain trained contextualized embeddings. For this we use the pre-trained LMs as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers. We compare BERT (Devlin et al. 2018), ELMo (Peters et al., 2018), BioBERT (Lee et al., 2019) and BioELMo, a biomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We use visualization and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority.","['Qiao Jin', 'Bhuwan Dhingra', 'William W. Cohen', 'Xinghua Lu']","Jin, Qiao; Dhingra, Bhuwan; Cohen, William; Lu, Xinghua",https://export.arxiv.org/abs/1904.02181,https://aclanthology.org/W19-2011,2021-08-03
9792.0,,acl,Towards Automatic Generation of Shareable Synthetic Clinical Notes Using   Neural Language Models,Towards Automatic Generation of Shareable Synthetic Clinical Notes Using Neural Language Models,"Large-scale clinical data is invaluable to driving many computational scientific advances today. However, understandable concerns regarding patient privacy hinder the open dissemination of such data and give rise to suboptimal siloed research. De-identification methods attempt to address these concerns but were shown to be susceptible to adversarial attacks. In this work, we focus on the vast amounts of unstructured natural language data stored in clinical notes and propose to automatically generate synthetic clinical notes that are more amenable to sharing using generative models trained on real de-identified records. To evaluate the merit of such notes, we measure both their privacy preservation properties as well as utility in training clinical NLP models. Experiments using neural language models yield notes whose utility is close to that of the real ones in some clinical NLP tasks, yet leave ample room for future improvements.","Large-scale clinical data is invaluable to driving many computational scientific advances today. However, understandable concerns regarding patient privacy hinder the open dissemination of such data and give rise to suboptimal siloed research. De-identification methods attempt to address these concerns but were shown to be susceptible to adversarial attacks. In this work, we focus on the vast amounts of unstructured natural language data stored in clinical notes and propose to automatically generate synthetic clinical notes that are more amenable to sharing using generative models trained on real de-identified records. To evaluate the merit of such notes, we measure both their privacy preservation properties as well as utility in training clinical NLP models. Experiments using neural language models yield notes whose utility is close to that of the real ones in some clinical NLP tasks, yet leave ample room for future improvements.","['Oren Melamud', 'Chaitanya Shivade']","Melamud, Oren; Shivade, Chaitanya",https://export.arxiv.org/abs/1905.07002,https://aclanthology.org/W19-1905,2021-08-03
9765.0,,acl,Re-Ranking Words to Improve Interpretability of Automatically Generated   Topics,Re-Ranking Words to Improve Interpretability of Automatically Generated Topics,"Topics models, such as LDA, are widely used in Natural Language Processing. Making their output interpretable is an important area of research with applications to areas such as the enhancement of exploratory search interfaces and the development of interpretable machine learning models. Conventionally, topics are represented by their n most probable words, however, these representations are often difficult for humans to interpret. This paper explores the re-ranking of topic words to generate more interpretable topic representations. A range of approaches are compared and evaluated in two experiments. The first uses crowdworkers to associate topics represented by different word rankings with related documents. The second experiment is an automatic approach based on a document retrieval task applied on multiple domains. Results in both experiments demonstrate that re-ranking words improves topic interpretability and that the most effective re-ranking schemes were those which combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements.","Topics models, such as LDA, are widely used in Natural Language Processing. Making their output interpretable is an important area of research with applications to areas such as the enhancement of exploratory search interfaces and the development of interpretable machine learning models. Conventionally, topics are represented by their n most probable words, however, these representations are often difficult for humans to interpret. This paper explores the re-ranking of topic words to generate more interpretable topic representations. A range of approaches are compared and evaluated in two experiments. The first uses crowdworkers to associate topics represented by different word rankings with related documents. The second experiment is an automatic approach based on a document retrieval task applied on multiple domains. Results in both experiments demonstrate that re-ranking words improves topic interpretability and that the most effective re-ranking schemes were those which combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements.","['Areej Alokaili', 'Nikolaos Aletras', 'Mark Stevenson']","Alokaili, Areej; Aletras, Nikolaos; Stevenson, Mark",https://export.arxiv.org/abs/1903.12542,https://aclanthology.org/W19-0404,2021-08-03
9844.0,,acl,Beyond English-Only Reading Comprehension: Experiments in Zero-Shot   Multilingual Transfer for Bulgarian,Beyond English-Only Reading Comprehension: Experiments in Zero-shot Multilingual Transfer for Bulgarian,"Recently, reading comprehension models achieved near-human performance on large-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely due to the release of pre-trained contextualized representations such as BERT and ELMo, which can be fine-tuned for the target task. Despite those advances and the creation of more challenging datasets, most of the work is still done for English. Here, we study the effectiveness of multilingual BERT fine-tuned on large-scale English datasets for reading comprehension (e.g., for RACE), and we apply it to Bulgarian multiple-choice reading comprehension. We propose a new dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects -history, biology, geography and philosophy-, and 412 additional questions from online quizzes in history. While the quiz authors gave no relevant context, we incorporate knowledge from Wikipedia, retrieving documents matching the combination of question + each answer option. Moreover, we experiment with different indexing and pre-training strategies. The evaluation results show accuracy of 42.23%, which is well above the baseline of 24.89%.","Recently, reading comprehension models achieved near-human performance on large-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely due to the release of pre-trained contextualized representations such as BERT and ELMo, which can be fine-tuned for the target task. Despite those advances and the creation of more challenging datasets, most of the work is still done for English. Here, we study the effectiveness of multilingual BERT fine-tuned on large-scale English datasets for reading comprehension (e.g., for RACE), and we apply it to Bulgarian multiple-choice reading comprehension. We propose a new dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects ---history, biology, geography and philosophy---, and 412 additional questions from online quizzes in history. While the quiz authors gave no relevant context, we incorporate knowledge from Wikipedia, retrieving documents matching the combination of question + each answer option. Moreover, we experiment with different indexing and pre-training strategies. The evaluation results show accuracy of 42.23%, which is well above the baseline of 24.89%.","['Momchil Hardalov', 'Ivan Koychev', 'Preslav Nakov']","Hardalov, Momchil; Koychev, Ivan; Nakov, Preslav",https://export.arxiv.org/abs/1908.01519,https://aclanthology.org/R19-1053,2021-08-03
9820.0,,acl,Mitigating Gender Bias in Natural Language Processing: Literature Review,Mitigating Gender Bias in Natural Language Processing: Literature Review,"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Mitigating Gender Bias in Natural Language Processing: Literature Review.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs http://arxiv.org/abs/1906.08976; Years 2019; Authors Tony Sun; Andrew Gaut; Shirlyn Tang; Yuxin Huang; Mai ElSherief; Jieyu Zhao; Diba Mirza; Elizabeth M. Belding; Kai-Wei Chang; William Yang Wang; Deduplication_Notes ; X Mitigating Gender Bias in Natural Language Processing: Literature Review.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0] CHECK DUPLICATE [Source dblp; Title Mitigating Gender Bias in Natural Language Processing: Literature Review.; Abstract NaN; Keywords NaN; DOIs 10.18653/v1/p19-1159; DOI_original 10.18653/v1/p19-1159; URLs https://doi.org/10.18653/v1/p19-1159 ; https://www.aclweb.org/anthology/P19-1159/; Years 2019; Authors Tony Sun; Andrew Gaut; Shirlyn Tang; Yuxin Huang; Mai ElSherief; Jieyu Zhao; Diba Mirza; Elizabeth M. Belding; Kai-Wei Chang; William Yang Wang; Deduplication_Notes ; X Mitigating Gender Bias in Natural Language Processing: Literature Review.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]","As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.","['Tony Sun', 'Andrew Gaut', 'Shirlyn Tang', 'Yuxin Huang', 'Mai ElSherief', 'Jieyu Zhao', 'Diba Mirza', 'Elizabeth Belding', 'Kai-Wei Chang', 'William Yang Wang']","Sun, Tony; Gaut, Andrew; Tang, Shirlyn; Huang, Yuxin; ElSherief, Mai; Zhao, Jieyu; Mirza, Diba; Belding, Elizabeth; Chang, Kai-Wei; Wang, William Yang",https://export.arxiv.org/abs/1906.08976,https://aclanthology.org/P19-1159,2021-08-03
9811.0,,acl,Improving Low-Resource Cross-lingual Document Retrieval by Reranking   with Deep Bilingual Representations,Improving Low-Resource Cross-lingual Document Retrieval by Reranking with Deep Bilingual Representations,"In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra features, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space, the model can also be directly applied to another language pair without any training label. Experimental results on the MATERIAL dataset show that our model outperforms the competitive translation-based baselines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks.","In this paper, we propose to boost low-resource cross-lingual document retrieval performance with deep bilingual query-document representations. We match queries and documents in both source and target languages with four components, each of which is implemented as a term interaction-based deep neural network with cross-lingual word embeddings as input. By including query likelihood scores as extra features, our model effectively learns to rerank the retrieved documents by using a small number of relevance labels for low-resource language pairs. Due to the shared cross-lingual word embedding space, the model can also be directly applied to another language pair without any training label. Experimental results on the Material dataset show that our model outperforms the competitive translation-based baselines on English-Swahili, English-Tagalog, and English-Somali cross-lingual information retrieval tasks.","['Rui Zhang', 'Caitlin Westerfield', 'Sungrok Shim', 'Garrett Bingham', 'Alexander Fabbri', 'Neha Verma', 'William Hu', 'Dragomir Radev']","Zhang, Rui; Westerfield, Caitlin; Shim, Sungrok; Bingham, Garrett; Fabbri, Alexander; Hu, William; Verma, Neha; Radev, Dragomir",https://export.arxiv.org/abs/1906.03492,https://aclanthology.org/P19-1306,2021-08-03
9812.0,,acl,Adversarial Learning of Privacy-Preserving Text Representations for   De-Identification of Medical Records,Adversarial Learning of Privacy-Preserving Text Representations for De-Identification of Medical Records,"De-identification is the task of detecting protected health information (PHI) in medical text. It is a critical step in sanitizing electronic health records (EHRs) to be shared for research. Automatic de-identification classifierscan significantly speed up the sanitization process. However, obtaining a large and diverse dataset to train such a classifier that works wellacross many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records. We introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no PHI) that does not require expensive manual pseudonymization. These representations can be shared between organizations to create unified datasets for training de-identification models. Our representation allows training a simple LSTM-CRF de-identification model to an F1 score of 97.4%, which is comparable to a strong baseline that exposes private information in its representation. A robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly.","De-identification is the task of detecting protected health information (PHI) in medical text. It is a critical step in sanitizing electronic health records (EHR) to be shared for research. Automatic de-identification classifiers can significantly speed up the sanitization process. However, obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records. We introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no PHI) that does not require expensive manual pseudonymization. These representations can be shared between organizations to create unified datasets for training de-identification models. Our representation allows training a simple LSTM-CRF de-identification model to an F1 score of 97.4%, which is comparable to a strong baseline that exposes private information in its representation. A robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly.","['Max Friedrich', 'Arne Köhn', 'Gregor Wiedemann', 'Chris Biemann']","Friedrich, Max; K""ohn, Arne; Wiedemann, Gregor; Biemann, Chris",https://export.arxiv.org/abs/1906.05000,https://aclanthology.org/P19-1584,2021-08-03
9897.0,,acl,A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking,A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking,"Automated fact-checking based on machine learning is a promising approach to identify false information distributed on the web. In order to achieve satisfactory performance, machine learning methods require a large corpus with reliable annotations for the different tasks in the fact-checking process. Having analyzed existing fact-checking corpora, we found that none of them meets these criteria in full. They are either too small in size, do not provide detailed annotations, or are limited to a single domain. Motivated by this gap, we present a new substantially sized mixed-domain corpus with annotations of good quality for the core fact-checking tasks: document retrieval, evidence extraction, stance detection, and claim validation. To aid future corpus construction, we describe our methodology for corpus creation and annotation, and demonstrate that it results in substantial inter-annotator agreement. As baselines for future research, we perform experiments on our corpus with a number of model architectures that reach high performance in similar problem settings. Finally, to support the development of future models, we provide a detailed error analysis for each of the tasks. Our results show that the realistic, multi-domain setting defined by our data poses new challenges for the existing models, providing opportunities for considerable improvement by future systems.","Automated fact-checking based on machine learning is a promising approach to identify false information distributed on the web. In order to achieve satisfactory performance, machine learning methods require a large corpus with reliable annotations for the different tasks in the fact-checking process. Having analyzed existing fact-checking corpora, we found that none of them meets these criteria in full. They are either too small in size, do not provide detailed annotations, or are limited to a single domain. Motivated by this gap, we present a new substantially sized mixed-domain corpus with annotations of good quality for the core fact-checking tasks: document retrieval, evidence extraction, stance detection, and claim validation. To aid future corpus construction, we describe our methodology for corpus creation and annotation, and demonstrate that it results in substantial inter-annotator agreement. As baselines for future research, we perform experiments on our corpus with a number of model architectures that reach high performance in similar problem settings. Finally, to support the development of future models, we provide a detailed error analysis for each of the tasks. Our results show that the realistic, multi-domain setting defined by our data poses new challenges for the existing models, providing opportunities for considerable improvement by future systems.","['Andreas Hanselowski', 'Christian Stab', 'Claudia Schulz', 'Zile Li', 'Iryna Gurevych']","Hanselowski, Andreas; Stab, Christian; Schulz, Claudia; Li, Zile; Gurevych, Iryna",https://export.arxiv.org/abs/1911.01214,https://aclanthology.org/K19-1046,2021-08-03
9874.0,,acl,BioNLP-OST 2019 RDoC Tasks: Multi-grain Neural Relevance Ranking Using   Topics and Attention Based Query-Document-Sentence Interactions,BioNLP-OST 2019 RDoC Tasks: Multi-grain Neural Relevance Ranking Using Topics and Attention Based Query-Document-Sentence Interactions,"This paper presents our system details and results of participation in the RDoC Tasks of BioNLP-OST 2019. Research Domain Criteria (RDoC) construct is a multi-dimensional and broad framework to describe mental health disorders by combining knowledge from genomics to behaviour. Non-availability of RDoC labelled dataset and tedious labelling process hinders the use of RDoC framework to reach its full potential in Biomedical research community and Healthcare industry. Therefore, Task-1 aims at retrieval and ranking of PubMed abstracts relevant to a given RDoC construct and Task-2 aims at extraction of the most relevant sentence from a given PubMed abstract. We investigate (1) attention based supervised neural topic model and SVM for retrieval and ranking of PubMed abstracts and, further utilize BM25 and other relevance measures for re-ranking, (2) supervised and unsupervised sentence ranking models utilizing multi-view representations comprising of query-aware attention-based sentence representation (QAR), bag-of-words (BoW) and TF-IDF. Our best systems achieved 1st rank and scored 0.86 mean average precision (mAP) and 0.58 macro average accuracy (MAA) in Task-1 and Task-2 respectively.","This paper presents our system details and results of participation in the RDoC Tasks of BioNLP-OST 2019. Research Domain Criteria (RDoC) construct is a multi-dimensional and broad framework to describe mental health disorders by combining knowledge from genomics to behaviour. Non-availability of RDoC labelled dataset and tedious labelling process hinders the use of RDoC framework to reach its full potential in Biomedical research community and Healthcare industry. Therefore, Task-1 aims at retrieval and ranking of PubMed abstracts relevant to a given RDoC construct and Task-2 aims at extraction of the most relevant sentence from a given PubMed abstract. We investigate (1) attention based supervised neural topic model and SVM for retrieval and ranking of PubMed abstracts and, further utilize BM25 and other relevance measures for re-ranking, (2) supervised and unsupervised sentence ranking models utilizing multi-view representations comprising of query-aware attention-based sentence representation (QAR), bag-of-words (BoW) and TF-IDF. Our best systems achieved 1st rank and scored 0.86 mAP and 0.58 macro average accuracy in Task-1 and Task-2 respectively.","['Yatin Chaudhary', 'Pankaj Gupta', 'Hinrich Schütze']","Gupta, Pankaj; Chaudhary, Yatin; Sch""utze, Hinrich",https://export.arxiv.org/abs/1910.00314,https://aclanthology.org/D19-5730,2021-08-03
9889.0,,acl,Generating a Common Question from Multiple Documents using Multi-source   Encoder-Decoder Models,Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder Models,"Ambiguous user queries in search engines result in the retrieval of documents that often span multiple topics. One potential solution is for the search engine to generate multiple refined queries, each of which relates to a subset of the documents spanning the same topic. A preliminary step towards this goal is to generate a question that captures common concepts of multiple documents. We propose a new task of generating common question from multiple documents and present simple variant of an existing multi-source encoder-decoder framework, called the Multi-Source Question Generator (MSQG). We first train an RNN-based single encoder-decoder generator from (single document, question) pairs. At test time, given multiple documents, the 'Distribute' step of our MSQG model predicts target word distributions for each document using the trained model. The 'Aggregate' step aggregates these distributions to generate a common question. This simple yet effective strategy significantly outperforms several existing baseline models applied to the new task when evaluated using automated metrics and human judgments on the MS-MARCO-QA dataset.","Ambiguous user queries in search engines result in the retrieval of documents that often span multiple topics. One potential solution is for the search engine to generate multiple refined queries, each of which relates to a subset of the documents spanning the same topic. A preliminary step towards this goal is to generate a question that captures common concepts of multiple documents. We propose a new task of generating common question from multiple documents and present simple variant of an existing multi-source encoder-decoder framework, called the Multi-Source Question Generator (MSQG). We first train an RNN-based single encoder-decoder generator from (single document, question) pairs. At test time, given multiple documents, the Distribute step of our MSQG model predicts target word distributions for each document using the trained model. The Aggregate step aggregates these distributions to generate a common question. This simple yet effective strategy significantly outperforms several existing baseline models applied to the new task when evaluated using automated metrics and human judgments on the MS-MARCO-QA dataset.","['Woon Sang Cho', 'Yizhe Zhang', 'Sudha Rao', 'Chris Brockett', 'Sungjin Lee']","Cho, Woon Sang; Zhang, Yizhe; Rao, Sudha; Brockett, Chris; Lee, Sungjin",https://export.arxiv.org/abs/1910.11483,https://aclanthology.org/D19-5604,2021-08-03
9862.0,,acl,PubMedQA: A Dataset for Biomedical Research Question Answering,PubMedQA: A Dataset for Biomedical Research Question Answering,"We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.","We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.","['Qiao Jin', 'Bhuwan Dhingra', 'Zhengping Liu', 'William W. Cohen', 'Xinghua Lu']","Jin, Qiao; Dhingra, Bhuwan; Liu, Zhengping; Cohen, William; Lu, Xinghua",https://export.arxiv.org/abs/1909.06146,https://aclanthology.org/D19-1259,2021-08-03
9884.0,,acl,Answering Complex Open-domain Questions Through Iterative Query   Generation,Answering Complex Open-domain Questions Through Iterative Query Generation,"It is challenging for current one-step retrieve-and-read question answering (QA) systems to answer questions like 'Which novel by the author of 'Armada' will be adapted as a feature film by Steven Spielberg?' because the question seldom contains retrievable clues about the missing entity (here, the author). Answering such a question requires multi-hop reasoning where one must gather information about the missing entity (or facts) to proceed with further reasoning. We present GoldEn (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions. Instead of using opaque and computationally expensive neural retrieval models, GoldEn Retriever generates natural language search queries given the question and available context, and leverages off-the-shelf information retrieval systems to query for missing entities. This allows GoldEn Retriever to scale up efficiently for open-domain multi-hop reasoning while maintaining interpretability. We evaluate GoldEn Retriever on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and demonstrate that it outperforms the best previously published model despite not using pretrained language models such as BERT.","It is challenging for current one-step retrieve-and-read question answering (QA) systems to answer questions like ``Which novel by the author of `Armada' will be adapted as a feature film by Steven Spielberg?'' because the question seldom contains retrievable clues about the missing entity (here, the author). Answering such a question requires multi-hop reasoning where one must gather information about the missing entity (or facts) to proceed with further reasoning. We present GoldEn (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions. Instead of using opaque and computationally expensive neural retrieval models, GoldEn Retriever generates natural language search queries given the question and available context, and leverages off-the-shelf information retrieval systems to query for missing entities. This allows GoldEn Retriever to scale up efficiently for open-domain multi-hop reasoning while maintaining interpretability. We evaluate GoldEn Retriever on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and demonstrate that it outperforms the best previously published model despite not using pretrained language models such as BERT.","['Peng Qi', 'Xiaowen Lin', 'Leo Mehr', 'Zijian Wang', 'Christopher D. Manning']","Qi, Peng; Lin, Xiaowen; Mehr, Leo; Wang, Zijian; Manning, Christopher D.",https://export.arxiv.org/abs/1910.07000,https://aclanthology.org/D19-1261,2021-08-03
9678.0,,acl,UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification,UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification,"The Fact Extraction and VERification (FEVER) shared task was launched to support the development of systems able to verify claims by extracting supporting or refuting facts from raw text. The shared task organizers provide a large-scale dataset for the consecutive steps involved in claim verification, in particular, document retrieval, fact extraction, and claim classification. In this paper, we present our claim verification pipeline approach, which, according to the preliminary results, scored third in the shared task, out of 23 competing systems. For the document retrieval, we implemented a new entity linking approach. In order to be able to rank candidate facts and classify a claim on the basis of several selected facts, we introduce two extensions to the Enhanced LSTM (ESIM).","The Fact Extraction and VERification (FEVER) shared task was launched to support the development of systems able to verify claims by extracting supporting or refuting facts from raw text. The shared task organizers provide a large-scale dataset for the consecutive steps involved in claim verification, in particular, document retrieval, fact extraction, and claim classification. In this paper, we present our claim verification pipeline approach, which, according to the preliminary results, scored third in the shared task, out of 23 competing systems. For the document retrieval, we implemented a new entity linking approach. In order to be able to rank candidate facts and classify a claim on the basis of several selected facts, we introduce two extensions to the Enhanced LSTM (ESIM).","['Andreas Hanselowski', 'Hao Zhang', 'Zile Li', 'Daniil Sorokin', 'Benjamin Schiller', 'Claudia Schulz', 'Iryna Gurevych']","Hanselowski, Andreas; Zhang, Hao; Li, Zile; Sorokin, Daniil; Schiller, Benjamin; Schulz, Claudia; Gurevych, Iryna",https://export.arxiv.org/abs/1809.01479,https://aclanthology.org/W18-5516,2021-08-03
9676.0,,acl,"DeFactoNLP: Fact Verification using Entity Recognition, TFIDF Vector   Comparison and Decomposable Attention","DeFactoNLP: Fact Verification using Entity Recognition, TFIDF Vector Comparison and Decomposable Attention","In this paper, we describe DeFactoNLP, the system we designed for the FEVER 2018 Shared Task. The aim of this task was to conceive a system that can not only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia. In our approach, the Wikipedia documents whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are most similar to the vector of the claim and those documents whose names are similar to those of the named entities (NEs) mentioned in the claim are identified as the documents which might contain evidence. The sentences in these documents are then supplied to a textual entailment recognition module. This module calculates the probability of each sentence supporting the claim, contradicting the claim or not providing any relevant information to assess the veracity of the claim. Various features computed using these probabilities are finally used by a Random Forest classifier to determine the overall truthfulness of the claim. The sentences which support this classification are returned as evidence. Our approach achieved a 0.4277 evidence F1-score, a 0.5136 label accuracy and a 0.3833 FEVER score.","In this paper, we describe DeFactoNLP, the system we designed for the FEVER 2018 Shared Task. The aim of this task was to conceive a system that can not only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia. In our approach, the Wikipedia documents whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are most similar to the vector of the claim and those documents whose names are similar to those of the named entities (NEs) mentioned in the claim are identified as the documents which might contain evidence. The sentences in these documents are then supplied to a textual entailment recognition module. This module calculates the probability of each sentence supporting the claim, contradicting the claim or not providing any relevant information to assess the veracity of the claim. Various features computed using these probabilities are finally used by a Random Forest classifier to determine the overall truthfulness of the claim. The sentences which support this classification are returned as evidence. Our approach achieved a 0.4277 evidence F1-score, a 0.5136 label accuracy and a 0.3833 FEVER score.","['Aniketh Janardhan Reddy', 'Gil Rocha', 'Diego Esteves']","Reddy, Aniketh Janardhan; Rocha, Gil; Esteves, Diego",https://export.arxiv.org/abs/1809.00509,https://aclanthology.org/W18-5522,2021-08-03
9681.0,,acl,AUEB at BioASQ 6: Document and Snippet Retrieval,AUEB at BioASQ 6: Document and Snippet Retrieval,"We present AUEB's submissions to the BioASQ 6 document and snippet retrieval tasks (parts of Task 6b, Phase A). Our models use novel extensions to deep learning architectures that operate solely over the text of the query and candidate document/snippets. Our systems scored at the top or near the top for all batches of the challenge, highlighting the effectiveness of deep learning for these tasks.","We present AUEB's submissions to the BioASQ 6 document and snippet retrieval tasks (parts of Task 6b, Phase A). Our models use novel extensions to deep learning architectures that operate solely over the text of the query and candidate document/snippets. Our systems scored at the top or near the top for all batches of the challenge, highlighting the effectiveness of deep learning for these tasks.","['Georgios-Ioannis Brokos', 'Polyvios Liosis', 'Ryan McDonald', 'Dimitris Pappas', 'Ion Androutsopoulos']","Brokos, George; Liosis, Polyvios; McDonald, Ryan; Pappas, Dimitris; Androutsopoulos, Ion",https://export.arxiv.org/abs/1809.06366,https://aclanthology.org/W18-5304,2021-08-03
9648.0,,acl,The Historical Significance of Textual Distances,The Historical Significance of Textual Distances,"Measuring similarity is a basic task in information retrieval, and now often a building-block for more complex arguments about cultural change. But do measures of textual similarity and distance really correspond to evidence about cultural proximity and differentiation? To explore that question empirically, this paper compares textual and social measures of the similarities between genres of English-language fiction. Existing measures of textual similarity (cosine similarity on tf-idf vectors or topic vectors) are also compared to new strategies that use supervised learning to anchor textual measurement in a social context.","Measuring similarity is a basic task in information retrieval, and now often a building-block for more complex arguments about cultural change. But do measures of textual similarity and distance really correspond to evidence about cultural proximity and differentiation? To explore that question empirically, this paper compares textual and social measures of the similarities between genres of English-language fiction. Existing measures of textual similarity (cosine similarity on tf-idf vectors or topic vectors) are also compared to new strategies that strive to anchor textual measurement in a social context.",['Ted Underwood'],"Underwood, Ted",https://export.arxiv.org/abs/1807.00181,https://aclanthology.org/W18-4507,2021-08-03
9653.0,,acl,Replicated Siamese LSTM in Ticketing System for Similarity Learning and   Retrieval in Asymmetric Texts,Replicated Siamese LSTM in Ticketing System for Similarity Learning and Retrieval in Asymmetric Texts,"The goal of our industrial ticketing system is to retrieve a relevant solution for an input query, by matching with historical tickets stored in knowledge base. A query is comprised of subject and description, while a historical ticket consists of subject, description and solution. To retrieve a relevant solution, we use textual similarity paradigm to learn similarity in the query and historical tickets. The task is challenging due to significant term mismatch in the query and ticket pairs of asymmetric lengths, where subject is a short text but description and solution are multi-sentence texts. We present a novel Replicated Siamese LSTM model to learn similarity in asymmetric text pairs, that gives 22% and 7% gain (Accuracy@10) for retrieval task, respectively over unsupervised and supervised baselines. We also show that the topic and distributed semantic features for short and long texts improved both similarity learning and retrieval.","The goal of our industrial ticketing system is to retrieve a relevant solution for an input query, by matching with historical tickets stored in knowledge base. A query is comprised of subject and description, while a historical ticket consists of subject, description and solution. To retrieve a relevant solution, we use textual similarity paradigm to learn similarity in the query and historical tickets. The task is challenging due to significant term mismatch in the query and ticket pairs of asymmetric lengths, where subject is a short text but description and solution are multi-sentence texts. We present a novel Replicated Siamese LSTM model to learn similarity in asymmetric text pairs, that gives 22% and 7% gain (Accuracy@10) for retrieval task, respectively over unsupervised and supervised baselines. We also show that the topic and distributed semantic features for short and long texts improved both similarity learning and retrieval.","['Pankaj Gupta', 'Bernt Andrassy', 'Hinrich Schütze']","Gupta, Pankaj; Andrassy, Bernt; Sch""utze, Hinrich",https://export.arxiv.org/abs/1807.02854,https://aclanthology.org/W18-4001,2021-08-03
9642.0,,acl,Comparative Analysis of Neural QA models on SQuAD,Comparative Analysis of Neural QA models on SQuAD,"The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language. Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks. Different components in these neural architectures are intended to tackle different challenges. As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them. We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper.","The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language. Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks. Different components in these neural architectures are intended to tackle different challenges. As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them. We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper.","['Soumya Wadhwa', 'Khyathi Raghavi Chandu', 'Eric Nyberg']","Wadhwa, Soumya; Chandu, Khyathi; Nyberg, Eric",https://export.arxiv.org/abs/1806.06972,https://aclanthology.org/W18-2610,2021-08-03
9656.0,,acl,Hierarchical Losses and New Resources for Fine-grained Entity Typing and   Linking,Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking,"Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and TypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.","Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: \textitMedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and \textitTypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.","['Shikhar Murty*', 'Patrick Verga*', 'Luke Vilnis', 'Irena Radovanovic', 'Andrew McCallum']","Murty, Shikhar; Verga, Patrick; Vilnis, Luke; Radovanovic, Irena; McCallum, Andrew",https://export.arxiv.org/abs/1807.05127,https://aclanthology.org/P18-1010,2021-08-03
2748.0,,acl,"A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature","A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature","We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the 'PICO' elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.","We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the `PICO' elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.","Nye, B.
 and Jessy Li, J.
 and Patel, R.
 and Yang, Y.
 and Marshall, I. J.
 and Nenkova, A.
 and Wallace, B. C.","Nye, Benjamin; Li, Junyi Jessy; Patel, Roma; Yang, Yinfei; Marshall, Iain; Nenkova, Ani; Wallace, Byron",not available,https://aclanthology.org/P18-1019,2021-08-03
9744.0,,acl,Dating Documents using Graph Convolution Networks,Dating Documents using Graph Convolution Networks,"Document date is essential for many important tasks, such as document retrieval, summarization, event detection, etc. While existing approaches for these tasks assume accurate knowledge of the document date, this is not always available, especially for arbitrary documents from the Web. Document Dating is a challenging problem which requires inference over the temporal structure of the document. Prior document dating systems have largely relied on handcrafted features while ignoring such document internal structures. In this paper, we propose NeuralDater, a Graph Convolutional Network (GCN) based document dating approach which jointly exploits syntactic and temporal graph structures of document in a principled way. To the best of our knowledge, this is the first application of deep learning for the problem of document dating. Through extensive experiments on real-world datasets, we find that NeuralDater significantly outperforms state-of-the-art baseline by 19% absolute (45% relative) accuracy points.","Document date is essential for many important tasks, such as document retrieval, summarization, event detection, etc. While existing approaches for these tasks assume accurate knowledge of the document date, this is not always available, especially for arbitrary documents from the Web. Document Dating is a challenging problem which requires inference over the temporal structure of the document. Prior document dating systems have largely relied on handcrafted features while ignoring such document-internal structures. In this paper, we propose NeuralDater, a Graph Convolutional Network (GCN) based document dating approach which jointly exploits syntactic and temporal graph structures of document in a principled way. To the best of our knowledge, this is the first application of deep learning for the problem of document dating. Through extensive experiments on real-world datasets, we find that NeuralDater significantly outperforms state-of-the-art baseline by 19% absolute (45% relative) accuracy points.","['Shikhar Vashishth', 'Shib Sankar Dasgupta', 'Swayambhu Nath Ray', 'Partha Talukdar']","Vashishth, Shikhar; Dasgupta, Shib Sankar; Ray, Swayambhu Nath; Talukdar, Partha",https://export.arxiv.org/abs/1902.00175,https://aclanthology.org/P18-1149,2021-08-03
9684.0,,acl,Ranking Paragraphs for Improving Answer Recall in Open-Domain Question   Answering,Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering,"Recently, open-domain question answering (QA) has been combined with machine comprehension models to find answers in a large knowledge source. As open-domain QA requires retrieving relevant documents from text corpora to answer questions, its performance largely depends on the performance of document retrievers. However, since traditional information retrieval systems are not effective in obtaining documents with a high probability of containing answers, they lower the performance of QA systems. Simply extracting more documents increases the number of irrelevant documents, which also degrades the performance of QA systems. In this paper, we introduce Paragraph Ranker which ranks paragraphs of retrieved documents for a higher answer recall with less noise. We show that ranking paragraphs and aggregating answers using Paragraph Ranker improves performance of open-domain QA pipeline on the four open-domain QA datasets by 7.8% on average.","Recently, open-domain question answering (QA) has been combined with machine comprehension models to find answers in a large knowledge source. As open-domain QA requires retrieving relevant documents from text corpora to answer questions, its performance largely depends on the performance of document retrievers. However, since traditional information retrieval systems are not effective in obtaining documents with a high probability of containing answers, they lower the performance of QA systems. Simply extracting more documents increases the number of irrelevant documents, which also degrades the performance of QA systems. In this paper, we introduce Paragraph Ranker which ranks paragraphs of retrieved documents for a higher answer recall with less noise. We show that ranking paragraphs and aggregating answers using Paragraph Ranker improves performance of open-domain QA pipeline on the four open-domain QA datasets by 7.8% on average.","['Jinhyuk Lee', 'Seongjun Yun', 'Hyunjae Kim', 'Miyoung Ko', 'Jaewoo Kang']","Lee, Jinhyuk; Yun, Seongjun; Kim, Hyunjae; Ko, Miyoung; Kang, Jaewoo",https://export.arxiv.org/abs/1810.00494,https://aclanthology.org/D18-1053,2021-08-03
9667.0,,acl,Adaptive Document Retrieval for Deep Question Answering,Adaptive Document Retrieval for Deep Question Answering,"State-of-the-art systems in deep question answering proceed as follows: (1) an initial document retrieval selects relevant documents, which (2) are then processed by a neural network in order to extract the final answer. Yet the exact interplay between both components is poorly understood, especially concerning the number of candidate documents that should be retrieved. We show that choosing a static number of documents -- as used in prior research -- suffers from a noise-information trade-off and yields suboptimal results. As a remedy, we propose an adaptive document retrieval model. This learns the optimal candidate number for document retrieval, conditional on the size of the corpus and the query. We report extensive experimental results showing that our adaptive approach outperforms state-of-the-art methods on multiple benchmark datasets, as well as in the context of corpora with variable sizes.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Adaptive Document Retrieval for Deep Question Answering.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs http://arxiv.org/abs/1808.06528; Years 2018; Authors Bernhard Kratzwald; Stefan Feuerriegel; Deduplication_Notes ; X Adaptive Document Retrieval for Deep Question Answering.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0] CHECK DUPLICATE [Source dblp; Title Adaptive Document Retrieval for Deep Question Answering.; Abstract NaN; Keywords NaN; DOIs 10.18653/v1/d18-1055; DOI_original 10.18653/v1/d18-1055; URLs https://doi.org/10.18653/v1/d18-1055 ; https://www.aclweb.org/anthology/D18-1055/; Years 2018; Authors Bernhard Kratzwald; Stefan Feuerriegel; Deduplication_Notes ; X Adaptive Document Retrieval for Deep Question Answering.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]","State-of-the-art systems in deep question answering proceed as follows: (1)an initial document retrieval selects relevant documents, which (2) are then processed by a neural network in order to extract the final answer. Yet the exact interplay between both components is poorly understood, especially concerning the number of candidate documents that should be retrieved. We show that choosing a static number of documents - as used in prior research - suffers from a noise-information trade-off and yields suboptimal results. As a remedy, we propose an adaptive document retrieval model. This learns the optimal candidate number for document retrieval, conditional on the size of the corpus and the query. We report extensive experimental results showing that our adaptive approach outperforms state-of-the-art methods on multiple benchmark datasets, as well as in the context of corpora with variable sizes.","['Bernhard Kratzwald', 'Stefan Feuerriegel']","Kratzwald, Bernhard; Feuerriegel, Stefan",https://export.arxiv.org/abs/1808.06528,https://aclanthology.org/D18-1055,2021-08-03
9674.0,,acl,Learning a Policy for Opportunistic Active Learning,Learning a Policy for Opportunistic Active Learning,"Active learning identifies data points to label that are expected to be the most useful in improving a supervised model. Opportunistic active learning incorporates active learning into interactive tasks that constrain possible queries during interactions. Prior work has shown that opportunistic active learning can be used to improve grounding of natural language descriptions in an interactive object retrieval task. In this work, we use reinforcement learning for such an object retrieval task, to learn a policy that effectively trades off task completion with model improvement that would benefit future tasks.","Active learning identifies data points to label that are expected to be the most useful in improving a supervised model. Opportunistic active learning incorporates active learning into interactive tasks that constrain possible queries during interactions. Prior work has shown that opportunistic active learning can be used to improve grounding of natural language descriptions in an interactive object retrieval task. In this work, we use reinforcement learning for such an object retrieval task, to learn a policy that effectively trades off task completion with model improvement that would benefit future tasks.","['Aishwarya Padmakumar', 'Peter Stone', 'Raymond J. Mooney']","Padmakumar, Aishwarya; Stone, Peter; Mooney, Raymond",https://export.arxiv.org/abs/1808.10009,https://aclanthology.org/D18-1165,2021-08-03
9672.0,,acl,A strong baseline for question relevancy ranking,A strong baseline for question relevancy ranking,"The best systems at the SemEval-16 and SemEval-17 community question answering shared tasks -- a task that amounts to question relevancy ranking -- involve complex pipelines and manual feature engineering. Despite this, many of these still fail at beating the IR baseline, i.e., the rankings provided by Google's search engine. We present a strong baseline for question relevancy ranking by training a simple multi-task feed forward network on a bag of 14 distance measures for the input question pair. This baseline model, which is fast to train and uses only language-independent features, outperforms the best shared task systems on the task of retrieving relevant previously asked questions.","The best systems at the SemEval-16 and SemEval-17 community question answering shared tasks -- a task that amounts to question relevancy ranking -- involve complex pipelines and manual feature engineering. Despite this, many of these still fail at beating the IR baseline, i.e., the rankings provided by Google's search engine. We present a strong baseline for question relevancy ranking by training a simple multi-task feed forward network on a bag of 14 distance measures for the input question pair. This baseline model, which is fast to train and uses only language-independent features, outperforms the best shared task systems on the task of retrieving relevant previously asked questions.","['Ana V. González-Garduño', 'Isabelle Augenstein', 'Anders Søgaard']","Gonzalez, Ana; Augenstein, Isabelle; Sogaard, Anders",https://export.arxiv.org/abs/1808.08836,https://aclanthology.org/D18-1515,2021-08-03
9530.0,,acl,Key-Value Retrieval Networks for Task-Oriented Dialogue,Key-Value Retrieval Networks for Task-Oriented Dialogue,"Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.","Neural task-oriented dialogue systems often struggle to smoothly interface with a knowledge base. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The model is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space: calendar scheduling, weather information retrieval, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.","['Mihail Eric', 'Christopher D. Manning']","Eric, Mihail; Krishnan, Lakshmi; Charette, Francois; Manning, Christopher D.",https://export.arxiv.org/abs/1705.05414,https://aclanthology.org/W17-5506,2021-08-03
9548.0,,acl,Cross-genre Document Retrieval: Matching between Conversational and   Formal Writings,Cross-genre Document Retrieval: Matching between Conversational and Formal Writings,"This paper challenges a cross-genre document retrieval task, where the queries are in formal writing and the target documents are in conversational writing. In this task, a query, is a sentence extracted from either a summary or a plot of an episode in a TV show, and the target document consists of transcripts from the corresponding episode. To establish a strong baseline, we employ the current state-of-the-art search engine to perform document retrieval on the dataset collected for this work. We then introduce a structure reranking approach to improve the initial ranking by utilizing syntactic and semantic structures generated by NLP tools. Our evaluation shows an improvement of more than 4% when the structure reranking is applied, which is very promising.","This paper challenges a cross-genre document retrieval task, where the queries are in formal writing and the target documents are in conversational writing. In this task, a query, is a sentence extracted from either a summary or a plot of an episode in a TV show, and the target document consists of transcripts from the corresponding episode. To establish a strong baseline, we employ the current state-of-the-art search engine to perform document retrieval on the dataset collected for this work. We then introduce a structure reranking approach to improve the initial ranking by utilizing syntactic and semantic structures generated by NLP tools. Our evaluation shows an improvement of more than 4% when the structure reranking is applied, which is very promising.","['Tomasz Jurczyk', 'Jinho D. Choi']","Jurczyk, Tomasz; Choi, Jinho D.",https://export.arxiv.org/abs/1707.04538,https://aclanthology.org/W17-5407,2021-08-03
2392.0,,acl,Automating Biomedical Evidence Synthesis: RobotReviewer,Automating Biomedical Evidence Synthesis: RobotReviewer,"We present RobotReviewer, an open-source web-based system that uses machine learning and NLP to semi-automate biomedical evidence synthesis, to aid the practice of Evidence-Based Medicine. RobotReviewer processes full-text journal articles (PDFs) describing randomized controlled trials (RCTs). It appraises the reliability of RCTs and extracts text describing key trial characteristics (e.g., descriptions of the population) using novel NLP methods. RobotReviewer then automatically generates a report synthesising this information. Our goal is for RobotReviewer to automatically extract and synthesise the full-range of structured data needed to inform evidence-based practice.",,"Marshall, I. J.
 and Kuiper, J.
 and Banner, E.
 and Wallace, B. C.","Marshall, Iain; Kuiper, Jo""el; Banner, Edward; Wallace, Byron C.",not available,https://aclanthology.org/P17-4002,2021-08-03
9522.0,,acl,Reading Wikipedia to Answer Open-Domain Questions,Reading Wikipedia to Answer Open-Domain Questions,This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.,This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.,"['Danqi Chen', 'Adam Fisch', 'Jason Weston', 'Antoine Bordes']","Chen, Danqi; Fisch, Adam; Weston, Jason; Bordes, Antoine",https://export.arxiv.org/abs/1704.00051,https://aclanthology.org/P17-1171,2021-08-03
10282.0,,acl,<U+4F7F><U+7528><U+67E5><U+8A62><U+610F><U+5411><U+63A2><U+7D22><U+8207><U+985E><U+795E><U+7D93><U+7DB2><U+8DEF><U+65BC><U+8A9E><U+97F3><U+6587><U+4EF6><U+6AA2><U+7D22><U+4E4B><U+7814><U+7A76> (Exploring Query Intent and Neural Network modeling Techniques for Spoken Document Retrieval) [In Chinese],<U+4F7F><U+7528><U+67E5><U+8A62><U+610F><U+5411><U+63A2><U+7D22><U+8207><U+985E><U+795E><U+7D93><U+7DB2><U+8DEF><U+65BC><U+8A9E><U+97F3><U+6587><U+4EF6><U+6AA2><U+7D22><U+4E4B><U+7814><U+7A76> (Exploring Query Intent and Neural Network modeling Techniques for Spoken Document Retrieval) [In Chinese],not available,,"Tien-Hong Lo
 and Ying-Wen Chen
 and Berlin Chen
 and Kuan-Yu Chen
 and Hsin-Min Wang","Lo, Tien-Hong; Chen, Ying-Wen; Chen, Berlin; Chen, Kuan-Yu; Wang, Hsin-Min",https://www.aclweb.org/anthology/O17-1015/,https://aclanthology.org/O17-1015,2021-08-03
9564.0,,acl,DOC: Deep Open Classification of Text Documents,DOC: Deep Open Classification of Text Documents,"Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem. This problem is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.","Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem. This problem is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.","['Lei Shu', 'Hu Xu', 'Bing Liu']","Shu, Lei; Xu, Hu; Liu, Bing",https://export.arxiv.org/abs/1709.08716,https://aclanthology.org/D17-1314,2021-08-03
10305.0,,acl,A Recurrent Neural Network Architecture for De-identifying Clinical Records,A Recurrent Neural Network Architecture for De-identifying Clinical Records,not available,,"Shweta
 and Ankit Kumar
 and Asif Ekbal
 and Sriparna Saha 0001
 and Pushpak Bhattacharyya","Shweta; Kumar, Ankit; Ekbal, Asif; Saha, Sriparna; Bhattacharyya, Pushpak",https://www.aclweb.org/anthology/W16-6325/,https://aclanthology.org/W16-6325,2021-08-03
9478.0,,acl,Using Centroids of Word Embeddings and Word Mover's Distance for   Biomedical Document Retrieval in Question Answering,Using Centroids of Word Embeddings and Word Mover's Distance for Biomedical Document Retrieval in Question Answering,"We propose a document retrieval method for question answering that represents documents and questions as weighted centroids of word embeddings and reranks the retrieved documents with a relaxation of Word Mover's Distance. Using biomedical questions and documents from BIOASQ, we show that our method is competitive with PUBMED. With a top-k approximation, our method is fast, and easily portable to other domains and languages.        Lena: ignore if you agree that this is a duplicate CHECK DUPLICATE [Source dblp; Title Using Centroids of Word Embeddings and Word Mover's Distance for Biomedical Document Retrieval in Question Answering.; Abstract NaN; Keywords NaN; DOIs NaN; DOI_original NaN; URLs http://arxiv.org/abs/1608.03905; Years 2016; Authors Georgios-Ioannis Brokos; Prodromos Malakasiotis; Ion Androutsopoulos; Deduplication_Notes ; X Using Centroids of Word Embeddings and Word Mover's Distance for Biomedical Document Retrieval in Question Answering.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0] CHECK DUPLICATE [Source dblp; Title Using Centroids of Word Embeddings and Word Mover's Distance for Biomedical Document Retrieval in Question Answering.; Abstract NaN; Keywords NaN; DOIs 10.18653/v1/w16-2915; DOI_original 10.18653/v1/W16-2915; URLs https://doi.org/10.18653/v1/W16-2915; Years 2016; Authors Georgios-Ioannis Brokos; Prodromos Malakasiotis; Ion Androutsopoulos; Deduplication_Notes ; X Using Centroids of Word Embeddings and Word Mover's Distance for Biomedical Document Retrieval in Question Answering.; yHat_svm_lose 0; yHat_svm_tight 0; yHat_svm_tiabs 0; yHat_decisiontree_tiabs 0; yHat_decisiontree_all 0; yHat_sum 0]",,"['Georgios-Ioannis Brokos', 'Prodromos Malakasiotis', 'Ion Androutsopoulos']","Brokos, Georgios-Ioannis; Malakasiotis, Prodromos; Androutsopoulos, Ion",https://export.arxiv.org/abs/1608.03905,https://aclanthology.org/W16-2915,2021-08-03
11026.0,,acl,Corpus and Method for Identifying Citations in Non-Academic Text,Corpus and Method for Identifying Citations in Non-Academic Text,"We attempt to identify citations in non-academic text such as patents. Unlike academic articles which often provide bibliographies and follow consistent citation styles, non-academic text cites scientific research in a more ad-hoc manner. We manually annotate citations in 50 patents, train a CRF classifier to find new citations, and apply a reranker to incorporate non-local information. Our best system achieves 0.83 F-score on 5-fold cross validation.","We attempt to identify citations in non-academic text such as patents. Unlike academic articles which often provide bibliographies and follow consistent citation styles, non-academic text cites scientific research in a more ad-hoc manner. We manually annotate citations in 50 patents, train a CRF classifier to find new citations, and apply a reranker to incorporate non-local information. Our best system achieves 0.83 F-score on 5-fold cross validation.","He, Y. F. Meyers, A.","He, Yifan; Meyers, Adam",<Go to ISI>://WOS:000355611005156,http://www.lrec-conf.org/proceedings/lrec2014/pdf/1036_Paper.pdf,2021-08-03
10325.0,,acl,CRAB 20: A text mining tool for supporting literature review in chemical cancer risk assessment,CRAB 20: A text mining tool for supporting literature review in chemical cancer risk assessment,not available,,"Yufan Guo
 and Diarmuid Ó Séaghdha
 and Ilona Silins
 and Lin Sun 0003
 and Johan Högberg
 and Ulla Stenius
 and Anna Korhonen","Guo, Yufan; 'O S'eaghdha, Diarmuid; Silins, Ilona; Sun, Lin; H""ogberg, Johan; Stenius, Ulla; Korhonen, Anna",https://www.aclweb.org/anthology/C14-2017/,https://aclanthology.org/C14-2017,2021-08-03
10146.0,,acl,A Comparative Study of Methods for Topic Modeling in Spoken Document Retrieval,A Comparative Study of Methods for Topic Modeling in Spoken Document Retrieval,not available,,"Shih-Hsiang Lin
 and Berlin Chen","Lin, Shih-Hsiang; Chen, Berlin",http://www.aclclp.org.tw/clclp/v17n1/v17n1a4.pdf,https://aclanthology.org/O12-2004,2021-08-03
15920.0,,acl,A statistical relational learning approach to identifying evidence based medicine categories,A Statistical Relational Learning Approach to Identifying Evidence Based Medicine Categories,"Evidence-based medicineis an approachwhereby clinical decisions are supported bythe best available findings gained from scien-tific research. This requires efficient accessto such evidence. To this end, abstracts inevidence-based medicine can be labeled usinga set of predefined medical categories, the so-calledPICOcriteria. This paper presents anapproach to automatically annotate sentencesin medical abstracts with these labels. Sinceboth structural and sequential information areimportant for this classification task, we usekLog, a new language for statistical relationallearning with kernels. Our results show a clearimprovement with respect to state-of-the-artsystems.",,"Verbeke M, Van Asch V, Morante R, Frasconi P, Daelemans W, De Raedt L. A","Verbeke, Mathias; Van Asch, Vincent; Morante, Roser; Frasconi, Paolo; Daelemans, Walter; De Raedt, Luc",https://www.aclweb.org/anthology/D12-1053/,https://aclanthology.org/D12-1053,2021-08-03
15928.0,,acl,Extracting formulaic and free text clinical research articles metadata using conditional random fields,Extracting Formulaic and Free Text Clinical Research Articles Metadata using Conditional Random Fields,"We explore the use of conditional randomfields (CRFs) to automatically extract impor-tant metadata from clinical research articles.These metadata fields include formulaic meta-data about the authors, extracted from the titlepage, as well as free text fields concerning thestudyâ€™s critical parameters, such as longitudi-nal variables and medical intervention meth-ods, extracted from the body text of the arti-cle. Extracting such information can help bothreaders conduct deep semantic search of arti-cles and policy makers and sociologists trackmacro level trends in research. Preliminary re-sults show an acceptable level of performancefor formulaic metadata and a high precisionfor those found in the free text.",,"Lin S, Ng J-P, Pradhan S, Shah J, Pietrobon R, Kan M-Y","Lin, Sein; Ng, Jun-Ping; Pradhan, Shreyasee; Shah, Jatin; Pietrobon, Ricardo; Kan, Min-Yen",https://www.aclweb.org/anthology/W10-1114/,https://aclanthology.org/W10-1114,2021-08-03
9881.0,,acl,Corpus Exploitation from Wikipedia for Ontology Construction,Corpus Exploitation from Wikipedia for Ontology Construction,"Ontology construction usually requires a domain-specific corpus for building corresponding concept hierarchy. The domain corpus must have a good coverage of domain knowledge. Wikipedia(Wiki), the world's largest online encyclopaedic knowledge source, is open-content, collaboratively edited, and free of charge. It covers millions of articles and still keeps on expanding continuously. These characteristics make Wiki a good candidate as domain corpus resource in ontology construction. However, the selected article collection must have considerable quality and quantity. In this paper, a novel approach is proposed to identify articles in Wiki as domain-specific corpus by using available classification information in Wiki pages. The main idea is to generate a domain hierarchy from the hyperlinked pages of Wiki. Only articles strongly linked to this hierarchy are selected as the domain corpus. The proposed approach makes use of linked category information in Wiki pages to produce the hierarchy as a directed graph for obtaining a set of pages in the same connected branch. Ranking and filtering are then done on these pages based on the classification tree generated by the traversal algorithm. The experiment and evaluation results show that Wiki is a good resource for acquiring a relative high quality domain-specific corpus for ontology construction.","Ontology construction usually requires a domain-specific corpus for building corresponding concept hierarchy. The domain corpus must have a good coverage of domain knowledge. Wikipedia(Wiki), the world<U+0092>s largest online encyclopaedic knowledge source, is open-content, collaboratively edited, and free of charge. It covers millions of articles and still keeps on expanding continuously. These characteristics make Wiki a good candidate as domain corpus resource in ontology construction. However, the selected article collection must have considerable quality and quantity. In this paper, a novel approach is proposed to identify articles in Wiki as domain-specific corpus by using available classification information in Wiki pages. The main idea is to generate a domain hierarchy from the hyperlinked pages of Wiki. Only articles strongly linked to this hierarchy are selected as the domain corpus. The proposed approach makes use of linked category information in Wiki pages to produce the hierarchy as a directed graph for obtaining a set of pages in the same connected branch. Ranking and filtering are then done on these pages based on the classification tree generated by the traversal algorithm. The experiment and evaluation results show that Wiki is a good resource for acquiring a relative high quality domain-specific corpus for ontology construction.","Cui, G. Y. Lu, Q. Li, W. J. Chen, Y. R. European Language Resources, Association","Cui, Gaoying; Lu, Qin; Li, Wenjie; Chen, Yirong",<Go to ISI>://WOS:000324028902033,http://www.lrec-conf.org/proceedings/lrec2008/pdf/541_paper.pdf,2021-08-03
19315.0,pubmed,eppimirosoftacademic,A rule-based approach for automatically extracting data from systematic reviews and their updates to model the risk of conclusion change,A rule-based approach for automatically extracting data from systematic reviews and their updates to model the risk of conclusion change,"Few data-driven approaches are available to estimate the risk of conclusion change in systematic review updates. We developed a rule-based approach to automatically extract information from reviews and updates to be used as features for modelling conclusion change risk. Rules were developed to extract relevant information from published Cochrane reviews and used to construct four features: the number of included trials and participants in the reviews, a measure based on the number of participants, and the time elapsed between the search dates. We compared the performance of random forest, decision tree, and logistic regression to predict the conclusion change risk. The performance was measured by accuracy, precision, recall, F<sub>1</sub> -score, and area under ROC (AU-ROC). One rule was developed to extract the conclusion change information (96% accuracy, 100 reviews), one for the search date (100% accuracy, 100 reviews), one for the number of included clinical trials (100% accuracy, 100 reviews), and 22 for the number of participants (97.3% accuracy, 200 reviews). For unseen reviews, the random forest classifier showed the highest accuracy (80.8%) and AU-ROC (0.80). All classifiers showed relatively similar performance with overlapping 95% confidence interval (CI). The coverage score was shown to be the most useful feature for predicting the conclusion change risk. Features mined from Cochrane reviews and updates can estimate conclusion change risk. If data from more published reviews and updates were made accessible, data-driven methods to predict the conclusion change risk may be a feasible way to support decisions about updating reviews.","Background  Few data-driven approaches are available to estimate the risk of conclusion change in systematic review updates. We developed a rule-based approach to automatically extract information from reviews and updates to be used as features for modelling conclusion change risk.  Methods  Rules were developed to extract relevant information from published Cochrane reviews and used to construct four features: the number of included trials and participants in the reviews, a measure based on the number of participants, and the time elapsed between the search dates. We compared the performance of random forest, decision tree, and logistic regression to predict the conclusion change risk. The performance was measured by accuracy, precision, recall, F1 -score, and area under ROC (AU-ROC).  Results  One rule was developed to extract the conclusion change information (96% accuracy, 100 reviews), one for the search date (100% accuracy, 100 reviews), one for the number of included clinical trials (100% accuracy, 100 reviews), and 22 for the number of participants (97.3% accuracy, 200 reviews). For unseen reviews, the random forest classifier showed the highest accuracy (80.8%) and AU-ROC (0.80). All classifiers showed relatively similar performance with overlapping 95% confidence interval (CI). The coverage score was shown to be the most useful feature for predicting the conclusion change risk.  Conclusion  Features mined from Cochrane reviews and updates can estimate conclusion change risk. If data from more published reviews and updates were made accessible, data-driven methods to predict the conclusion change risk may be a feasible way to support decisions about updating reviews. This article is protected by copyright. All rights reserved.","Bashir, Dunn, Surian",Bashir Rabia; Dunn Adam G; Dunn Adam G; Surian Didi,https://doi.org/10.1002/jrsm.1473,https://doi.org/10.1002/JRSM.1473,2021-08-03
19414.0,pubmed,eppimirosoftacademic,Text mining approaches for dealing with the rapidly expanding literature on COVID-19,Text mining approaches for dealing with the rapidly expanding literature on COVID-19,"More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a qualitative description and assessment of the system's performance, unique data or user interface features and modeling decisions. Many systems focus on search and discovery, though several systems provide novel features, such as the ability to summarize findings over multiple documents or linking between scientific articles and clinical trials. We also describe the public corpora, models and shared tasks that have been introduced to help reduce repeated effort among community members; some of these resources (especially shared tasks) can provide a basis for comparing the performance of different systems. Finally, we summarize promising results and open challenges for text mining the COVID-19 literature.","More than 50 000 papers have been published about COVID-19 since the beginning of 2020 and several hundred new papers continue to be published every day. This incredible rate of scientific productivity leads to information overload, making it difficult for researchers, clinicians and public health officials to keep up with the latest findings. Automated text mining techniques for searching, reading and summarizing papers are helpful for addressing information overload. In this review, we describe the many resources that have been introduced to support text mining applications over the COVID-19 literature; specifically, we discuss the corpora, modeling resources, systems and shared tasks that have been introduced for COVID-19. We compile a list of 39 systems that provide functionality such as search, discovery, visualization and summarization over the COVID-19 literature. For each system, we provide a qualitative description and assessment of the system's performance, unique data or user interface features and modeling decisions. Many systems focus on search and discovery, though several systems provide novel features, such as the ability to summarize findings over multiple documents or linking between scientific articles and clinical trials. We also describe the public corpora, models and shared tasks that have been introduced to help reduce repeated effort among community members; some of these resources (especially shared tasks) can provide a basis for comparing the performance of different systems. Finally, we summarize promising results and open challenges for text mining the COVID-19 literature.","Wang, Lo",Wang Lucy Lu; Lo Kyle,https://doi.org/10.1093/bib/bbaa296,https://doi.org/10.1093/BIB/BBAA296,2021-08-03
17771.0,arxiv,eppimirosoftacademic,Machine Reading of Hypotheses for Organizational Research Reviews and   Pre-trained Models via R Shiny App for Non-Programmers,Machine Reading of Hypotheses for Organizational Research Reviews and Pre-trained Models via R Shiny App for Non-Programmers,"The volume of scientific publications in organizational research becomes exceedingly overwhelming for human researchers who seek to timely extract and review knowledge. This paper introduces natural language processing (NLP) models to accelerate the discovery, extraction, and organization of theoretical developments (i.e., hypotheses) from social science publications. We illustrate and evaluate NLP models in the context of a systematic review of stakeholder value constructs and hypotheses. Specifically, we develop NLP models to automatically 1) detect sentences in scholarly documents as hypotheses or not (Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs) and links (causal/associative relationships) (Relationship Deconstruction ), and 3) classify the features of links in terms causality (versus association) and direction (positive, negative, versus nonlinear) (Feature Classification). Our models have reported high performance metrics for all three tasks. While our models are built in Python, we have made the pre-trained models fully accessible for non-programmers. We have provided instructions on installing and using our pre-trained models via an R Shiny app graphic user interface (GUI). Finally, we suggest the next paths to extend our methodology for computer-assisted knowledge synthesis.","The volume of scientific publications in organizational research becomes exceedingly overwhelming for human researchers who seek to timely extract and review knowledge. This paper introduces natural language processing (NLP) models to accelerate the discovery, extraction, and organization of theoretical developments (i.e., hypotheses) from social science publications. We illustrate and evaluate NLP models in the context of a systematic review of stakeholder value constructs and hypotheses. Specifically, we develop NLP models to automatically 1) detect sentences in scholarly documents as hypotheses or not (Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs) and links (causal/associative relationships) (Relationship Deconstruction ), and 3) classify the features of links in terms causality (versus association) and direction (positive, negative, versus nonlinear) (Feature Classification). Our models have reported high performance metrics for all three tasks. While our models are built in Python, we have made the pre-trained models fully accessible for non-programmers. We have provided instructions on installing and using our pre-trained models via an R Shiny app graphic user interface (GUI). Finally, we suggest the next paths to extend our methodology for computer-assisted knowledge synthesis.","['Victor Zitian Chen', 'Felipe Montano-Campos', 'Wlodek Zadrozny', 'Evan Canfield']",Chen Victor Zitian; Montano-Campos Felipe; Zadrozny Wlodek; Canfield Evan,https://export.arxiv.org/abs/2106.16102,https://academic.microsoft.com/paper/3176001288,2021-08-03
